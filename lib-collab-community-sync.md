---
title: lib-collab-community-sync
tags: [collaboration, community, synchronization]
created: 2022-11-29T20:41:00.894Z
modified: 2022-11-29T20:41:25.566Z
---

# lib-collab-community-sync

# guide
- sync-xp
  - åŸºäºç¼“å­˜å®ç°syncæœ‰ç‚¹ç±»ä¼¼äºreact-query
  - å‚è€ƒvlcn

- åä½œæ–¹æ¡ˆå‚è€ƒ
  - Liveblocks, synced-store, FluidFramework, gun, pouchdb
  - automerge (2017), yjs (2015), sharedb (2013)

- æˆç†Ÿçš„è§£å†³æ–¹æ¡ˆä¸€èˆ¬éƒ½ä¼šè®¾è®¡å’Œå…¬å¼€è‡ªå·±çš„åŒæ­¥åè®®
  - database: couchdb, rxdb, 
  - framework: meteor-ddp, feathers-sync, gnu, realm/Atlas-Device-Sync
  - editing: yjs-protocols, automerge-sync

- [Building an offline realtime sync engine](https://gist.github.com/pesterhazy/3e039677f2e314cb77ffe3497ebca07b)
  - figma, linear
  - pouchdb, fluid, watermelonDB, Liveblocks

- è€ƒè™‘åˆ°å®¢æˆ·ç«¯å‡çº§çš„é—®é¢˜
  - åŒæ­¥å‰ä¸€å®šè¦æ£€æŸ¥ä¸€ä¸ªversionï¼Œå‚è€ƒindexeddb upgrade

- æ”¯æŒofflineçš„æ¶æ„
  - è¿˜å¯ä»¥è€ƒè™‘ä½¿ç”¨å¤šçº§ç¼“å­˜ï¼Œä¸ä¸€å®šå…¨é‡æ•°æ®åº“ï¼Œç±»ä¼¼react-query + indexeddb
# linear-realtime-sync

## [linear sync åˆ†äº«_202002](https://www.youtube.com/watch?v=WxK11RsLqp4&t=2169s)

- object graph
  - ä½¿ç”¨mobxè¿›è¡Œstate management, è‡ªåŠ¨æ›´æ–°view
  - object graphæ”¯æŒobject reference itself

- object pool
  - just normalize all your data structures into one pool
  - there's one big array of all the objects that represent your entire dataset in your application
- from this object pool, we are able to create this object graph, then pass this object graph to the views
- there are 3 objects in the object pool, but there are 5 objects in the object graph
- in the beginning , the object pool is empty
  - somewhere stream data into the pool
  - find team id in object pool, update user will update team, åœ¨modelå±‚å®ç°
  - object poolé‡Œé¢çš„å¯¹è±¡å¤§å¤šæ˜¯æ‰å¹³çš„
  - object poolé‡Œé¢å¯¹è±¡åˆ é™¤åï¼Œä¹Ÿä¼šåˆ é™¤object graphå¯¹åº”çš„å¯¹è±¡
  - object poolé‡Œé¢çš„crudéƒ½ä¼šè‡ªåŠ¨æ›´æ–°object graphï¼Œç„¶åè‡ªåŠ¨æ›´æ–°view

- transaction queue
  - viewè§¦å‘çš„changeså…ˆåœ¨å‰ç«¯æ‰§è¡Œï¼Œç„¶åæ‰å‘é€åˆ°åç«¯
  - å¦‚æœåç«¯accept changeï¼Œå› ä¸ºå‰ç«¯å·²æ‰§è¡Œï¼Œå°±ä¸è¿”å›æ•°æ®ä¿¡æ¯
  - å¦‚æœåç«¯reject changeï¼Œå› ä¸ºå†²çªã€æƒé™ç­‰åŸå› ï¼ŒtransactionæŒæœ‰æ—§æ•°æ®ï¼Œå¯ä»¥ç”¨æ¥å›æ»šï¼Œæ­¤æ—¶ç•Œé¢å¯èƒ½æœ‰é—ªçƒ
- realtime syncæ›´é€‚åˆåç«¯ä¸æ‹’ç»çš„åœºæ™¯ï¼Œè¿™æ ·å›æ»šé—ªçƒä¼šè¾ƒå°‘
  - å‰ç«¯å…ˆä¹è§‚æ›´æ–°
  - figma teamå®è·µå‡ºçš„ç»“è®ºä¹Ÿæ˜¯è¿™æ ·çš„

- backend to frontend
  - backend has a queue of all the changes made to the database
  - backend broadcast changes to clients
  - å®¢æˆ·ç«¯çš„changeä¼šå‘é€åˆ°åç«¯ï¼Œåç«¯ä¼šå‘é€ç»™å…¶ä»–å®¢æˆ·ç«¯

- optimizationï¼š object store
  - åœ¨å‰ç«¯æŒä¹…åŒ–æ•°æ®
  - every change from backend gets stored locally in the indexeddb
  - clients reload/åˆ·æ–°æ—¶ï¼Œä¸ä¼šä»åç«¯è¯·æ±‚æ•°æ®ï¼Œè€Œæ˜¯åœ¨å¯åŠ¨æ—¶å…ˆä»æœ¬åœ°idbæ„å»ºobject pool, ç„¶åå†è¿æ¥åˆ°åç«¯ï¼Œåç«¯å†å‘æ¥æ•°æ®
  - å¦‚æœç¦»çº¿æ—¶é—´è¿‡é•¿ï¼Œåç«¯å‘æ¥çš„æ•°æ®å°±è¾ƒå¤šï¼Œæ­¤æ—¶å‰ç«¯å·²ç»å±•ç¤ºæ•°æ®äº†
  - transactionsä¹Ÿæ˜¯è¿™æ ·ï¼Œæœ¬åœ°ä¹ŸæŒä¹…åŒ–äº†æ‰€æœ‰transactionï¼Œåˆ·æ–°å®¢æˆ·ç«¯æ—¶ï¼Œä¼šæ›´æ–°objectså’Œtransations
- æ”¯æŒoffline modeï¼Œæ¢å¤åœ¨çº¿åï¼Œæœ¬åœ°æŒä¹…åŒ–çš„transactionsä¼šè¢«å‘é€åˆ°åç«¯

- entire workflow(01:07:45)
  - å¯åŠ¨æ—¶ä»local dbåˆ›å»ºobject pool
  - æ ¹æ®decoratorä»object poolåˆ›å»ºobject graph, render view
  - æ›´æ–°issueæ—¶ï¼Œé€šçŸ¥object poolå±æ€§æ›´æ–°äº†, create transaction å‘é€åˆ°åç«¯ï¼Œåç«¯å‘é€ç»™å…¶ä»–å®¢æˆ·ç«¯è¿›è¡ŒåŒæ­¥æ›´æ–°object pool

- å®¢æˆ·ç«¯æ›´æ–°æ•°æ®å¾ˆç®€å•ï¼Œç±»ä¼¼setState(newData)ï¼Œsync engineä¼šå¤„ç†åŒæ­¥ã€æŒä¹…åŒ–ã€å†²çªç­‰é—®é¢˜

- discussions

- ç¦»çº¿å†²çªçš„é—®é¢˜
  - é»˜è®¤last-write-win
  - æ›´å¤šæ˜¯ä¸šåŠ¡é€»è¾‘é—®é¢˜ï¼Œè€Œä¸æ˜¯æŠ€æœ¯é—®é¢˜
  - åœ¨linear llwå¯ä»¥workï¼Œä½†åœ¨grouponçš„äº¤æ˜“å†²çªæ—¶ä¼šæç¤ºç”¨æˆ·é€‰åŒºç‰ˆæœ¬

- ## When we started work on @linear , we felt real-time sync was a core functionality we had to invest in from the get-go. 
- https://twitter.com/artman/status/1558081796914483201
  - It turns out sync was important, but not for the reasons we thought.
- Our gut feeling was that real-time updates were required from a modern tool like Linear. Who wants to refresh to see the latest data? But how often do you find yourself in a situation where multiple people update data simultaneously in a project management tool?
  - Not that often, it turned out. Aside from special cases where your team gets together to operate on data - like planning your next cycle - edits are made across the entire dataset, with the same data being touched at the same time relatively infrequently.
  - ğŸ‘‰ğŸ» Donâ€™t get me wrong, we still believe that real-time sync is essential, 
  - but there are two more valuable things we got out of real-time sync that we did not appropriately anticipate: **App speed** and **Ship speed**.
- Amen! Noticed the same while working on http://syncedstore.org and yjs; being forced to really separate the data layer for sync comes with many additional benefits (local first, pluggable storage, dev speed etc)

- ğŸ‘‰ğŸ» The most straightforward way to implement real-time sync is to load the entire app state and then keep it up-to-date with real-time changes. 
  - While weâ€™ve had to add complexity to this simple initial implementation to support larger workspaces, the core tenant/tenet still holds.
  - Clients have the vast majority of their workspace data stored locally. Hence page loads are all but eliminated. As a result, startup times are fast, filters work instantly, and there are no page loads.
- So how well do you handle large workspaces now?  When I first talked about to you about the impl I always wondered what the design would end up being for large corps.
  - We do handle them pretty well, at least from the realtime sync aspect. A few more major changes and then we can pretty well scale to any kind of company size while keeping sync active for the data you are usually interested in.
- Iâ€™m curious to know if you are planning to modify the engine to improve the experience for large workspace, the current architecture of loading all data doesnâ€™t seem to scale. Would loading only a subset of the data break the nice abstraction that the sync engine seems to give?
  - ğŸ¤·ğŸ» no answer yet
- How do startup times stay fast when loading the entire state to the client?
  - We first load data that youâ€™ll immediately need, and then selectively stream in data that your likely to access next. And results are stored, so only the first load will be a bit slower.
- Noted, thank you. Do you store on localstorage, so the next full load will be faster? Or by first load do you just mean first request in the browser lifecycle?
  - We store the users dataset in IndexDB, which is really the only viable option, yet is not very good for relational data.
- ğŸ¤” But what if the size of the data gets really large? Like the data from many yearsâ€¦ Is still everything loaded into the local storage?
  - No. When the dataset gets larger we selectively preload only the data your likely to access into the local database, and the dynamically load data that you access outside of this dataset.

- But arguably even more essential and surprising was that real-time sync helped us ship new functionality much faster than regular architectures. How? By eliminating a vast swathe of complex and error-prone code.
- ğŸ‘‰ğŸ» Sync automatically takes care of generating API calls, creating transactions, applying them on the backend, handling conflicts and errors, reverting erroneous changes, rebasing in-fight changes, and offline capabilities.
  - To create a new feature as an engineer, you essentially render and modify local in-memory data structures to build new functionality. 
  - All the complexity that comes with requests, conflicts, network errors and retries are handled by sync for free.
- All UI code automatically re-renders when the data that they accessed updates. Whether the data changes come from the user or the network doesn't matter. So you get multi-player for free, too.
- As you can imagine, reducing the number of layers engineers have to work on dramatically improves the speed at which we can ship new functionality. After experiencing this architecture at scale, I'm spoiled for life.
- For a pretty old - but still relevant - talk on our sync engine, check out

- Linear is great! What did you use specifically for sync and did you roll out all the reconciliation code yourself or did you leverage other tools?
  - ws for sockets, and idb to make working with IndexDB a bit more pleasant, but other than that itâ€™s a custom solution.

- ğŸ¤” How do you see this scaling down the road? You mentioned some modifications for larger workspaces. I suppose there are a lot of assumptions built on top of the current sync engine. If you need to radically update it in the future, wouldn't that force a huge client re-write?
  - Data access in most places is already async and the sync client has three tiers of data: in memory, local database and network. Client code is agnostic to where the data is coming from. Was a a lot of work, but thatâ€™s really the scaling story.
- Do you think it could work for an app with much more content, for example something like Notion or Confluence? Could be difficult to maintain a full copy of everything on every device, especially on smartphones. Same problem as hit monorepos  too big to be cloned.

- It turns out that a sync engine is actually a much more general solution because of functional purity and managed effects. Essentially you move all the effects (async calls) into the sync engine service layer.

- when will linear open source the react-query for sync?
  - Haha there are also a lot of other options: replicache, http://convex.dev, http://clientdb.dev, a new one called aphrodite
# discuss
- ## 

- ## Toying around with OS level multiplayer...
- https://twitter.com/ronithhh/status/1630733879220011008
- tbh i believe "multiplayer" should be platform level

- ## Today's coding adventure: choose a syncing storage layer for a tech demo I'm working on.
- https://twitter.com/jessmartin/status/1630658249371295758
- When you search for something like this, a bunch of things pop up, some of which I've heard of:
  - LiteFS from http://Fly.io
  - Litestream
  - dsqlite
  - sql.js
  - AbsurdSQL
  - http://vlcn.io
- there's LiteFS which is like Litestream but better. Rather than async writing your sqlite db to some replica, LiteFS actually reads each *transaction* (each change) and stores them and replays them. This allows for cool things like rollbacks since LiteFS has all of history.
  - Now what's wild is how LiteFS pulls this off: they run a separate process that reads directly from the filesystem (using FUSE) to watch the sqlite db (sqlite dbs are just one giant file, remember!) for changes, then nabs them as they happen.
- Q: If a sqlite database is normally stored in a file on the file system and browsers don't have file systems, how does sqlite-wasm store the db?
  - A: ğŸ˜± local-storage and session-storage, ofc! pretty gnarly limitations: <5mb, strings only, etc
  - ğŸ’¡ OPFS runs in a worker thread, so it doesn't block the main thread, which allows the UI to be more responsive.

    - opfsåªæ”¯æŒworkerçš„api: createSyncAccessHandle(),FileSystemSyncAccessHandle

- One important note about sqlite-in-the-browser: the excellent sql.js has been around for years, *but* it only supports *in-memory* changes to sqlite. No persistence. Refresh 
  - While sql.js does support exporting the database as JavaScript-typed array, that's not what I want. I want streaming replication
- Dqlite is distributed sqlite that is distributing a single sqlite db across a cluster or peers. Also, importantly it runs as a library inside your app, not as a sidecar process watching the filesystem. This would be more amenable(é¡ºä»çš„ï¼›é¡ºæœçš„) to the web. Unfortunately, it only runs on C/Linux. There aren't any other client libraries for other languages / platforms. Not sure why this hasn't gotten much adoption...

- ## I find that in distributed systems, metadata size is often inversely proportional to message size.
- https://twitter.com/aboodman/status/1628166157667831808
  - Decreasing message size means increasing message rate. Each message caries less data, but increased rate means more metadata is needed to correctly run and maintain the system.

- ## [è¯·æ•™ä¸€ä¸‹ï¼Œç±»ä¼¼ LOLï¼Œç‹è€…è£è€€ï¼Œ Diablo3 è¿™æ ·çš„ç½‘ç»œæ¸¸æˆï¼Œå¦‚ä½•åŒæ­¥å¤šæœºå®æ—¶çš„æ•°æ®ï¼Ÿ - V2EX](https://www.v2ex.com/t/763822)
- microsoft Fluid ç±»ä¼¼äº CRDT åˆ†å¸ƒå¼æ¡†æ¶ï¼ŒCRDT ä¸»è¦æ— ä¸­å¿ƒæœåŠ¡å™¨ï¼Œp2p æƒ…å†µä¸‹å¯ä»¥æœ€ç»ˆä¸€è‡´ç»“æœã€‚CRDT p2p å…·ä½“åº”ç”¨ èŠå¤© æ–‡å­—ååŒç¼–è¾‘
  - æ¸¸æˆæœåŠ¡å™¨æ˜¯ä¸€å°ä¸­å¿ƒæœåŠ¡å™¨ï¼Œä¸­å¿ƒæœåŠ¡å™¨å†³å®šå®¢æˆ·ç«¯è¯·æ±‚å…ˆåé¡ºåºï¼Œå¹¿æ’­ç»™å…¶ä»–å®¢æˆ·ç«¯ï¼Œæ²¡æœ‰ä¸€è‡´æ€§é—®é¢˜ã€‚

- å¸§åŒæ­¥ã€çŠ¶æ€åŒæ­¥
  - å¸§åŒæ­¥ï¼Œæ‰€æœ‰å®¢æˆ·ç«¯æ ¹æ®æœåŠ¡å™¨ä¸‹å‘çš„é€»è¾‘å¸§ï¼ˆåŒ…å«çš„ä¿¡æ¯æ˜¯åå°è®¾å¤‡çš„æ“ä½œè¾“å…¥ï¼‰ï¼Œåœ¨å®¢æˆ·ç«¯æ¼”ç®—ä¸€éï¼Œè¾“å…¥ä¸€è‡´ç®—æ³•ä¸€è‡´ æ‰€ä»¥æ¯ä¸ªäººçœ‹åˆ°çš„ç»“æœä¸€è‡´ï¼Œè°å»¶è¿Ÿé«˜è°åƒäº
  - çŠ¶æ€åŒæ­¥ï¼šæœåŠ¡å™¨ä»¥ä¸€å®šçš„é¢‘ç‡ä¸‹å‘å„ä¸ªç©å®¶è§’è‰²çš„çŠ¶æ€ç»™å„ä¸ªå®¢æˆ·ç«¯ï¼Œå®¢æˆ·ç«¯ä»¥æœåŠ¡å™¨ä¿¡æ¯ä¸ºç»å¯¹çœŸç†ï¼ŒåŠªåŠ›å¾€è¿™ä¸ªç»“æœä¸Šé 
- wow æ˜¯çŠ¶æ€åŒæ­¥ï¼Œå®¢æˆ·ç«¯ä¸‹å‘æ“ä½œå‘½ä»¤ï¼ŒæœåŠ¡å™¨è¿ç®—æ“ä½œç»“æœï¼Œäº§ç”Ÿçš„ç»“æœæ¨é€åˆ°åŒå±çš„å®¢æˆ·ç«¯ï¼Œå¦‚æœæ‰€æœ‰äººéƒ½åœ¨ä¸åœçš„åŠ¨ï¼ŒæœåŠ¡å™¨è´Ÿè½½æ˜¯äººæ•°çš„å¹³æ–¹ï¼Œ70 çº§çš„æ—¶å€™å± åŸåªè¦ 7 ä¸ªå›¢åœ¨ä¸€ä¸ªæˆ¿é—´å†…å°±ä¼šå®•æœºï¼Œå¤§æ¦‚ 300 äºº
- war3 æ˜¯å¸§åŒæ­¥ï¼Œå®¢æˆ·ç«¯ä¸‹å‘æ“ä½œå‘½ä»¤ï¼ŒæœåŠ¡å™¨ç›´æ¥æ¨é€æ“ä½œæŒ‡ä»¤ç»™åŒå±å®¢æˆ·ç«¯ï¼Œæ¥æ”¶çš„å®¢æˆ·ç«¯è´Ÿè´£è¿ç®—ç»“æœï¼Œè¦æ±‚æ‰€æœ‰å®¢æˆ·ç«¯ç‰ˆæœ¬ç›¸åŒï¼Œä¸å…è®¸è·¨ç‰ˆæœ¬è¿æ¥ã€‚å½•åƒæ–‡ä»¶å¯ä»¥å¾ˆå°ï¼Œå› ä¸ºåªéœ€è¦è®°å½•æ“ä½œè¡Œä¸ºï¼Œå®é™…ç»“æœæ˜¯å½•åƒ+å®¢æˆ·ç«¯å…±åŒå®Œæˆçš„

- å®æ—¶ç«æŠ€éƒ½æ˜¯å¸§åŒæ­¥, æ¯ä¸ªå®¢æˆ·ç«¯æŠŠå½“å‰å¸§(ä¾‹å¦‚è¿‡å» 1/60 ç§’)çš„åŠ¨ä½œ, ç”¨ UDP å‘å‡ºå». è€Œä¸”åŒ…å¾—å¾ˆå°, ä¸ºäº†é¿å…åšæ’åºé‡å‘. ä¸€èˆ¬éƒ½æŠŠå½“å‰å¸§å’Œä¹‹å‰çš„ 2(æˆ–å¤š)å¸§æ”¾åˆ°ä¸€èµ·å‘å‡ºå», è¿˜å¾—åœ¨ä¸€ä¸ª MTU å¤§å°å†…, é¿å…è¢«æ‹†åŒ….

- ## [Couchdbæœ‰åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨çš„ä¾‹å­å—ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/20112928/answers/updated)
- CouchDBæ˜¯HTTP Restful APIæ¥æ“ä½œæ•°æ®åº“çš„ï¼Œå…¶å®ƒæ•°æ®åº“ç³»ç»Ÿä½¿ç”¨TCPï¼Œåœ¨ä¼ è¾“å¤§é‡æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒHTTPåè®®åœ¨TCPåè®®ä¹‹ä¸Šï¼Œå¯èƒ½HTTPåè®®ä¼šæ¯”æ•°æ®åº“è‡ªèº«å®ç°çš„æ•°æ®äº¤äº’åè®®payloadè¦å¤§ï¼Œé€ æˆç½‘ç»œæ€§èƒ½ç•¥å·®

- ## [Ask HN: The state of Firebase alternatives in 2020? | Hacker News](https://news.ycombinator.com/item?id=24843664)
- There really isnâ€™t a competitor. 
- Supabase is trying to combine OSS tooling into a somewhat similar offering but itâ€™s not nearly as feature rich as Firebase. 
  - That having been said, most people use Firebase for real time DB and auth, and Supabase supports that now. 
  - It doesnâ€™t, however, support offline use cases, or any of the advanced functionality of firebase.
- Pouch and couch solve the offline data scenario and live replication, but no auth story and the mapping of users to data is problematic (unless you build a proxy layer, thereâ€™s not an easy way to have some data be public, some private, and some shared).
- Realm is paid these days, I believe, and it solves the data replication, but again, no auth.

- ## [meteor: Improve offline support_202110](https://github.com/meteor/meteor/discussions/11656)
  - With Minimongo there is a touch of offline support. This could be taken further and improved. First step most likely being the ability to not loose offline only changes when the app is closed and then improved syncing once connection is restored. The final step possibly being that you could create offline-first Meteor app.
  - There is also Hoodie which uses PouchDB and CouchDB as a pair to achieve this

- ## [Nano-SQL: Offline use with Sync to Server_201801](https://github.com/only-cliches/Nano-SQL/issues/18)
- Hey gents, I'm getting close to implementing this as a core feature.
- [Here's what I'm thinking:](github.com/only-cliches/Nano-SQL/issues/18#issuecomment-392220931)
  - 1. Implement a conflict resolution feature nearly identical to CouchDB that works on the client and the server. 
  - 2. Use websockets with ajax polling fallback to allow syncing between client side databases and servers alike. 
  - 3. Include a simple JSON Web Tokens feature in the client/server model with security baked in. 
  - 4. Make three way data binding super simple using the new observer feature
- I don't think we actually include any conflict resolution code, I really like CouchDBs approach 
  - here where if a record is conflicting at all, everything gets saved as revisions of that row and a winner is selected in a deterministic way. 
  - This lets the application developers handle conflict resolution in a way that suits their use case, prevents the build from bloating into hundreds of kilobytes and prevents loss of data.

- One of the "big deal" features for me that I haven't really seen in CouchDB, Gun and many others is a flexible security model, they seem to almost exclusively be all or nothing. 
  - Let me give you a very simple example: a system with blog posts like Wordpress. Everyone should have read capability, but only specific users should have write capability
  - These are the kinds of problems I'd like to solve with nanoSQL's offline/syncing system, and honestly where I've seen many of the existing solutions fall short.
- This is definitely an issue that Gun does not resolve. 
  - CouchDB does this though. 
  - Using a revision history for a document. 
  - In projects I have worked on revisions are kept for historical purposes. And could be used for manually merging or choosing revisions. 
  - So in any storage engine that Nano-SQL uses a revisions table for all changes(or one for each table's changes) could be managed to provide a history. 
  - However conflict determination would need to take place so that we can let the User know if a conflict occurred and let them take action if they are allowed to.

- Unfortunately implementing offline db use and eventual consistency is a fairly complex beast. 
  - Multiple users can have updated the server while you are offline, so the server needs to keep track of this. 
  - The clients need a way of finding out what has changed since they were last online and if there updates are newer, push them to the server. 
  - Deletes also need to be handled. 
  - Couch uses sequence numbers for part of this.
- Some articles etc. which will help here are:
http://docs.couchdb.org/en/2.1.1/replication/protocol.html
couchbase/couchbase-lite-ios/wiki/Replication-Algorithm (maybe out of date)
http://offlinefirst.org/sync
npmjs.com/package/dexie-syncable (possibly what I'll end up using)
share/sharedb
paldepind/synceddb
kinto.readthedocs.io/en/stable
forbesmyester/SyncIt
- Browser/Server communication should be abstracted so either http or websockets can be used. In Clibu I use Websockets.

- yjs. There is a good CRDT implementation that allows 100% synchronisation. 
  - Its used in production and very easy to use.
  - I really think you should look at this, because its a leap frog technology. The way CouchDB and others work is to use the `Last Write Wins` which does not guarantee that all changes on a type ( or datbase row as it were) that are from many offline users does resolve without anyones data being overwritten.
  - ğŸ‘‰ğŸ» The interesting thing about y.js is that all the reconciliation happens clientside.
  - This means that server side you can either hold the "last know version of a type" or hold all versions known to be out there. You can do either depending on the use case.

- yjs is interesting and works well, however I have several concerns. 
  - The changes stored in IndexedDb etc. appear to grow forever. I posted on Gitter back on Mar 13 and have not had a response. This level of support which just seems to be a single developer is another concern.
- Automerge is impressive but really only suitable for in memory objects. 
  - So for example if you want to merge database doc's for offline use it isn't suitable. 
  - Also I think it's memory use keeps growing with every change. ie. No garbage collection as @gedw99 mentioned.
- hypercore which is part of DAT seems only suitable for synchronizing files, not JS Objects or Database documents.
- Orbit.js
  - I can't see any way to specify/use database indexes. I've written a Gitter post on this. 21 Feb 18
  - It looks like all data is kept in memory which can be backed up to various stores, such as IndexedDB. However I can't see that it is possible to lose the in-memory store/cache and just use IndexedDB?
  - I can't find any documentation on how synchronization works. Latest wins, CRDT ...?  Docs say it can be used to sync editor content?
- I have real trouble thinking of GunDB as a "real" database.
  - To me it is more of a distributed in-memory cache. 
  - It has no query language, no indexes and the entire "DB" is in memory. 
  - Further it is unreliable - see this long outstanding issue
- Another new entrant is TurtleDB, which has very good docs and this article however as it is unusable for my specific use case. It is also an all-in-one library vs. separate independent modules as per your comment.

- There's also an actor based model of sync and state.
  - I can't really write much more about it right now (gotta run) but hopefully I'll remember to later.
  - Here's a project that implements this system http://ceptr.org/projects/holochain#local-source-chain
  - https://github.com/holochain/holochain-proto

- If remote sync is implemented I think it could make Nano-SQL an excellent choice for progressive web apps (PWAs).
- These are some of the alternatives I have evaluated and in my opinion their pros and cons:

- https://github.com/amark/gun
  - An open source cybersecurity protocol for syncing decentralized graph data.
  - GUN is an ecosystem of tools that let you build community run and encrypted applications - like an Open Source Firebase or a Decentralized Dropbox.
  - No support for relationships, join queries nor aggregate queries
- https://github.com/dfahlander/Dexie.js
  - Limited support for relationships and no support of join nor agregate queries
- https://github.com/google/lovefield
  - No remote sync, can use either a local or a remote adapter

- `Dexie.Syncable` the Sync Server just uses Nedb as a quick way to get a sample running. You would replace that with MongoDB or whatever backend DB you wanted so you con isn't relevant here.
- Automerge is indeed impressive however last I looked (and asked) it only works with in-memory objects and isn't durable as @gedw99 mentioned.
- Like y-js I'm also concerned about lack of garbage collection with automerge which could end up consuming a lot of memory if the objects are a reasonable size and change frequently. For example a 5KB Markdown document which was edited 20 times (in a day) would use 100KB + metadata.
- Logux is another library I've been evaluating and not had time to mention before (been travelling) but shows promise. 
  - What I like about Logux, is it is completely independent of the front/backend database. Which `Dexie.Syncable` is largely.
