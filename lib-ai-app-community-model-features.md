---
title: lib-ai-app-community-model-features
tags: [ai, architecture, large-language-model]
created: 2025-11-05T19:04:29.203Z
modified: 2025-11-05T19:04:50.350Z
---

# lib-ai-app-community-model-features

# guide

# discuss-stars
- ## 

- ## 

- ## 
# discuss-workflow/pipeline
- ## 

- ## 

- ## 
# discuss-memory
- ## 

- ## 

- ## 

- ## 

- ## [What are some approaches taken for the problem of memory in LLMs? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1op7kmw/what_are_some_approaches_taken_for_the_problem_of/)
  - Long-term memory is currently one of the most important problems in LLMs. What are some approaches taken by you or researchers to solve this problem?

- When reaching the context limit, you can have the model summarize the previous content and only maintain the highly relevant details.
  - You can also have the model create structured notes, for example like writing to a file akin to a notepad so it can keep track of progress what it needs to do and what it already finished like a to-do list.
  - There's this blog post by Anthropic that might be relevant.

- AI Memory and RAG are two pair of shows. Robust memory requires semantic context, ontologies, and a hybrid stack that combines vectors (similarity) with graphs (relationships). Handling embeddings and relational structure is also required.
- Current leaders in the field are
  - cognee - Strong at semantic understanding and graph-based reasoning, useful when relationships, entities, and multi-step logic matter; requires a bit more setup but scales well with complexity.
  - mem0 - Lightweight, simple to integrate, and fast for personalization or “assistant remembers what you said” use cases; less focused on structured or relational reasoning.
  - zep - Optimized for evolving conversations and timelines, making it good for session history and narrative continuity; not primarily aimed at deep semantic graph reasoning.
# discuss-news-model
- ## 

- ## 

- ## 

- ## [Instead of predicting one token at a time, CALM (Continuous Autoregressive Language Models) predicts continuous vectors that represent multiple tokens at once : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1opabzi/instead_of_predicting_one_token_at_a_time_calm/)

# discuss-memory
- ## 

- ## 

- ## 
