---
title: lib-ai-app-examples-rag-knowledge-base
tags: [ai, examples, knowledge-base, large-language-model, rag]
created: 2025-11-30T17:26:44.502Z
modified: 2025-11-30T17:27:16.720Z
---

# lib-ai-app-examples-rag-knowledge-base

# guide

- ä¼˜åŒ–æå–ä¸­æ–‡æ–‡æ¡£æ–‡å­—çš„æ–¹æ¡ˆ
  - å¯å‚è€ƒåäººå›¢é˜Ÿçš„æ–¹æ¡ˆ, å¦‚ WeKnora, LightRAG
# popular
- https://github.com/pipeshub-ai/pipeshub-ai /2kStar/apache2/202511/python/ts
  - https://pipeshub.com/
  - The OpenSource Alternative to Glean's Workplace AI
  - PipesHub AI helps you quickly find the right information using natural language searchâ€”just like Google.
  - The platform not only delivers the most relevant results but also shows where the information came from, with proper citations, using Knowledge Graphs and Page Ranking
  - Beyond search, our platform allows enterprises to create custom apps and AI agents using a No-Code interface.
  - Knowledge Graph Backbone â€“ All data is seamlessly structured into a powerful knowledge graph.
  - Enterprise-Grade Connectors â€“ Scalable, reliable, and built for secure access across your organization.
  - Modular & Scalable Architecture â€“ Every service is loosely coupled to scale independently and adapt to your needs.
  - å‰ç«¯: Material UIã€React Hook Formã€zod
  - åç«¯: fastapi, LangGraph, LangChain, Qdrant(vector), ArangoDB(graph), Kafka, Redis, Docling, PyMuPDF, OCRmyPDF
  - ğŸ›¢ï¸ åº”ç”¨å±‚æ•°æ®åœ¨pythonä¾§å’Œnodejsä¾§éƒ½å¤§é‡ä½¿ç”¨arangodbæ¥å­˜å‚¨å›¾ç»“æ„çš„å…³ç³»
  - ğŸ› ä¸æ”¯æŒå¤–éƒ¨æœç´¢å¼•æ“å¦‚ Tavily/EXA/SearxNG, ä½†æ–¹ä¾¿çº¯æœ¬åœ°éƒ¨ç½²
  - [Work AI for all - AI platform for agents, assistant, search](https://www.glean.com/)
  - [Best way to extract data from PDFs and HTML : r/Rag _202510](https://www.reddit.com/r/Rag/comments/1oavnx4/best_way_to_extract_data_from_pdfs_and_html/)
    - At PipesHub, we use docling, pymupdf (faster than docling but need to use layout parser on top of it), ocrmupdf/Azure DI (scanned pdfs).
    - If you are looking for Higher Accuracy, Visual Citations, Cleaner UI, Direct integration with Google Drive, OneDrive, SharePoint Online, Dropbox and more. PipesHub is free and fully open source, extensible. 
  - prå·²åˆå¹¶ [Backend Support for Ollama Models Â· Pull Request _202507](https://github.com/pipeshub-ai/pipeshub-ai/pull/475)
    - [Ollama Embedding model support Â· Pull Request ](https://github.com/pipeshub-ai/pipeshub-ai/pull/480)

- https://github.com/MODSetter/SurfSense /10.6kStar/apache2/202511/python/ts
  - https://www.surfsense.com/
  - Open source alternative to NotebookLM, Perplexity, and Glean.
  - Connects to search engines, Slack, Linear, Jira, ClickUp, Notion, YouTube, GitHub, Discord, and more. 
  - Save content from your own personal files (Documents, images, videos and supports 50+ file extensions) to your own personal knowledge base .
  - â›“ï¸ Get Cited answers just like Perplexity.
  - Works Flawlessly with Ollama local LLMs.
    - ğŸ› embeddingæ¨¡å‹å¿…é¡»ä½¿ç”¨azure, ä¸æ”¯æŒæœ¬åœ°æ¨¡å‹
  - åç«¯: FastAPI, SQLAlchemy, pgvector, LangGraph, LangChain, Hybrid Search, Rerankers, Redis
  - å‰ç«¯: next, Vercel AI SDK Kit UI Stream Protocol, Shadcn, Framer Motion, React Hook Form, tanstack/table
  - ä¾èµ–chonkieå¯¹docè¿›è¡Œembedding
  - ETL Service (choose one)
    - Docling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)
    - LlamaIndex API key (enhanced parsing, supports 50+ formats)
  - ğŸ‘‡ xp
    - è·¨workspaceä¸èƒ½å…±äº«source-document
    - èŠå¤©æ—¶~~åªèƒ½é€‰æ‹©å…¨éƒ¨æ–‡æ¡£æˆ–ä¸é€‰æ–‡æ¡£ï¼Œä¸~~å¯ä»¥åªé€‰æ‹©éƒ¨åˆ†æ–‡æ¡£
    - æ”¯æŒæ ¹æ®åœºæ™¯é…ç½®ä¸åŒllm: fast, long, reasoning
    - ğŸ› èŠå¤©ä¸­çš„å†…å®¹æ”¯æŒç‚¹å‡»è·³è½¬åˆ°æ–‡æ¡£çš„chunkä½ç½®ï¼Œè€Œä¸æ˜¯æºæ–‡ä»¶ï¼Œä¸”ä¸­æ–‡æ–‡æ¡£çš„chunkç»å¸¸æ˜¯ä¹±ç 
  - ğŸ“¡ [ã€Feature Requestã€‘ Streaming Response for Research Agent _202505](https://github.com/MODSetter/SurfSense/issues/86)
  - https://discord.com/channels/1359368468260192417/1359416865939787837/1409642464792412220
    - I was considering installing Surfsense but it needs API keys, doesn't it? How much does it cost to use it?
    - Every service has a local alternative other than Speech to Text service. No need to put any API keys if you use everything local.
  - https://discord.com/channels/1359368468260192417/1359416891831222362/1437030441910669403  _202511
    - how does embedding work for large documents (not chunks)  if my embedding modelâ€™s context window is only 256 or 512 tokens, but the document is tens of thousands of tokens long? 
    - We generate embedding of the summary of doc.

- https://github.com/infiniflow/ragflow /68.2kStar/apache2/202511/python/ts/åäººå›¢é˜Ÿ
  - https://ragflow.io/
  - https://demo.ragflow.io/
  - open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding
  - Template-based chunking: Plenty of template options to choose from
  - Grounded citations with reduced hallucinations
  - Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.
  - Multiple recall paired with fused re-ranking.
  - ä¾èµ– Crawl4AIã€elasticsearchã€flask-loginã€minioã€pandasã€voyageaiã€pyobvector
  - æœªä½¿ç”¨langchain/aisdk, å¤šmodel-providerçš„é›†æˆå®Œå…¨è‡ªå®šä¹‰å®ç°
  - ğŸŒ¹ å¯¹ä¸­æ–‡çš„æ”¯æŒè¾ƒå¥½
  - å®æµ‹æœ¬åœ°è¿è¡Œå¾ˆä¸å‹å¥½
    - elasticsearch/infinity æ•°æ®å±‚è¿˜åœ¨è¿ç§»ä¸ä¼˜åŒ–
    - æœ¬åœ°è¿è¡Œçš„å…¥å£ä¾èµ– libjemalloc.so
  - Prerequisites: RAM >= 16 GB
    - gVisor: Required only if you intend to use the code executor (sandbox) feature of RAGFlow.
  - âœ¨ å®æµ‹, ragåçš„chunkæ–‡æœ¬å¯æŸ¥çœ‹chunkä¸åŸæ–‡å¯¹åº”çš„ç»†èŠ‚
    - èŠå¤©çš„å›ç­”ä¸­, hoverå›¾æ ‡èƒ½æ˜¾ç¤ºå¼•æ–‡ï¼Œå¹¶å¯ç‚¹å‡»å¼•æ–‡åå¯åœ¨é¡µé¢é®ç½©ä¾§é¢å¼¹çª—ä¸­æŸ¥çœ‹pdfåŸæ–‡ä½ç½®
  - ğŸ› æ•´ä¸ª çŸ¥è¯†åº“/èŠå¤©/æŸ¥è¯¢ çš„åˆ›å»ºåŠä½¿ç”¨äº¤äº’è¿‡äºå¤æ‚ï¼Œ ä¸å¤Ÿç®€æ´å’Œè‡ªç„¶
  - DeepDoc (Default) - RAGFlowPdfParser
    - Full OCR, Most comprehensive but slower
    - Stream Reading Capability: âœ… YES
    - Page-by-page processing with configurable page_from/page_to
    - Uses `pdfplumber.open()` with `page_from` and `page_to` parameters
    - ä½¿ç”¨pdfplumber.open()å’Œpypdf.PdfReader(), ä¸€æ¬¡æ€§åŠ è½½æ•´ä¸ªPDFæ–‡ä»¶åˆ°å†…å­˜
  - Plain Text Parser - PlainParser
    - Uses `pypdf` for text extraction only
    - Only for text-based PDFs
    - Stream Reading Capability: âœ… YES
    - Uses `pypdf.PdfReader` with page ranges
    - Memory Usage: Very low - only text content loaded
    - å¯ä»¥é€é¡µå¤„ç†ï¼Œä½†ä»éœ€è¦åŠ è½½å®Œæ•´PDF
  - Docling Parser - DoclingParser
    -  Uses `docling.DocumentConverter` with full file
  - MinerU Parser - MinerUParser
    - External tool, entire file processing
  - TCADP Parser - TCADPParser
    - Tencent Cloud API integration
    - Cloud-based, file upload to Tencent Cloud
    - Reading Method: Converts entire file to `base64` and uploads, entire file loaded into memory for Base64 conversion
  - Vision Parser - VisionParser
    - Visual model processing, full file
    - Higher memory requirements
    - Uses `pdfplumber` with full file access
  - [HARD -- Efficient way to use enterprise dataset without uploading all files? _202509](https://github.com/orgs/infiniflow/discussions/10388)
    - You have to upload the data from Azure to RAGFlow. And currently you can do that through API. From 0.22 which is going to be launched in this Nov, we will provide some data sources and you can ingest data by just click several buttons. And more data sources could be easily added.
  - [[Question]: Why can't knowledge graphs be used? _202509](https://github.com/infiniflow/ragflow/issues/10017)
    - Knowledge graphs canâ€™t currently be used with the Table chunking method in RAGFlow. The reason is that table chunking treats each row as a separate chunk and stores field mappings for retrieval, but it does not run those chunks through the knowledge graph extraction pipeline. As a result, entityâ€“relationship extraction is skipped for tables.
    - If you need a knowledge graph from table data, youâ€™d have to implement a custom post-processing step (e.g., parsing the rows yourself and generating triples).
  - [Select PDF parser | RAGFlow](https://ragflow.io/docs/dev/select_pdf_parser)
  - [Deploy local models | RAGFlow](https://ragflow.io/docs/dev/deploy_local_llm)
    - RAGFlow supports deploying models locally using Ollama, Xinference, IPEX-LLM, or jina.
  - ğŸ› [[Bug]: Fail to access model(text-embedding-bge-reranker-v2-m3). The LmStudioRerank has not been implement _202502](https://github.com/infiniflow/ragflow/issues/5354)
    - Unable to add reranker model for LM STUDIOï¼ŒFail to access model(text-embedding-bge-reranker-v2-m3).The LmStudioRerank has not been implement
  - ğŸ› [[Question]: Support for calling the OLLAMA Reranker models ï¼Ÿ _202509](https://github.com/infiniflow/ragflow/issues/8988)
    - Ollama supports very few rerank models and is not recommended.
  - [FAQs | RAGFlow](https://ragflow.io/docs/dev/faq)
    - RAGFlow supports MinerU (>= 2.6.3) as an optional PDF parser with multiple backends. RAGFlow acts only as a client for MinerU, calling it to parse documents, reading the output files, and ingesting the parsed content.
  - [[Question]: Big File Parsing ](https://github.com/infiniflow/ragflow/issues/10986)
  - [[Question]: High Memory and CPU Usage During PDF Parsing _202505](https://github.com/infiniflow/ragflow/issues/7602)
    - åŸå› æ‰¾åˆ°äº†ï¼Œç”¨äº†deepdocè§£ææ–¹å¼ï¼Œå°±éå¸¸æ¶ˆè€—èµ„æºï¼Œæˆ‘ç”¨nativeå°±æ²¡é—®é¢˜ã€‚ç›®å‰ä¸éœ€è¦å›¾ç‰‡è¯†åˆ«ï¼Œå°±ä¸éœ€è¦deepdoc
  - [Questions about Ragflow _202507](https://github.com/orgs/infiniflow/discussions/8904)
    - RAGFlow uses Elasticsearch by default. Is Elasticsearch your recommendation over Infinity (at version 0.19.1)?
    - Pls choose Elasticsearch. We're refactoring Infinity.
  - ğŸ› [[Question]: Parsing resumes is not open source yet. Are there any plans to open source it in the future? ](https://github.com/infiniflow/ragflow/issues/1053)
    - 202406: currently, we don't have such plan.
  - [[Question]: How to using this "Team" option for open-source? ](https://github.com/infiniflow/ragflow/issues/7050)
    - 202504: It's a feature of enterprise version.

- https://github.com/Tencent/WeKnora /7.5kStar/MIT/202511/go/ts/vue
  - https://weknora.weixin.qq.com/
  - LLM-powered framework for deep document understanding, semantic retrieval, and context-aware answers using RAG paradigm
  - ğŸ“¡ [WeKnora Roadmap ](https://github.com/Tencent/WeKnora/issues/414)
    - Vision: ç‹¬ç«‹éƒ¨ç½²ä¸ªäººçŸ¥è¯†åº“ï¼Œæ”¯æŒæ–‡æ¡£ã€æ•°æ®ã€å›¾ç‰‡ï¼ŒåŸºäºä¼ ç»ŸRAGæ¡†æ¶æ‹“å±•æ›´å¤šLLMåº”ç”¨åœºæ™¯
  - [[Question]: è¯·é—®æœ‰è®¡åˆ’æ”¯æŒMinerUã€PP-Structure-V3ç­‰ç¬¬ä¸‰æ–¹è§£ææœåŠ¡çš„è°ƒç”¨ä¹ˆ? ä»¥åŠæ˜¯å¦æ”¯æŒæ›´å¤šåƒlightragè¿™ç§graphçš„æ„å»ºï¼Ÿ _202509](https://github.com/Tencent/WeKnora/issues/248)
    - ç°åœ¨æ”¯æŒäº†PPOCR-V4.PP-Structure-V3å½“ç„¶ä¹Ÿæ˜¯å¯ä»¥çš„ï¼Œå…·ä½“è§£æé€»è¾‘åœ¨docreaderè¿™ä¸ªæ¨¡å—ä¸­ã€‚
    - MinerUè¿™å—æš‚æ—¶æ²¡æœ‰è¯¦ç»†è°ƒç ”ï¼Œdocreaderåœ¨è®¾è®¡ä¸Šæ˜¯ä¸€ä¸ªPythonçš„ç‹¬ç«‹æ¨¡å—ï¼Œç†è®ºä¸Šå¯ä»¥æ¯”è¾ƒè½»æ¾çš„æ¥å…¥å…¶ä»–SDKã€‚
    - graphRagè¿™å—æˆ‘ä»¬ç°åœ¨æœ‰æŒ‰ç…§æˆ‘ä»¬çš„ç´¢å¼•ç»“æ„è¿›è¡Œå®ç°ã€‚å¦‚æœæœ‰æ›´å¥½çš„æ–¹å¼ï¼Œæ„Ÿè§‰å¯ä»¥æŒ‰ç…§å¤šè·¯ç´¢å¼•çš„æ€è·¯è¿›è¡Œå•ç‹¬ä¸€è·¯çš„å®ç°
  - ğŸ› [æ–‡æ¡£è§£æå¤±è´¥ _202508](https://github.com/Tencent/WeKnora/issues/77)
    - åˆšåˆšæ‰¾åˆ°è§£å†³æ–¹æ³•äº†ï¼Œç–‘ä¼¼æ˜¯è°ƒç”¨é€šè¿‡æœ¬åœ°ollama æ¨¡å‹çš„é—®é¢˜ï¼› è§£å†³æ–¹æ³•å°±æ˜¯é€šè¿‡apiå»è°ƒç”¨embeddingæ¨¡å‹ï¼Œè™½ç„¶é…ç½®æ—¶ä¼šæç¤ºæŠ¥é”™ï¼Œä½†æ˜¯å¯ä»¥æ˜¯ä¿å­˜çš„ï¼Œä¿å­˜åä¹Ÿå¯ä»¥ä½¿ç”¨ã€‚å†æ¬¡ä¸Šä¼ æ—¶å°±è§£ææˆåŠŸäº†
  - [[Question]: ä¸Šä¼ å¤§æ–‡ä»¶æ€ä¹ˆåŠï¼Ÿ _202510](https://github.com/Tencent/WeKnora/issues/348)
    - è¿™ä¸ªåº”è¯¥æ˜¯nginxçš„é…ç½®é—®é¢˜å§ã€‚å…ˆè¿›å…¥å‰ç«¯æ–‡ä»¶å¤¹ï¼Œä¿®æ”¹nginx.confæ–‡ä»¶
    - æ¯”å¦‚ï¼Œå°†æœ€å¤§æ–‡ä»¶ä¿®æ”¹ä¸º200M,å°†ä¸Šä¼ æ—¶é—´ä¿®æ”¹ä¸º300s;
      - å°†client_max_body_size 50M ;ä¿®æ”¹ä¸º200M;
      - client_max_body_size 200M
      - proxy_connect_timeout 300s;
  - [[Question]: è§£é™¤äº†30Mçš„é™åˆ¶ä¹‹åè§£æå¤±è´¥äº† ](https://github.com/Tencent/WeKnora/issues/245)
    - æ‰¾åˆ°äº†é—®é¢˜äº† ç»™ gRPC æ¶ˆæ¯å¤§å°è®¾ç½®ä¸º 5GB è¿™å¥½åƒæ˜¯è¶…èŒƒå›´äº† ä¿®æ”¹ä¸º500Må å°±èƒ½æ­£å¸¸è§£æäº†ï¼Œå¤§æ¦‚åœ¨65Må·¦å³çš„ä¸€ä¸ªpdfè§£æäº†å°†è¿‘15åˆ†é’Ÿ ä¸çŸ¥é“æ˜¯ä¸æ˜¯æˆ‘ç”µè„‘æ€§èƒ½çš„é—®é¢˜ m1 pro 32
    - ç›®å‰æœ‰å¹¶å‘æ€§èƒ½é—®é¢˜ï¼Œåé¢ä¼šè¿›ä¸€æ­¥ä¼˜åŒ–
  - [[Bug]: ä½¿ç”¨bge-m3å¯¼å…¥æ–‡æœ¬çŸ¥è¯†åº“è¶…è¿‡ä¸€å®šå­—ç¬¦å°±å¤±è´¥ ](https://github.com/Tencent/WeKnora/issues/410)
    - ä½¿ç”¨bge-m3å‘é‡åŒ–æ¨¡å‹ï¼Œæœ¬åœ°txtæ–‡æœ¬æ–‡ä»¶å¯¼å…¥çŸ¥è¯†åº“çš„è¿‡ç¨‹ä¸­ï¼Œå½“æ–‡æ¡£å­—ç¬¦å¤§äºç­‰äº2wæ—¶ï¼Œå°±ä¼šå¯¼å…¥å¤±è´¥ï¼Œè€Œé€‚å½“åˆ é™¤å­—ç¬¦ï¼Œæ¯”å¦‚19997ï¼Œåˆ™å¯ä»¥æ­£å¸¸å¯¼å…¥ã€‚åˆæ­¥æ¨æµ‹æœ‰ä¸€ä¸ª2ä¸‡çš„é™åˆ¶ï¼Ÿä»ä»»åŠ¡ç®¡ç†å™¨çš„èµ„æºæƒ…å†µæ¥çœ‹ï¼Œæœºå™¨èµ„æºåœ¨å¥åº·æ°´ä½ï¼Œæ‰€ä»¥ä¸æ˜¯æœºå™¨èµ„æºçš„é—®é¢˜
  - [[Feature]: çŸ¥è¯†åº“æ˜¯å¦èƒ½æ”¯æŒä¸Šä¼ æ–‡ä»¶å¤¹ _202511](https://github.com/Tencent/WeKnora/issues/388)
    - è¿˜æ²¡æœ‰è®¡åˆ’æ”¾å¼€è¿™ä¸ªåŠŸèƒ½ï¼Œå¦‚æœä¸€æ¬¡æ€§ä¸Šä¼ æ–‡ä»¶è¿‡å¤šï¼Œå¤šæœåŠ¡è´Ÿè½½æ¥è¯´ä¸æ˜¯ä¸€ä»¶å¥½äº‹æƒ…
  - [[Question]: çŸ¥è¯†å›¾è°±  ](https://github.com/Tencent/WeKnora/issues/364)
    - å‘ç°é—®é¢˜äº†ï¼Œæ˜¯æ¨¡å‹çš„é—®é¢˜ï¼ŒçŸ¥è¯†å›¾è°±çš„æ„å»ºéœ€è¦jsonæ ¼å¼ï¼Œä¹‹å‰ä½¿ç”¨çš„æ˜¯æ¨ç†æ¨¡å‹ï¼ŒåŒ…å«äº†thinkæ ‡ç­¾ï¼Œæ‰€ä»¥ä¸€ç›´å¤±è´¥ã€‚ 
    - å¦‚æœæˆ‘æƒ³å…³é—­çŸ¥è¯†å›¾è°±çš„æ„å»ºä»¥åŠç¦æ­¢å®ƒä¾æ®çŸ¥è¯†å›¾è°±è¿›è¡Œæ£€ç´¢ï¼Œåªéœ€è¦â€œENABLE_GRAPH_RAG=Falseâ€å°±å¯ä»¥äº†å—ï¼Ÿæƒ³åšä¸€ä¸‹æœ‰æ— çŸ¥è¯†å›¾è°±æƒ…å†µä¸‹çš„æ£€ç´¢æ€§èƒ½çš„å¯¹æ¯”
    - æ˜¯çš„ã€‚ç„¶åé‡å¯é‡æ–°ä¸Šä¼ å°±å¥½
  - [[Question]: åªèƒ½é€šè¿‡çŸ¥è¯†åº“çš„å†…å®¹é—®ç­”ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰å†…å®¹æ—¶ï¼Œæ— è¿”å› ](https://github.com/Tencent/WeKnora/issues/359)
    - æœŸæœ›åœ¨çŸ¥è¯†åº“æ£€ç´¢ä¸åˆ°æ—¶ï¼Œå¤§æ¨¡å‹ä¹Ÿå¯ä»¥è¾“å‡º
    - è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ä¸ºäº†é˜²æ­¢æ¨¡å‹è¾“å‡ºä¸€äº›å±é™©çš„ï¼Œæ— æ³•é¢„æµ‹çš„å†…å®¹
    - å¯ä»¥ä¿®æ”¹summary prompt, æŠŠ NO_MATCH çš„è¯æœ¯ä» Promptä¸­å»æ‰ï¼Œæ”¹æˆä½ æƒ³è¦çš„

- https://github.com/UnicomAI/wanwu /2.7kStar/apache2/202511/go/ğŸ†š
  - å…ƒæ™¯ä¸‡æ‚Ÿæ™ºèƒ½ä½“å¹³å°æ˜¯ä¸€æ¬¾é¢å‘ä¼ä¸šçº§åœºæ™¯çš„ä¸€ç«™å¼ã€å•†ç”¨licenseå‹å¥½çš„æ™ºèƒ½ä½“å¼€å‘å¹³å°
  - å¤šç§Ÿæˆ·æ¶æ„ï¼šæä¾›å¤šç§Ÿæˆ·è´¦å·ä½“ç³»ï¼Œæ»¡è¶³ç”¨æˆ·æˆæœ¬æ§åˆ¶ã€æ•°æ®å®‰å…¨éš”ç¦»ã€ä¸šåŠ¡å¼¹æ€§æ‰©å±•ã€è¡Œä¸šå®šåˆ¶åŒ–ã€å¿«é€Ÿä¸Šçº¿åŠç”Ÿæ€ååŒç­‰æ ¸å¿ƒéœ€æ±‚
  - ğŸ†š æä¾›äº†ç«å“æ¯”è¾ƒ: dify, fastgpt, ragflow, coze
  - ä¿¡åˆ›é€‚é…ï¼šå·²é€‚é…å›½äº§ä¿¡åˆ›æ•°æ®åº“TiDBå’ŒOceanBase
  - model hub: æ”¯æŒç”¨æˆ·å¯¼å…¥åŒ…æ‹¬è”é€šå…ƒæ™¯ã€OpenAI-API-compatibleã€Ollamaã€é€šä¹‰åƒé—®ã€ç«å±±å¼•æ“ç­‰æ¨¡å‹ä¾›åº”å•†çš„LLMã€Embeddingã€Rerankæ¨¡å‹
  - æä¾› å¤šæ¨ç†åç«¯æ”¯æŒï¼ˆvLLMã€TGIç­‰ï¼‰ä¸ è‡ªæ‰˜ç®¡è§£å†³æ–¹æ¡ˆï¼Œæ»¡è¶³ä¸åŒè§„æ¨¡ä¼ä¸šçš„ç®—åŠ›éœ€æ±‚
  - è”ç½‘æ£€ç´¢ï¼ˆWeb Searchï¼‰
  - å¯è§†åŒ–å·¥ä½œæµï¼ˆWorkflow Studioï¼‰
  - ä¼ä¸šçº§çŸ¥è¯†åº“ã€RAG Pipeline:  æä¾›çŸ¥è¯†åº“åˆ›å»ºâ†’ æ–‡æ¡£è§£æâ†’å‘é‡åŒ–â†’æ£€ç´¢â†’ç²¾æ’ çš„å…¨æµç¨‹çŸ¥è¯†ç®¡ç†èƒ½åŠ›ï¼Œæ”¯æŒpdf/docx/txt/xlsx/csv/pptxç­‰ å¤šç§æ ¼å¼ æ–‡æ¡£ï¼Œè¿˜æ”¯æŒç½‘é¡µèµ„æºçš„æŠ“å–å’Œæ¥å…¥
  - æä¾› RESTful API ï¼Œæ”¯æŒä¸ä¼ä¸šç°æœ‰ç³»ç»Ÿï¼ˆOA/CRM/ERPç­‰ï¼‰æ·±åº¦é›†æˆ

- https://github.com/pingcap/autoflow /2.6kStar/apache2/202507/python
  - https://tidb.ai/
  - a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage.
  - An open source GraphRAG (Knowledge Graph) built on top of TiDB Vector and LlamaIndex and DSPy.
  - UIäº¤äº’ç±»ä¼¼chatgpt
  - https://x.com/9hills/status/1862522244527972625
    - RAG Demo åˆ° RAG Application éš¾åº¦çš„å®Œç¾è¡¨ç°ï¼Œå…¶å®åŠŸèƒ½ä¸ç®—ä¸°å¯Œï¼ˆå¢åŠ äº† Graph RAGå’Œ Agent RAG çš„æ€æƒ³ï¼‰ï¼Œ
    - ä»£ç å´ä¸å¾—ä¸åšçš„éå¸¸å¤æ‚ï¼Œå¤§éƒ¨åˆ†å…¶å®æ˜¯åº”ç”¨é€»è¾‘ã€‚ P. S. ä»£ç å·²ç»æˆç†Ÿåˆ°å¯ä»¥ç›´æ¥æŠ„äº†ï¼Œç›´æ¥å¤åˆ»å°±å®Œäº†

- https://github.com/netease-youdao/QAnything /13.8kStar/apache2 > AGPL/202503/python/vue/inactive
  - https://qanything.ai/
  - å¼€æºçš„ä¼ä¸šçº§æœ¬åœ°çŸ¥è¯†åº“é—®ç­”è§£å†³æ–¹æ¡ˆï¼Œè‡´åŠ›äºæ”¯æŒä»»æ„æ ¼å¼æ–‡ä»¶æˆ–æ•°æ®åº“çš„é—®ç­”
  - æ¨¡å‹æ•°æ®å…¨åœ¨æœ¬åœ°ï¼Œå¯æ–­ç½‘ä½¿ç”¨
  - Support selecting multiple knowledge bases for Q&A
  - Currently supported formats include: PDF(pdf), Word(docx), PPT(pptx), XLS(xlsx), Markdown(md), Email(eml), TXT(txt), Image(jpgï¼Œjpegï¼Œpng), CSV(csv), Web links(html) and more
  - https://github.com/netease-youdao/BCEmbedding /apache2/202409/python/inactive
    - ç”±ç½‘æ˜“æœ‰é“å¼€å‘çš„ä¸­è‹±åŒè¯­å’Œè·¨è¯­ç§è¯­ä¹‰è¡¨å¾ç®—æ³•æ¨¡å‹åº“ï¼Œå…¶ä¸­åŒ…å« EmbeddingModelå’Œ RerankerModelä¸¤ç±»åŸºç¡€æ¨¡å‹
    - EmbeddingModelä¸“é—¨ç”¨äºç”Ÿæˆè¯­ä¹‰å‘é‡ï¼Œåœ¨è¯­ä¹‰æœç´¢å’Œé—®ç­”ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œè€Œ RerankerModelæ“…é•¿ä¼˜åŒ–è¯­ä¹‰æœç´¢ç»“æœå’Œè¯­ä¹‰ç›¸å…³é¡ºåºç²¾æ’ã€‚
    - BCEmbeddingä½œä¸ºæœ‰é“çš„æ£€ç´¢å¢å¼ºç”Ÿæˆå¼åº”ç”¨ï¼ˆRAGï¼‰çš„åŸºçŸ³ï¼Œç‰¹åˆ«æ˜¯åœ¨QAnythingä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨
    - åŒè¯­å’Œè·¨è¯­ç§èƒ½åŠ›ï¼šåŸºäºæœ‰é“ç¿»è¯‘å¼•æ“çš„å¼ºå¤§èƒ½åŠ›ï¼ŒBCEmbeddingå®ç°å¼ºå¤§çš„ä¸­è‹±åŒè¯­å’Œè·¨è¯­ç§è¯­ä¹‰è¡¨å¾èƒ½åŠ›
    - é¢å‘RAGåšé’ˆå¯¹æ€§ä¼˜åŒ–ï¼Œå¯é€‚é…å¤§å¤šæ•°ç›¸å…³ä»»åŠ¡ï¼Œæ¯”å¦‚ç¿»è¯‘ï¼Œæ‘˜è¦ï¼Œé—®ç­”ç­‰ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ é—®é¢˜ç†è§£ï¼ˆquery understandingï¼‰ ä¹Ÿåšäº†é’ˆå¯¹ä¼˜åŒ–

- https://github.com/Future-House/paper-qa /7.9kStar/apache2/202511/python
  - https://futurehouse.gitbook.io/futurehouse-cookbook
  - PaperQA2 is a package for doing high-accuracy retrieval augmented generation (RAG) on PDFs, text files, Microsoft Office documents, and source code files, with a focus on the scientific literature.
  - A usable full-text search engine for a local repository of PDF/text files.
  - You can use llama.cpp to be the LLM. You won't get good performance with 7B models.
  - [save and import embeddings for faster answer generation _202411](https://github.com/Future-House/paper-qa/issues/721)
    - Iâ€™ve been able to use it to ask questions by providing over 100 papers as input, and Iâ€™ve been using only local models via Ollama. Everything is working well, but Iâ€™d like to know how I can avoid reloading the same files and retraining an embedding model each time I have a new Question.
    - ğŸ”¡ éœ€è¦æ‰‹åŠ¨ä¿®æ”¹embeddingæ¨¡å‹é…ç½®, è€Œä¸æ˜¯ä½¿ç”¨é»˜è®¤openaiæ¨¡å‹
  - [usage with large local paper results exceeding `text-embeeded`'s maximium prompt limit _202409](https://github.com/Future-House/paper-qa/issues/453)
    - I'm using paperqa with CLI frontend, while this could apply to code usage too. just run pqa ask cusum in a directory with a 10MiB Chinese paper pdf, results an error 'exceeding 8192 tokens limit' using OpenAI third-party proxy API (did some trick in litellm to actually set api_base)
    - We could call tokenizer, or ask litellm to support pre-check before api call (this will be better)
# rag-examples
- https://github.com/pymupdf/pymupdf4llm /1.2kStar/AGPL/202511/python/lib
  - https://pymupdf.readthedocs.io/en/latest/pymupdf4llm
  - a specialized extension of PyMuPDF designed specifically for extracting content from PDFs in a format that's optimized for LLMs
  - Converts PDFs to clean, structured Markdown format
  - Automatically identifies headers, paragraphs, tables, and images
  - Preserves document hierarchy (headers, lists, tables)
  - Maintains document layout and reading order: process multi-column pages
  - Extracts images from PDFs: save images separately or encode them inline
  - [Build a Multimodal RAG using â€” PyMuPDF4LLM-llamaindex-Qdrant _202411](https://ai.gopubby.com/build-a-multimodal-rag-using-pymupdf4llm-llamaindex-qdrant-e9d23a4409cc)

- https://github.com/danny-avila/rag_api /710Star/MIT/202508/python/librechat
  - [RAG API - Chat with files](https://www.librechat.ai/docs/features/rag_api)
  - This project integrates Langchain with FastAPI in an Asynchronous, Scalable manner, providing a framework for document indexing and retrieval, using PostgreSQL/pgvector.
  - Files are organized into embeddings by `file_id`. The primary use case is for integration with LibreChat, but this simple API can be used for any ID-based use case.
  - The main reason to use the ID approach is to work with embeddings on a file-level. This makes for targeted queries when combined with file metadata stored in a database, such as is done by LibreChat.
  - [[Enhancement] Reduce memory consumption greatly, and speed up process slighlty by processing file async and inserting to vectordb in chunks rather than all at once _202510](https://github.com/danny-avila/rag_api/issues/213)
    - Currently RAG creates embeddings for the file holding them all in memory then bulk inserts them all into the vectordb. When embedding very large files this consumes a vast amount of memory
    - This is largely solved by changing the logic such that it breaks the file up into chucks and embeds each separately, and asynchronously bulk inserting each chunk individually as the embedding process completes each chunk. 
    - præœªåˆå¹¶
  - [[Issue] `/health` endpoint becomes unresponsive during large file processing _202510](https://github.com/danny-avila/rag_api/issues/216)
    - When uploading and processing large files, the /health endpoint becomes unresponsive until the processing is complete.

- https://github.com/clusterzx/paperless-ai /4.6kStar/MIT/202511/python/js
  - https://clusterzx.github.io/paperless-ai/
  - an AI-powered extension for Paperless-ngx that brings automatic document classification, smart tagging, and semantic search using OpenAI-compatible APIs and Ollama.
  - [Paperless-AI: Now including a RAG Chat for all of your documents : r/selfhosted _202505](https://www.reddit.com/r/selfhosted/comments/1kqbouu/paperlessai_now_including_a_rag_chat_for_all_of/)
  - [Paperless-ai runs well on local chatgpt-oss installation  _202510](https://github.com/clusterzx/paperless-ai/discussions/749)
    - I installed chatgpt-oss-20b FP16 model, using koboldcpp on a Ryzen 7 7840HS mini pc with 32G RAM. Using -- usevulkan all 25 layers of the model can offloaded to gpu using VRAM. There is no major speed reduction when using FP16, as compared to Q4 etc.! Running on proxmox / debian LXC.
    - For the first 100 docs, the results are very good for my mostly German documents. Processing time is mostly below 10 seconds per document for smaller 1-3 page and 30-40 seconds for larger documents. Works fine for me.

- https://github.com/ari99/lm_studio_big_rag_plugin /202511/ts
  - LM Studio Plugin to mark a directory full of documents as a RAG source and another directory as the Vectorstore and build a RAG for use in your queries. Built using Cursor.
  - Massive Scale: Designed to handle large document collections (GB to TB scale)
  - Deep Directory Scanning: Recursively scans all subdirectories
  - PDF parsing via pdf-parse
  - Multiple File Formats: Supports HTM, HTML, XHTML, PDF, EPUB, TXT, TEXT, Markdown variants (MD/MDX), BMP, JPEG, PNG
  - OCR Support: Optional OCR for image files using Tesseract.js
  - Vector Search: Uses LanceDB for efficient vector storage and retrieval
  - Persistent Storage: Vector embeddings are stored locally and persist across sessions
  - Incremental Indexing: Automatically detects and skips already-indexed files
  - Concurrent Processing: Configurable concurrency for optimal performance
  - Disk Space: The vector store requires additional disk space (typically 10-20% of original document size)
  - Initial Indexing: Can take several hours for TB-scale collections
  - Memory Usage: Scales with concurrent processing (reduce maxConcurrentFiles if needed)
  - Limitations
    - RAR Archives: Not yet implemented (files are skipped)
    - Very Large Files: Individual files >100MB may cause memory issues
    - Non-English OCR: Currently only English OCR is configured

- https://github.com/2dogsandanerd/Knowledge-Base-Self-Hosting-Kit /MIT/202511/python
  - A Docker-powered RAG system that understands the difference between code and prose. 
  - Ingest your codebase and documentation, then query them with full privacy and zero configuration.
  - Different chunking strategies for code vs. prose
    - Code collections use AST-based chunking that respects function boundaries
    - Document collections use semantic chunking optimized for prose
  - Backend: FastAPI, LlamaIndex 0.12.9
  - Vector DB: ChromaDB 0.5.23
  - LLM/Embeddings: Ollama (configurable)
  - Document Parser: Docling 2.13.0 (advanced OCR, table extraction)
  - Frontend: Vanilla HTML/JS (no build step)
  - Batch Ingestion â€” Process multiple files (sequential processing in Community Edition)
  - ğŸ§ª
    - chroma run --host 0.0.0.0 --port 8000 --path ~/Documents/repos/libfwk/ai-llm/all-rag/Knowledge-Base-Self-Hosting-Kit/backend/ENV/ingestdb
    - OLLAMA_DEBUG=2 ollama serve
    - uv run --env-file .env -- uvicorn src.main:app --port 8080
  - [I spent 2 years building privacy-first local AI. My conclusion: Ingestion is the bottleneck, not the Model. (Showcase: Ollama + Docling RAG Kit) : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pamu5t/i_spent_2_years_building_privacyfirst_local_ai_my/)
    - Iâ€™ve been working on strictly local, data-privacy-compliant AI solutions for about two years
    - The biggest lesson I learned: We spend 90% of our time debating model quantization, VRAM, and context windows. But in real-world implementations, the project usually fails long before the prompt hits the LLM. It fails at Ingestion.
    - I built a self-hosting starter kit that focuses heavily on fixing the Input Layer before worrying about the model.
    - Ingestion: Docling (v2). I chose this over PyPDF/LangChain splitters because it actually performs layout analysis. It reconstructs tables and headers into Markdown, so the LLM isn't guessing when reading a row.
    - Iâ€™d love to hear your thoughts on the "Ingestion First" approach. For me, switching from simple text-splitting to layout-aware parsing was the game changer for retrieval accuracy.
  - ğŸ‘¾ prompts-agent
    - i have run this project fully locally without docker and nginx.
    - the backend server starts by `cd backend && uv run --env-file .env -- uvicorn src.main:app --port 8080`.

- https://github.com/djleamen/doc-reader /MIT/202511/python/django
  - A Django-based document Q&A system using Retrieval-Augmented Generation (RAG) to process and query large documents with AI-powered responses.
  - Large Document Support: Handle documents up to 800k+ words
  - Multiple Formats: PDF, DOCX, TXT, and Markdown support
  - REST API: Django REST Framework for integrations
  - Vector Search: FAISS/ChromaDB/Pinecone vector databases
  - CLI Tools: Command-line interface for batch operations
  - Memory issues with large docs: Reduce `CHUNK_SIZE` in `.env` or process documents individually

- https://github.com/renton4code/pdf-rag /AGPL/202502/ts/inactive
  - A production-ready template for building Retrieval-Augmented Generation (RAG) applications. 
  - This template provides a complete setup for document processing, vector storage, and AI-powered question answering with kickass UI.
  - PDF document processing with OCR
  - Milvus DB with billions of vectors scale support: Vector DB for storing embeddings
  - PostgreSQL: Relational DB for metadata storage
  - Click-to-view document references with highlighting
  - Large documents processing and status updates
  - AI/ML: Google Gemini (LLM), HuggingFace Transformers (Embeddings)
  - Backend: Bun
  - Based on Q&A with a large document (700+ pages) in comparison to RagFlow

- https://github.com/gptme/gptme-rag /MIT/202511/python/cli
  - Local RAG as a simple CLI
  - Built primarily for gptme, but can be used standalone.
  - Document indexing with ChromaDB
  - Streaming large file handling: chunking splits large documents into manageable pieces
  - File watching and auto-indexing: Real-time index updates, batch processing
  - https://github.com/gptme/gptme /4.1kStar/MIT/202511/python
    - Your agent in your terminal, equipped with local tools: writes code, uses the terminal, browses the web, vision

- https://github.com/thiswillbeyourgithub/wdoc /GPL/202511/python
  - https://wdoc.readthedocs.io/en/stable/
  - Summarize and query from a lot of heterogeneous documents. 

- https://github.com/IlyaRice/RAG-Challenge-2 /MIT/202503/python/inactive
  - [Ilya Rice: How I Won the Enterprise RAG Challenge _202503](https://abdullin.com/ilya/how-to-build-best-rag/)
  - Custom PDF parsing with Docling
  - Vector search with parent document retrieval: Faiss from Meta for efficient similarity search and clustering of dense vectors.
  - LLM reranking for improved context relevance
  - Structured output prompting with chain-of-thought reasoning
  - Query routing for multi-company comparisons
  - You'll need your own API keys for OpenAI/Gemini
  - [5.4 Enterprise-level RAG Practice (I) | AI Roadmap](https://comfyai.app/article/llm-applications/enterprise-level-rag-hands-on-practice)

- https://github.com/in-tech-gration/LangChain-RAG /202503/js/inactive
  - A simple RAG application for doing question-answering on a PDF document
  - This repository contains the JavaScript version of the python RAG implementation by Jodie Burchell using LangChain as demoed in her Beyond the Hype: A Realistic Look at Large Language Models GOTO 2024 presentation.
  - Uses LangChain.js v0.2
  - Key differences between the original python repository and the JavaScript version
    - This code uses two types of Vector stores instead of one. The original code used the ChromaDB vector store, whereas this repo contains code using ChromaDB but also code using the In-memory vector store module provided by LangChain.js.
    - The original repo contains a large PDF (pycharm-documentation.pdf which is around 174MB) that is used in the demo. This is a great source to test and also compare the results with the demo, but it turns out that it takes quite a lot of time to get vectorized. 
    - https://github.com/slvg01/90.10d_RAG_OnTheFly
      - you can upload any PDF document, with a limit of 200 MB per file. You can upload as many PDFs as you'd like.
      - Upload PDF files which are combined into one big text chunk.
    - [Machine-Learning/Optimizing RAG with Document Chunking Techniques Using Python.md at main Â· xbeat/Machine-Learning](https://github.com/xbeat/Machine-Learning/blob/main/Optimizing%20RAG%20with%20Document%20Chunking%20Techniques%20Using%20Python.md)

- https://github.com/Gopendranath/Rag-pdf-chat-backend /202511/js
  - This backend server provides comprehensive file upload functionality for handling images, documents, and messages in a chat application.
  - Multiple File Upload: Support for uploading multiple images and documents simultaneously
  - File Size Limits: 10MB maximum file size per file
  - Static File Serving: Uploaded files are served statically via /uploads/* endpoint
  - This project can use a vector-enabled PostgreSQL (pgvector) for embeddings and MongoDB for document storage.
  - https://github.com/Gopendranath/frontend-rag-chat-agent /ts

- https://github.com/iamarunbrahma/rag-ingest /MIT/202411/python/inactive
  - tool designed to seamlessly convert PDF documents into markdown format while preserving the original layout and formatting
  - The extracted content is indexed in a vector database (Qdrant) to enhance RAG
  - Advanced text extraction with layout preservation using PyMuPDF
  - Uses Ollama model for contextual enhancement of document chunks
  - Combines dense and sparse embeddings via Qdrant for improved retrieval
  - Memory Issues: Process large PDFs in smaller batches

- https://github.com/yash1912/colpali-rag /202410/python/å•æ–‡ä»¶/inactive
  - RAG solution leveraging cutting-edge AI models like ColPali and Qwen2VL.
  - converts PDFs into searchable embeddings using ColPali's AI model

- https://github.com/datawhalechina/all-in-rag /202511/python
  - https://datawhalechina.github.io/all-in-rag/
  - å¤§æ¨¡å‹åº”ç”¨å¼€å‘å®æˆ˜ä¸€ï¼šRAG æŠ€æœ¯å…¨æ ˆæŒ‡å—ï¼Œåœ¨çº¿é˜…è¯»
  - https://github.com/yksanjo/all-in-rag

- https://github.com/ZohaibCodez/document-qa-rag-system /MIT/202509/python/inactive
  - https://document-rag-system.streamlit.app/
  - RAG project built with LangChain and Streamlit. 
  - Upload documents (PDF/TXT) and interact with them using natural language questions powered by embeddings and vector search.
  - Vector-based similarity search using FAISS
  - Frontend: Streamlit
  - Limitations
    - File Types: Currently supports only PDF and TXT formats
    - Large documents (>50 pages) may take longer to process
    - Scanned PDFs: Does not support OCR for image-based PDFs
    - Large PDF files (>100MB) may cause memory issues
    - Embedded images in PDFs are not processed
    - Some complex PDF layouts may not parse correctly

- https://github.com/pratheeshkumar99/Document-based-Question-Answering-System /202408/python/inactive
  - This project demonstrates a Retrieval-Augmented Generation (RAG) system for question answering. 
  - It integrates OpenAIâ€™s GPT-4 model with FAISS for vector similarity search
  - Vector Store: The embeddings are stored in FAISS (Facebook AI Similarity Search)
  - Handles large documents by splitting them into smaller chunks, ensuring efficient processing and retrieval.

- https://github.com/BjornMelin/docmind-ai-llm /MIT/202509/python/åŠŸèƒ½ä¸°å¯Œ
  - open-source Streamlit application leveraging LlamaIndex, LangGraph, and local Large Language Models (LLMs) via Ollama, LMStudio, llama.cpp, or vLLM for advanced document analysis. 
  - provides local document analysis with zero cloud dependency
  - This system combines hybrid search (dense + sparse embeddings), knowledge graph extraction, and a 5-agent coordination system to extract and analyze information from your PDFs, Office docs, and multimedia content.
  - Built on LlamaIndex pipelines with LangGraph supervisor orchestration and Qwen3-4B-Instruct-2507's FULL 262K context capability through INT8 KV cache optimization
  - Offline-First Design: 100% local processing with no external API dependencies.
  - Enable Chunked Analysis for large documents, Late Chunking for accuracy, or Multi-Vector Embeddings for enhanced retrieval.
  - LlamaIndex `IngestionPipeline` orchestrates Unstructured parsing, deterministic hashing, DuckDB caching, and AES-GCM page image handling with OpenTelemetry spans for each run.
  - Retrieval/Router: RouterQueryEngine composed via router_factory with tools semantic_search, hybrid_search (Qdrant serverâ€‘side fusion), and optional knowledge_graph; uses async/batching where appropriate.
  - Hybrid Retrieval: Qdrant Query API serverâ€‘side fusion (RRF default, DBSF optional) over named vectors text-dense (BGEâ€‘M3; COSINE) and text-sparse (FastEmbed BM42/BM25 with IDF). Dense via LlamaIndex; sparse via FastEmbed.
  - Feature Flags: Boolean environment variables for experimental features

- https://github.com/ricardolx/rag-query /202401/ts/inactive
  - [Implement RAG for large document searchâ€” semantic search with a vector database and LLM _202401](https://medium.com/@ricrivero3/implement-large-document-search-using-rag-semantic-search-with-a-vector-database-and-llm-b798675dca08)
  - Backend â€” Firebase Cloud Functions
  - Database â€” Firestore
  - Vector DB â€” Pinecone
  - LLM â€” OpenAI

- https://github.com/chaanakyaaM/QueryPDF /202507/python/ä»£ç å°‘/inactive
  - a local-first, terminal-based RAG tool that allows users to interact with PDF documents using natural language, entirely offline.
  - It extracts text from user-specified pages, chunks it intelligently, embeds the content, and stores it in a local vector database.
  - Local Caching: Caches both tokenizers and embedding models locally for offline use and faster future runs.
  - Graceful Exit Handling: Automatically cleans up ChromaDB collections on exit.
  - Visual Progress Feedback: Shows embedding generation progress.
  - Uses semantic search to find the most relevant chunks for your questions.
  - Fully Customizable: Users can configure embedding model, LLMs, and chunk parameters via a simple JSON file.

- https://github.com/raghavan/pdfgptindexer-offline /MIT/202511/python
  - local RAG system for indexing and querying PDF documents
  - PyMuPDF for text extraction
  - Text Splitting: LangChain's RecursiveCharacterTextSplitter
  - Embeddings: HuggingFace Sentence Transformers (local)
  - Vector Store: FAISS for efficient similarity search
  - LLM: Ollama (local LLM runner)
  - Larger PDFs take longer to index - be patient
  - The FAISS index can be reused - you don't need to re-index unless PDFs change
  - https://github.com/amithkoujalgi/ollama-pdf-bot /202502/strealit
  - https://github.com/tonykipkemboi/ollama_pdf_rag /202501/streamlit

- https://github.com/drisskhattabi6/Chat-with-PDF-Locally /202505/python/inactive
  - An advanced chatbot using Ollama / Sambanova LLMs to interactively extract information from PDFs, Using Streamlit & Ollama / Sambanova API and langchain
  - uses the Marker library to convert PDFs to Markdown format. 
  - Text Chunking: Large documents are split into manageable chunks for easier processing.
  - Embedded Models: The app supports Ollama models for document embeddings and generating answers based on the content.
  - Chroma Vector Store: All the processed documents are stored in a Chroma vector store for efficient retrieval.

- https://github.com/krey-yon/Cliven /MIT/202506/python/inactive
  - a command-line tool that allows you to process PDF documents and have interactive conversations with their content using local AI models. 
  - using ChromaDB for vector storage and Ollama for AI inference.

- https://github.com/codesniper99/LLM-Tinkering /202508/python/inactive
  - This repository enables semantic search and question-answering on research papers using PDF ingestion, vector databases, and local or cloud-hosted LLM

- https://github.com/jacoblee93/fully-local-pdf-chatbot /MIT/202410/ts/inactive
  - another chat over documents implementation... but this one is entirely local
  - Exposing a port to a local LLM running on your desktop via Ollama.
  - Downloading weights into your browser and running via WebLLM.

- https://github.com/AvaAvarai/Local_Small_LM_Document_RAG /MIT/202409/python/inactive
  - Local Document Retrieval Augmented Generation (RAG) with sentence embedding context for cited question answering with small language models (LM).
  - Currently runs in terminal again, will add GUI back soon.

- https://github.com/snexus/llm-search /MIT/202506/python/inactive
  - RAG with a simple YAML-based configuration that enables interaction with a collection of local documents.
  - The package is designed to work with custom Large Language Models (LLMs) â€“ whether from OpenAI or installed locally.

- https://github.com/swirlai/swirl-search /2.9kStar/apache2/202511/python
  - https://swirlaiconnect.com/
  - AI Search & RAG Without Moving Your Data. Get instant answers from your company's knowledge across 100+ apps
  - Warning: The Docker version of Swirl does not retain any data or configuration when shut down
  - Swirl comes configured to search Arxiv, European PMC and Google News right out of the box.

- https://github.com/KeiranHome/Rag_demo /202505/python/inactive
  - æœ¬é¡¹ç›®åˆ©ç”¨FAISSå‘é‡åº“å’Œåƒé—®æ¨¡å‹APIæ„å»ºäº†ä¸€ä¸ªæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œæ—¨åœ¨ä¸ºç”¨æˆ·æä¾›é’ˆå¯¹å…¬å¸å¹´åº¦æ€»ç»“æ•°æ®çš„æ™ºèƒ½é—®ç­”æœåŠ¡
  - ä¸­æ–‡åˆ†è¯å·¥å…·ï¼šJieba/THULACï¼Œç”¨äºæ–­å¥å’Œå…³é”®è¯æå–ï¼Œæå‡åç»­Embeddingè¯­ä¹‰è¡¨å¾ç²¾åº¦ã€‚
  - é€šä¹‰åƒé—®Embeddingæ¨¡å‹ï¼šä¸“ä¸ºä¸­æ–‡ä¼˜åŒ–ï¼Œæ”¯æŒé•¿æ–‡æœ¬è¯­ä¹‰ç¼–ç ï¼ˆæœ€é•¿16K tokensï¼‰ï¼Œä½™å¼¦ç›¸ä¼¼åº¦å‡†ç¡®ç‡è¾ƒå¼€æºæ¨¡å‹ï¼ˆå¦‚BERT-baseï¼‰æå‡15%ï¼›
# rag-fwk
- https://github.com/phbst/tinyRAG /202409/jupyter/inactive
  - å…¨æ‰‹å†™çš„ä¸€ä¸ªRAGåº”ç”¨ã€‚Langchainçš„å¤§éƒ¨åˆ†åº“ä¼šå¾ˆæ–¹ä¾¿ï¼Œä½†æ˜¯ä½ ä¸ä¸€å®šç†è§£å…¶ä¸­åŸç†ï¼Œæ‰€ä»¥ä»£ç å°½å¯èƒ½å±•ç°åŸºæœ¬ç®—æ³•ï¼Œä¸»æ‰“ç†è§£RAGçš„åŸç†

- https://github.com/codemilestones/TinyCodeBase /apache2/202509/python
  - æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ä»£ç æ™ºèƒ½ç³»ç»Ÿï¼ŒåŒ…å«äº†RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ã€Agentï¼ˆæ™ºèƒ½ä»£ç†ï¼‰å’Œè¯„ä¼°ç³»ç»Ÿçš„å®Œæ•´å®ç°ã€‚
  - é¡¹ç›®ä»[TinyRAG](https://github.com/KMnO4-zx/TinyRAG)æ‰©å±•è€Œæ¥ï¼Œä¸“æ³¨äºä»£ç åœºæ™¯çš„ä¼˜åŒ–å’Œå®è·µã€‚
  - [ä¹‹å‰æœ‰å¤šå«Œå¼ƒå¤§æ¨¡å‹æ¡†æ¶ï¼Œç°åœ¨ç”¨ LangGraph å°±æœ‰å¤šé¦™ - çŸ¥ä¹ _202509](https://zhuanlan.zhihu.com/p/1946396924342177830)

- https://github.com/wzdavid/ThinkRAG /MIT/202507/python/inactive
  - å¤§æ¨¡å‹æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿï¼Œå¯ä»¥è½»æ¾éƒ¨ç½²åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šï¼Œå®ç°æœ¬åœ°çŸ¥è¯†åº“æ™ºèƒ½é—®ç­”
  - built on LlamaIndex and Streamlit, and has been optimized for Chinese users in various fields such as model selection and text processing.
  - Supports locally deployed models and offline use
  - a lot of customizations and optimizations for Chinese users:
    - Uses Spacy text splitter for better handling of Chinese characters
    - Uses Chinese prompt templates for Q&A and refinement processes
    - Uses bilingual embedding models, such as `bge-large-zh-v1.5` from BAAI

- https://github.com/HKUDS/RAG-Anything /10.7kStar/MIT/202511/python
  - All-in-One Multimodal Document Processing RAG system built on LightRAG.
  - As a unified solution, RAG-Anything eliminates the need for multiple specialized tools. It provides seamless processing and querying across all content modalities within a single integrated framework. 
  - [[Question]: æˆ‘ä½¿ç”¨ä¸­æ–‡æ•°æ®æ„å»ºäº†RAGï¼Œä½†æ˜¯ä¸ºä»€ä¹ˆæ£€ç´¢å‡ºæ¥çš„å†…å®¹æ˜¯è‹±æ–‡çš„ï¼Ÿ _202510](https://github.com/HKUDS/RAG-Anything/issues/136)
    - .envé‡Œå¥½åƒæœ‰ä¸ªè¯­è¨€è®¾ç½®ï¼Œé»˜è®¤æ˜¯è‹±æ–‡ï¼Œå¯ä»¥çœ‹çœ‹å’Œè¿™ä¸ªæ˜¯å¦æœ‰å…³
  - [[Question]: ç›®å‰æ˜¯å¦æ”¯æŒé•¿è¡¨æ ¼çš„å¯¼å…¥ _202509](https://github.com/HKUDS/RAG-Anything/issues/100)
  - https://github.com/HKUDS/LightRAG /20.1kStar/MIT/202508/python
    - Simple and Fast Retrieval-Augmented Generation
    - [2025.06.16]ğŸ¯ Our team has released RAG-Anything an All-in-One Multimodal RAG System for seamless text, image, table, and equation processing.
- https://github.com/xerrors/Yuxi-Know /2.6kStar/MIT/202512/python/vue
  - https://xerrors.github.io/Yuxi-Know/
  - åŠŸèƒ½å¼ºå¤§çš„æ™ºèƒ½ä½“å¹³å°ï¼Œèåˆäº† RAG çŸ¥è¯†åº“ä¸çŸ¥è¯†å›¾è°±æŠ€æœ¯ï¼ŒåŸºäº LangGraph v1 + Vue.js + FastAPI + LightRAG æ¶æ„æ„å»º
  - é›†æˆä¸»æµå¤§æ¨¡å‹ã€LightRAGã€MinerUã€PP-Structureã€Neo4j ã€è”ç½‘æ£€ç´¢ã€å·¥å…·è°ƒç”¨ã€‚

- https://github.com/getzep/graphiti /20.6kStar/apache2/202511/python
  - https://help.getzep.com/graphiti
  - a framework for building and querying temporally-aware knowledge graphs
  - Unlike traditional RAG, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. 
  - The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation
  - Graphiti powers the core of Zep, a turn-key context engineering platform for AI Agents.
  - Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. 
  - Real-Time Incremental Updates: Immediate integration of new data episodes without batch recomputation.
  - Scalability: Efficiently manages large datasets with parallel processing, suitable for enterprise environments.

- https://github.com/chunkhound/chunkhound /118Star/MIT/202509/python
  - https://chunkhound.github.io/
  - Deep Research for Code & Files
  - Transform your codebase into a searchable knowledge base for AI assistants using semantic search via cAST algorithm and regex search. 
  - Integrates with AI assistants via the Model Context Protocol (MCP).
  - cAST Algorithm - Research-backed semantic code chunking
  - Multi-Hop Semantic Search - Discovers interconnected code relationships beyond direct matches
  - Semantic search - Natural language queries like "find authentication code"
  - Regex search - Pattern matching without API keys
  - Local-first - Your code stays on your machine
  - 22 languages with structured parsing, via Tree-sitter

- https://github.com/memfreeme/memfree /MIT/202409/ts
  - https://www.memfree.me/
  - MemFree is a Hybrid AI Search Engine.
  - Search and ask questions with text, images, files, and web pages.
  - Multi AI Models: ChatGPT, Claude, Gemini
  - æ”¯æŒå¤šæ¨¡æ€ï¼ˆå›¾ç‰‡ã€æ–‡å­—ã€æ–‡ä»¶ï¼‰ï¼Œå¤šæ¥æºï¼ˆTwitterã€å­¦æœ¯ã€æœ¬åœ°çŸ¥è¯†åº“ï¼‰ï¼Œå¤šç§è¡¨ç°å½¢å¼ï¼ˆæ€ç»´å¯¼å›¾ã€å›¾ç‰‡éŸ³è§†é¢‘ï¼‰ç­‰æ··åˆæœç´¢å½¢æ€

- https://github.com/KruxAI/ragbuilder /apache2/202410/python
  - https://docs.ragbuilder.io/
  - A toolkit to create optimal Production-readyRetrieval Augmented Generation(RAG) setup for your data
  - By performing hyperparameter tuning on various RAG parameters (Eg: chunking strategy: semantic, character etc., chunk size: 1000, 2000 etc.), RagBuilder evaluates these configurations against a test dataset to identify the best-performing setup for your data.

- https://github.com/llmware-ai/llmware /14.5kStar/apache2/202507/python/inactive
  - https://llmware-ai.github.io/llmware/
  - a unified framework for building LLM-based applications (e.g., RAG, Agents), using small, specialized models that can be deployed privately
  - llmware has two main components:
    - RAG Pipeline - integrated components for the full lifecycle of connecting knowledge sources to ai models
    - 50+ small, specialized models fine-tuned for key tasks in enterprise process automation, including fact-based question-answering, classification, summarization, and extraction
  - RAG-Optimized Models - 1-7B parameter models designed for RAG workflow integration and running locally.

- https://github.com/VectifyAI/PageIndex /4.1kStar/MIT/202511/pyhton
  - https://pageindex.ai/
  - Inspired by AlphaGo, we propose PageIndex â€” a vectorless, reasoning-based RAG system that builds a hierarchical tree index for long documents and reasons over that index for retrieval. 
  - [RAG without Vectors â€“ PageIndex: Reasoning-Based Document Indexing Â· run-llama/llama_index _202504](https://github.com/run-llama/llama_index/discussions/18360)
    - We were frustrated by vector-based RAG systems that rely on semantic similarity and often fail on long, domain-specific documents.
    - PageIndex, a hierarchical indexing system that transforms large documents (like financial reports, regulatory documents, or textbooks) into semantic trees optimized for reasoning-based RAG.

- https://github.com/dataelement/bisheng /10.5kStar/apache2/202511/python/ts
  - http://www.bisheng.ai/
  - open LLM devops platform 
  - Powerful and comprehensive features include: GenAI workflow, RAG, Agent, Unified model management, Evaluation, SFT, Dataset Management, Enterprise-level System Management, Observability and more.
  - [æœ‰è®¡åˆ’å°†ä¼ä¸šæ•°æ®åº“ä½œä¸ºæ•°æ®æºä¹‹ä¸€å—ï¼Ÿ ](https://github.com/dataelement/bisheng/issues/500)
    - 202509: ç›®å‰å¯ä»¥é€šè¿‡åŠ©æ‰‹èŠ‚ç‚¹æ•°æ®æŸ¥è¯¢åŠŸèƒ½å®ç° nl2sql èƒ½åŠ›

- https://github.com/rag-web-ui/rag-web-ui /2.7kStar/apache2/202511/python/ts/æäº¤å°‘
  - RAG Web UI is an intelligent dialogue system based on RAG

- https://github.com/agentset-ai/agentset /1.6kStar/MIT/202512/ts
  - https://agentset.ai/
  - The open-source RAG platform: built-in citations, deep research, 22+ file formats, partitions, MCP server, and more.
  - It provides end-to-end tooling: ingestion, vector indexing, evaluation/benchmarks, chat playground, hosting, and a developer-friendly API.
  - Model agnostic: works with your choice of LLM, embeddings, and vector DB
  - Chat playground with message editing and citations
  - Built-in multi-tenancy
  - Built with TypeScript, Next.js, AI SDK, Prisma, Supabase, and Trigger.dev

- https://github.com/percent4/embedding_rerank_retrieval /202507/jupyter/inactive
  - æœ¬é¡¹ç›®æ˜¯é’ˆå¯¹RAGä¸­çš„Retrieveé˜¶æ®µçš„å¬å›æŠ€æœ¯åŠç®—æ³•æ•ˆæœæ‰€åšè¯„ä¼°å®éªŒã€‚ä½¿ç”¨ä¸»ä½“æ¡†æ¶ä¸ºLlamaIndex.
  - ä½¿ç”¨Gradioå®ç°ä¸­æ–‡Late-ChunkingæœåŠ¡: late_chunking/late_chunking_gradio_server.py
# rag-memory
- https://github.com/jakops88-hub/Long-Term-Memory-API /202512/ts
  - A Memory Server for AI Agents. Runs on Postgres + pgvector. 
  - Now supporting 100% Local/Offline execution via Ollama.
  - [I implemented Hybrid Search (BM25 + pgvector) in Postgres to fix RAG retrieval for exact keywords. Here is the logic. : r/Rag _202512](https://www.reddit.com/r/Rag/comments/1pcvtan/i_implemented_hybrid_search_bm25_pgvector_in/)
    - I don't stick to just one framework myself, most people jump between LangChain (for quick prototyping) and **Vercel AI SDK** (for production/streaming). MemVault is built to work with both. Regarding state, yeah, MemVault handles the state layer.
    - So the queries sent back to memvault are plain nlp texts ?
      - Exactly. You just send raw text (e.g., "What is the user's budget?")
      - The API handles the embedding/vectorization internally and matches it against the stored memories. So from the agent's perspective, it's just natural language in, relevant context out.
    - Does it work well enough for other languages? Did you try that?
      - you're right, 'english' is too aggressive with stemming. I haven't tested non-English languages extensively yet, but switching to the 'simple' config is the smart move to remove stemming issues, I'll update the config to that now.

- https://github.com/mem0ai/mem0 /apache2/202409/python
  - https://mem0.ai/
  - Mem0 (pronounced as "mem-zero") enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. 
  - Mem0 remembers user preferences, adapts to individual needs, and continuously improves over time, making it ideal for customer support chatbots, AI assistants, and autonomous systems.
  - Multi-Level Memory: User, Session, and AI Agent memory retention
  - Mem0 leverages a hybrid database approach to manage and retrieve long-term memories for AI agents and assistants. 
    - Each memory is associated with a unique identifier, such as a user ID or agent ID, allowing Mem0 to organize and access memories specific to an individual or context.
    - ğŸ›¢ï¸ When a message is added to the Mem0 using add() method, the system extracts relevant facts and preferences and stores it across data stores: a vector database, a key-value database, and a graph database. 
    - This hybrid approach ensures that different types of information are stored in the most efficient manner, making subsequent searches quick and effective.
    - Mem0 performs search across these data stores, retrieving relevant information from each source

- https://github.com/getzep/zep /apache2/202409/go
  - https://docs.getzep.com/
  - a long-term memory service for AI Assistant apps. 
  - With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant, while also reducing hallucinations, latency, and cost.
  - Zep persists and recalls chat histories, and automatically generates summaries and other artifacts from these chat histories. 
  - Zep also provides a simple, easy to use abstraction for document vector search called Document Collections. This is designed to complement Zep's core memory features, but is not designed to be a general purpose vector database.
# chunking/embedding
- https://github.com/CoreyFransen08/chunk-forge /apache2/202512/python/ts
  - A self-hosted document processing platform for converting PDFs to Markdown with semantic chunking, drag-and-drop editing, rich metadata management, and multi-format export.
  - Upload PDFs and convert them to Markdown using LlamaParse, MarkItDown, or Docling.
  - Multiple strategies (recursive, paragraph, heading, semantic, sentence, token, hierarchical) with configurable chunk sizes.
  - Frontend: React + Vite + TypeScript + Tailwind CSS + dnd-kit
  - Backend: Node.js + Express + TypeScript + Drizzle ORM
  - Parser Service: Python + FastAPI + LlamaParse/MarkItDown/Docling
  - Storage: Local filesystem
  - Built-in export presets for popular vector databases: Pinecone, Chroma

- https://github.com/sovit-123/local_file_search /MIT/202510/python
  - Scripts to replicate simple file search and RAG in a directory with embeddings and Language Models.

- https://github.com/AlwaysSany/huggingface-local-embedding /MIT/202507/python
  - A Fast API server that provides local text and multi-modal embedding using LlamaIndex Hugging Face Embedding

- https://github.com/huggingface/text-embeddings-inference /apache2/202511/rust
  - A blazing fast inference solution for text embeddings models.
  - From scratch implementation, no Vector DBs yet.
  - Steps to Chat with Any PDF in Gradio UI
# chat-docs/knowledgebase
- https://github.com/arc53/DocsGPT /17.4kStar/MIT/202511/python/ts/æäº¤å¤š
  - https://app.docsgpt.cloud/
  - https://docsgpt.arc53.com/
  - GPT-powered chat for documentation, chat with your documents
  - open-source AI platform for building intelligent agents and assistants. 
  - ä½¿ç”¨langchainçš„ä»£ç ä¸å¤š
  - Wide Format Support: Reads PDF, DOCX, CSV, XLSX, EPUB, MD, RST, HTML, MDX, JSON, PPTX, and images.
  - Web & Data Integration: Ingests from URLs, sitemaps, Reddit, GitHub and web crawlers.
  - Reliable Answers: Get accurate, hallucination-free responses with source citations 
  - Flexible Deployment: Works with major LLMs (OpenAI, Google, Anthropic) and local models (Ollama, llama_cpp).
  - Pre-built Integrations: Use readily available HTML/React chat widgets, search tools, Discord/Telegram bots, and more.
  - Secure & Scalable: Run privately and securely with Kubernetes support
  - Manually updating chunks in the app UI (Feb 2025)
  - [Show HN: DocsGPT, open-source documentation assistant, fully aware of libraries | Hacker News _202302](https://news.ycombinator.com/item?id=34648266)
  - [When ingesting data, ingest using chunks. _202302](https://github.com/arc53/DocsGPT/issues/54)
    - ä¼¼ä¹chunkingçš„å®ç°å­˜åœ¨é—®é¢˜

- https://github.com/PromtEngineer/localGPT /22kStar/MIT/202507/python/ts/æäº¤å°‘/inactive
  - LocalGPT is a fully private, on-premise Document Intelligence platform. 
  - Ask questions, summarise, and uncover insights from your files with AIâ€”no data ever leaves your machine.
  - LocalGPT features a hybrid search engine that blends semantic similarity, keyword matching, and Late Chunking for long-context precision
  - A smart router automatically selects between RAG and direct LLM answering for every query, while contextual enrichment and sentence-level Context Pruning surface only the most relevant content
  - An independent verification pass adds an extra layer of accuracy.
  - The architecture is modular and lightweightâ€”enable only the components you need. With a pure-Python core and minimal dependencies
  - ğŸ  The RAG system is pure python and does not require any additional dependencies.
  - models via Ollama.
  - Chat History: Remembers your previous conversations (in a session).
  - API: LocalGPT has an API that you can use for building RAG Applications.
  - GPU, CPU, HPU & MPS Support
  - Multi-format Support: PDF, DOCX, TXT, Markdown, and more (Currently only PDF is supported)
  - Contextual Enrichment: Enhanced document understanding with AI-generated context, inspired by Contextual Retrieval
  - Batch Processing: Handle multiple documents simultaneously
  - Smart Routing: Automatically chooses between RAG and direct LLM responses
  - Query Decomposition: Breaks complex queries into sub-questions for better answers
  - Intuitive Web UI: Clean, responsive design
  - Real-time Chat: Streaming responses for immediate feedback
  - [Ingest larger documents (over 800MB) _202306](https://github.com/PromtEngineer/localGPT/discussions/112)
    - Im currently using a system with 16GB RAM, and 3 nvidia TESLA GPUs (16GBs each) but the localGPT only took 4-5 GB of GPU space.
    - it took some time to finish the ingesting (I kept it running for a day). But the inferences from the model takes some time. Any way to reduce that?
  - [I am running on CPU and I have 6.4GB worth of file _202309](https://github.com/PromtEngineer/localGPT/issues/448)
    - The pdf size is too large. Please try using a smaller pdf. or export relevant text out of it (reduce the pdf size) and then use it. For this much big pdf you will certainly need NVIDIA cuda cores for faster processing.

- https://github.com/Cinnamon/kotaemon /23kStar/apache2/202507/python/æäº¤å°‘/inactive
  - https://cinnamon.github.io/kotaemon/
  - https://huggingface.co/spaces/cin-model/kotaemon-demo
  - open-source clean & customizable RAG UI for chatting with your documents. Built with both end users and developers in mind.
  - This project serves as a functional RAG UI for both end users who want to do QA on their documents and developers who want to build their own RAG pipeline.
  - Minimalistic UI: A user-friendly interface for RAG-based QA.
  - Support for Various LLMs: Compatible with LLM API providers (OpenAI, AzureOpenAI, Cohere, etc.) and local LLMs (via `ollama` and `llama-cpp-python`).
  - Framework for RAG Pipelines: Tools to build your own RAG-based document QA pipeline.
  - Customizable UI: See your RAG pipeline in action with the provided UI, built with `Gradio`.
  - Host your own document QA (RAG) web-UI: Support multi-user login, organize your files in private/public collections, collaborate and share your favorite chat with others.
  - Hybrid RAG pipeline: Sane default RAG pipeline with hybrid (full-text & vector) retriever 
  - Advanced citations with document preview: By default the system will provide detailed citations to ensure the correctness of LLM answers.
  - Extensible: Being built on `Gradio`, you are free to customize or add any UI elements as you like.
  - require `Unstructured` if you want to process files other than .pdf, .html, .mhtml, and .xlsx documents.
    - We support both `lite` & `full` version of Docker images. With `full` version, the extra packages of `unstructured` will be installed, which can support additional file types (`.doc, .docx`, ...)
  - [Kotaemon: An open-source RAG-based tool for chatting with your documents | Hacker News _202501](https://news.ycombinator.com/item?id=42571272)
  - [Kotaemon-papers: an open-source web app to chat with your academic papers | Hacker News _202501](https://news.ycombinator.com/item?id=42603357)
    - Our team has been working on a public demo to showcase the new advanced citation features in our RAG
  - [[BUG] Unable to index document in docker ](https://github.com/Cinnamon/kotaemon/issues/539)
    - I think its PDF file size thing. Doesn't seem to happen on PDFs with <100 pages...
  - [[BUG] - Local Model LLM works, but not Embedding _202409](https://github.com/Cinnamon/kotaemon/issues/240)
    - I tried to use the base URL that works for me in AnythingLLM but it still didn't work in kotaemon. 

- https://github.com/h2oai/h2ogpt /11.9kStar/apache2/202503/python/inactive
  - http://h2o.ai/
  - [Gradio Demo](https://gpt.h2o.ai/)
  - [OpenWebUI Demo](https://gpt-docs.h2o.ai/)
  - Query and summarize your documents or just chat with local private GPT LLMs using h2oGPT
  - Private offline database of any documents (PDFs, Excel, Word, Images, Video Frames, YouTube, Audio, Code, Text, MarkDown, etc.)
  - Persistent database (Chroma, Weaviate, or in-memory FAISS) using accurate embeddings (instructor-large, all-MiniLM-L6-v2, etc.)
  - Efficient use of context using instruct-tuned LLMs (no need for LangChain's few-shot approach)
  - Parallel summarization and extraction, reaching an output of 80 tokens per second with the 13B LLaMa2 model
  - Gradio UI or CLI with streaming of all models
  - [Feature request - to add support for nomic-ai/nomic-embed-text-v1 embeddings _202402](https://github.com/h2oai/h2ogpt/issues/1418)
    - 83333 is very large. I made the max 4096. Or you can control via env `CHROMA_MAX_BATCH_SIZE`.
    - 4096 is on high end, yes can make it smaller as required. If on CPU I expect should work pretty ok, but issue is bge-m3 has 8k context so uses alot more memory despite size if chunks are large.

- https://github.com/GitHamza0206/simba /1.4kStar/apache2/202505/python/jupyter/inactive
  - https://simba.mintlify.app/
  - Portable KMS (knowledge management system) designed to integrate seamlessly with any RAG
  - Modular Architecture: Flexible integration of vector stores, embedding models, chunkers, and parsers.

- https://github.com/labring/FastGPT /25.5kStar/apache2+LOGO+nonTenant/202508/ts
  - https://fastgpt.io/
  - ä¸€ä¸ª AI Agent æ„å»ºå¹³å°ï¼Œæä¾›å¼€ç®±å³ç”¨çš„æ•°æ®å¤„ç†ã€æ¨¡å‹è°ƒç”¨ç­‰èƒ½åŠ›ï¼ŒåŒæ—¶å¯ä»¥é€šè¿‡ Flow å¯è§†åŒ–è¿›è¡Œå·¥ä½œæµç¼–æ’ï¼Œä»è€Œå®ç°å¤æ‚çš„åº”ç”¨åœºæ™¯
  - é¡¹ç›®æŠ€æœ¯æ ˆï¼šNextJs + TS + ChakraUI + MongoDB + PostgreSQL (PG Vector æ’ä»¶)/Milvus
  - âš–ï¸ License
    - å…è®¸ä½œä¸ºåå°æœåŠ¡ç›´æ¥å•†ç”¨ï¼Œä½†ä¸å…è®¸æä¾› SaaS æœåŠ¡

- https://github.com/QuivrHQ/quivr /38.4kStar/apache2/202506/python
  - https://core.quivr.com/
  - Opiniated RAG for integrating GenAI in your apps
  - Opiniated RAG: We created a RAG that is opinionated, fast and efficient so you can focus on your product
  - LLMs: Quivr works with any LLM, you can use it with OpenAI, Anthropic, Mistral, Gemma, etc.
  - Any File: Quivr works with any file, you can use it with PDF, TXT, Markdown, etc and even add your own parsers.
  - Customize your RAG: Quivr allows you to customize your RAG, add internet search, add tools, etc.

- https://github.com/dontizi/rlama /1.1kStar/apache2/202508/go/js
  - https://rlama.dev/
  - a powerful AI-driven question-answering tool for your documents, seamlessly integrating with your local Ollama models.
  - It enables you to create, manage, and interact with Retrieval-Augmented Generation (RAG) systems tailored to your documentation needs.
  - This project is currently on pause due to my work and university commitments that take up a lot of my time
  - [RLAMA -- A document AI question-answering tool that connects to your local Ollama models. : r/ollama _202503](https://www.reddit.com/r/ollama/comments/1j66pg3/rlama_a_document_ai_questionanswering_tool_that/)
    - I developed RLAMA to solve a straightforward but frustrating problem: how to easily query my own documents with a local LLM without using cloud services.
    - RLAMA extracts text from the documents and generates embeddings via Ollama
    - it uses Tesseract OCR to extract text from image-based PDFs or scanned documents where text can't be directly selected.

- https://github.com/morphik-org/morphik-core /3.2kStar/BSL/202508/python/ts
  - https://morphik.ai/docs
  - The most accurate document search and store for building AI apps
  - We are building the best way for developers to integrate context (however complex and nuanced) into their AI applications. 
  - Morphik provides developers the tools to ingest, search (deep and shallow), transform, and manage unstructured and multimodal documents.
  - Multimodal Search: We employ techniques such as ColPali to build search that actually understands the visual content of documents you provide. Search over images, PDFs, videos, and more

- https://github.com/jorge-armando-navarro-flores/chat_with_your_docs /MIT/202409/python
  - ChatWithYourDocs Chat App is a Python application that allows you to chat with multiple Docs formats like PDF, WEB pages and YouTube videos. 
  - This app utilizes a language model to generate accurate answers to your queries

- https://github.com/zylon-ai/private-gpt /56.5kStar/apache2/202409/python/inactive
  - a production-ready AI project that allows you to ask questions about your documents using LLM
  - Conceptually, PrivateGPT is an API that wraps a RAG pipeline and exposes its primitives.
    - The API is built using FastAPI and follows OpenAI's API scheme.
    - The RAG pipeline is based on LlamaIndex.
  - The design of PrivateGPT allows to easily extend and adapt both the API and the RAG implementation.
    - Dependency Injection, decoupling the different components and layers.
    - Usage of `LlamaIndex` abstractions such as `LLM, BaseEmbedding or VectorStore`, making it immediate to change the actual implementations of those abstractions.
    - Ready to use, providing a full implementation of the API and RAG pipeline.
  - The project provides an API offering all the primitives required to build private, context-aware AI applications. 
  - It follows and extends the OpenAI API standard, and supports both normal and streaming responses.
  - [Ollama Embedding Fails with Large PDF files _202410](https://github.com/zylon-ai/private-gpt/discussions/2097)
  - [Ingesting large number of files _202402](https://github.com/zylon-ai/private-gpt/issues/1660)
    - Its using SQLLite as its using simple (filesystem based) indexes and doc stores. Once this #1706 is pulled, those things can be moved into Postgres.

- https://github.com/thiswillbeyourgithub/WDoc /478Star/GPL/202511/python
  - https://wdoc.readthedocs.io/en/stable/
  - Summarize and query from a lot of heterogeneous documents. 
  - Any LLM provider, any filetype, advanced RAG, advanced summaries, scriptable, etc
  - Created by a medical student who needed a way to get a definitive answer from multiple sources at the same time (audio recordings, video lectures, Anki flashcards, PDFs, EPUBs, etc.). wdoc was born from frustration with existing RAG solutions for querying and summarizing.
  - It uses mostly `LangChain` and `LiteLLM` as backends.
  - High recall and specificity: it was made to find A LOT of documents using carefully designed embedding search then carefully aggregate gradually each answer using semantic batch to produce a single answer that mentions the source pointing to the exact portion of the source document.
    - Use both an expensive and cheap LLM to make recall as high as possible because we can afford fetching a lot of documents per query (via embeddings)
  - Extensible: this is both a tool and a library. It was even turned into an Open-WebUI Tool
  - Local and Private LLM
  - Web Search: Preliminary web search support using DuckDuckGo (via the ddgs library)
  - Fast: Parallel document loading, parsing, embeddings, querying, etc.
  - Roadmap
    - figure out a good way to skip merging batches that are too large before trying to merge them

- https://github.com/kqlade/dory-frontend /AGPL/202504/ts/inactive
  - [Dory â€“ AI Knowledge Base Powered by Browser History | Hacker News _202504](https://news.ycombinator.com/item?id=43619021)
  - Dynamic Online Recall for You (DORY) helps you find anything you've seen before online using whatever you remember about it. 
  - DORY also builds a cognitive context graph to organize your work into auto-updating workflows and serve them to you in real time based on your browsing context.
  - DORY uses Neo4j to store browsing data with weighted edge relationships (semantic, behavioral, temporal) then applies stochastic block modeling for community detection and temporal-behavioral profiling.
  - It's an approach similar to how banks analyze transaction networks to detect fraud but works surprisingly well for teasing apart browser workflows.

- https://github.com/Zipstack/unstract /5.9kStar/AGPL/202511/python
  - https://unstract.com/
  - No-code LLM Platform to launch APIs and ETL Pipelines to structure unstructured documents

- https://github.com/freeCodeCamp/devdocs /37.9kStar/MPL/202511/ruby
  - https://devdocs.io/
  - DevDocs combines multiple developer documentations in a clean and organized web UI with instant search, offline support, mobile version, dark theme, keyboard shortcuts, and more
  - DevDocs was created by Thibaut Courouble and is operated by freeCodeCamp.

- https://github.com/1Panel-dev/MaxKB /15.5kStar/GPL/202504/python/ts/vue
  - https://maxkb.pro/
  - åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„å¼€æºçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ
  - it is a ready-to-use RAG chatbot that features robust workflow and MCP tool-use capabilities. 
  - ä¾èµ–djangoã€langchainã€pgvectorã€Vue
  - Flexible Orchestration: Equipped with a powerful workflow engine, function library and MCP tool-use, enabling the orchestration of AI processes to meet the needs of complex business scenarios.

- https://github.com/chaitin/PandaWiki /8.3kStar/AGPL/202512/go/ts
  - https://pandawiki.docs.baizhi.cloud/
  - AI å¤§æ¨¡å‹é©±åŠ¨çš„å¼€æºçŸ¥è¯†åº“æ­å»ºç³»ç»Ÿï¼Œå¸®åŠ©ä½ å¿«é€Ÿæ„å»ºæ™ºèƒ½åŒ–çš„ äº§å“æ–‡æ¡£ã€æŠ€æœ¯æ–‡æ¡£ã€FAQã€åšå®¢ç³»ç»Ÿï¼Œå€ŸåŠ©å¤§æ¨¡å‹çš„åŠ›é‡ä¸ºä½ æä¾› AI åˆ›ä½œã€AI é—®ç­”ã€AI æœç´¢ç­‰èƒ½åŠ›ã€‚
  - å¼ºå¤§çš„å¯Œæ–‡æœ¬ç¼–è¾‘èƒ½åŠ›ï¼šå…¼å®¹ Markdown å’Œ HTMLï¼Œæ”¯æŒå¯¼å‡ºä¸º wordã€pdfã€markdown ç­‰å¤šç§æ ¼å¼ã€‚
  - è½»æ¾ä¸ç¬¬ä¸‰æ–¹åº”ç”¨è¿›è¡Œé›†æˆï¼šæ”¯æŒåšæˆç½‘é¡µæŒ‚ä»¶æŒ‚åœ¨å…¶ä»–ç½‘ç«™ä¸Šï¼Œæ”¯æŒåšæˆé’‰é’‰ã€é£ä¹¦ã€ä¼ä¸šå¾®ä¿¡ç­‰èŠå¤©æœºå™¨äººã€‚
  - é€šè¿‡ç¬¬ä¸‰æ–¹æ¥æºå¯¼å…¥å†…å®¹ï¼šæ ¹æ®ç½‘é¡µ URL å¯¼å…¥ã€é€šè¿‡ç½‘ç«™ Sitemap å¯¼å…¥ã€é€šè¿‡ RSS è®¢é˜…ã€é€šè¿‡ç¦»çº¿æ–‡ä»¶å¯¼å…¥ç­‰ã€‚
# chat-excel
- https://github.com/huggingface/aisheets /1.5kStar/apache2/202510/python/ts
  - https://huggingface.co/spaces/aisheets/sheets
  - an open-source tool for building, enriching, and transforming datasets using AI models with no code. 
  - The tool can be deployed locally or on the Hub. 
  - By default, AI Sheets is configured to use the Huggingface Inference Providers API to run inference on the latest open-source models. However, you can also run Sheets with own custom LLMs, such as those hosted on your own infrastructure or other cloud providers. The only requirement is that your LLMs must support the OpenAI API specification.
  - This app has a minimal Expressjs server implementation

- https://github.com/weijunext/smart-excel-ai /MIT/202312/ts
  - https://smartexcel.cc/
  - Generate the Excel formulas you need in seconds using ChatGPT
  - https://twitter.com/weijunext/status/1744225202228089173
    - è¿™æ˜¯ä¸€ä¸ªè¶³å¤Ÿç®€å•ï¼ˆè°ƒç”¨ChatGPTçš„APIï¼‰å´åˆåŠŸèƒ½ä¿±å…¨ï¼ˆæœ‰ç™»å½•å’Œæ”¯ä»˜ï¼‰çš„demoçº§äº§å“ã€‚
    - å‰åç«¯ï¼šNext.js+Tailwind+Prisma
    - ç™»å½•ï¼šNext-Auth
    - æ”¯ä»˜ï¼šLemon Squeezy
    - éƒ¨ç½²ï¼šVercel
# chat-pdf
- https://github.com/AKSarav/pdfstract /202509/python/inactive
  - web application built with FastAPI and HTML that converts PDF files to Markdown format using various conversion libraries.
  - [Built a small tool to compare PDF â†’ Markdown libraries (for RAG / LLM workflows) : r/Rag _202507](https://www.reddit.com/r/Rag/comments/1m1j10e/built_a_small_tool_to_compare_pdf_markdown/)
  - Iâ€™ve been exploring different libraries for converting PDFs to Markdown to use in a Retrieval-Augmented Generation (RAG) setup.
  - But testing each library turned out to be quite a hassle â€” environment setup, dependencies, version conflicts, etc.
  - Currently, it supports: docling pymupdf4llm markitdown marker

- https://github.com/weiwill88/Local_Pdf_Chat_RAG /202510/python/inactive
  - æœ¬åœ°åŒ–æ™ºèƒ½é—®ç­”ç³»ç»Ÿ (FAISSç‰ˆ)
  - æ··åˆæ£€ç´¢ï¼šç»“åˆFAISSè¿›è¡Œè¯­ä¹‰æ£€ç´¢å’ŒBM25è¿›è¡Œå…³é”®è¯æ£€ç´¢ï¼Œæé«˜æ£€ç´¢å¬å›ç‡å’Œå‡†ç¡®æ€§
  - ç»“æœé‡æ’åºï¼šæ”¯æŒäº¤å‰ç¼–ç å™¨ï¼ˆCrossEncoderï¼‰å’ŒLLMå¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åºï¼Œä¼˜åŒ–ç›¸å…³æ€§
  - å¯é€‰æ‹©ä½¿ç”¨æœ¬åœ°Ollamaå¤§æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1ç³»åˆ—ï¼‰æˆ–äº‘ç«¯SiliconFlow APIè¿›è¡Œæ¨ç†
  - åŸºäºGradioæ„å»ºäº¤äº’å¼Webç•Œé¢ï¼Œæ”¯æŒå¤šç§ä¸»é¢˜ç•Œé¢

- https://github.com/shibing624/ChatPDF /801Star/apache2/202409/python/inactive
  - çº¯åŸç”Ÿå®ç°RAGåŠŸèƒ½ï¼ŒåŸºäºæœ¬åœ°LLMã€embeddingæ¨¡å‹ã€rerankeræ¨¡å‹å®ç°ï¼Œæ”¯æŒGraphRAGï¼Œæ— é¡»å®‰è£…ä»»ä½•ç¬¬ä¸‰æ–¹agentåº“ã€‚
  - æœ¬é¡¹ç›®å®ç°äº†è½»é‡ç‰ˆçš„GraphRAG
  - æ”¯æŒlocalæ¨¡å¼çš„å…³ç³»å›¾æ£€ç´¢çš„æ–‡æ¡£é—®ç­”
  - æ”¯æŒOpenai API, Deepseek API, Ollama APIç­‰ï¼Œå¯è‡ªè¡Œæ‰©å±•æ”¯æŒæ›´å¤šLLM
  - æ”¯æŒopenai embeddingã€æœ¬åœ° text2vec embeddingã€huggingface embeddingã€sentence-transformers embeddingç­‰
  - æœ¬é¡¹ç›®æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼ï¼ŒåŒ…æ‹¬PDFã€docxã€markdownã€txtç­‰
  - ä¼˜åŒ–äº†RAGå‡†ç¡®ç‡: Chinese chunkåˆ‡åˆ†ä¼˜åŒ–ï¼Œé€‚é…ä¸­è‹±æ–‡æ··åˆæ–‡æ¡£
  - æœ¬é¡¹ç›®åŸºäº`gradio`å¼€å‘äº†RAGå¯¹è¯é¡µé¢ï¼Œæ”¯æŒæµå¼å¯¹è¯

- https://github.com/mayooear/ai-pdf-chatbot-langchain /15.8kStar/MIT/202502/ts/inactive
  - PDF chatbot agent built with LangChain & LangGraph
  - This monorepo is a customizable template example of an AI chatbot agent that "ingests" PDF documents, stores embeddings in a vector database (Supabase), and then answers user queries using OpenAI (or another LLM provider) utilising LangChain and LangGraph as orchestration frameworks.
  - This template is also an accompanying example to the book Learning LangChain (O'Reilly): Building AI and LLM applications with LangChain and LangGraph.
  - [Support for BIG PDFs?(like 500pages) ](https://github.com/mayooear/ai-pdf-chatbot-langchain/issues/409)
    - you must be mindful of the pinecone.io free tier's pod capacity, which may be exceeded
  - https://github.com/mayooear/ai-pdf-chatbot-langchain/tree/feat/chroma

- https://github.com/williamcaseylucas/quill-pdf-chat /202401/ts
  - ä¾èµ–prisma, shadcn-ui, react-pdf, AWS S3 buckets
  - https://github.com/khaledmk20/quill
    - Quill redefines PDF interaction with intelligent conversations.

- https://github.com/Byaidu/PDFMathTranslate /29.8kStar/AGPLv3/202511/python 
  - https://pdf2zh.com/
  - åŸºäº AI å®Œæ•´ä¿ç•™æ’ç‰ˆçš„ PDF æ–‡æ¡£å…¨æ–‡åŒè¯­ç¿»è¯‘ï¼Œæ”¯æŒ Google/DeepL/Ollama/OpenAI ç­‰æœåŠ¡ï¼Œæä¾› CLI/GUI/MCP/Docker/Zotero
  - Preserve formulas, charts, table of contents, and annotations
  - Support multiple languages, and diverse translation services.
  - https://github.com/PDFMathTranslate/PDFMathTranslate-next /AGPLv3
    - pdf2zh 2.0 does not currently provide an online demo

- https://github.com/bhaskatripathi/pdfGPT /7.2kStar/MIT/202306/python/ä»£ç å°‘/inactive
  - chat with the contents of your PDF file by using GPT capabilities. 
  - When you pass a large text to Open AI, it suffers from a 4K token limit. It cannot take an entire pdf file as an input
  - The application intelligently breaks the document into smaller chunks and employs a powerful Deep Averaging Network Encoder to generate embeddings.
  - A semantic search is first performed on your pdf content and the most relevant embeddings are passed to the Open AI.
  - https://github.com/tuxxon/PDFGPT /202408/inactive
    - I rebuilt it because I thought this repository was no longer being maintained.
# chat-workspace
- https://github.com/lfnovo/open-notebook /10.2kStar/MIT/202511/python/ts/æäº¤å°‘
  - https://www.open-notebook.ai/
  - Open Source implementation of Notebook LM with more flexibility and features
  - AI-powered note-taking/research platform that respects your privacy

- https://github.com/rmusser01/tldw_server /1.1kStar/GPL/202511/python
  - https://tldwproject.com/
  - Too Long; Didn't Watch - API-first media analysis & research platform
  - tldw_server is an open-source research multi-tool / backend for ingesting, transcribing, analyzing, and retrieving knowledge from video, audio, documents, websites, and more.
  - It consists of a FastAPI API-first server architecture backed by SQLite or Postgres depending on user choice, with OpenAI-compatible Chat and Audio APIs, a unified RAG pipeline, knowledge management, and integrations with local or hosted LLM providers (with cost/usage tracking).
  - https://github.com/the-crypt-keeper/tldw /inactive

- https://github.com/CaviraOSS/PageLM /NonCommercial/202511/ts
  - https://pagelm.spotit.dev/
  - a community driven version of NotebookLM & a education platform that transforms study materials into interactive resources like quizzes, flashcards, notes, and podcasts.

- https://github.com/souzatharsis/podcastfy /5.6kStar/apache2/202510/python
  - Open Source Python alternative to NotebookLM's podcast feature: Transforming Multimodal Content into Captivating Multilingual Audio Conversations with GenAI
  - https://github.com/gabrielchua/open-notebooklm
    - Convert any PDF into a podcast episode

- https://github.com/run-llama/notebookllama /1.6kStar/MIT/202508/python/inactive
  - open-source, LlamaCloud-backed alternative to NotebookLM
  - [NotebookLlama: An open source version of NotebookLM | Hacker News _202410](https://news.ycombinator.com/item?id=41964980)

- https://github.com/xynehq/xyne /635Star/apache2/202511/ts
  - https://xynehq.com/
  - AI-first Search & Answer Engine for work. 
  - Open-source alternative to Glean, Gemini and MS Copilot
  - Your work information has become fragmented â€” across so many SaaS apps, docs, files, repos
  - Xyne connects to your applications (Google Workspace, Atlassian suite, Slack, Github, etc), securely indexes your data, and maps a graph of relationships.
  - ğŸ› å¿…é¡»ä½¿ç”¨googleè´¦æˆ·ç™»å½•æ‰èƒ½ä½¿ç”¨?
    - ä¼¼ä¹ä¸æ”¯æŒä¸Šä¼ æœ¬åœ°pdfæ–‡ä»¶
  - Model Agnostic: Plugs into any LLM of your choice. You can even point it to a local Deepseek via ollama.
  - ä½¿ç”¨è‡ªç ”aiæ¡†æ¶ jaf  ç»“åˆ aisdk
  - https://github.com/xynehq/jaf
    - functional agent framework built on immutable state, type safety, and composable policies.

- https://github.com/deta/surf /2.7kStar/apache2/202511/rust/ts/svelte
  - https://deta.surf/
  - Personal AI Notebooks. Organize files & webpages and generate notes from them. 
  - built in Svelte, TypeScript and Rust, runs on MacOS, Windows & Linux, stores data locally in open formats 
  - PDF Notes: open a PDF and ask a question
  - Create an applet: use the "app generation" tool and ask for an app
  - ğŸ“¡ [Optimized Embeddings CPU Usage _202510](https://github.com/deta/surf/pull/43)
    - Implemented Lazy Embeddings for Large Document Types
    - Embeddings will then be generated on-demand when documents are accessed in chat/search
    - Larger chunks (2000 â†’ 2500): fewer embeddings to generate and store
  - [[FEATURE] Migrate to a better browser engine _202510](https://github.com/deta/surf/issues/27)
    - We understand the pros and cons of using Electron, and for the moment the pros outweigh the cons, specially for a small team.
  - [Show HN: Deta Surf â€“ An open source and local-first AI notebook | Hacker News _202510](https://news.ycombinator.com/item?id=45680937)
    - We took inspiration from analog notebooks as a tool for thought, but wanted something for multi-media. We also see NotebookLM as the closest mainstream product to Surf.
    - ğŸ†š The big difference UX wise between chatbots and Surf is that Surf is built entirely on editable documents that you can mold / craft into an output (vs chat).
      - We actually had a chatbot, but our explorations showed that notes were a more effective in many cases!
    - Is this an open source equivalent to googleâ€™s NotebookLM? I can tell. How does it stack up features wise?
      - Surf is built entirely on editable WYSIWYG documents, NotebookLM's main AI is built on chat. Surf is built to be a bit more open, NotebookLM was a bit locked down for our taste.
      - An example I'd highlight is taking notes against a PDF.
      - NotebookLM will convert the PDF to simple text, and the chat responses are read only. NotebookLM also has a lot of strict walls between chat, artifacts & sources. You have to "save" responses as (read only) notes, and move notes to sources.
      - With Surf you can generate notes that deep link to specific pages in the PDF, and Surf will open those pages in the original PDF. You can remove the fluff you don't want in your notes. The intention is to be a little more open -- all notes are sources from the get go, you don't have to save or migrate anything.

- https://github.com/ItzCrazyKns/Perplexica /27.3kStar/MIT/202511/ts
  - Open source alternative to Perplexity AI
  - a privacy-focused AI answering engine that runs entirely on your own hardware. It combines knowledge from the vast internet with support for local LLMs (Ollama) and cloud providers (OpenAI, Claude, Groq), delivering accurate answers with cited sources while keeping your searches completely private
  - Web search powered by SearxNG - Support for Tavily and Exa coming soon
  - Upload documents and ask questions about them. PDFs, text files, images - Perplexica understands them all.
    - æœªä½¿ç”¨docling/libreoffice
  - Get intelligent search suggestions as you type, helping you formulate better queries.
  - Perplexica also provides an API for developers looking to integrate its powerful search engine into their own applications.
  - Perplexica runs on Next.js and handles all API requests.
  - ğŸ› å®˜æ–¹æœªæä¾›å¤–éƒ¨æ•°æ®æºçš„é›†æˆï¼Œå¦‚slack/notion/gmail
    - [Add private search connectors _202508](https://github.com/ItzCrazyKns/Perplexica/issues/856)
  - [fix(uploads): Resolve the bug of large document attachment upload failure](https://github.com/ItzCrazyKns/Perplexica/pull/675)
    - Resolve the bug where large document uploads prompt an "exceeded maximum allowed batch size" error. The error message is as follows: error: Error in uploading file results: 413 input batch size 512 > maximum allowed batch size 32 (request id: 2025031909320299661815003514673)

- https://github.com/zaidmukaddam/scira /11.4kStar/AGPL/202511/ts
  - https://scira.ai/
  - Scira (Formerly MiniPerplx) is a minimalistic AI-powered search engine that helps you find information on the internet and cites it too. 
  - Powered by Vercel AI SDK
  - using multiple AI models including xAI's Grok, Anthropic's Claude, Google's Gemini, and OpenAI's GPT models
  - Search the web using Exa's API with support for multiple queries, search depths, and topics
  - Extract and analyze content from any URL using Exa AI with live crawling capabilities
    - Search Reddit content with time range filtering using Tavily API
    - X (Twitter) search: Search X posts with date ranges and specific handle filtering using xAI Live Search
  - Advanced multi-step search capability for complex queries
  - Academic search: Search for academic papers and research using Exa AI with abstracts and summaries
  - YouTube search: Find YouTube videos with detailed information, captions, and timestamps powered by Exa AI
# citation/sourcing
- https://github.com/stanford-oval/WikiChat /1.5kStar/apache2/202504/python/inactive
  - https://wikichat.genie.stanford.edu/
  - WikiChat is an improved RAG. It stops the hallucination of large language models by retrieving data from a corpus.
  - WikiChat uses Wikipedia and the following 7-stage pipeline to makes sure its responses are factual. Each numbered stage involves one or more LLM calls.

- https://github.com/AdyTech99/volo /GPL/202501/python/inactive
  - combining AI with Wikipedia knowledge via a RAG pipeline
  - It utilizes an offline database of Wikipedia created by Kiwix, ensuring fast and reliable access to information without requiring constant internet connectivity.
  - Volo uses a tiny model (Qwen2.5:3b) and gives it the knowledge of nearly 7 million Wikipedia articles, making it a more reliable source of information than giant closed-source models like OpenAI's GPT4o and Anthropic's Claude 3.5 Sonnet, which are prone to hallucinations.
  - Offline Wikipedia Database: Leverages a `.zim` file from Kiwix, offering a snapshot of Wikipedia for offline access.
  - OpenAI-Compatible REST APIs: Use Volo with interfaces like Open WebUI or your own API client.

- https://github.com/imDelivered/WikiRAG /202512/python/ä»£ç å°‘
  - [I built an offline AI chat app that automatically pulls Wikipedia articles for factual answers - runs completely locally with Ollama : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1pd2x8u/i_built_an_offline_ai_chat_app_that_automatically/)
    - We really need a universal Kiwix API endpoint that Just Works with tools calls from whatever inference engine someone is using
# more
