---
title: lib-ai-app-examples-rag-knowledge-base
tags: [ai, examples, knowledge-base, large-language-model, rag]
created: 2025-11-30T17:26:44.502Z
modified: 2025-11-30T17:27:16.720Z
---

# lib-ai-app-examples-rag-knowledge-base

# guide

- ‰ºòÂåñÊèêÂèñ‰∏≠ÊñáÊñáÊ°£ÊñáÂ≠óÁöÑÊñπÊ°à
  - ÂèØÂèÇËÄÉÂçé‰∫∫Âõ¢ÈòüÁöÑÊñπÊ°à, Â¶Ç ragflow/WeKnora/LightRAG

- tips
  - ÂÆöÂà∂Êàñ‰ºòÂåñÊñπÊ°àÊó∂, Êó©ÊôöÈÉΩÈúÄË¶ÅÊ∑±ÂÖ•ragÁöÑÂêÑ‰∏™ÁéØËäÇ: parse > chunk > embed > retrieval > rerank > summarize
  - Ê≤°ÊúâÊåÅ‰πÖÂåñvector embeddingsÁöÑÁ§∫‰æãÈÉΩÊòØdemo, ÊØèÊ¨°ÂêØÂä®ÈÉΩË¶ÅÂ∞ÜÂ§ßÊñá‰ª∂index‰∏ÄÈÅç
  - Âü∫‰∫éÊñá‰ª∂Á≥ªÁªüÔºåËøòÊòØÂü∫‰∫éÊï∞ÊçÆÂ∫ì Êù•ÂÆûÁé∞Âíå‰ºòÂåñrag
  - ragÊòØÂæàÂ§ö‰∫ßÂìÅÈÉΩÈúÄË¶ÅÁöÑÂü∫Á°ÄËÉΩÂäõ‰πã‰∏Ä, ÂèØÊõø‰ª£text-search, ÂèØÂèÇËÄÉÊàêÂäüÁöÑ‰∫ßÂìÅÊàñÈíàÂØπÂú∫ÊôØ/codebase/local‰ºòÂåñÁöÑ‰∫ßÂìÅ

- tech-stack
  - ÂæàÂ§öragÊñπÊ°àÂØπ‰∏≠ÊñáÁöÑÊîØÊåÅÂæàÂ∑Æ
  - ÂèÇËÄÉ cli-agent ÁöÑragÊ°à‰æãÊù•ÂÆûÁé∞ragÂêéÁ´Ø, Â§ßÂ§öÊï∞Âü∫‰∫émcp-serverÊàñÈõÜÊàê‰ª£Á†ÅÂ∫ì
    - mcp-memory-serviceÊîØÊåÅÂú®‰∏çÂêåcli‰∏≠ÂÖ±‰∫´memory, ËøòË¶ÅÊîØÊåÅÂêØÁî®/Á¶ÅÊ≠¢memory
    - migrate code indexing ÁöÑÈÄªËæëÂà∞ pdf rag
  - ÁªìÂêà cli-agent + cli-rag ÁöÑÊñπÊ°à, cli-agentÁöÑsearchÈÄöÂ∏∏ÈÄöËøáMCPÊù•ÂÆûÁé∞
  - coding-agent is good at text and filesystem
    - store extracted text in db
    - use just-bash to interact with db

- who is NOT using #code-rag
  - claude-code
  - cline
- who is using #code-rag
  - roocode

- features: 
  - citations: ÊêúÁ¥¢rag citationÁöÑÊ°à‰æã
  - local sources: pdf/docx
  - external sources: slack, github
  - external search: SearXNG, Tavily
  - large-file/pdf
  - ollama-embeddings
  - ‰∏≠Êñá‰ºòÂåñ
# citation/backlinks
- ÊùÉÂ®ÅÊï∞ÊçÆÊ∫ê
  - arxiv
  - Á±ª‰ºº google-scholar, web-of-science
  - ÂõΩÂÆ∂ÁªüËÆ°Â±Ä
  - ‰∏≠ÂõΩË£ÅÂà§Êñá‰π¶ÁΩë
  - ÂõΩÂÆ∂Âì≤Â≠¶Á§æ‰ºöÁßëÂ≠¶(ÊâÄÊúâÊñáÁåÆÂÖçË¥π‰∏ãËΩΩ)

- Âõæ‰π¶
  - [Project Gutenberg - Free eBooks ](https://www.gutenberg.org/)
    - focus on older works for which U.S. copyright has expired
  - ÂõΩÂÆ∂Êï∞Â≠óÂõæ‰π¶È¶Ü
    - [ÁîµÂ≠êÂõæ‰π¶ÂÖ¨ÁõäÈòÖËØª](http://read.nlc.cn/gyyd/index)
  - ‰π¶Ê†º, ÁâàÊùÉËøáÊúüÂõæ‰π¶
# popular
- https://github.com/pipeshub-ai/pipeshub-ai /2kStar/apache2/202511/python/ts
  - https://pipeshub.com/
  - The OpenSource Alternative to Glean's Workplace AI
  - PipesHub AI helps you quickly find the right information using natural language search‚Äîjust like Google.
  - The platform not only delivers the most relevant results but also shows where the information came from, with proper citations, using Knowledge Graphs and Page Ranking
  - Beyond search, our platform allows enterprises to create custom apps and AI agents using a No-Code interface.
  - Knowledge Graph Backbone ‚Äì All data is seamlessly structured into a powerful knowledge graph.
  - Enterprise-Grade Connectors ‚Äì Scalable, reliable, and built for secure access across your organization.
  - Modular & Scalable Architecture ‚Äì Every service is loosely coupled to scale independently and adapt to your needs.
  - ÂâçÁ´Ø: Material UI„ÄÅReact Hook Form„ÄÅzod
  - ÂêéÁ´Ø: fastapi, LangGraph, LangChain, Qdrant(vector), ArangoDB(graph), Kafka, Redis, Docling, PyMuPDF, OCRmyPDF
  - üõ¢Ô∏è Â∫îÁî®Â±ÇÊï∞ÊçÆÂú®python‰æßÂíånodejs‰æßÈÉΩÂ§ßÈáè‰ΩøÁî®arangodbÊù•Â≠òÂÇ®ÂõæÁªìÊûÑÁöÑÂÖ≥Á≥ª
  - üêõ ‰∏çÊîØÊåÅÂ§ñÈÉ®ÊêúÁ¥¢ÂºïÊìéÂ¶Ç Tavily/EXA/SearxNG, ‰ΩÜÊñπ‰æøÁ∫ØÊú¨Âú∞ÈÉ®ÁΩ≤
  - [Work AI for all - AI platform for agents, assistant, search](https://www.glean.com/)
  - [Best way to extract data from PDFs and HTML : r/Rag _202510](https://www.reddit.com/r/Rag/comments/1oavnx4/best_way_to_extract_data_from_pdfs_and_html/)
    - At PipesHub, we use docling, pymupdf (faster than docling but need to use layout parser on top of it), ocrmupdf/Azure DI (scanned pdfs).
    - If you are looking for Higher Accuracy, Visual Citations, Cleaner UI, Direct integration with Google Drive, OneDrive, SharePoint Online, Dropbox and more. PipesHub is free and fully open source, extensible. 
  - prÂ∑≤ÂêàÂπ∂ [Backend Support for Ollama Models ¬∑ Pull Request _202507](https://github.com/pipeshub-ai/pipeshub-ai/pull/475)
    - [Ollama Embedding model support ¬∑ Pull Request ](https://github.com/pipeshub-ai/pipeshub-ai/pull/480)

- https://github.com/MODSetter/SurfSense /12.7kStar/apache2/202601/python/ts
  - https://www.surfsense.com/
  - Open source alternative to NotebookLM, Perplexity, and Glean.
  - Connects to search engines, Slack, Linear, Jira, ClickUp, Notion, YouTube, GitHub, Discord, and more. 
  - Save content from your own personal files (Documents, images, videos and supports 50+ file extensions) to your own personal knowledge base .
  - ‚õìÔ∏è Get Cited answers just like Perplexity.
  - Works Flawlessly with Ollama local LLMs.
    - üêõ embeddingÊ®°ÂûãÂøÖÈ°ª‰ΩøÁî®azure, ‰∏çÊîØÊåÅÊú¨Âú∞Ê®°Âûã
  - ÂêéÁ´Ø: litellm, FastAPI, SQLAlchemy, pgvector, LangGraph, LangChain, Hybrid Search, Rerankers, Redis
  - ÂâçÁ´Ø: next, Vercel AI SDK Kit UI Stream Protocol, Shadcn, Framer Motion, React Hook Form, tanstack/table
  - ‰æùËµñchonkieÂØπdocËøõË°åembedding
  - ETL Service (choose one)
    - Docling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)
    - LlamaIndex API key (enhanced parsing, supports 50+ formats)
  - üë∑ xp
    - Ë∑®workspace‰∏çËÉΩÂÖ±‰∫´source-document
    - ËÅäÂ§©Êó∂~~Âè™ËÉΩÈÄâÊã©ÂÖ®ÈÉ®ÊñáÊ°£Êàñ‰∏çÈÄâÊñáÊ°£Ôºå‰∏ç~~ÂèØ‰ª•Âè™ÈÄâÊã©ÈÉ®ÂàÜÊñáÊ°£
    - ÊîØÊåÅÊ†πÊçÆÂú∫ÊôØÈÖçÁΩÆ‰∏çÂêållm: fast, long, reasoning
    - üêõ ËÅäÂ§©‰∏≠ÁöÑÂÜÖÂÆπÊîØÊåÅÁÇπÂáªË∑≥ËΩ¨Âà∞ÊñáÊ°£ÁöÑchunk‰ΩçÁΩÆÔºåËÄå‰∏çÊòØÊ∫êÊñá‰ª∂Ôºå‰∏î‰∏≠ÊñáÊñáÊ°£ÁöÑchunkÁªèÂ∏∏ÊòØ‰π±Á†Å
  - [„ÄêFeature Request„Äë Streaming Response for Research Agent _202505](https://github.com/MODSetter/SurfSense/issues/86)
    - 202512 Â∑≤ÂêàÂπ∂pr
  - https://discord.com/channels/1359368468260192417/1359416865939787837/1409642464792412220
    - I was considering installing Surfsense but it needs API keys, doesn't it? How much does it cost to use it?
    - Every service has a local alternative other than Speech to Text service. No need to put any API keys if you use everything local.
  - https://discord.com/channels/1359368468260192417/1359416891831222362/1437030441910669403  _202511
    - how does embedding work for large documents (not chunks)  if my embedding model‚Äôs context window is only 256 or 512 tokens, but the document is tens of thousands of tokens long? 
    - We generate embedding of the summary of doc.
  - [OSS Alternative to Glean : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1qbgdu2/oss_alternative_to_glean/)
    - We use litellm to route our LLM calls and it supports nearly everything

- https://github.com/infiniflow/ragflow /68.2kStar/apache2/202511/python/ts/Âçé‰∫∫Âõ¢Èòü
  - https://ragflow.io/
  - https://demo.ragflow.io/
  - open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding
  - Template-based chunking: Plenty of template options to choose from
  - Grounded citations with reduced hallucinations
  - Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.
  - Multiple recall paired with fused re-ranking.
  - ‰æùËµñ Crawl4AI„ÄÅelasticsearch„ÄÅflask-login„ÄÅminio„ÄÅpandas„ÄÅvoyageai„ÄÅpyobvector
  - Êú™‰ΩøÁî®langchain/aisdk, Â§ömodel-providerÁöÑÈõÜÊàêÂÆåÂÖ®Ëá™ÂÆö‰πâÂÆûÁé∞
  - üá®üá≥ ÂØπ‰∏≠ÊñáÁöÑÊîØÊåÅËæÉÂ•Ω
  - ÂÆûÊµãÊú¨Âú∞ËøêË°åÂæà‰∏çÂèãÂ•Ω
    - elasticsearch/infinity Êï∞ÊçÆÂ±ÇËøòÂú®ËøÅÁßª‰∏é‰ºòÂåñ
    - Êú¨Âú∞ËøêË°åÁöÑÂÖ•Âè£‰æùËµñ libjemalloc.so
  - Prerequisites: RAM >= 16 GB
    - gVisor: Required only if you intend to use the code executor (sandbox) feature of RAGFlow.
  - ‚ú® ÂÆûÊµã, ragÂêéÁöÑchunkÊñáÊú¨ÂèØÊü•Áúãchunk‰∏éÂéüÊñáÂØπÂ∫îÁöÑÁªÜËäÇ
    - ËÅäÂ§©ÁöÑÂõûÁ≠î‰∏≠, hoverÂõæÊ†áËÉΩÊòæÁ§∫ÂºïÊñáÔºåÂπ∂ÂèØÁÇπÂáªÂºïÊñáÂêéÂèØÂú®È°µÈù¢ÈÅÆÁΩ©‰æßÈù¢ÂºπÁ™ó‰∏≠Êü•ÁúãpdfÂéüÊñá‰ΩçÁΩÆ
  - üêõ Êï¥‰∏™ Áü•ËØÜÂ∫ì/ËÅäÂ§©/Êü•ËØ¢ ÁöÑÂàõÂª∫Âèä‰ΩøÁî®‰∫§‰∫íËøá‰∫éÂ§çÊùÇÔºå ‰∏çÂ§üÁÆÄÊ¥ÅÂíåËá™ÁÑ∂
  - DeepDoc (Default) - RAGFlowPdfParser
    - Full OCR, Most comprehensive but slower
    - Stream Reading Capability: ‚úÖ YES
    - Page-by-page processing with configurable page_from/page_to
    - Uses `pdfplumber.open()` with `page_from` and `page_to` parameters
    - ‰ΩøÁî®pdfplumber.open()Âíåpypdf.PdfReader(), ‰∏ÄÊ¨°ÊÄßÂä†ËΩΩÊï¥‰∏™PDFÊñá‰ª∂Âà∞ÂÜÖÂ≠ò
  - Plain Text Parser - PlainParser
    - Uses `pypdf` for text extraction only
    - Only for text-based PDFs
    - Stream Reading Capability: ‚úÖ YES
    - Uses `pypdf.PdfReader` with page ranges
    - Memory Usage: Very low - only text content loaded
    - ÂèØ‰ª•ÈÄêÈ°µÂ§ÑÁêÜÔºå‰ΩÜ‰ªçÈúÄË¶ÅÂä†ËΩΩÂÆåÊï¥PDF
  - Docling Parser - DoclingParser
    -  Uses `docling.DocumentConverter` with full file
  - MinerU Parser - MinerUParser
    - External tool, entire file processing
  - TCADP Parser - TCADPParser
    - Tencent Cloud API integration
    - Cloud-based, file upload to Tencent Cloud
    - Reading Method: Converts entire file to `base64` and uploads, entire file loaded into memory for Base64 conversion
  - Vision Parser - VisionParser
    - Visual model processing, full file
    - Higher memory requirements
    - Uses `pdfplumber` with full file access
  - [HARD -- Efficient way to use enterprise dataset without uploading all files? _202509](https://github.com/orgs/infiniflow/discussions/10388)
    - You have to upload the data from Azure to RAGFlow. And currently you can do that through API. From 0.22 which is going to be launched in this Nov, we will provide some data sources and you can ingest data by just click several buttons. And more data sources could be easily added.
  - [[Question]: Why can't knowledge graphs be used? _202509](https://github.com/infiniflow/ragflow/issues/10017)
    - Knowledge graphs can‚Äôt currently be used with the Table chunking method in RAGFlow. The reason is that table chunking treats each row as a separate chunk and stores field mappings for retrieval, but it does not run those chunks through the knowledge graph extraction pipeline. As a result, entity‚Äìrelationship extraction is skipped for tables.
    - If you need a knowledge graph from table data, you‚Äôd have to implement a custom post-processing step (e.g., parsing the rows yourself and generating triples).
  - [Select PDF parser | RAGFlow](https://ragflow.io/docs/dev/select_pdf_parser)
  - [Deploy local models | RAGFlow](https://ragflow.io/docs/dev/deploy_local_llm)
    - RAGFlow supports deploying models locally using Ollama, Xinference, IPEX-LLM, or jina.
  - üêõ [[Bug]: Fail to access model(text-embedding-bge-reranker-v2-m3). The LmStudioRerank has not been implement _202502](https://github.com/infiniflow/ragflow/issues/5354)
    - Unable to add reranker model for LM STUDIOÔºåFail to access model(text-embedding-bge-reranker-v2-m3).The LmStudioRerank has not been implement
  - üêõ [[Question]: Support for calling the OLLAMA Reranker models Ôºü _202509](https://github.com/infiniflow/ragflow/issues/8988)
    - Ollama supports very few rerank models and is not recommended.
  - [FAQs | RAGFlow](https://ragflow.io/docs/dev/faq)
    - RAGFlow supports MinerU (>= 2.6.3) as an optional PDF parser with multiple backends. RAGFlow acts only as a client for MinerU, calling it to parse documents, reading the output files, and ingesting the parsed content.
  - [[Question]: Big File Parsing ](https://github.com/infiniflow/ragflow/issues/10986)
  - [[Question]: High Memory and CPU Usage During PDF Parsing _202505](https://github.com/infiniflow/ragflow/issues/7602)
    - ÂéüÂõ†ÊâæÂà∞‰∫ÜÔºåÁî®‰∫ÜdeepdocËß£ÊûêÊñπÂºèÔºåÂ∞±ÈùûÂ∏∏Ê∂àËÄóËµÑÊ∫êÔºåÊàëÁî®nativeÂ∞±Ê≤°ÈóÆÈ¢ò„ÄÇÁõÆÂâç‰∏çÈúÄË¶ÅÂõæÁâáËØÜÂà´ÔºåÂ∞±‰∏çÈúÄË¶Ådeepdoc
  - [Questions about Ragflow _202507](https://github.com/orgs/infiniflow/discussions/8904)
    - RAGFlow uses Elasticsearch by default. Is Elasticsearch your recommendation over Infinity (at version 0.19.1)?
    - Pls choose Elasticsearch. We're refactoring Infinity.
  - üêõ [[Question]: Parsing resumes is not open source yet. Are there any plans to open source it in the future? ](https://github.com/infiniflow/ragflow/issues/1053)
    - 202406: currently, we don't have such plan.
  - [[Question]: How to using this "Team" option for open-source? ](https://github.com/infiniflow/ragflow/issues/7050)
    - 202504: It's a feature of enterprise version.
  - https://github.com/tedhappy/ragflow-admin /apache2/202512/python/ts
    - Âü∫‰∫éragflow‰∫åÊ¨°ÂºÄÂèëÁöÑÂêéÂè∞ÁÆ°ÁêÜÁ≥ªÁªüÔºåÂèØ‰ª•Áã¨Á´ãËøêË°åÔºåÊîØÊåÅÊâπÈáèÁÆ°ÁêÜÁü•ËØÜÂ∫ì„ÄÅËÅäÂ§©„ÄÅÊô∫ËÉΩ‰Ωì„ÄÅÁî®Êà∑Á≠âÔºåËß£ÂÜ≥ragflowÂú®ÂêéÂè∞ÁÆ°ÁêÜ‰∏äÁöÑÁóõÁÇπ„ÄÇ
    - ÂÜÖÁΩÆÁÆ°ÁêÜÁïåÈù¢Âú®Áîü‰∫ßÁéØÂ¢É‰∏≠Â≠òÂú®‰∏Ä‰∫õÂ±ÄÈôêÔºöÂçïÁü•ËØÜÂ∫ìËßÜÂõæ, Êó†ÊâπÈáèÊìç‰Ωú, ‰ªªÂä°ÈòüÂàóÈöêËóè
  - https://github.com/knowflow-ai/KnowFlow /AGPL/202512/python
    - https://knowflowchat.cn/
    - Âü∫‰∫é RAGFlow ÁöÑ‰ºÅ‰∏öÁ∫ßÂºÄÊ∫êÁü•ËØÜÂ∫ìËß£ÂÜ≥ÊñπÊ°àÔºå‰∏ìÊ≥®‰∫é‰∏∫‰ºÅ‰∏öÊèê‰æõÁúüÊ≠£ËêΩÂú∞ÁöÑÊúÄÂêé‰∏ÄÂÖ¨ÈáåÊúçÂä°
    - ÊåÅÁª≠ÂÖºÂÆπ RAGFlow ÂÆòÊñπÁâàÊú¨ÔºàÂΩìÂâçÈÄÇÈÖç RAGFlow v0.20.1ÔºâÔºåÂêåÊó∂Â∞ÜÁ§æÂå∫ÊúÄ‰Ω≥ÂÆûË∑µÊï¥ÂêàËøõÊù•Ôºå‰∏∫‰ºÅ‰∏öÁü•ËØÜÁÆ°ÁêÜÊèê‰æõÊõ¥Âä†ÂÆåÂñÑÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ
    - Êèí‰ª∂ÂåñÂ¢ûÂº∫Âπ≥Âè∞ÔºöÈÄöËøáÁã¨Á´ãÊúçÂä°ÊñπÂºèÊâ©Â±ï RAGFlow ÂäüËÉΩ
    - ‰ºÅ‰∏öÁ∫ßÁü•ËØÜÁÆ°ÁêÜÁ≥ªÁªüÔºöÊèê‰æõÂÆåÊï¥ÁöÑÁî®Êà∑ÊùÉÈôê„ÄÅÂõ¢ÈòüÂçè‰Ωú„ÄÅÊï∞ÊçÆÂÆâÂÖ®‰øùÈöú
    - ÂïÜ‰∏öÁâà: PaddleOCR, RAG ËØÑ‰º∞Á≥ªÁªü, RBAC ÂÆåÊï¥ÁâàÊú¨, Dify Ê∑±Â∫¶ÈõÜÊàê, ‰ΩøÁî®ÁªüËÆ°

- https://github.com/Tencent/WeKnora /7.5kStar/MIT/202511/go/ts/vue
  - https://weknora.weixin.qq.com/
  - LLM-powered framework for deep document understanding, semantic retrieval, and context-aware answers using RAG paradigm
  - üì° [WeKnora Roadmap ](https://github.com/Tencent/WeKnora/issues/414)
    - Vision: Áã¨Á´ãÈÉ®ÁΩ≤‰∏™‰∫∫Áü•ËØÜÂ∫ìÔºåÊîØÊåÅÊñáÊ°£„ÄÅÊï∞ÊçÆ„ÄÅÂõæÁâáÔºåÂü∫‰∫é‰º†ÁªüRAGÊ°ÜÊû∂ÊãìÂ±ïÊõ¥Â§öLLMÂ∫îÁî®Âú∫ÊôØ
  - [[Question]: ËØ∑ÈóÆÊúâËÆ°ÂàíÊîØÊåÅMinerU„ÄÅPP-Structure-V3Á≠âÁ¨¨‰∏âÊñπËß£ÊûêÊúçÂä°ÁöÑË∞ÉÁî®‰πà? ‰ª•ÂèäÊòØÂê¶ÊîØÊåÅÊõ¥Â§öÂÉèlightragËøôÁßçgraphÁöÑÊûÑÂª∫Ôºü _202509](https://github.com/Tencent/WeKnora/issues/248)
    - Áé∞Âú®ÊîØÊåÅ‰∫ÜPPOCR-V4.PP-Structure-V3ÂΩìÁÑ∂‰πüÊòØÂèØ‰ª•ÁöÑÔºåÂÖ∑‰ΩìËß£ÊûêÈÄªËæëÂú®docreaderËøô‰∏™Ê®°Âùó‰∏≠„ÄÇ
    - MinerUËøôÂùóÊöÇÊó∂Ê≤°ÊúâËØ¶ÁªÜË∞ÉÁ†îÔºådocreaderÂú®ËÆæËÆ°‰∏äÊòØ‰∏Ä‰∏™PythonÁöÑÁã¨Á´ãÊ®°ÂùóÔºåÁêÜËÆ∫‰∏äÂèØ‰ª•ÊØîËæÉËΩªÊùæÁöÑÊé•ÂÖ•ÂÖ∂‰ªñSDK„ÄÇ
    - graphRagËøôÂùóÊàë‰ª¨Áé∞Âú®ÊúâÊåâÁÖßÊàë‰ª¨ÁöÑÁ¥¢ÂºïÁªìÊûÑËøõË°åÂÆûÁé∞„ÄÇÂ¶ÇÊûúÊúâÊõ¥Â•ΩÁöÑÊñπÂºèÔºåÊÑüËßâÂèØ‰ª•ÊåâÁÖßÂ§öË∑ØÁ¥¢ÂºïÁöÑÊÄùË∑ØËøõË°åÂçïÁã¨‰∏ÄË∑ØÁöÑÂÆûÁé∞
  - üêõ [ÊñáÊ°£Ëß£ÊûêÂ§±Ë¥• _202508](https://github.com/Tencent/WeKnora/issues/77)
    - ÂàöÂàöÊâæÂà∞Ëß£ÂÜ≥ÊñπÊ≥ï‰∫ÜÔºåÁñë‰ººÊòØË∞ÉÁî®ÈÄöËøáÊú¨Âú∞ollama Ê®°ÂûãÁöÑÈóÆÈ¢òÔºõ Ëß£ÂÜ≥ÊñπÊ≥ïÂ∞±ÊòØÈÄöËøáapiÂéªË∞ÉÁî®embeddingÊ®°ÂûãÔºåËôΩÁÑ∂ÈÖçÁΩÆÊó∂‰ºöÊèêÁ§∫Êä•ÈîôÔºå‰ΩÜÊòØÂèØ‰ª•ÊòØ‰øùÂ≠òÁöÑÔºå‰øùÂ≠òÂêé‰πüÂèØ‰ª•‰ΩøÁî®„ÄÇÂÜçÊ¨°‰∏ä‰º†Êó∂Â∞±Ëß£ÊûêÊàêÂäü‰∫Ü
  - [[Question]: ‰∏ä‰º†Â§ßÊñá‰ª∂ÊÄé‰πàÂäûÔºü _202510](https://github.com/Tencent/WeKnora/issues/348)
    - Ëøô‰∏™Â∫îËØ•ÊòØnginxÁöÑÈÖçÁΩÆÈóÆÈ¢òÂêß„ÄÇÂÖàËøõÂÖ•ÂâçÁ´ØÊñá‰ª∂Â§πÔºå‰øÆÊîπnginx.confÊñá‰ª∂
    - ÊØîÂ¶ÇÔºåÂ∞ÜÊúÄÂ§ßÊñá‰ª∂‰øÆÊîπ‰∏∫200M,Â∞Ü‰∏ä‰º†Êó∂Èó¥‰øÆÊîπ‰∏∫300s;
      - Â∞Üclient_max_body_size 50M ;‰øÆÊîπ‰∏∫200M;
      - client_max_body_size 200M
      - proxy_connect_timeout 300s;
  - [[Question]: Ëß£Èô§‰∫Ü30MÁöÑÈôêÂà∂‰πãÂêéËß£ÊûêÂ§±Ë¥•‰∫Ü ](https://github.com/Tencent/WeKnora/issues/245)
    - ÊâæÂà∞‰∫ÜÈóÆÈ¢ò‰∫Ü Áªô gRPC Ê∂àÊÅØÂ§ßÂ∞èËÆæÁΩÆ‰∏∫ 5GB ËøôÂ•ΩÂÉèÊòØË∂ÖËåÉÂõ¥‰∫Ü ‰øÆÊîπ‰∏∫500MÂêé Â∞±ËÉΩÊ≠£Â∏∏Ëß£Êûê‰∫ÜÔºåÂ§ßÊ¶ÇÂú®65MÂ∑¶Âè≥ÁöÑ‰∏Ä‰∏™pdfËß£Êûê‰∫ÜÂ∞ÜËøë15ÂàÜÈíü ‰∏çÁü•ÈÅìÊòØ‰∏çÊòØÊàëÁîµËÑëÊÄßËÉΩÁöÑÈóÆÈ¢ò m1 pro 32
    - ÁõÆÂâçÊúâÂπ∂ÂèëÊÄßËÉΩÈóÆÈ¢òÔºåÂêéÈù¢‰ºöËøõ‰∏ÄÊ≠•‰ºòÂåñ
  - [[Bug]: ‰ΩøÁî®bge-m3ÂØºÂÖ•ÊñáÊú¨Áü•ËØÜÂ∫ìË∂ÖËøá‰∏ÄÂÆöÂ≠óÁ¨¶Â∞±Â§±Ë¥• ](https://github.com/Tencent/WeKnora/issues/410)
    - ‰ΩøÁî®bge-m3ÂêëÈáèÂåñÊ®°ÂûãÔºåÊú¨Âú∞txtÊñáÊú¨Êñá‰ª∂ÂØºÂÖ•Áü•ËØÜÂ∫ìÁöÑËøáÁ®ã‰∏≠ÔºåÂΩìÊñáÊ°£Â≠óÁ¨¶Â§ß‰∫éÁ≠â‰∫é2wÊó∂ÔºåÂ∞±‰ºöÂØºÂÖ•Â§±Ë¥•ÔºåËÄåÈÄÇÂΩìÂà†Èô§Â≠óÁ¨¶ÔºåÊØîÂ¶Ç19997ÔºåÂàôÂèØ‰ª•Ê≠£Â∏∏ÂØºÂÖ•„ÄÇÂàùÊ≠•Êé®ÊµãÊúâ‰∏Ä‰∏™2‰∏áÁöÑÈôêÂà∂Ôºü‰ªé‰ªªÂä°ÁÆ°ÁêÜÂô®ÁöÑËµÑÊ∫êÊÉÖÂÜµÊù•ÁúãÔºåÊú∫Âô®ËµÑÊ∫êÂú®ÂÅ•Â∫∑Ê∞¥‰ΩçÔºåÊâÄ‰ª•‰∏çÊòØÊú∫Âô®ËµÑÊ∫êÁöÑÈóÆÈ¢ò
  - [[Feature]: Áü•ËØÜÂ∫ìÊòØÂê¶ËÉΩÊîØÊåÅ‰∏ä‰º†Êñá‰ª∂Â§π _202511](https://github.com/Tencent/WeKnora/issues/388)
    - ËøòÊ≤°ÊúâËÆ°ÂàíÊîæÂºÄËøô‰∏™ÂäüËÉΩÔºåÂ¶ÇÊûú‰∏ÄÊ¨°ÊÄß‰∏ä‰º†Êñá‰ª∂ËøáÂ§öÔºåÂ§öÊúçÂä°Ë¥üËΩΩÊù•ËØ¥‰∏çÊòØ‰∏Ä‰ª∂Â•Ω‰∫ãÊÉÖ
  - [[Question]: Áü•ËØÜÂõæË∞±  ](https://github.com/Tencent/WeKnora/issues/364)
    - ÂèëÁé∞ÈóÆÈ¢ò‰∫ÜÔºåÊòØÊ®°ÂûãÁöÑÈóÆÈ¢òÔºåÁü•ËØÜÂõæË∞±ÁöÑÊûÑÂª∫ÈúÄË¶ÅjsonÊ†ºÂºèÔºå‰πãÂâç‰ΩøÁî®ÁöÑÊòØÊé®ÁêÜÊ®°ÂûãÔºåÂåÖÂê´‰∫ÜthinkÊ†áÁ≠æÔºåÊâÄ‰ª•‰∏ÄÁõ¥Â§±Ë¥•„ÄÇ 
    - Â¶ÇÊûúÊàëÊÉ≥ÂÖ≥Èó≠Áü•ËØÜÂõæË∞±ÁöÑÊûÑÂª∫‰ª•ÂèäÁ¶ÅÊ≠¢ÂÆÉ‰æùÊçÆÁü•ËØÜÂõæË∞±ËøõË°åÊ£ÄÁ¥¢ÔºåÂè™ÈúÄË¶Å‚ÄúENABLE_GRAPH_RAG=False‚ÄùÂ∞±ÂèØ‰ª•‰∫ÜÂêóÔºüÊÉ≥ÂÅö‰∏Ä‰∏ãÊúâÊó†Áü•ËØÜÂõæË∞±ÊÉÖÂÜµ‰∏ãÁöÑÊ£ÄÁ¥¢ÊÄßËÉΩÁöÑÂØπÊØî
    - ÊòØÁöÑ„ÄÇÁÑ∂ÂêéÈáçÂêØÈáçÊñ∞‰∏ä‰º†Â∞±Â•Ω
  - [[Question]: Âè™ËÉΩÈÄöËøáÁü•ËØÜÂ∫ìÁöÑÂÜÖÂÆπÈóÆÁ≠îÔºåÁü•ËØÜÂ∫ì‰∏≠Ê≤°ÊúâÂÜÖÂÆπÊó∂ÔºåÊó†ËøîÂõû ](https://github.com/Tencent/WeKnora/issues/359)
    - ÊúüÊúõÂú®Áü•ËØÜÂ∫ìÊ£ÄÁ¥¢‰∏çÂà∞Êó∂ÔºåÂ§ßÊ®°Âûã‰πüÂèØ‰ª•ËæìÂá∫
    - ËøôÊ†∑ËÆæËÆ°ÁöÑÁõÆÁöÑÊòØ‰∏∫‰∫ÜÈò≤Ê≠¢Ê®°ÂûãËæìÂá∫‰∏Ä‰∫õÂç±Èô©ÁöÑÔºåÊó†Ê≥ïÈ¢ÑÊµãÁöÑÂÜÖÂÆπ
    - ÂèØ‰ª•‰øÆÊîπsummary prompt, Êää NO_MATCH ÁöÑËØùÊúØ‰ªé Prompt‰∏≠ÂéªÊéâÔºåÊîπÊàê‰Ω†ÊÉ≥Ë¶ÅÁöÑ
  - [[Question]: ‰∏∫‰ªÄ‰πàpdfÁî®minerUÁöÑparserÂêéÔºåËøòÂçïÁã¨Áî®‰∫Üpaddle ocrÂ§ÑÁêÜÂõæÁâáÔºü _202512](https://github.com/Tencent/WeKnora/issues/461)
    - ÁõÆÂâç minuerU ÁöÑ PDF Ëß£ÊûêÂäüËÉΩÊöÇÊú™ÈõÜÊàê OCR ËÉΩÂäõÔºåËã•‰Ω†ÂØπÊ≠§ÂäüËÉΩÊÑüÂÖ¥Ë∂£ÔºåÊ¨¢ËøéÊèê‰∫§ PR Ë¥°ÁåÆ‰ª£Á†Å„ÄÇ

- https://github.com/UnicomAI/wanwu /2.7kStar/apache2/202511/go/üÜö
  - ÂÖÉÊôØ‰∏áÊÇüÊô∫ËÉΩ‰ΩìÂπ≥Âè∞ÊòØ‰∏ÄÊ¨æÈù¢Âêë‰ºÅ‰∏öÁ∫ßÂú∫ÊôØÁöÑ‰∏ÄÁ´ôÂºè„ÄÅÂïÜÁî®licenseÂèãÂ•ΩÁöÑÊô∫ËÉΩ‰ΩìÂºÄÂèëÂπ≥Âè∞
  - Â§öÁßüÊà∑Êû∂ÊûÑÔºöÊèê‰æõÂ§öÁßüÊà∑Ë¥¶Âè∑‰ΩìÁ≥ªÔºåÊª°Ë∂≥Áî®Êà∑ÊàêÊú¨ÊéßÂà∂„ÄÅÊï∞ÊçÆÂÆâÂÖ®ÈöîÁ¶ª„ÄÅ‰∏öÂä°ÂºπÊÄßÊâ©Â±ï„ÄÅË°å‰∏öÂÆöÂà∂Âåñ„ÄÅÂø´ÈÄü‰∏äÁ∫øÂèäÁîüÊÄÅÂçèÂêåÁ≠âÊ†∏ÂøÉÈúÄÊ±Ç
  - üÜö Êèê‰æõ‰∫ÜÁ´ûÂìÅÊØîËæÉ: dify, fastgpt, ragflow, coze
  - ‰ø°ÂàõÈÄÇÈÖçÔºöÂ∑≤ÈÄÇÈÖçÂõΩ‰∫ß‰ø°ÂàõÊï∞ÊçÆÂ∫ìTiDBÂíåOceanBase
  - model hub: ÊîØÊåÅÁî®Êà∑ÂØºÂÖ•ÂåÖÊã¨ËÅîÈÄöÂÖÉÊôØ„ÄÅOpenAI-API-compatible„ÄÅOllama„ÄÅÈÄö‰πâÂçÉÈóÆ„ÄÅÁÅ´Â±±ÂºïÊìéÁ≠âÊ®°Âûã‰æõÂ∫îÂïÜÁöÑLLM„ÄÅEmbedding„ÄÅRerankÊ®°Âûã
  - Êèê‰æõ Â§öÊé®ÁêÜÂêéÁ´ØÊîØÊåÅÔºàvLLM„ÄÅTGIÁ≠âÔºâ‰∏é Ëá™ÊâòÁÆ°Ëß£ÂÜ≥ÊñπÊ°àÔºåÊª°Ë∂≥‰∏çÂêåËßÑÊ®°‰ºÅ‰∏öÁöÑÁÆóÂäõÈúÄÊ±Ç
  - ËÅîÁΩëÊ£ÄÁ¥¢ÔºàWeb SearchÔºâ
  - ÂèØËßÜÂåñÂ∑•‰ΩúÊµÅÔºàWorkflow StudioÔºâ
  - ‰ºÅ‰∏öÁ∫ßÁü•ËØÜÂ∫ì„ÄÅRAG Pipeline:  Êèê‰æõÁü•ËØÜÂ∫ìÂàõÂª∫‚Üí ÊñáÊ°£Ëß£Êûê‚ÜíÂêëÈáèÂåñ‚ÜíÊ£ÄÁ¥¢‚ÜíÁ≤æÊéí ÁöÑÂÖ®ÊµÅÁ®ãÁü•ËØÜÁÆ°ÁêÜËÉΩÂäõÔºåÊîØÊåÅpdf/docx/txt/xlsx/csv/pptxÁ≠â Â§öÁßçÊ†ºÂºè ÊñáÊ°£ÔºåËøòÊîØÊåÅÁΩëÈ°µËµÑÊ∫êÁöÑÊäìÂèñÂíåÊé•ÂÖ•
  - Êèê‰æõ RESTful API ÔºåÊîØÊåÅ‰∏é‰ºÅ‰∏öÁé∞ÊúâÁ≥ªÁªüÔºàOA/CRM/ERPÁ≠âÔºâÊ∑±Â∫¶ÈõÜÊàê

- https://github.com/pingcap/autoflow /2.6kStar/apache2/202601/python/inactive
  - https://tidb.ai/
  - a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage.
  - An open source GraphRAG (Knowledge Graph) built on top of TiDB Vector and LlamaIndex and DSPy.
  - UI‰∫§‰∫íÁ±ª‰ººchatgpt
  - https://x.com/9hills/status/1862522244527972625
    - RAG Demo Âà∞ RAG Application ÈöæÂ∫¶ÁöÑÂÆåÁæéË°®Áé∞ÔºåÂÖ∂ÂÆûÂäüËÉΩ‰∏çÁÆó‰∏∞ÂØåÔºàÂ¢ûÂä†‰∫Ü Graph RAGÂíå Agent RAG ÁöÑÊÄùÊÉ≥ÔºâÔºå
    - ‰ª£Á†ÅÂç¥‰∏çÂæó‰∏çÂÅöÁöÑÈùûÂ∏∏Â§çÊùÇÔºåÂ§ßÈÉ®ÂàÜÂÖ∂ÂÆûÊòØÂ∫îÁî®ÈÄªËæë„ÄÇ P. S. ‰ª£Á†ÅÂ∑≤ÁªèÊàêÁÜüÂà∞ÂèØ‰ª•Áõ¥Êé•ÊäÑ‰∫ÜÔºåÁõ¥Êé•Â§çÂàªÂ∞±ÂÆå‰∫Ü

- https://github.com/dataelement/bisheng /10.5kStar/apache2/202511/python/ts
  - http://www.bisheng.ai/
  - open LLM devops platform 
  - Powerful and comprehensive features include: GenAI workflow, RAG, Agent, Unified model management, Evaluation, SFT, Dataset Management, Enterprise-level System Management, Observability and more.
  - [ÊúâËÆ°ÂàíÂ∞Ü‰ºÅ‰∏öÊï∞ÊçÆÂ∫ì‰Ωú‰∏∫Êï∞ÊçÆÊ∫ê‰πã‰∏ÄÂêóÔºü ](https://github.com/dataelement/bisheng/issues/500)
    - 202509: ÁõÆÂâçÂèØ‰ª•ÈÄöËøáÂä©ÊâãËäÇÁÇπÊï∞ÊçÆÊü•ËØ¢ÂäüËÉΩÂÆûÁé∞ nl2sql ËÉΩÂäõ

- https://github.com/coze-dev/coze-studio /18.8kStar/apache2/202512/go/ts
  - agent development platform with all-in-one visual tools, simplifying agent creation, debugging, and deployment like never before
  - Provides all core technologies needed for AI agent development: prompt, RAG, plugin, workflow, enabling developers to focus on creating the core value of AI.
  - https://github.com/cloudwego/eino /apache2/go
    - ultimate LLM/AI application development framework in Golang
  - https://github.com/bytedance/flowgram.ai /MIT/ts
    - providing a high-quality workflow building engine for Coze Studio's frontend workflow canvas editor
  - https://github.com/cloudwego/hertz /apache2/go
    - Go HTTP framework with high-performance and strong-extensibility for building micro-services

- https://github.com/netease-youdao/QAnything /13.8kStar/apache2 > AGPL/202503/python/vue/inactive
  - https://qanything.ai/
  - ÂºÄÊ∫êÁöÑ‰ºÅ‰∏öÁ∫ßÊú¨Âú∞Áü•ËØÜÂ∫ìÈóÆÁ≠îËß£ÂÜ≥ÊñπÊ°àÔºåËá¥Âäõ‰∫éÊîØÊåÅ‰ªªÊÑèÊ†ºÂºèÊñá‰ª∂ÊàñÊï∞ÊçÆÂ∫ìÁöÑÈóÆÁ≠î
  - Ê®°ÂûãÊï∞ÊçÆÂÖ®Âú®Êú¨Âú∞ÔºåÂèØÊñ≠ÁΩë‰ΩøÁî®
  - Support selecting multiple knowledge bases for Q&A
  - Currently supported formats include: PDF(pdf), Word(docx), PPT(pptx), XLS(xlsx), Markdown(md), Email(eml), TXT(txt), Image(jpgÔºåjpegÔºåpng), CSV(csv), Web links(html) and more
  - https://github.com/netease-youdao/BCEmbedding /apache2/202409/python/inactive
    - Áî±ÁΩëÊòìÊúâÈÅìÂºÄÂèëÁöÑ‰∏≠Ëã±ÂèåËØ≠ÂíåË∑®ËØ≠ÁßçËØ≠‰πâË°®ÂæÅÁÆóÊ≥ïÊ®°ÂûãÂ∫ìÔºåÂÖ∂‰∏≠ÂåÖÂê´ EmbeddingModelÂíå RerankerModel‰∏§Á±ªÂü∫Á°ÄÊ®°Âûã
    - EmbeddingModel‰∏ìÈó®Áî®‰∫éÁîüÊàêËØ≠‰πâÂêëÈáèÔºåÂú®ËØ≠‰πâÊêúÁ¥¢ÂíåÈóÆÁ≠î‰∏≠Ëµ∑ÁùÄÂÖ≥ÈîÆ‰ΩúÁî®ÔºåËÄå RerankerModelÊìÖÈïø‰ºòÂåñËØ≠‰πâÊêúÁ¥¢ÁªìÊûúÂíåËØ≠‰πâÁõ∏ÂÖ≥È°∫Â∫èÁ≤æÊéí„ÄÇ
    - BCEmbedding‰Ωú‰∏∫ÊúâÈÅìÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÂºèÂ∫îÁî®ÔºàRAGÔºâÁöÑÂü∫Áü≥ÔºåÁâπÂà´ÊòØÂú®QAnything‰∏≠ÂèëÊå•ÁùÄÈáçË¶Å‰ΩúÁî®
    - ÂèåËØ≠ÂíåË∑®ËØ≠ÁßçËÉΩÂäõÔºöÂü∫‰∫éÊúâÈÅìÁøªËØëÂºïÊìéÁöÑÂº∫Â§ßËÉΩÂäõÔºåBCEmbeddingÂÆûÁé∞Âº∫Â§ßÁöÑ‰∏≠Ëã±ÂèåËØ≠ÂíåË∑®ËØ≠ÁßçËØ≠‰πâË°®ÂæÅËÉΩÂäõ
    - Èù¢ÂêëRAGÂÅöÈíàÂØπÊÄß‰ºòÂåñÔºåÂèØÈÄÇÈÖçÂ§ßÂ§öÊï∞Áõ∏ÂÖ≥‰ªªÂä°ÔºåÊØîÂ¶ÇÁøªËØëÔºåÊëòË¶ÅÔºåÈóÆÁ≠îÁ≠â„ÄÇÊ≠§Â§ñÔºåÈíàÂØπ ÈóÆÈ¢òÁêÜËß£Ôºàquery understandingÔºâ ‰πüÂÅö‰∫ÜÈíàÂØπ‰ºòÂåñ

- https://github.com/Future-House/paper-qa /7.9kStar/apache2/202511/python
  - https://futurehouse.gitbook.io/futurehouse-cookbook
  - PaperQA2 is a package for doing high-accuracy retrieval augmented generation (RAG) on PDFs, text files, Microsoft Office documents, and source code files, with a focus on the scientific literature.
  - A usable full-text search engine for a local repository of PDF/text files.
  - You can use llama.cpp to be the LLM. You won't get good performance with 7B models.
  - [save and import embeddings for faster answer generation _202411](https://github.com/Future-House/paper-qa/issues/721)
    - I‚Äôve been able to use it to ask questions by providing over 100 papers as input, and I‚Äôve been using only local models via Ollama. Everything is working well, but I‚Äôd like to know how I can avoid reloading the same files and retraining an embedding model each time I have a new Question.
    - üî° ÈúÄË¶ÅÊâãÂä®‰øÆÊîπembeddingÊ®°ÂûãÈÖçÁΩÆ, ËÄå‰∏çÊòØ‰ΩøÁî®ÈªòËÆ§openaiÊ®°Âûã
  - [usage with large local paper results exceeding `text-embeeded`'s maximium prompt limit _202409](https://github.com/Future-House/paper-qa/issues/453)
    - I'm using paperqa with CLI frontend, while this could apply to code usage too. just run pqa ask cusum in a directory with a 10MiB Chinese paper pdf, results an error 'exceeding 8192 tokens limit' using OpenAI third-party proxy API (did some trick in litellm to actually set api_base)
    - We could call tokenizer, or ask litellm to support pre-check before api call (this will be better)

- https://github.com/timescale/pgai /5.5kStar/PGLic/202512/python
  - A suite of tools to develop RAG, semantic search, and other AI applications more easily with PostgreSQL
  - A Python library that transforms PostgreSQL into a robust, production-ready retrieval engine for RAG and Agentic applications.
  - Automatically create and synchronize vector embeddings from PostgreSQL data and S3 documents. Embeddings update automatically as data changes.
  - Enable natural language to SQL with AI. 
  - Works with any PostgreSQL database, including Timescale Cloud, Amazon RDS, Supabase and more.
  - Architecture: The system consists of an application you write, a PostgreSQL database, and stateless vectorizer workers.
    - data modifications made by the application are decoupled from the embedding process, ensuring that failures in the embedding service do not affect the core data operations.
- https://github.com/neondatabase-labs/pgrag /apache2/202510/rust
  - Postgres extensions to support end-to-end Retrieval-Augmented Generation (RAG) pipelines
  - Text extraction and conversion, using pdf-extract
  - Text chunking
  - Local embedding and reranking models
- https://github.com/postgresml/korvus /1.5kStar/MIT/202501/rust/inactive
  - a search SDK that unifies the entire RAG pipeline in a single database query. 
  - Built on top of Postgres with bindings for Python, JavaScript, Rust and C.

- https://github.com/joelhooks/pdf-brain /157Star/MIT/202601/ts
  - Local PDF & Markdown knowledge base with semantic search and AI-powered enrichment.
  - CLI
  - extract-pdf/text > ollama-enrichment(ÊâìÊ†á) > embeding > libsql-vector-hnsw-index
    - Extract - PDF text via `pdf-parse`, Markdown parsed directly
    - Enrich (optional) - LLM extracts metadata, matches taxonomy concepts
    - Chunk - Text split into ~512 token chunks with overlap
    - Embed - Each chunk embedded via Ollama (1024 dimensions)
    - Store - `libSQL` with vector index (HNSW) + FTS5
    - Search - Query embedded, compared via cosine similarity
  - PDF + Markdown - Index .pdf and .md files with the same workflow
  - Local-first - Everything runs on your machine, no API costs
  - AI enrichment - LLM extracts titles, summaries, tags, and concepts
  - Organize documents with hierarchical concepts
    - The taxonomy(ÁîüÁâ©ÂàÜÁ±ªÂ≠¶; ÂàÜÁ±ªÁ≥ªÁªü) is a hierarchical concept system for organizing documents
  - Vector search - Semantic search via Ollama embeddings
  - Hybrid search - Combine vector similarity with full-text search
  - MCP server - Use with Claude, Cursor, and other AI assistants
  - The database can get large due to vector index overhead. 
    - Text content	~180MB	Actual chunk text(~500k chunks)
    - FTS index	~200MB	Full-text search
    - Embeddings	~1.9GB	500k √ó 1024 dims √ó 4 bytes
    - Vector index	~48GB	HNSW neighbor graphs (~100KB/row)

- https://github.com/Hamza5/file-brain /GPL/202601/python/ts
  - https://file-brain.com/
  - open-source desktop tool that lets you search your local files using natural language.
  - Uses advanced Semantic Search -in addition to full text search- to understand the intent behind your query 
  - Extracts the content of over 1400+ file formats (PDF, Word, Excel, PowerPoint, images, archives, and more).
  - Semantic Search: It uses a multilingual embedding model to understand intent. You can search in one language and find docs in another.
  - OCR Support: Automatically extracts text from screenshots, and scanned documents.
  - Auto-Indexing: Detects changes in real-time and updates the index instantly.
  - Read-Only & Safe: File Brain only reads your files to index them. It never modifies, deletes, or alters your data in any way.
  - All indexing and AI processing happens 100% locally on your machine. 
  - Python/FastAPI/watchdog for backend and the custom filesystem crawler/monitor.
  - React + PrimeReact for the UI
  - Typesense for indexing and search.
  - Apache Tika for file content extraction.
  - üí∞ PRO version is on the way with advanced capabilities:
    - Chat with Files
    - Video Search
    - Cloud & Network Drives: Connect Google Drive, Dropbox, Box, and network drives.
  - [Local file search engine that understands your documents (OCR + Semantic Search) - Open Source. : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1qiuxko/local_file_search_engine_that_understands_your/)
  - if you are usin embeddings to search, does that mean you are maintaing a vector database of all files on disk? that would be a huge memory overhead?
    - Yes that's it. As you can see in the screenshot, the app displays the index size, which is always above 1 Go, because the embedding itself takes around 1.1 Go.

- https://github.com/AKSarav/pdfstract /108Star/apache2/202601/python/js
  - The Extraction and Chunking Layer in Your RAG Pipeline - Available as CLI - WEBUI - API
  - Extract structured text, tables, and metadata from PDFs using various libraries (PyMuPDF4LLM, MarkItDown, Marker, Docling, PaddleOCR, DeepSeek-OCR, Tesseract, MinerU, Unstructured, and more)
  - Chunk the text into smaller chunks using various libraries (Token, Sentence, Recursive, Table, Semantic, Code, Late, Neural, Slumber, and more)
    - 10+ chunking methods powered by `Chonkie`.
  - Embed the chunks using various libraries (Sentence Transformers, OpenAI, etc.)
  - Multiple Output Formats: Markdown, JSON, and Plain Text
  - On-Demand Model Downloads: Download ML models only when needed
  - Batch Processing: Parallel conversion of 100+ PDFs with detailed reporting
  - ‰æùËµñfastapi„ÄÅChonkie„ÄÅPyMuPDF, Marker, Docling
  - üÜö [Built a small tool to compare PDF ‚Üí Markdown libraries (for RAG / LLM workflows) : r/Rag _202507](https://www.reddit.com/r/Rag/comments/1m1j10e/built_a_small_tool_to_compare_pdf_markdown/)
    - I‚Äôve been exploring different libraries for converting PDFs to Markdown to use in a Retrieval-Augmented Generation (RAG) setup.
    - But testing each library turned out to be quite a hassle ‚Äî environment setup, dependencies, version conflicts, etc.
    - Currently, it supports: docling pymupdf4llm markitdown marker
# rag-examples
- https://github.com/pymupdf/pymupdf4llm /1.2kStar/AGPL/202601/python/lib
  - https://pymupdf.readthedocs.io/en/latest/pymupdf4llm
  - a specialized extension of PyMuPDF designed specifically for extracting content from PDFs in a format that's optimized for LLMs
  - Converts PDFs to clean, structured Markdown format
  - Automatically identifies headers, paragraphs, tables, and images
  - Preserves document hierarchy (headers, lists, tables)
  - Maintains document layout and reading order: process multi-column pages
  - Extracts images from PDFs: save images separately or encode them inline
  - [Build a Multimodal RAG using ‚Äî PyMuPDF4LLM-llamaindex-Qdrant _202411](https://ai.gopubby.com/build-a-multimodal-rag-using-pymupdf4llm-llamaindex-qdrant-e9d23a4409cc)
- https://github.com/intercepted16/pymupdf4llm-C /AGPL/202512/c/python
  - "Blazingly-fast" PDF extractor; a high quality 300 pages/s alternative to Markitdown, PyMUPDF4LLM & others.
  - a PDF extractor in C using MuPDF, inspired by pymupdf4llm. i took many of its heuristics and approach but rewrote it in C for speed, then bound it to Python and Rust so it's easy to use.
    - primarily intended for use with python bindings. but for some reason i got bored and added Rust ones too if ya want.
  - outputs JSON for every block: text, type, bounding box, font metrics, tables. you get the raw data to process however you need.
  - [I made a fast, structured PDF extractor for RAG; 300 pages a second : r/Rag _202512](https://www.reddit.com/r/Rag/comments/1pzwot0/i_made_a_fast_structured_pdf_extractor_for_rag/)
  - [I ported PyMuPDF4LLM to Go/C, and made it 100x faster (literally), while keeping comparable quality : r/Python _202602](https://www.reddit.com/r/Python/comments/1qxb67f/i_ported_pymupdf4llm_to_goc_and_made_it_100x/)
    - About 1000 pages/s on a 1600 page document, and 500 pages/s on a 149 page document

- https://github.com/danny-avila/rag_api /710Star/MIT/202508/python/librechat
  - [RAG API - Chat with files](https://www.librechat.ai/docs/features/rag_api)
  - This project integrates Langchain with FastAPI in an Asynchronous, Scalable manner, providing a framework for document indexing and retrieval, using PostgreSQL/pgvector.
  - Files are organized into embeddings by `file_id`. The primary use case is for integration with LibreChat, but this simple API can be used for any ID-based use case.
  - The main reason to use the ID approach is to work with embeddings on a file-level. This makes for targeted queries when combined with file metadata stored in a database, such as is done by LibreChat.
  - [[Enhancement] Reduce memory consumption greatly, and speed up process slighlty by processing file async and inserting to vectordb in chunks rather than all at once _202510](https://github.com/danny-avila/rag_api/issues/213)
    - Currently RAG creates embeddings for the file holding them all in memory then bulk inserts them all into the vectordb. When embedding very large files this consumes a vast amount of memory
    - This is largely solved by changing the logic such that it breaks the file up into chucks and embeds each separately, and asynchronously bulk inserting each chunk individually as the embedding process completes each chunk. 
    - prÂ∑≤ÂêàÂπ∂ _202512
  - [[Issue] `/health` endpoint becomes unresponsive during large file processing _202510](https://github.com/danny-avila/rag_api/issues/216)
    - When uploading and processing large files, the /health endpoint becomes unresponsive until the processing is complete.

- https://github.com/clusterzx/paperless-ai /4.6kStar/MIT/202511/python/js
  - https://clusterzx.github.io/paperless-ai/
  - an AI-powered extension for Paperless-ngx that brings automatic document classification, smart tagging, and semantic search using OpenAI-compatible APIs and Ollama.
  - [Paperless-AI: Now including a RAG Chat for all of your documents : r/selfhosted _202505](https://www.reddit.com/r/selfhosted/comments/1kqbouu/paperlessai_now_including_a_rag_chat_for_all_of/)
  - [Paperless-ai runs well on local chatgpt-oss installation  _202510](https://github.com/clusterzx/paperless-ai/discussions/749)
    - I installed chatgpt-oss-20b FP16 model, using koboldcpp on a Ryzen 7 7840HS mini pc with 32G RAM. Using -- usevulkan all 25 layers of the model can offloaded to gpu using VRAM. There is no major speed reduction when using FP16, as compared to Q4 etc.! Running on proxmox / debian LXC.
    - For the first 100 docs, the results are very good for my mostly German documents. Processing time is mostly below 10 seconds per document for smaller 1-3 page and 30-40 seconds for larger documents. Works fine for me.

- https://github.com/ari99/lm_studio_big_rag_plugin /202511/ts
  - LM Studio Plugin to mark a directory full of documents as a RAG source and another directory as the Vectorstore and build a RAG for use in your queries. Built using Cursor.
  - Massive Scale: Designed to handle large document collections (GB to TB scale)
  - Deep Directory Scanning: Recursively scans all subdirectories
  - PDF parsing via pdf-parse
  - Multiple File Formats: Supports HTM, HTML, XHTML, PDF, EPUB, TXT, TEXT, Markdown variants (MD/MDX), BMP, JPEG, PNG
  - OCR Support: Optional OCR for image files using Tesseract.js
  - Vector Search: Uses LanceDB for efficient vector storage and retrieval
  - Persistent Storage: Vector embeddings are stored locally and persist across sessions
  - Incremental Indexing: Automatically detects and skips already-indexed files
  - Concurrent Processing: Configurable concurrency for optimal performance
  - Disk Space: The vector store requires additional disk space (typically 10-20% of original document size)
  - Initial Indexing: Can take several hours for TB-scale collections
  - Memory Usage: Scales with concurrent processing (reduce maxConcurrentFiles if needed)
  - Limitations
    - RAR Archives: Not yet implemented (files are skipped)
    - Very Large Files: Individual files >100MB may cause memory issues
    - Non-English OCR: Currently only English OCR is configured

- https://github.com/2dogsandanerd/Knowledge-Base-Self-Hosting-Kit /MIT/202511/python
  - A Docker-powered RAG system that understands the difference between code and prose. 
  - Ingest your codebase and documentation, then query them with full privacy and zero configuration.
  - Different chunking strategies for code vs. prose
    - Code collections use AST-based chunking that respects function boundaries
    - Document collections use semantic chunking optimized for prose
  - Backend: FastAPI, LlamaIndex 0.12.9
  - Vector DB: ChromaDB 0.5.23
  - LLM/Embeddings: Ollama (configurable)
  - Document Parser: Docling 2.13.0 (advanced OCR, table extraction)
  - Frontend: Vanilla HTML/JS (no build step)
  - Batch Ingestion ‚Äî Process multiple files (sequential processing in Community Edition)
  - üõù
    - chroma run --host 0.0.0.0 --port 8000 --path ~/Documents/repos/libfwk/ai-llm/all-rag/Knowledge-Base-Self-Hosting-Kit/backend/ENV/ingestdb
    - OLLAMA_DEBUG=2 ollama serve
    - uv run --env-file .env -- uvicorn src.main:app --port 8080
  - [I spent 2 years building privacy-first local AI. My conclusion: Ingestion is the bottleneck, not the Model. (Showcase: Ollama + Docling RAG Kit) : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pamu5t/i_spent_2_years_building_privacyfirst_local_ai_my/)
    - I‚Äôve been working on strictly local, data-privacy-compliant AI solutions for about two years
    - The biggest lesson I learned: We spend 90% of our time debating model quantization, VRAM, and context windows. But in real-world implementations, the project usually fails long before the prompt hits the LLM. It fails at Ingestion.
    - I built a self-hosting starter kit that focuses heavily on fixing the Input Layer before worrying about the model.
    - Ingestion: Docling (v2). I chose this over PyPDF/LangChain splitters because it actually performs layout analysis. It reconstructs tables and headers into Markdown, so the LLM isn't guessing when reading a row.
    - I‚Äôd love to hear your thoughts on the "Ingestion First" approach. For me, switching from simple text-splitting to layout-aware parsing was the game changer for retrieval accuracy.
  - üëæ prompts-agent
    - i have run this project fully locally without docker and nginx.
    - the backend server starts by `cd backend && uv run --env-file .env -- uvicorn src.main:app --port 8080`.

- https://github.com/djleamen/doc-reader /MIT/202511/python/django
  - A Django-based document Q&A system using Retrieval-Augmented Generation (RAG) to process and query large documents with AI-powered responses.
  - Large Document Support: Handle documents up to 800k+ words
  - ‰æùËµñlangchain
  - üêõ ÂÆûÊµãÊü•ËØ¢ÊïàÊûúÂæàÂ∑Æ, ÂæàÂ§öÂÖ≥ÈîÆËØçÈÉΩÊêú‰∏çÂá∫ÂÜÖÂÆπ The provided context does not contain any information related to your query
    - È°µÈù¢ÊòæÁ§∫scoreÈÉΩÊòØ 1.0
  - Multiple Formats: PDF, DOCX, TXT, and Markdown support
  - REST API: Django REST Framework for integrations
  - Vector Search: FAISS/ChromaDB/Pinecone vector databases
  - CLI Tools: Command-line interface for batch operations
  - Memory issues with large docs: Reduce `CHUNK_SIZE` in `.env` or process documents individually
  - üõù
    - chroma run --host 0.0.0.0 --port 8000 --path ~/Documents/repos/libfwk/ai-llm/all-rag/doc-reader/chroma_db
    - OLLAMA_DEBUG=2 ollama serve
    - uv run --env-file .env -- python main.py start
    - ÈúÄË¶ÅÊâãÂä®ÂàõÂª∫ ÂàõÂª∫ÈªòËÆ§ÁöÑ DocumentIndex(ÊâæAIÂçèÂä©): DocumentIndex.objects.create(name='default'
  - üëæ prompt
    - i have run this django rag webapp fully locally by `uv run --env-file .env -- python main.py start`.
    - when i open http://localhost:8000/ , the browser shows 

- https://github.com/itanishqshelar/SmartRAG /106Star/MIT/202512/python
  - a privacy-first multimodal RAG system that lets you chat intelligently with your documents, images, and audio. 
  - Documents: PDF, DOCX, TXT, MD with intelligent chunking
  - Images: OCR + visual understanding via BLIP
  - Local AI Stack
    - Ollama (Llama 3.1 8B) for generation
    - Nomic Embed Text (768-dim) for embeddings
    - ChromaDB for vector storage
    - Complete offline operation

- https://github.com/MLNativeAI/paperjet /AGPL/202512/ts
  - https://getpaperjet.com/
  - Open-source platform to securely extract data from any document. Build custom workflows while keeping your data private.
  - Fully open-source - The web and self-hosted versions have the same feature set
  - Zero cloud dependencies - PaperJet doesn‚Äôt depend on any cloud services. Everything is self-contained in Docker
  - Built for large documents: easily ingest hundreds of pages at once
  - Use any LLM with your own keys (BYOK)
  - local providers: VLLM, LM Studio and Ollama

- https://github.com/renton4code/pdf-rag /AGPL/202502/ts/inactive
  - A production-ready template for building Retrieval-Augmented Generation (RAG) applications. 
  - This template provides a complete setup for document processing, vector storage, and AI-powered question answering with kickass UI.
  - PDF document processing with OCR
  - Milvus DB with billions of vectors scale support: Vector DB for storing embeddings
  - PostgreSQL: Relational DB for metadata storage
  - Click-to-view document references with highlighting
  - Large documents processing and status updates
  - AI/ML: Google Gemini (LLM), HuggingFace Transformers (Embeddings)
  - Backend: Bun
  - Based on Q&A with a large document (700+ pages) in comparison to RagFlow

- https://github.com/Arterning/DeepParseX /MIT/202512/python
  - Âº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅÊñáÊ°£Ëß£Êûê‰∏éÁü•ËØÜÁÆ°ÁêÜÂπ≥Âè∞ÔºåÊîØÊåÅ PDF„ÄÅWord„ÄÅExcel„ÄÅPPT„ÄÅÂõæÁâá„ÄÅËßÜÈ¢ë„ÄÅÈü≥È¢ë Á≠âÂ§öÁßçÊñá‰ª∂Ê†ºÂºèÁöÑÊô∫ËÉΩËß£ÊûêÔºåËá™Âä®ÊèêÂèñÂÖ≥ÈîÆ‰ø°ÊÅØÔºåÂπ∂ÊûÑÂª∫ Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâ Âíå Áü•ËØÜÂõæË∞±ÔºàKnowledge GraphÔºâ Á≥ªÁªüÔºåÂÆûÁé∞ÁªìÊûÑÂåñÊï∞ÊçÆÁöÑÊô∫ËÉΩÊ£ÄÁ¥¢‰∏éÊé®ÁêÜ
  - ÂêëÈáèÂ≠òÂÇ®ÔºöParadeDB
  - Áü•ËØÜÂõæË∞±ÔºöNetworkX
  - ÂêéÁ´ØÔºöFastAPI, Docker, MinIO
  - https://github.com/Arterning/DeepParseXWeb

- https://github.com/gptme/gptme-rag /MIT/202511/python/cli
  - Local RAG as a simple CLI
  - Built primarily for gptme, but can be used standalone.
  - Document indexing with ChromaDB
  - Streaming large file handling: chunking splits large documents into manageable pieces
  - File watching and auto-indexing: Real-time index updates, batch processing
  - https://github.com/gptme/gptme /4.1kStar/MIT/202602/python
    - https://gptme.org/docs/
    - Your agent in your terminal, equipped with local tools: writes code, uses the terminal, browses the web, vision
    - An unconstrained local free and open-source alternative to Claude Code, Codex, Cursor Agents, etc.
  - https://github.com/gptme/gptme-tauri /NALic/202507/rust/ts/inactive
    - Desktop app for gptme built with Tauri

- https://github.com/thiswillbeyourgithub/wdoc /GPL/202511/python
  - https://wdoc.readthedocs.io/en/stable/
  - Summarize and query from a lot of heterogeneous documents. 

- https://github.com/IlyaRice/RAG-Challenge-2 /MIT/202503/python/inactive
  - [Ilya Rice: How I Won the Enterprise RAG Challenge _202503](https://abdullin.com/ilya/how-to-build-best-rag/)
  - Custom PDF parsing with Docling
  - Vector search with parent document retrieval: Faiss from Meta for efficient similarity search and clustering of dense vectors.
  - LLM reranking for improved context relevance
  - Structured output prompting with chain-of-thought reasoning
  - Query routing for multi-company comparisons
  - You'll need your own API keys for OpenAI/Gemini
  - [5.4 Enterprise-level RAG Practice (I) | AI Roadmap](https://comfyai.app/article/llm-applications/enterprise-level-rag-hands-on-practice)

- https://github.com/in-tech-gration/LangChain-RAG /202503/js/inactive
  - A simple RAG application for doing question-answering on a PDF document
  - This repository contains the JavaScript version of the python RAG implementation by Jodie Burchell using LangChain as demoed in her Beyond the Hype: A Realistic Look at Large Language Models GOTO 2024 presentation.
  - Uses LangChain.js v0.2
  - Key differences between the original python repository and the JavaScript version
    - This code uses two types of Vector stores instead of one. The original code used the ChromaDB vector store, whereas this repo contains code using ChromaDB but also code using the In-memory vector store module provided by LangChain.js.
    - The original repo contains a large PDF (pycharm-documentation.pdf which is around 174MB) that is used in the demo. This is a great source to test and also compare the results with the demo, but it turns out that it takes quite a lot of time to get vectorized. 
    - https://github.com/slvg01/90.10d_RAG_OnTheFly
      - you can upload any PDF document, with a limit of 200 MB per file. You can upload as many PDFs as you'd like.
      - Upload PDF files which are combined into one big text chunk.
    - [Machine-Learning/Optimizing RAG with Document Chunking Techniques Using Python.md at main ¬∑ xbeat/Machine-Learning](https://github.com/xbeat/Machine-Learning/blob/main/Optimizing%20RAG%20with%20Document%20Chunking%20Techniques%20Using%20Python.md)

- https://github.com/Gopendranath/Rag-pdf-chat-backend /202511/js
  - This backend server provides comprehensive file upload functionality for handling images, documents, and messages in a chat application.
  - Multiple File Upload: Support for uploading multiple images and documents simultaneously
  - File Size Limits: 10MB maximum file size per file
  - Static File Serving: Uploaded files are served statically via /uploads/* endpoint
  - This project can use a vector-enabled PostgreSQL (pgvector) for embeddings and MongoDB for document storage.
  - https://github.com/Gopendranath/frontend-rag-chat-agent /ts

- https://github.com/iamarunbrahma/rag-ingest /MIT/202411/python/inactive
  - tool designed to seamlessly convert PDF documents into markdown format while preserving the original layout and formatting
  - The extracted content is indexed in a vector database (Qdrant) to enhance RAG
  - Advanced text extraction with layout preservation using PyMuPDF
  - Uses Ollama model for contextual enhancement of document chunks
  - Combines dense and sparse embeddings via Qdrant for improved retrieval
  - Memory Issues: Process large PDFs in smaller batches

- https://github.com/yash1912/colpali-rag /202410/python/ÂçïÊñá‰ª∂/inactive
  - RAG solution leveraging cutting-edge AI models like ColPali and Qwen2VL.
  - converts PDFs into searchable embeddings using ColPali's AI model

- https://github.com/datawhalechina/all-in-rag /202511/python
  - https://datawhalechina.github.io/all-in-rag/
  - Â§ßÊ®°ÂûãÂ∫îÁî®ÂºÄÂèëÂÆûÊàò‰∏ÄÔºöRAG ÊäÄÊúØÂÖ®Ê†àÊåáÂçóÔºåÂú®Á∫øÈòÖËØª
  - https://github.com/yksanjo/all-in-rag

- https://github.com/ZohaibCodez/document-qa-rag-system /MIT/202509/python/inactive
  - https://document-rag-system.streamlit.app/
  - RAG project built with LangChain and Streamlit. 
  - Upload documents (PDF/TXT) and interact with them using natural language questions powered by embeddings and vector search.
  - Vector-based similarity search using FAISS
  - Frontend: Streamlit
  - Limitations
    - File Types: Currently supports only PDF and TXT formats
    - Large documents (>50 pages) may take longer to process
    - Scanned PDFs: Does not support OCR for image-based PDFs
    - Large PDF files (>100MB) may cause memory issues
    - Embedded images in PDFs are not processed
    - Some complex PDF layouts may not parse correctly

- https://github.com/pratheeshkumar99/Document-based-Question-Answering-System /202408/python/inactive
  - This project demonstrates a Retrieval-Augmented Generation (RAG) system for question answering. 
  - It integrates OpenAI‚Äôs GPT-4 model with FAISS for vector similarity search
  - Vector Store: The embeddings are stored in FAISS (Facebook AI Similarity Search)
  - Handles large documents by splitting them into smaller chunks, ensuring efficient processing and retrieval.

- https://github.com/BjornMelin/docmind-ai-llm /MIT/202509/python/ÂäüËÉΩ‰∏∞ÂØå
  - open-source Streamlit application leveraging LlamaIndex, LangGraph, and local Large Language Models (LLMs) via Ollama, LMStudio, llama.cpp, or vLLM for advanced document analysis. 
  - provides local document analysis with zero cloud dependency
  - This system combines hybrid search (dense + sparse embeddings), knowledge graph extraction, and a 5-agent coordination system to extract and analyze information from your PDFs, Office docs, and multimedia content.
  - Built on LlamaIndex pipelines with LangGraph supervisor orchestration and Qwen3-4B-Instruct-2507's FULL 262K context capability through INT8 KV cache optimization
  - Offline-First Design: 100% local processing with no external API dependencies.
  - Enable Chunked Analysis for large documents, Late Chunking for accuracy, or Multi-Vector Embeddings for enhanced retrieval.
  - LlamaIndex `IngestionPipeline` orchestrates Unstructured parsing, deterministic hashing, DuckDB caching, and AES-GCM page image handling with OpenTelemetry spans for each run.
  - Retrieval/Router: RouterQueryEngine composed via router_factory with tools semantic_search, hybrid_search (Qdrant server‚Äëside fusion), and optional knowledge_graph; uses async/batching where appropriate.
  - Hybrid Retrieval: Qdrant Query API server‚Äëside fusion (RRF default, DBSF optional) over named vectors text-dense (BGE‚ÄëM3; COSINE) and text-sparse (FastEmbed BM42/BM25 with IDF). Dense via LlamaIndex; sparse via FastEmbed.
  - Feature Flags: Boolean environment variables for experimental features

- https://github.com/ricardolx/rag-query /202401/ts/inactive
  - [Implement RAG for large document search‚Äî semantic search with a vector database and LLM _202401](https://medium.com/@ricrivero3/implement-large-document-search-using-rag-semantic-search-with-a-vector-database-and-llm-b798675dca08)
  - Backend ‚Äî Firebase Cloud Functions
  - Database ‚Äî Firestore
  - Vector DB ‚Äî Pinecone
  - LLM ‚Äî OpenAI

- https://github.com/chaanakyaaM/QueryPDF /202507/python/‰ª£Á†ÅÂ∞ë/inactive
  - a local-first, terminal-based RAG tool that allows users to interact with PDF documents using natural language, entirely offline.
  - It extracts text from user-specified pages, chunks it intelligently, embeds the content, and stores it in a local vector database.
  - Local Caching: Caches both tokenizers and embedding models locally for offline use and faster future runs.
  - Graceful Exit Handling: Automatically cleans up ChromaDB collections on exit.
  - Visual Progress Feedback: Shows embedding generation progress.
  - Uses semantic search to find the most relevant chunks for your questions.
  - Fully Customizable: Users can configure embedding model, LLMs, and chunk parameters via a simple JSON file.

- https://github.com/raghavan/pdfgptindexer-offline /MIT/202511/python
  - local RAG system for indexing and querying PDF documents
  - PyMuPDF for text extraction
  - Text Splitting: LangChain's RecursiveCharacterTextSplitter
  - Embeddings: HuggingFace Sentence Transformers (local)
  - Vector Store: FAISS for efficient similarity search
  - LLM: Ollama (local LLM runner)
  - Larger PDFs take longer to index - be patient
  - The FAISS index can be reused - you don't need to re-index unless PDFs change
  - https://github.com/amithkoujalgi/ollama-pdf-bot /202502/strealit
  - https://github.com/tonykipkemboi/ollama_pdf_rag /202501/streamlit

- https://github.com/drisskhattabi6/Chat-with-PDF-Locally /202505/python/inactive
  - An advanced chatbot using Ollama / Sambanova LLMs to interactively extract information from PDFs, Using Streamlit & Ollama / Sambanova API and langchain
  - uses the Marker library to convert PDFs to Markdown format. 
  - Text Chunking: Large documents are split into manageable chunks for easier processing.
  - Embedded Models: The app supports Ollama models for document embeddings and generating answers based on the content.
  - Chroma Vector Store: All the processed documents are stored in a Chroma vector store for efficient retrieval.

- https://github.com/krey-yon/Cliven /MIT/202506/python/inactive
  - a command-line tool that allows you to process PDF documents and have interactive conversations with their content using local AI models. 
  - using ChromaDB for vector storage and Ollama for AI inference.

- https://github.com/codesniper99/LLM-Tinkering /202508/python/inactive
  - This repository enables semantic search and question-answering on research papers using PDF ingestion, vector databases, and local or cloud-hosted LLM

- https://github.com/jacoblee93/fully-local-pdf-chatbot /MIT/202410/ts/inactive
  - another chat over documents implementation... but this one is entirely local
  - Exposing a port to a local LLM running on your desktop via Ollama.
  - Downloading weights into your browser and running via WebLLM.

- https://github.com/joelhooks/pdf-library /MIT/202512/ts
  - Local PDF knowledge base with vector search using PGlite + pgvector
  - i've got a crazy deep pdf library so I vibed up this @opencode (et al) tool that slurps pdfs into a pglite database with pgvector using ollama for embeddings and gives me semantic search over library of alexandria for how i think

- https://github.com/AvaAvarai/Local_Small_LM_Document_RAG /MIT/202409/python/inactive
  - Local Document Retrieval Augmented Generation (RAG) with sentence embedding context for cited question answering with small language models (LM).
  - Currently runs in terminal again, will add GUI back soon.

- https://github.com/snexus/llm-search /MIT/202506/python/inactive
  - RAG with a simple YAML-based configuration that enables interaction with a collection of local documents.
  - The package is designed to work with custom Large Language Models (LLMs) ‚Äì whether from OpenAI or installed locally.

- https://github.com/swirlai/swirl-search /2.9kStar/apache2/202511/python
  - https://swirlaiconnect.com/
  - AI Search & RAG Without Moving Your Data. Get instant answers from your company's knowledge across 100+ apps
  - Warning: The Docker version of Swirl does not retain any data or configuration when shut down
  - Swirl comes configured to search Arxiv, European PMC and Google News right out of the box.

- https://github.com/KeiranHome/Rag_demo /202505/python/inactive
  - Êú¨È°πÁõÆÂà©Áî®FAISSÂêëÈáèÂ∫ìÂíåÂçÉÈóÆÊ®°ÂûãAPIÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÁ≥ªÁªüÔºåÊó®Âú®‰∏∫Áî®Êà∑Êèê‰æõÈíàÂØπÂÖ¨Âè∏Âπ¥Â∫¶ÊÄªÁªìÊï∞ÊçÆÁöÑÊô∫ËÉΩÈóÆÁ≠îÊúçÂä°
  - ‰∏≠ÊñáÂàÜËØçÂ∑•ÂÖ∑ÔºöJieba/THULACÔºåÁî®‰∫éÊñ≠Âè•ÂíåÂÖ≥ÈîÆËØçÊèêÂèñÔºåÊèêÂçáÂêéÁª≠EmbeddingËØ≠‰πâË°®ÂæÅÁ≤æÂ∫¶„ÄÇ
  - ÈÄö‰πâÂçÉÈóÆEmbeddingÊ®°ÂûãÔºö‰∏ì‰∏∫‰∏≠Êñá‰ºòÂåñÔºåÊîØÊåÅÈïøÊñáÊú¨ËØ≠‰πâÁºñÁ†ÅÔºàÊúÄÈïø16K tokensÔºâÔºå‰ΩôÂº¶Áõ∏‰ººÂ∫¶ÂáÜÁ°ÆÁéáËæÉÂºÄÊ∫êÊ®°ÂûãÔºàÂ¶ÇBERT-baseÔºâÊèêÂçá15%Ôºõ

- https://github.com/jayshah5696/pravah /apache2/202409/python/inactive
  - LLM powered Local Perplexity-Inspired Search Engine

- https://github.com/ragpi/ragpi /MIT/202511/python
  - https://docs.ragpi.io/
  - open-source AI assistant that answers questions using your documentation, GitHub issues, and READMEs. 
  - supports multiple providers like OpenAI, Ollama, and Deepseek, and has built-in integrations with Discord and Slack.
  - A web widget integration is also available to embed the assistant in your website.
  - After adding a source, documents will be synced automatically. You can monitor the sync process through the /tasks endpoint.

- https://github.com/NotYuSheng/OmniPDF /MIT/202510/python/inactive
  - OmniPDF is a PDF analyzer capable of translation, summarization, captioning and conversational capabilities through Retrieval-Augmented-Generation (RAG).
  - OmniPDF follows a microservices architecture with centralized orchestration:
    - pdf-processor-service: Main hub that coordinates all processing workflows
    - Processing services: Specialized services for extraction, translation, rendering, and embedding
    - Data layer: Redis (sessions), ChromaDB (vectors), MinIO (files)
    - Service mesh layer: Istio for mTLS, traffic management, and observability (prestaging/staging/production)

- https://github.com/ikantkode/pdfLLM /MIT/202509/python/inactive
  - open source, proof of concept RAG app.
  - uses PostgreSQL for session management, Qdrant for vector storage, Dgraph for graph-based indexing, and Celery for asynchronous task processing.

- https://github.com/pega2077/ai_file_manager /MIT/202601/ts/Electron
  - file manager powered by AI. It automatically classifies your imported files into the most suitable folders and tags them intelligently based on their content, making future search and retrieval easy.
  - Document Import and Management - Supports multiple document formats, automatically converts to Markdown format
  - Semantic Search - Intelligent document retrieval based on vector database
  - Supports Windows and macOS systems
  - Embedded Node.js (Express) server inside the Electron main process
  - Data Storage: SQLite (document metadata via Sequelize) + Faiss vector index (faiss-node)
  - AI Models: Pluggable LLM / embedding providers (OpenAI, Azure, OpenRouter, Bailian, Ollama, etc.)
# rag-fwk
- https://github.com/run-llama/semtools /1.5kStar/MIT/202511/rust/ts
  - Semantic search and document parsing tools for the command line
  - A collection of high-performance CLI tools for document processing and semantic search, built with Rust for speed and reliability.
  - parse - Parse documents (PDF, DOCX, etc.) using, by default, the `LlamaParse` API into markdown format
  - search - Local semantic keyword search using multilingual embeddings with cosine similarity matching and per-line context matching
  - ask - AI agent with search and read tools for answering questions over document collections (defaults to OpenAI, but see the config section to learn more about connecting to any OpenAI-Compatible API)
  - By default, parse uses LlamaParse as a backend. search and workspace remain local-only. ask requires an OpenAI API key.
  - Multi-format support for parsing documents (PDF, DOCX, PPTX, etc.)
  - Fast semantic search using model2vec embeddings from minishlab/potion-multilingual-128M
  - üîÄ Concurrent processing for better parsing performance
  - üì° roadmap
    - More parsing backends (something local-only would be great!)
  - https://x.com/jerryjliu0/status/1999163155898069206
    - We made a simple cli command `ask` which lets you ask questions over any arbitrary folder in your filesystem. 
    - It's a specialized agent that does super-efficient indexing/search. Can natively help you parse all your pdfs/powerpoints/word docs.
    - You can also plug it into Claude Code to give it an efficient way to query your filesystem for context, without polluting the context window with a ton of raw file text. 

- https://github.com/phbst/tinyRAG /202409/jupyter/inactive
  - ÂÖ®ÊâãÂÜôÁöÑ‰∏Ä‰∏™RAGÂ∫îÁî®„ÄÇLangchainÁöÑÂ§ßÈÉ®ÂàÜÂ∫ì‰ºöÂæàÊñπ‰æøÔºå‰ΩÜÊòØ‰Ω†‰∏ç‰∏ÄÂÆöÁêÜËß£ÂÖ∂‰∏≠ÂéüÁêÜÔºåÊâÄ‰ª•‰ª£Á†ÅÂ∞ΩÂèØËÉΩÂ±ïÁé∞Âü∫Êú¨ÁÆóÊ≥ïÔºå‰∏ªÊâìÁêÜËß£RAGÁöÑÂéüÁêÜ

- https://github.com/codemilestones/TinyCodeBase /apache2/202509/python
  - Êú¨È°πÁõÆÊòØ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑ‰ª£Á†ÅÊô∫ËÉΩÁ≥ªÁªüÔºåÂåÖÂê´‰∫ÜRAGÔºàÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºâ„ÄÅAgentÔºàÊô∫ËÉΩ‰ª£ÁêÜÔºâÂíåËØÑ‰º∞Á≥ªÁªüÁöÑÂÆåÊï¥ÂÆûÁé∞„ÄÇ
  - È°πÁõÆ‰ªé[TinyRAG](https://github.com/KMnO4-zx/TinyRAG)Êâ©Â±ïËÄåÊù•Ôºå‰∏ìÊ≥®‰∫é‰ª£Á†ÅÂú∫ÊôØÁöÑ‰ºòÂåñÂíåÂÆûË∑µ„ÄÇ
  - [‰πãÂâçÊúâÂ§öÂ´åÂºÉÂ§ßÊ®°ÂûãÊ°ÜÊû∂ÔºåÁé∞Âú®Áî® LangGraph Â∞±ÊúâÂ§öÈ¶ô - Áü•‰πé _202509](https://zhuanlan.zhihu.com/p/1946396924342177830)

- https://github.com/onyx-dot-app/onyx /16.7kStar/MIT+EE/202512/python/ts/ÂÅè‰∏öÂä°app
  - https://github.com/danswer-ai/danswer /legacy
  - https://onyx.app/
  - Onyx is a feature-rich, self-hostable Chat UI that works with any LLM.
  - Onyx comes loaded with advanced features like Agents, Web Search, RAG, MCP, Deep Research, Connectors to 40+ knowledge sources, and more.
  - RAG: Best in class hybrid-search + knowledge graph for uploaded files and connectors
  - Connectors: Pull knowledge, metadata, and access information from over 40 applications.
  - üí∞ enterprise: Knowledge Graph, Query History, Usage Dashboards, dev api, rbac
  - ÂæàÂ§öissueÊó†‰∫∫Â§ÑÁêÜÂ∞±Ëá™Âä®ÂÖ≥Èó≠‰∫Ü
  - üêõ ÂÆûÊµãÊúçÂä°ÂêØÂä®ÂêéÈ¶ñÊ¨°‰∏ä‰º†Êñá‰ª∂‰ºö‰∏ÄÁõ¥loading/processing, Ê≠§Êó∂ÈáçÂêØÊúçÂä°Â∞±ËÉΩÊ≠£Â∏∏‰∏ä‰º†‰∫ÜÔºåÂéüÂõ†ÂæÖÊéíÊü•
    - ÂØπ‰∏≠ÊñáÁöÑembeddingÂíåsearchÊúâÈóÆÈ¢òÔºåÂÖ≥ÈîÆËØçÊêú‰∏çÂá∫Êù•: The provided document does not contain any information about your query
    - ÂõûÁ≠îËÉΩÊòæÁ§∫ÂºïÁî®ÁöÑÊñá‰ª∂Ôºå‰ΩÜÁÇπÂáªÂºïÁî®Êñá‰ª∂ÁöÑÂõæÊ†áÊòæÁ§∫ÁöÑÊòØÊñáÊú¨ÔºåËÄå‰∏çÊòØpdfÂéüÊñá
  - Code Interpreter: Execute code to analyze data, render graphs and create files.
  - Image Generation: Generate images based on user prompts.
  - üëæ Onyx works with all LLMs (like OpenAI, Anthropic, Gemini, etc.) and self-hosted LLMs (like Ollama, vLLM, etc.)
  - Enterprise Search: far more than simple RAG, Onyx has custom indexing and retrieval that remains performant and accurate for scales of up to tens of millions of documents.
  - [Configure Onyx - Onyx Documentation](https://docs.onyx.app/deployment/configuration/configuration)
    - Although Onyx assumes English by default, the system can be configured to support multiple languages
  - üêõ [Enable to work with other Vector Database ](https://github.com/onyx-dot-app/onyx/issues/1165)
    - Danswer has both vector search and BM25, followed by re-ranking. Additionally, Vespa is not a database; it is more like an application server that handles both roles effectively.
    - So, I want to say that it would not be possible to migrate to something else, as it is quite a big job due to much of the logic being handled by Vespa itself. Vespa is not just a dummy storage.
  - üêõüè† [High memory consumption _202412](https://github.com/onyx-dot-app/onyx/issues/3427)
    - we are experiencing a similar issue, is it expected that the index container's memory usage grows proportionately to number of indexed documents? that seems like a poor design decision if that's the case - acts more like a memory leak. isn't the Vespa database meant to prevent a need for loading every document in ram?
      - Not really ... in fact being in memory is a key component of being able to perform similarity searches across documents quickly. There are probably some significant optimizations we can apply here, but generally speaking this is expected behavior.
    - A solution is to use an external vector database instead of running it within the VM. To achieve this, a new class inheriting from DocumentIndex can be implemented
    - Using an external vector database such as Qdrant Cloud, Elasticsearch, or Vespa Cloud etc. is often a better approach than running the vector index directly within the VM. The vector database is typically the component that demands the most memory.
  - üêõ [markdown files (.md / .mdx) not supported yet ](https://github.com/onyx-dot-app/onyx/issues/5621)
  - [Feature Request: Support embedding via ollama _202511](https://github.com/onyx-dot-app/onyx/issues/6189)
    - onyx expects the embedding models to return vectors of 1536 or 3072 dimensions, respectively. I don't think that I can work around this without building my own Onyx docker image.
  - [Only getting 1 chunk per document _202402](https://github.com/onyx-dot-app/onyx/issues/1081)
    - When im querying say a pdf document, the results shown to me are based solely on 1 chunk from that document. The one with the highest semantic proximity to the user query.
  - [When I ask questions in Chinese, why are the responses always in English? _202401](https://github.com/onyx-dot-app/onyx/issues/1024)
  - [Introducing Onyx - a fully open source chat UI with RAG, web search, deep research, and MCP : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1nw52ad/introducing_onyx_a_fully_open_source_chat_ui_with/)
    - Onyx actually is Danswer. We re-named the project ~6 months ago
    - The way I think about it is that any/every feature needed to have a great chat experience should be completely free to use.
      - The project actually started as a pure RAG/enterprise search system called Danswer, and the enterprise features were built for that world. I‚Äôm moving all features from ee to MIT that fit into the bucket above (e.g. advanced SSO).
    - You have a list of what features are included and which are pay walled?
      - Every chat/core experience feature is completely open-source! So custom assistants, RAG, web search, MCP, image gen, etc.
      - Currently, the ee features are permission syncing (e.g. for RAG, pulling in permissions from enterprise tools and applying them within the tool), a few admin dashboards, and some whitelabeling (ofc the code is MIT, so you can just edit things yourself if you want).
    - üÜö What is the main differences between Openwebui besides being fully open source
      - One of the biggest differences is native RAG/file indexing that scales. In my experience, it's a huge pain to set up OpenWebUI with private docs. Onyx has data connectors to apps like drive, a vector db, and indexing and retrieval pipelines pre-configured for documentation search. System can comfortably do a few hundred thousand docs order of magnitude.
  - [Launch HN: Onyx (YC W24) ‚Äì Open-source chat UI | Hacker News _202511](https://news.ycombinator.com/item?id=46045987)
  - üõù 
    - minio server --address :9004 --console-address :9005  ~/Documents/repos/libfwk/ai-llm/all-rag/onyx/dataruntime/onyxs3
    - docker compose up index , docker start 
    - OLLAMA_DEBUG=2 ollama serve
    - uv run --env-file .env -- alembic upgrade head
    - uv run --env-file .env -- uvicorn model_server.main:app --reload --port 9000
    - uv run --env-file .env -- python ./scripts/dev_run_background_jobs.py
    - uv run --env-file .env -- uvicorn onyx.main:app --reload --port 8080
  - üëæ prompts
    - i have run this project fully locally without docker and nginx, only vespa is hosted using docker, all other services are running locally on my macos.
    - the backend server starts by `cd backend && uv run --env-file .env -- uvicorn onyx.main:app --reload --port 8080`.

- https://github.com/deepset-ai/haystack /23.5kStar/apache2/202512/python
  - https://haystack.deepset.ai/
  - https://docs.haystack.deepset.ai/docs
  - Haystack is an end-to-end LLM framework that allows you to build applications powered by LLMs, Transformer models, vector search and more
  - Haystack can orchestrate state-of-the-art embedding models and LLMs into pipelines to build end-to-end NLP applications 
  - üë∑ xp
    - ÊòØÁ±ª‰ºº langchain/llama_index ÁöÑÊ°ÜÊû∂Ôºå‰∏çÊòØÁ±ª‰ººragflowËøôÊ†∑ÁöÑÂ∏¶uiÁöÑÂèØÁî®‰∫ßÂìÅ
  - Model agnostic: Allow users the flexibility to decide what vendor or technology they want. 
  - Explicit: Make it transparent how different moving parts can ‚Äútalk‚Äù to each other so it's easier to fit your tech stack and use case.
  - Haystack provides all tooling in one place: database access, file conversion, cleaning, splitting, training, eval, inference, and more
  - Extensible: Provide a uniform and easy way for the community and third parties to build their own components
  - deepset AI Platform is our fully managed, end-to-end platform to integrate LLMs with your data, which uses Haystack for the LLM pipelines architecture.
  - https://github.com/deepset-ai/haystack-cookbook /202511
    - A collection of example notebooks using Haystack
  - https://github.com/deepset-ai/haystack-demos
  - https://github.com/deepset-ai/hayhooks /apache2/python
    - Easily deploy Haystack pipelines as REST APIs and MCP Tools.
  - [Ollama: support Embedders ](https://github.com/deepset-ai/haystack-core-integrations/issues/190)
    - 202411: TextEmbedder done. Document Embedder has been merged some time ago.
  - [Investigate support for images in `ToolCallResult` _202505](https://github.com/deepset-ai/haystack/issues/9432)
    - Currently, providers that expose a tool role for messages (OpenAI, Gemini, Ollama) do not support images in the tool result.
    - Conversely, Anthropic has no tool role and requires tool results to be included in a user message, which can contain images.
    - Since multimodal support in Amazon Bedrock is mostly based on Anthropic, Bedrock supports this use case as well. 
    - Supporting this use case would require refactoring our `ToolCallResult` dataclass.
    - a simple workaround is to create a user message with the image returned by the tool.
  - [Handling rich data files such as PDF/CSV/XLSX _202511](https://github.com/deepset-ai/haystack/discussions/10034)
    - I‚Äôd like to ask how you usually handle document ingestion for large files ‚Äî for example, CSVs, XLSX, or PDFs. I‚Äôm currently facing out-of-memory (OOM) issues when uploading rich data files. Our production server has 24 GB of GPU memory, but I‚Äôd like to make the process as optimized as possible.
    - Reproducing: What's the batch size for embedder? I guess you could lower that to 4-5 and see? Your splitter looks good but still do for just one file and check if it is working as expected and breaking into proper chunks. But I guess batch size is the issue
  - [Can this framework be used to Chinese full-text searches? _202307](https://github.com/deepset-ai/haystack/discussions/5471)
    - ÂÆòÊñπÊ∫êÁ†Å‰∏çÊîØÊåÅ„ÄÇ ÂèØ‰ª•ÈÄöËøáÊîπÊ∫êÁ†ÅÂÆûÁé∞„ÄÇ ÂÖ∑‰ΩìÂú®ÂàÜËØçÂàÜÂè•ÈÉ®ÂàÜÔºå ÂÖ∂Ê∫êÁ†Å‰ΩøÁî®ÁöÑnltkÂ∫ìÔºåÊ≤°Êúâ‰∏≠Êñá„ÄÇ ‰ΩÜÊòØÂèØ‰ª•Ëá™Â∑±‰øÆÊîπËøôÈÉ®ÂàÜÁöÑÂáΩÊï∞„ÄÇ‰ΩøÂÖ∂ÊîØÊåÅ‰∏≠ÊñáÁöÑÂàÜËØçÂàÜÂè•ËßÑÂàô
    - 202412: The support for Chinese is very poor. I hope v2 supports Chinese.
    - 202508: We have a new integration for processing Chinese Text with Haystack https://haystack.deepset.ai/integrations/hanlp
    - [for Chinese text processing, can we still only use regular expressions? I wonder if there is a new way now _202303](https://github.com/deepset-ai/haystack/discussions/4312)
  - [Best pipeline for similarity question search _202205](https://github.com/deepset-ai/haystack/discussions/2516)
    - Your advice saved me! Now I use `sentence-transformers/distiluse-base-multilingual-cased` for question embedding. It seems to have a nice result. 

- https://github.com/neuml/txtai /11.9kStar/apache2/202512/python
  - https://neuml.github.io/txtai
  - an all-in-one AI framework for semantic search, LLM orchestration and language model workflows
  - The key component of txtai is an embeddings database, which is a union of vector indexes (sparse and dense), graph networks and relational databases.
  - Vector search with SQL, object storage, topic modeling, graph analysis and multimodal indexing
  - Workflows to join pipelines together and aggregate business logic
  - Run local or scale out with container orchestration
  - built with Python 3.10+, Hugging Face Transformers, Sentence Transformers and FastAPI

- https://github.com/agentset-ai/agentset /1.6kStar/MIT/202512/ts
  - https://agentset.ai/
  - The open-source RAG platform: built-in citations, deep research, 22+ file formats, partitions, MCP server, and more.
  - It provides end-to-end tooling: ingestion, vector indexing, evaluation/benchmarks, chat playground, hosting, and a developer-friendly API.
  - Model agnostic: works with your choice of LLM, embeddings, and vector DB
  - Chat playground with message editing and citations
  - Built-in multi-tenancy
  - Built with TypeScript, Next.js, AI SDK, Prisma, Supabase, and Trigger.dev

- https://github.com/incidentfox/OpenRag /MIT/202602/python
  - Multi-strategy RAG system achieving 74% Recall@10 on MultiHop-RAG.
  - Combines RAPTOR hierarchical retrieval, knowledge graphs, HyDE, BM25, and Cohere neural reranking.
  - Full RAG Implementation (ultimate_rag/) - RAPTOR + Graph + HyDE + BM25 + Neural Reranking
  - This system uses Cohere's rerank API for neural reranking, which provides the best benchmark results (+9.3% improvement).
    - For privacy-sensitive use cases, consider these alternatives
    - Local cross-encoder: The system includes `CrossEncoderReranker` using `BAAI/bge-reranker-base` (runs locally, no external API)
  - [We open-sourced our code that outperforms RAPTOR on multi-hop retrieval : r/Rag _202602](https://www.reddit.com/r/Rag/comments/1qv17hv/we_opensourced_our_code_that_outperforms_raptor/)

- https://github.com/wzdavid/ThinkRAG /MIT/202512/python/inactive
  - https://bluedigit.ai/
  - Â§ßÊ®°ÂûãÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÁ≥ªÁªüÔºåÂèØ‰ª•ËΩªÊùæÈÉ®ÁΩ≤Âú®Á¨îËÆ∞Êú¨ÁîµËÑë‰∏äÔºåÂÆûÁé∞Êú¨Âú∞Áü•ËØÜÂ∫ìÊô∫ËÉΩÈóÆÁ≠î
  - Âü∫‰∫é LlamaIndex Âíå Streamlit ÊûÑÂª∫ÔºåÈíàÂØπÂõΩÂÜÖÁî®Êà∑Âú®Ê®°ÂûãÈÄâÊã©„ÄÅÊñáÊú¨Â§ÑÁêÜÁ≠âËØ∏Â§öÈ¢ÜÂüüËøõË°å‰∫Ü‰ºòÂåñ
  - ÂºÄÂèëÊ®°ÂºèÊîØÊåÅÊú¨Âú∞Êñá‰ª∂Â≠òÂÇ®ÔºåÊó†ÈúÄÂÆâË£Ö‰ªª‰ΩïÊï∞ÊçÆÂ∫ì
  - ‰∏∫ÂõΩÂÜÖÁî®Êà∑ÂÅö‰∫ÜÂ§ßÈáèÂÆöÂà∂Âíå‰ºòÂåñÔºö
    - ‰ΩøÁî® Spacy ÊñáÊú¨ÂàÜÂâ≤Âô®ÔºåÊõ¥Â•ΩÂú∞Â§ÑÁêÜ‰∏≠ÊñáÂ≠óÁ¨¶
    - ÈááÁî®‰∏≠ÊñáÊ†áÈ¢òÂ¢ûÂº∫ÂäüËÉΩ
    - ‰ΩøÁî®‰∏≠ÊñáÊèêÁ§∫ËØçÊ®°ÊùøËøõË°åÈóÆÁ≠îÂíåÁªÜÂåñËøáÁ®ã
    - ÈªòËÆ§ÊîØÊåÅÂõΩÂÜÖÂ§ßÊ®°ÂûãÂéÇÂïÜÔºåÂ¶ÇDeepSeekÔºåMoonshotÂíåZhipuÁ≠â
    - ‰ΩøÁî®ÂèåËØ≠ÂµåÂÖ•Ê®°ÂûãÔºåÂ¶Ç BAAIÁöÑbge-large-zh-v1.5

- https://github.com/Bessouat40/RAGLight /612Star/MIT/202512/python
  - a lightweight and modular Python library for implementing RAG
  - provides modular components to easily integrate various LLMs, embeddings, and vector stores
  - supports: Ollama Google LMStudio vLLM OpenAI API Mistral API

- https://github.com/HKUDS/RAG-Anything /10.7kStar/MIT/202511/python
  - All-in-One Multimodal Document Processing RAG system built on LightRAG.
  - As a unified solution, RAG-Anything eliminates the need for multiple specialized tools. It provides seamless processing and querying across all content modalities within a single integrated framework. 
  - [[Question]: Êàë‰ΩøÁî®‰∏≠ÊñáÊï∞ÊçÆÊûÑÂª∫‰∫ÜRAGÔºå‰ΩÜÊòØ‰∏∫‰ªÄ‰πàÊ£ÄÁ¥¢Âá∫Êù•ÁöÑÂÜÖÂÆπÊòØËã±ÊñáÁöÑÔºü _202510](https://github.com/HKUDS/RAG-Anything/issues/136)
    - .envÈáåÂ•ΩÂÉèÊúâ‰∏™ËØ≠Ë®ÄËÆæÁΩÆÔºåÈªòËÆ§ÊòØËã±ÊñáÔºåÂèØ‰ª•ÁúãÁúãÂíåËøô‰∏™ÊòØÂê¶ÊúâÂÖ≥
  - [[Question]: ÁõÆÂâçÊòØÂê¶ÊîØÊåÅÈïøË°®Ê†ºÁöÑÂØºÂÖ• _202509](https://github.com/HKUDS/RAG-Anything/issues/100)
  - https://github.com/HKUDS/LightRAG /20.1kStar/MIT/202508/python
    - Simple and Fast Retrieval-Augmented Generation
    - [2025.06.16]üéØ Our team has released RAG-Anything an All-in-One Multimodal RAG System for seamless text, image, table, and equation processing.
  - https://github.com/BukeLy/rag-api
    - Multi-tenant RAG API powered by LightRAG/RAG-Anything. Auto-selects best parser (DeepSeek-OCR/MinerU/Docling) via complexity scoring
- https://github.com/xerrors/Yuxi-Know /2.6kStar/MIT/202512/python/vue
  - https://xerrors.github.io/Yuxi-Know/
  - ÂäüËÉΩÂº∫Â§ßÁöÑÊô∫ËÉΩ‰ΩìÂπ≥Âè∞ÔºåËûçÂêà‰∫Ü RAG Áü•ËØÜÂ∫ì‰∏éÁü•ËØÜÂõæË∞±ÊäÄÊúØÔºåÂü∫‰∫é LangGraph v1 + Vue.js + FastAPI + LightRAG Êû∂ÊûÑÊûÑÂª∫
  - ÈõÜÊàê‰∏ªÊµÅÂ§ßÊ®°Âûã„ÄÅLightRAG„ÄÅMinerU„ÄÅPP-Structure„ÄÅNeo4j „ÄÅËÅîÁΩëÊ£ÄÁ¥¢„ÄÅÂ∑•ÂÖ∑Ë∞ÉÁî®„ÄÇ

- https://github.com/SciPhi-AI/R2R /7.5kStar/MIT/202511/python
  - an advanced AI retrieval system supporting RAG
  - Built around a RESTful API, R2R offers multimodal content ingestion, hybrid search, knowledge graphs, and comprehensive document management.
  - Multimodal Ingestion: Parse .txt, .pdf, .json, .png, .mp3, and more
  - Hybrid Search: Semantic + keyword search with reciprocal rank fusion
  - User & Access Management: Complete authentication & collection system

- https://github.com/getzep/graphiti /20.6kStar/apache2/202511/python
  - https://help.getzep.com/graphiti
  - a framework for building and querying temporally-aware knowledge graphs
  - Unlike traditional RAG, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. 
  - The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation
  - Graphiti powers the core of Zep, a turn-key context engineering platform for AI Agents.
  - Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. 
  - Real-Time Incremental Updates: Immediate integration of new data episodes without batch recomputation.
  - Scalability: Efficiently manages large datasets with parallel processing, suitable for enterprise environments.

- https://github.com/memfreeme/memfree /MIT/202409/ts
  - https://www.memfree.me/
  - MemFree is a Hybrid AI Search Engine.
  - Search and ask questions with text, images, files, and web pages.
  - Multi AI Models: ChatGPT, Claude, Gemini
  - ÊîØÊåÅÂ§öÊ®°ÊÄÅÔºàÂõæÁâá„ÄÅÊñáÂ≠ó„ÄÅÊñá‰ª∂ÔºâÔºåÂ§öÊù•Ê∫êÔºàTwitter„ÄÅÂ≠¶ÊúØ„ÄÅÊú¨Âú∞Áü•ËØÜÂ∫ìÔºâÔºåÂ§öÁßçË°®Áé∞ÂΩ¢ÂºèÔºàÊÄùÁª¥ÂØºÂõæ„ÄÅÂõæÁâáÈü≥ËßÜÈ¢ëÔºâÁ≠âÊ∑∑ÂêàÊêúÁ¥¢ÂΩ¢ÊÄÅ

- https://github.com/KruxAI/ragbuilder /apache2/202410/python
  - https://docs.ragbuilder.io/
  - A toolkit to create optimal Production-readyRetrieval Augmented Generation(RAG) setup for your data
  - By performing hyperparameter tuning on various RAG parameters (Eg: chunking strategy: semantic, character etc., chunk size: 1000, 2000 etc.), RagBuilder evaluates these configurations against a test dataset to identify the best-performing setup for your data.

- https://github.com/llmware-ai/llmware /14.5kStar/apache2/202507/python/inactive
  - https://llmware-ai.github.io/llmware/
  - a unified framework for building LLM-based applications (e.g., RAG, Agents), using small, specialized models that can be deployed privately
  - llmware has two main components:
    - RAG Pipeline - integrated components for the full lifecycle of connecting knowledge sources to ai models
    - 50+ small, specialized models fine-tuned for key tasks in enterprise process automation, including fact-based question-answering, classification, summarization, and extraction
  - RAG-Optimized Models - 1-7B parameter models designed for RAG workflow integration and running locally.

- https://github.com/VectifyAI/PageIndex /4.1kStar/MIT/202511/pyhton
  - https://pageindex.ai/
  - Inspired by AlphaGo, we propose PageIndex ‚Äî a vectorless, reasoning-based RAG system that builds a hierarchical tree index for long documents and reasons over that index for retrieval. 
  - [RAG without Vectors ‚Äì PageIndex: Reasoning-Based Document Indexing ¬∑ run-llama/llama_index _202504](https://github.com/run-llama/llama_index/discussions/18360)
    - We were frustrated by vector-based RAG systems that rely on semantic similarity and often fail on long, domain-specific documents.
    - PageIndex, a hierarchical indexing system that transforms large documents (like financial reports, regulatory documents, or textbooks) into semantic trees optimized for reasoning-based RAG.

- https://github.com/OoriData/OgbujiPT /apache2/202512/python
  - Client-side toolkit for using large language models, including where self-hosted
  - It provides a unified API for storing, retrieving, and managing semantic knowledge across multiple backends, with support for dense vector search, sparse retrieval, hybrid search, and more.
  - Storage backends: in-memory, pgvector, qdrant
  - You can use the OpenAI cloud LLM API and APIs which conform to this, including Anthropic's, local LM Studio, Ollama, etc.

- https://github.com/yichuan-w/LEANN /6.8kStar/MIT/202512/python
  - an open-source vector database, compresses RAG indexes by an impressive 97% using graph-based recomputation and on-demand embedding calculation.
  - LEANN achieves this through graph-based selective recomputation with high-degree preserving pruning, computing embeddings on-demand instead of storing them all

- https://github.com/rag-web-ui/rag-web-ui /2.7kStar/apache2/202511/python/ts/Êèê‰∫§Â∞ë
  - RAG Web UI is an intelligent dialogue system based on RAG

- https://github.com/percent4/embedding_rerank_retrieval /202507/jupyter/inactive
  - Êú¨È°πÁõÆÊòØÈíàÂØπRAG‰∏≠ÁöÑRetrieveÈò∂ÊÆµÁöÑÂè¨ÂõûÊäÄÊúØÂèäÁÆóÊ≥ïÊïàÊûúÊâÄÂÅöËØÑ‰º∞ÂÆûÈ™å„ÄÇ‰ΩøÁî®‰∏ª‰ΩìÊ°ÜÊû∂‰∏∫LlamaIndex.
  - ‰ΩøÁî®GradioÂÆûÁé∞‰∏≠ÊñáLate-ChunkingÊúçÂä°: late_chunking/late_chunking_gradio_server.py

- https://github.com/devflowinc/trieve /2.6kStar/MIT/202510/rust/js/inactive
  - https://trieve.ai/
  - All-in-one platform for search, recommendations, RAG, and analytics offered via API

- https://github.com/autollama/autollama /26Star/MIT/202509/js
  - https://autollama.io/
  - Anthropic's Contextual Retrieval implementation with visual chunk comparison. Preview context enrichment before/after embedding.
  - For too long, RAG has been about finding chunks, not understanding documents. AutoLlama changes that. 
  - Built on Anthropic's breakthrough contextual retrieval methodology, it's the first JavaScript-first RAG framework that actually comprehends your documents the way humans do.

- https://github.com/RapidFireAI/rapidfireai /apache2/python/ts
  - https://rapidfire.ai/
  - RapidFire AI is a new experiment execution framework that transforms your AI customization experimentation from slow, sequential processes into rapid, intelligent workflows with hyperparallelized execution, dynamic real-time experiment control, and automatic system optimization.

- https://github.com/OpenBMB/UltraRAG /2.6kStar/apache2/202601/python/js
  - https://ultrarag.openbmb.cn/
  - the first lightweight RAG development framework based on the Model Context Protocol (MCP) architecture design
# graph-knowledge

## graph-examples

- https://github.com/liujuntao123/DeepRead /MIT/202509/ts
  - https://deepread.aizhi.site/
  - AI È©±Âä®ÁöÑÊô∫ËÉΩ‰π¶Á±çÁü•ËØÜÂõæË∞±, Â∞Ü‰π¶Á±çËΩ¨Âåñ‰∏∫Áõ∏‰∫íÂÖ≥ËÅîÁöÑÁü•ËØÜÁΩëÁªú
  - Âü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ®‰π¶ÂÜÖÂÆπÂàÜÊûê
  - Á±ª‰ºº deepwiki/codewiki ÁöÑ‰π¶Á±çÁâàÊú¨

- https://github.com/abhigyanpatwari/GitNexus /202601/ts
    - https://gitnexus.vercel.app/
    - GitNexus is a client-side knowledge graph creator that runs entirely in your browser. 
    - Drop in a GitHub repo or ZIP file, and get an interactive knowledge graph wit a built in Graph RAG Agent. Perfect for code exploration
    - Zero-Server, Graph-Based Code Intelligence Engine Works fully in-browser through WebAssembly. (DB engine, Embeddings model, AST parsing, all happens inside browser)
# rag-memory/context
- https://github.com/volcengine/OpenViking /839Star/apache2/202602/python/cpp
  - https://openviking.ai/
  - ‰∏ì‰∏∫ AI Agent ËÆæËÆ°ÁöÑ‰∏ä‰∏ãÊñáÊï∞ÊçÆÂ∫ì
  - OpenViking ÊëíÂºÉ‰∫Ü‰º†Áªü RAG ÁöÑÁ¢éÁâáÂåñÂêëÈáèÂ≠òÂÇ®Ê®°ÂºèÔºåÂàõÊñ∞ÊÄßÂú∞ÈááÁî® ‚ÄúÊñá‰ª∂Á≥ªÁªüËåÉÂºè‚ÄùÔºåÂ∞Ü Agent ÊâÄÈúÄÁöÑËÆ∞ÂøÜ„ÄÅËµÑÊ∫êÂíåÊäÄËÉΩËøõË°åÁªü‰∏ÄÁöÑÁªìÊûÑÂåñÁªÑÁªá„ÄÇ
  - ÂÉèÁÆ°ÁêÜÊú¨Âú∞Êñá‰ª∂‰∏ÄÊ†∑ÊûÑÂª∫ Agent ÁöÑÂ§ßËÑë
  - Êñá‰ª∂Á≥ªÁªüÁÆ°ÁêÜËåÉÂºè ‚Üí Ëß£ÂÜ≥Á¢éÁâáÂåñÈóÆÈ¢òÔºöÂü∫‰∫éÊñá‰ª∂Á≥ªÁªüËåÉÂºèÔºåÂ∞ÜËÆ∞ÂøÜ„ÄÅËµÑÊ∫ê„ÄÅÊäÄËÉΩËøõË°åÁªü‰∏Ä‰∏ä‰∏ãÊñáÁÆ°ÁêÜ
  - ÂàÜÂ±Ç‰∏ä‰∏ãÊñáÊåâÈúÄÂä†ËΩΩ ‚Üí Èôç‰Ωé Token Ê∂àËÄóÔºöL0/L1/L2 ‰∏âÂ±ÇÁªìÊûÑÔºåÊåâÈúÄÂä†ËΩΩÔºåÂ§ßÂπÖËäÇÁúÅÊàêÊú¨
  - ÁõÆÂΩïÈÄíÂΩíÊ£ÄÁ¥¢ ‚Üí ÊèêÂçáÊ£ÄÁ¥¢ÊïàÊûúÔºöÊîØÊåÅÂéüÁîüÊñá‰ª∂Á≥ªÁªüÊ£ÄÁ¥¢ÊñπÂºèÔºåËûçÂêàÁõÆÂΩïÂÆö‰Ωç‰∏éËØ≠‰πâÊêúÁ¥¢ÔºåÂÆûÁé∞ÈÄíÂΩíÂºèÁ≤æÂáÜ‰∏ä‰∏ãÊñáËé∑Âèñ

- https://github.com/jakops88-hub/Long-Term-Memory-API /202512/ts
  - A Memory Server for AI Agents. Runs on Postgres + pgvector. 
  - Now supporting 100% Local/Offline execution via Ollama.
  - [I implemented Hybrid Search (BM25 + pgvector) in Postgres to fix RAG retrieval for exact keywords. Here is the logic. : r/Rag _202512](https://www.reddit.com/r/Rag/comments/1pcvtan/i_implemented_hybrid_search_bm25_pgvector_in/)
    - I don't stick to just one framework myself, most people jump between LangChain (for quick prototyping) and **Vercel AI SDK** (for production/streaming). MemVault is built to work with both. Regarding state, yeah, MemVault handles the state layer.
    - So the queries sent back to memvault are plain nlp texts ?
      - Exactly. You just send raw text (e.g., "What is the user's budget?")
      - The API handles the embedding/vectorization internally and matches it against the stored memories. So from the agent's perspective, it's just natural language in, relevant context out.
    - Does it work well enough for other languages? Did you try that?
      - you're right, 'english' is too aggressive with stemming. I haven't tested non-English languages extensively yet, but switching to the 'simple' config is the smart move to remove stemming issues, I'll update the config to that now.

- https://github.com/mem0ai/mem0 /apache2/202409/python
  - https://mem0.ai/
  - Mem0 (pronounced as "mem-zero") enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. 
  - Mem0 remembers user preferences, adapts to individual needs, and continuously improves over time, making it ideal for customer support chatbots, AI assistants, and autonomous systems.
  - Multi-Level Memory: User, Session, and AI Agent memory retention
  - Mem0 leverages a hybrid database approach to manage and retrieve long-term memories for AI agents and assistants. 
    - Each memory is associated with a unique identifier, such as a user ID or agent ID, allowing Mem0 to organize and access memories specific to an individual or context.
    - üõ¢Ô∏è When a message is added to the Mem0 using add() method, the system extracts relevant facts and preferences and stores it across data stores: a vector database, a key-value database, and a graph database. 
    - This hybrid approach ensures that different types of information are stored in the most efficient manner, making subsequent searches quick and effective.
    - Mem0 performs search across these data stores, retrieving relevant information from each source

- https://github.com/getzep/zep /apache2/202409/go
  - https://docs.getzep.com/
  - a long-term memory service for AI Assistant apps. 
  - With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant, while also reducing hallucinations, latency, and cost.
  - Zep persists and recalls chat histories, and automatically generates summaries and other artifacts from these chat histories. 
  - Zep also provides a simple, easy to use abstraction for document vector search called Document Collections. This is designed to complement Zep's core memory features, but is not designed to be a general purpose vector database.
# chunking/embedding
- https://github.com/MichalZnalezniak/Chunking-Vis /MIT/202601/python
  - https://chunkingvis-production.up.railway.app/
  - A tool for visualizing different text-chunking methods.
  - Chunking Methods
  - [Built a tool for visualizing text chunking strategies : r/Rag](https://www.reddit.com/r/Rag/comments/1qnplpb/built_a_tool_for_visualizing_text_chunking/)
    - https://chunkviz.up.railway.app/

- https://github.com/chunkhound/chunkhound /974Star/MIT/202602/python
  - https://chunkhound.github.io/
  - Deep Research for Code & Files
  - Transform your codebase into a searchable knowledge base for AI assistants using semantic search via cAST algorithm and regex search. 
  - MCP integration - Works with Claude, VS Code, Cursor, Windsurf, Zed, etc
  - cAST Algorithm - Research-backed semantic code chunking
  - Multi-Hop Semantic Search - Discovers interconnected code relationships beyond direct matches
  - Semantic search - Natural language queries like "find authentication code"
  - Regex search - Pattern matching without API keys
  - Local-first - Your code stays on your machine
  - 22 languages with structured parsing, via Tree-sitter
  - Real-time indexing - Automatic file watching, smart diffs, seamless branch switching
  - prÂ∑≤ÂêàÂπ∂ [refactor: Extract BaseCLIProvider and improve OpenCode integration _202512](https://github.com/chunkhound/chunkhound/pull/122)

- https://github.com/CoreyFransen08/chunk-forge /apache2/202512/python/ts
  - A self-hosted document processing platform for converting PDFs to Markdown with semantic chunking, drag-and-drop editing, rich metadata management, and multi-format export.
  - Upload PDFs and convert them to Markdown using LlamaParse, MarkItDown, or Docling.
  - Multiple strategies (recursive, paragraph, heading, semantic, sentence, token, hierarchical) with configurable chunk sizes.
  - Frontend: React + Vite + TypeScript + Tailwind CSS + dnd-kit
  - Backend: Node.js + Express + TypeScript + Drizzle ORM
  - Parser Service: Python + FastAPI + LlamaParse/MarkItDown/Docling
  - Storage: Local filesystem
  - Built-in export presets for popular vector databases: Pinecone, Chroma

- https://github.com/zirkelc/chunkdown /MIT/202512/ts
  - a tree-based markdown text splitter to create semantically meaningful chunks for RAG applications.
  - Unlike traditional splitters that use simple character or regex-based methods, this library leverages markdown's hierarchical structure for optimal chunking. 
  - Chunkdown is built around a few core ideas that guide its design:
    - Markdown as Hierarchical Tree
  - https://x.com/zirkelc_/status/2000847712884068755
    - Chunkdown now supports split rules for fenced code blocks and inline code
  - https://x.com/zirkelc_/status/1992520261325771153
    - Chunkdown now compacts markdown tables thanks to a one-line PR
    - Markdown tables can be pretty formatted with additional dashes and spaces to align the columns vertically. This makes them easier for humans to read, but wastes tokens and embedding space on useless characters

- https://github.com/speedyk-005/chunklet-py /MIT/202512/python
  - https://speedyk-005.github.io/chunklet-py/
  - One library to split them all: Sentence, Code, Docs. Chunk smarter, not harder
  - Supports over 50 natural languages for text and document chunking 
  - formats including .pdf, .docx, .epub, .txt, .tex, .html, .hml, .md, .rst, .rtf, .odt, .csv, and .xlsx.
  - Triple Interface: CLI, Library & Web
  - [[Release] Chunklet-py v2.1.0: Interactive Web Visualizer & Expanded File Support! : r/Rag](https://www.reddit.com/r/Rag/comments/1prjdjp/release_chunkletpy_v210_interactive_web/)
    - Our `CodeChunker` is rule-based and language-agnostic, using clever patterns to identify functions, classes, and logical blocks without the overhead of heavy dependencies like tree-sitter.
  - ["Docling vs Chunklet-py: Which Document Processing Library Should You Use?" : r/Rag _202511](https://www.reddit.com/r/Rag/comments/1p42qik/docling_vs_chunkletpy_which_document_processing/)
    - Key Difference: Docling focuses on document understanding with complex chunking options, while Chunklet-py focuses on accessible, intelligent chunking with superior metadata and universal language support.

- https://github.com/supermemoryai/code-chunk /MIT/202512/ts
  - AST-aware code chunking for semantic search and RAG pipelines.
  - Uses `tree-sitter` to split source code at semantic boundaries (functions, classes, methods) rather than arbitrary character limits. 
  - Each chunk includes rich context: scope chain, imports, siblings, and entity signatures.
  - Batch processing: Process entire codebases with controlled concurrency
  - Streaming: Process large files incrementally
  - [Building code-chunk: AST Aware Code Chunking _202512](https://supermemory.ai/blog/building-code-chunk-ast-aware-code-chunking/)

- https://github.com/xuzeyu91/AntSK-FileChunk /202512/python
  - ‰∏Ä‰∏™Âü∫‰∫éËØ≠‰πâÁêÜËß£ÁöÑÊô∫ËÉΩÊñáÊú¨ÂàáÁâáÊúçÂä°Ôºå‰∏ìÈó®Áî®‰∫éÂ§ÑÁêÜPDFÂíåWordÊñáÊ°£ÔºåËÉΩÂ§üÊ†πÊçÆÊÆµËêΩËØ≠‰πâËøõË°åÂêàÁêÜÂàáÁâáÔºåÈÅøÂÖç‰º†ÁªüÂü∫‰∫éTokenÊï∞ÈáèÂàáÂàÜÂØºËá¥ÁöÑËØ≠‰πâÂâ≤Ë£ÇÈóÆÈ¢ò„ÄÇ
  - ÊîØÊåÅPDF„ÄÅWordÔºà.docx/.docÔºâ„ÄÅÁ∫ØÊñáÊú¨Á≠âÂ§öÁßçÊñáÊ°£Ê†ºÂºè

- https://github.com/sovit-123/local_file_search /MIT/202510/python
  - Scripts to replicate simple file search and RAG in a directory with embeddings and Language Models.

- https://github.com/AlwaysSany/huggingface-local-embedding /MIT/202507/python
  - A Fast API server that provides local text and multi-modal embedding using LlamaIndex Hugging Face Embedding

- https://github.com/huggingface/text-embeddings-inference /apache2/202511/rust
  - A blazing fast inference solution for text embeddings models.
  - From scratch implementation, no Vector DBs yet.
  - Steps to Chat with Any PDF in Gradio UI

- https://github.com/mburaksayici/smallevals /202512/python
  - A lightweight evaluation framework powered by tiny 0.6B models ‚Äî runs 100% locally on CPU/GPU/MPS, attach any vector DB connection and run, fast and free.
  - Evaluation tools requiring LLM-as-a-judge or external, that costs/doesn't scale easily. 

- https://github.com/kannandreams/latentlens /202512/python
  - https://latentlens.streamlit.app/
  - a powerful visual debugger and educational tool for exploring vector embeddings. 
  - It helps you peek inside the "black box" of semantic search by projecting high-dimensional vectors into an interactive 3D map.
  - 3D Projection: PCA ‚Üí UMAP reduction into an interactive 3D scatter plot.
  - Multiple Embedders: Support for Demo (synthetic), local MiniLM (sentence-transformers), and OpenAI (text-embedding-3-small).
  - Flexible Storage: Ships with an in-memory/local `ChromaDB` adapter.
  - matplotlib (for heatmap rendering)

- https://github.com/PotentiallyARobot/EmbeddingAdapters /CC-NC/202512/python
  - Universal embedding-space translation library. 
  - Plug-and-play adapters that map one embedding model‚Äôs vector space into another ‚Äî locally or via API ‚Äî enabling cross-model retrieval, routing, and interoperability.
  - a lightweight Python library and model collection that lets you map embeddings from one model‚Äôs space into another‚Äôs.
  - [I built a Python library that translates embeddings from MiniLM to OpenAI _202512](https://www.reddit.com/r/Rag/comments/1py8l8f/i_built_a_python_library_that_translates/)
# search üîç
- https://github.com/tobi/qmd /5.1kStar/MIT/202602/python/ts
  - mini cli search engine for your docs, knowledge bases, meeting notes, whatever. 
  - Tracking current sota approaches while being all local
  - use it on the command line, it also exposes an MCP (Model Context Protocol) server for tighter integration.
# mobile-devices
- https://github.com/shubham0204/OnDevice-RAG-Android /aapche2/202601/kotlin
  - A simple Android app that allows the user to add a PDF/DOCX document and ask natural-language questions whose answers are generated by the means of an LLM (remote or on-device)
  - A custom RAG pipeline for multi-document QA from PDF/DOCX documents, in Android
# chat-docs/knowledgebase
- https://github.com/hyperbrowserai/hyperbooklm /MIT/202512/ts/‰ªÖ‰∫ëÁ´ØÊ®°Âûã
  - https://hyperbrowser.ai/
  - powerful research assistant built with Next.js 15, React 19, and Hyperbrowser. 
  - It allows users to aggregate diverse sources (Web URLs, PDFs) and gain deep insights through interactive AI tools.
  - Web Scraping: Hyperbrowser SDK
  - React Flow (Mindmaps)
  - https://x.com/hyperbrowser/status/2000655095764267050
    - Ask questions across all your sources

- https://github.com/MrSibe/KnowNote /54Star/GPL/202601/ts/Electron
  - ‰∏Ä‰∏™Êú¨Âú∞‰ºòÂÖàÁöÑÁü•ËØÜÁ¨îËÆ∞Â∑•ÂÖ∑ÔºåÁÅµÊÑüÊù•Ëá™ Google NotebookLM„ÄÇÂÆÉÂ∞Ü‰Ω†ÁöÑ PDF„ÄÅWord ÊñáÊ°£„ÄÅPowerPoint ÂíåÁΩëÈ°µËΩ¨Âåñ‰∏∫ÂèØÊèêÈóÆ„ÄÅÂèØÂºïÁî®„ÄÅÂèØËøΩÊ∫ØÁöÑ‰∏™‰∫∫Áü•ËØÜÂ∫ì„ÄÇ
  - Ëá™ÂÆö‰πâ LLM - ÊîØÊåÅ OpenAI„ÄÅClaude„ÄÅÊú¨Âú∞Ê®°ÂûãÁ≠âÂ§öÁßç AI ÊúçÂä°
  - ‰ΩøÁî® SQLite Êú¨Âú∞Êï∞ÊçÆÂ∫ìÔºåÂø´ÈÄüÂèØÈù†
  - sqlite-vec ¬∑ Drizzle ORM ¬∑ pdfjs-dist ¬∑ mammoth ¬∑ officeparser ¬∑ Tiptap
  - [I built a local-first NotebookLM-style app without Docker (Electron-based) : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ptj78l/i_built_a_localfirst_notebooklmstyle_app_without/)
    - One of the main reasons I started KnowNote was that many similar projects (including open-notebook) rely on Docker-based setups, which can be a bit intimidating for non-technical users.
    - My goal here was to explore a lighter, desktop-first approach using Electron, so people can just download and run it without dealing with containers.

- https://github.com/Panda-995/wechat-editor /apache2/202601/ts
  - ‰∏ì‰∏∫ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑Âàõ‰ΩúËÄÖÊâìÈÄ†ÁöÑÊûÅËá¥ÂÉèÁ¥†È£é Markdown ÁºñËæëÂô®ÔºåÂÜÖÁΩÆ Google Gemini AI Âº∫Â§ßÂä©Êâã
  - [ÂëäÂà´ÊéíÁâàÂú∞Áã±ÔºÅ‰∏çÂè™ÊòØNASÔºåÊõ¥ÊòØÂÜô‰ΩúÂä©Êâã„ÄÅÊéíÁâàÂ§ßÂ∏àÂíåÁæéÂ≠¶ËÆæËÆ°Â∏à ](https://linux.do/t/topic/1506017)
    - ‰∏ä‰∏™ÊúàÔºåÁÜäÁå´ÂºÄÊ∫ê‰∫ÜÂºÄÂèëÁöÑÂ¶ôÁ¨îÁîüËä±È°πÁõÆÔºå‰∏ªË¶ÅÊòØÂà©Áî®AIÊù•Ê£ÄÊµãÊñáÁ´†‰∏≠ÁöÑÈîôÂà´Â≠ó‰ª•ÂèäËØ≠Âè•ÈóÆÈ¢òÔºåËøô‰πüÊòØÁÜäÁå´‰πãÂâçË¢´Á≤â‰∏ùÊã∑ÊâìÂæàÂ§öÁöÑÈóÆÈ¢ò„ÄÇ‰ΩÜÂè™ÊúâÊ£ÄÊµãÁöÑÂäüËÉΩÂ∞±ÂØºËá¥ÊàëÂæàÂ§öÊó∂ÂÄôÂÜôÂÆåÂπ∂‰∏çÊÑøÊÑèÁî®ÂÆÉÂçïÁã¨ÂéªË∑ë‰∏ÄÈÅç
    - ÂéüÂõ†ËøòÊòØÂú®‰∫éÂÆÉÁöÑÂäüËÉΩÂ§™Â∞ë‰∫ÜÔºå‰∫éÊòØÂú®Â¶ôÁ¨îÁîüËä±ÁöÑÂü∫Á°Ä‰∏äÔºåÊúÄËøëÁÜäÁå´ÂèàÊäòËÖæ‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂºÄÊ∫êÈ°πÁõÆ-ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ÁºñËæëÂô®
    - Áõ¥Êé•ÂÅö‰∫Ü‰∏™Âú®Á∫øÁöÑÁºñËæëÂô®ÔºåÈ¶ñÈ°µËÉΩÁúãÂà∞ÊúÄÂ∑¶‰æßÊòØÊñáÁ´†ÂàóË°®ÔºåÂè≥ËæπÂàôÊòØÁºñËæëÂå∫ÂíåÈ¢ÑËßàÂå∫ÔºåÁºñËæëÂå∫ÊîØÊåÅMarkdownËØ≠Ê≥ïÔºåÂêåÊó∂ÁºñËæëÂå∫ÂíåÈ¢ÑËßàÂå∫ÊîØÊåÅÂêåÊ≠•ÊªöÂä®
  - https://github.com/Panda-995/ai-writing-assistant /202512/ts
    - Â¶ôÁ¨îÁîüËä±ÊòØ‰∏ÄÊ¨æÂü∫‰∫é Google Gemini Âíå OpenAI ÊäÄÊúØÁöÑÁé∞‰ª£ÂåñÊô∫ËÉΩÂÜô‰ΩúËæÖÂä©Â∑•ÂÖ∑
    - Êô∫ËÉΩÁ∫†Èîô‰∏éÊ∂¶Ëâ≤ÔºöËá™Âä®Ê£ÄÊµãÈîôÂà´Â≠ó„ÄÅËØ≠Ê≥ïÈîôËØØ„ÄÅÊ†áÁÇπËØØÁî®ÔºåÂπ∂Êèê‰æõ‰∏ì‰∏öÁöÑÊ∂¶Ëâ≤Âª∫ËÆÆ„ÄÇ
    - ÈÄªËæëÁªìÊûÑÂèØËßÜÂåñÔºöÂà©Áî® D3.js ÁîüÊàêÊñáÁ´†ÈÄªËæëÊ†ëÁä∂ÂõæÔºåÁõ¥ËßÇÂ±ïÁ§∫Ê†∏ÂøÉËÆ∫ÁÇπ‰∏éÊîØÊíëËÆ∫ÊçÆÁöÑÂ±ÇÊ¨°ÁªìÊûÑ„ÄÇ
    - Â§öÊ®°ÂûãÊîØÊåÅÔºöÈªòËÆ§ÊîØÊåÅ Google Gemini (Flash/Pro)ÔºåÂÖºÂÆπ OpenAI (GPT-4o) ÂèäÂÖ∂‰ªñÂÖºÂÆπÊé•Âè£„ÄÇ
    - AI SDK: @google/genai (Gemini 2.0/1.5)
    - Markdown Ê∏≤Êüì: react-markdown

- https://github.com/arc53/DocsGPT /17.4kStar/MIT/202511/python/ts/Êèê‰∫§Â§ö
  - https://app.docsgpt.cloud/
  - https://docsgpt.arc53.com/
  - GPT-powered chat for documentation, chat with your documents
  - open-source AI platform for building intelligent agents and assistants. 
  - ‰ΩøÁî®langchainÁöÑ‰ª£Á†Å‰∏çÂ§ö
  - Wide Format Support: Reads PDF, DOCX, CSV, XLSX, EPUB, MD, RST, HTML, MDX, JSON, PPTX, and images.
  - Web & Data Integration: Ingests from URLs, sitemaps, Reddit, GitHub and web crawlers.
  - Reliable Answers: Get accurate, hallucination-free responses with source citations 
  - Flexible Deployment: Works with major LLMs (OpenAI, Google, Anthropic) and local models (Ollama, llama_cpp).
  - Pre-built Integrations: Use readily available HTML/React chat widgets, search tools, Discord/Telegram bots, and more.
  - Secure & Scalable: Run privately and securely with Kubernetes support
  - Manually updating chunks in the app UI (Feb 2025)
  - [Show HN: DocsGPT, open-source documentation assistant, fully aware of libraries | Hacker News _202302](https://news.ycombinator.com/item?id=34648266)
  - [When ingesting data, ingest using chunks. _202302](https://github.com/arc53/DocsGPT/issues/54)
    - ‰ºº‰πéchunkingÁöÑÂÆûÁé∞Â≠òÂú®ÈóÆÈ¢ò

- https://github.com/PromtEngineer/localGPT /22kStar/MIT/202507/python/ts/Êèê‰∫§Â∞ë/inactive
  - LocalGPT is a fully private, on-premise Document Intelligence platform. 
  - Ask questions, summarise, and uncover insights from your files with AI‚Äîno data ever leaves your machine.
  - LocalGPT features a hybrid search engine that blends semantic similarity, keyword matching, and Late Chunking for long-context precision
  - A smart router automatically selects between RAG and direct LLM answering for every query, while contextual enrichment and sentence-level Context Pruning surface only the most relevant content
  - An independent verification pass adds an extra layer of accuracy.
  - The architecture is modular and lightweight‚Äîenable only the components you need. With a pure-Python core and minimal dependencies
  - üè† The RAG system is pure python and does not require any additional dependencies.
  - models via Ollama.
  - Chat History: Remembers your previous conversations (in a session).
  - API: LocalGPT has an API that you can use for building RAG Applications.
  - GPU, CPU, HPU & MPS Support
  - Multi-format Support: PDF, DOCX, TXT, Markdown, and more (Currently only PDF is supported)
  - Contextual Enrichment: Enhanced document understanding with AI-generated context, inspired by Contextual Retrieval
  - Batch Processing: Handle multiple documents simultaneously
  - Smart Routing: Automatically chooses between RAG and direct LLM responses
  - Query Decomposition: Breaks complex queries into sub-questions for better answers
  - Intuitive Web UI: Clean, responsive design
  - Real-time Chat: Streaming responses for immediate feedback
  - [Ingest larger documents (over 800MB) _202306](https://github.com/PromtEngineer/localGPT/discussions/112)
    - Im currently using a system with 16GB RAM, and 3 nvidia TESLA GPUs (16GBs each) but the localGPT only took 4-5 GB of GPU space.
    - it took some time to finish the ingesting (I kept it running for a day). But the inferences from the model takes some time. Any way to reduce that?
  - [I am running on CPU and I have 6.4GB worth of file _202309](https://github.com/PromtEngineer/localGPT/issues/448)
    - The pdf size is too large. Please try using a smaller pdf. or export relevant text out of it (reduce the pdf size) and then use it. For this much big pdf you will certainly need NVIDIA cuda cores for faster processing.

- https://github.com/Cinnamon/kotaemon /23kStar/apache2/202507/python/Êèê‰∫§Â∞ë/inactive
  - https://cinnamon.github.io/kotaemon/
  - https://huggingface.co/spaces/cin-model/kotaemon-demo
  - open-source clean & customizable RAG UI for chatting with your documents. Built with both end users and developers in mind.
  - This project serves as a functional RAG UI for both end users who want to do QA on their documents and developers who want to build their own RAG pipeline.
  - Minimalistic UI: A user-friendly interface for RAG-based QA.
  - Support for Various LLMs: Compatible with LLM API providers (OpenAI, AzureOpenAI, Cohere, etc.) and local LLMs (via `ollama` and `llama-cpp-python`).
  - Framework for RAG Pipelines: Tools to build your own RAG-based document QA pipeline.
  - Customizable UI: See your RAG pipeline in action with the provided UI, built with `Gradio`.
  - Host your own document QA (RAG) web-UI: Support multi-user login, organize your files in private/public collections, collaborate and share your favorite chat with others.
  - Hybrid RAG pipeline: Sane default RAG pipeline with hybrid (full-text & vector) retriever 
  - Advanced citations with document preview: By default the system will provide detailed citations to ensure the correctness of LLM answers.
  - Extensible: Being built on `Gradio`, you are free to customize or add any UI elements as you like.
  - require `Unstructured` if you want to process files other than .pdf, .html, .mhtml, and .xlsx documents.
    - We support both `lite` & `full` version of Docker images. With `full` version, the extra packages of `unstructured` will be installed, which can support additional file types (`.doc, .docx`, ...)
  - Kotaemon has 3 GraphRAG options (NanoGraphRAG, LightRAG, and MS's hosted GraphRAG)
  - [Kotaemon: An open-source RAG-based tool for chatting with your documents | Hacker News _202501](https://news.ycombinator.com/item?id=42571272)
  - [Kotaemon-papers: an open-source web app to chat with your academic papers | Hacker News _202501](https://news.ycombinator.com/item?id=42603357)
    - Our team has been working on a public demo to showcase the new advanced citation features in our RAG
  - [[BUG] Unable to index document in docker ](https://github.com/Cinnamon/kotaemon/issues/539)
    - I think its PDF file size thing. Doesn't seem to happen on PDFs with <100 pages...
  - [[BUG] - Local Model LLM works, but not Embedding _202409](https://github.com/Cinnamon/kotaemon/issues/240)
    - I tried to use the base URL that works for me in AnythingLLM but it still didn't work in kotaemon. 

- https://github.com/h2oai/h2ogpt /11.9kStar/apache2/202503/python/inactive
  - http://h2o.ai/
  - [Gradio Demo](https://gpt.h2o.ai/)
  - [OpenWebUI Demo](https://gpt-docs.h2o.ai/)
  - Query and summarize your documents or just chat with local private GPT LLMs using h2oGPT
  - Private offline database of any documents (PDFs, Excel, Word, Images, Video Frames, YouTube, Audio, Code, Text, MarkDown, etc.)
  - Persistent database (Chroma, Weaviate, or in-memory FAISS) using accurate embeddings (instructor-large, all-MiniLM-L6-v2, etc.)
  - Efficient use of context using instruct-tuned LLMs (no need for LangChain's few-shot approach)
  - Parallel summarization and extraction, reaching an output of 80 tokens per second with the 13B LLaMa2 model
  - Gradio UI or CLI with streaming of all models
  - [Feature request - to add support for nomic-ai/nomic-embed-text-v1 embeddings _202402](https://github.com/h2oai/h2ogpt/issues/1418)
    - 83333 is very large. I made the max 4096. Or you can control via env `CHROMA_MAX_BATCH_SIZE`.
    - 4096 is on high end, yes can make it smaller as required. If on CPU I expect should work pretty ok, but issue is bge-m3 has 8k context so uses alot more memory despite size if chunks are large.

- https://github.com/khoj-ai/khoj /31.6kStar/AGPLv3/202511/python/ts
  - https://khoj.dev/
  - Khoj is an application that creates always-available, personal AI agents for you to extend your capabilities
  - Your AI agents have access to the internet, allowing you to incorporate realtime information.
  - Khoj is accessible on Desktop, Emacs, Obsidian, Web and Whatsapp.
  - Khoj is open-source, self-hostable. Always.
  - ÊîØÊåÅÁßÅÊúâÂåñÈÉ®ÁΩ≤„ÄÅÊú¨Âú∞Ê®°Âûã„ÄÅ‰∏é Obsidian Êó†ÁºùÊï¥Âêà
  - ÊîØÊåÅ github„ÄÅpdf/markdown„ÄÅnotion Á≠âÂÜÖÂÆπÊ∫ê

- https://github.com/GitHamza0206/simba /1.4kStar/apache2/202505/python/jupyter/inactive
  - https://simba.mintlify.app/
  - Portable KMS (knowledge management system) designed to integrate seamlessly with any RAG
  - Modular Architecture: Flexible integration of vector stores, embedding models, chunkers, and parsers.

- https://github.com/labring/FastGPT /25.5kStar/apache2+LOGO+nonTenant/202508/ts
  - https://fastgpt.io/
  - ‰∏Ä‰∏™ AI Agent ÊûÑÂª∫Âπ≥Âè∞ÔºåÊèê‰æõÂºÄÁÆ±Âç≥Áî®ÁöÑÊï∞ÊçÆÂ§ÑÁêÜ„ÄÅÊ®°ÂûãË∞ÉÁî®Á≠âËÉΩÂäõÔºåÂêåÊó∂ÂèØ‰ª•ÈÄöËøá Flow ÂèØËßÜÂåñËøõË°åÂ∑•‰ΩúÊµÅÁºñÊéíÔºå‰ªéËÄåÂÆûÁé∞Â§çÊùÇÁöÑÂ∫îÁî®Âú∫ÊôØ
  - È°πÁõÆÊäÄÊúØÊ†àÔºöNextJs + TS + ChakraUI + MongoDB + PostgreSQL (PG Vector Êèí‰ª∂)/Milvus
  - ‚öñÔ∏è License
    - ÂÖÅËÆ∏‰Ωú‰∏∫ÂêéÂè∞ÊúçÂä°Áõ¥Êé•ÂïÜÁî®Ôºå‰ΩÜ‰∏çÂÖÅËÆ∏Êèê‰æõ SaaS ÊúçÂä°

- https://github.com/QuivrHQ/quivr /38.7kStar/apache2/202506/python/inactive
  - https://core.quivr.com/
  - Opiniated RAG for integrating GenAI in your apps
  - Opiniated RAG: We created a RAG that is opinionated, fast and efficient so you can focus on your product
  - LLMs: Quivr works with any LLM, you can use it with OpenAI, Anthropic, Mistral, Gemma, etc.
  - Any File: Quivr works with any file, you can use it with PDF, TXT, Markdown, etc and even add your own parsers.
  - Customize your RAG: Quivr allows you to customize your RAG, add internet search, add tools, etc.

- https://github.com/dontizi/rlama /1.1kStar/apache2/202505/go/js/inactive
  - https://rlama.dev/
  - a powerful AI-driven question-answering tool for your documents, seamlessly integrating with your local Ollama models.
  - It enables you to create, manage, and interact with Retrieval-Augmented Generation (RAG) systems tailored to your documentation needs.
  - This project is currently on pause due to my work and university commitments that take up a lot of my time
  - [RLAMA -- A document AI question-answering tool that connects to your local Ollama models. : r/ollama _202503](https://www.reddit.com/r/ollama/comments/1j66pg3/rlama_a_document_ai_questionanswering_tool_that/)
    - I developed RLAMA to solve a straightforward but frustrating problem: how to easily query my own documents with a local LLM without using cloud services.
    - RLAMA extracts text from the documents and generates embeddings via Ollama
    - it uses Tesseract OCR to extract text from image-based PDFs or scanned documents where text can't be directly selected.

- https://github.com/morphik-org/morphik-core /3.2kStar/BSL/202508/python/ts
  - https://morphik.ai/docs
  - The most accurate document search and store for building AI apps
  - We are building the best way for developers to integrate context (however complex and nuanced) into their AI applications. 
  - Morphik provides developers the tools to ingest, search (deep and shallow), transform, and manage unstructured and multimodal documents.
  - Multimodal Search: We employ techniques such as ColPali to build search that actually understands the visual content of documents you provide. Search over images, PDFs, videos, and more

- https://github.com/jorge-armando-navarro-flores/chat_with_your_docs /MIT/202409/python
  - ChatWithYourDocs Chat App is a Python application that allows you to chat with multiple Docs formats like PDF, WEB pages and YouTube videos. 
  - This app utilizes a language model to generate accurate answers to your queries

- https://github.com/zylon-ai/private-gpt /56.5kStar/apache2/202409/python/inactive
  - a production-ready AI project that allows you to ask questions about your documents using LLM
  - Conceptually, PrivateGPT is an API that wraps a RAG pipeline and exposes its primitives.
    - The API is built using FastAPI and follows OpenAI's API scheme.
    - The RAG pipeline is based on LlamaIndex.
  - The design of PrivateGPT allows to easily extend and adapt both the API and the RAG implementation.
    - Dependency Injection, decoupling the different components and layers.
    - Usage of `LlamaIndex` abstractions such as `LLM, BaseEmbedding or VectorStore`, making it immediate to change the actual implementations of those abstractions.
    - Ready to use, providing a full implementation of the API and RAG pipeline.
  - The project provides an API offering all the primitives required to build private, context-aware AI applications. 
  - It follows and extends the OpenAI API standard, and supports both normal and streaming responses.
  - [Ollama Embedding Fails with Large PDF files _202410](https://github.com/zylon-ai/private-gpt/discussions/2097)
  - [Ingesting large number of files _202402](https://github.com/zylon-ai/private-gpt/issues/1660)
    - Its using SQLLite as its using simple (filesystem based) indexes and doc stores. Once this #1706 is pulled, those things can be moved into Postgres.

- https://github.com/thiswillbeyourgithub/WDoc /478Star/GPL/202511/python
  - https://wdoc.readthedocs.io/en/stable/
  - Summarize and query from a lot of heterogeneous documents. 
  - Any LLM provider, any filetype, advanced RAG, advanced summaries, scriptable, etc
  - Created by a medical student who needed a way to get a definitive answer from multiple sources at the same time (audio recordings, video lectures, Anki flashcards, PDFs, EPUBs, etc.). wdoc was born from frustration with existing RAG solutions for querying and summarizing.
  - It uses mostly `LangChain` and `LiteLLM` as backends.
  - High recall and specificity: it was made to find A LOT of documents using carefully designed embedding search then carefully aggregate gradually each answer using semantic batch to produce a single answer that mentions the source pointing to the exact portion of the source document.
    - Use both an expensive and cheap LLM to make recall as high as possible because we can afford fetching a lot of documents per query (via embeddings)
  - Extensible: this is both a tool and a library. It was even turned into an Open-WebUI Tool
  - Local and Private LLM
  - Web Search: Preliminary web search support using DuckDuckGo (via the ddgs library)
  - Fast: Parallel document loading, parsing, embeddings, querying, etc.
  - Roadmap
    - figure out a good way to skip merging batches that are too large before trying to merge them

- https://github.com/kqlade/dory-frontend /AGPL/202504/ts/inactive
  - [Dory ‚Äì AI Knowledge Base Powered by Browser History | Hacker News _202504](https://news.ycombinator.com/item?id=43619021)
  - Dynamic Online Recall for You (DORY) helps you find anything you've seen before online using whatever you remember about it. 
  - DORY also builds a cognitive context graph to organize your work into auto-updating workflows and serve them to you in real time based on your browsing context.
  - DORY uses Neo4j to store browsing data with weighted edge relationships (semantic, behavioral, temporal) then applies stochastic block modeling for community detection and temporal-behavioral profiling.
  - It's an approach similar to how banks analyze transaction networks to detect fraud but works surprisingly well for teasing apart browser workflows.

- https://github.com/Zipstack/unstract /5.9kStar/AGPL/202511/python
  - https://unstract.com/
  - No-code LLM Platform to launch APIs and ETL Pipelines to structure unstructured documents

- https://github.com/freeCodeCamp/devdocs /37.9kStar/MPL/202511/ruby
  - https://devdocs.io/
  - DevDocs combines multiple developer documentations in a clean and organized web UI with instant search, offline support, mobile version, dark theme, keyboard shortcuts, and more
  - DevDocs was created by Thibaut Courouble and is operated by freeCodeCamp.

- https://github.com/1Panel-dev/MaxKB /15.5kStar/GPL/202504/python/ts/vue
  - https://maxkb.pro/
  - Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÂºÄÊ∫êÁü•ËØÜÂ∫ìÈóÆÁ≠îÁ≥ªÁªü
  - it is a ready-to-use RAG chatbot that features robust workflow and MCP tool-use capabilities. 
  - ‰æùËµñdjango„ÄÅlangchain„ÄÅpgvector„ÄÅVue
  - Flexible Orchestration: Equipped with a powerful workflow engine, function library and MCP tool-use, enabling the orchestration of AI processes to meet the needs of complex business scenarios.

- https://github.com/chaitin/PandaWiki /8.3kStar/AGPL/202512/go/ts
  - https://pandawiki.docs.baizhi.cloud/
  - AI Â§ßÊ®°ÂûãÈ©±Âä®ÁöÑÂºÄÊ∫êÁü•ËØÜÂ∫ìÊê≠Âª∫Á≥ªÁªüÔºåÂ∏ÆÂä©‰Ω†Âø´ÈÄüÊûÑÂª∫Êô∫ËÉΩÂåñÁöÑ ‰∫ßÂìÅÊñáÊ°£„ÄÅÊäÄÊúØÊñáÊ°£„ÄÅFAQ„ÄÅÂçöÂÆ¢Á≥ªÁªüÔºåÂÄüÂä©Â§ßÊ®°ÂûãÁöÑÂäõÈáè‰∏∫‰Ω†Êèê‰æõ AI Âàõ‰Ωú„ÄÅAI ÈóÆÁ≠î„ÄÅAI ÊêúÁ¥¢Á≠âËÉΩÂäõ„ÄÇ
  - Âº∫Â§ßÁöÑÂØåÊñáÊú¨ÁºñËæëËÉΩÂäõÔºöÂÖºÂÆπ Markdown Âíå HTMLÔºåÊîØÊåÅÂØºÂá∫‰∏∫ word„ÄÅpdf„ÄÅmarkdown Á≠âÂ§öÁßçÊ†ºÂºè„ÄÇ
  - ËΩªÊùæ‰∏éÁ¨¨‰∏âÊñπÂ∫îÁî®ËøõË°åÈõÜÊàêÔºöÊîØÊåÅÂÅöÊàêÁΩëÈ°µÊåÇ‰ª∂ÊåÇÂú®ÂÖ∂‰ªñÁΩëÁ´ô‰∏äÔºåÊîØÊåÅÂÅöÊàêÈíâÈíâ„ÄÅÈ£û‰π¶„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Á≠âËÅäÂ§©Êú∫Âô®‰∫∫„ÄÇ
  - ÈÄöËøáÁ¨¨‰∏âÊñπÊù•Ê∫êÂØºÂÖ•ÂÜÖÂÆπÔºöÊ†πÊçÆÁΩëÈ°µ URL ÂØºÂÖ•„ÄÅÈÄöËøáÁΩëÁ´ô Sitemap ÂØºÂÖ•„ÄÅÈÄöËøá RSS ËÆ¢ÈòÖ„ÄÅÈÄöËøáÁ¶ªÁ∫øÊñá‰ª∂ÂØºÂÖ•Á≠â„ÄÇ

- https://github.com/signerlabs/klee /1.7kStar/MIT/202511/ts/inactive
  - a modern desktop application that combines AI-powered chat, knowledge base management, and note-taking capabilities.
  - It offers both Cloud Mode for seamless synchronization and Private Mode for complete offline functionality.
  - Integrated with OpenAI and local Ollama models
  - Tiptap-based collaborative editor with Markdown support
  - At its core, Klee is built on:
    - `Ollama`: For running local LLMs quickly and efficiently.
    - `LlamaIndex`: As the data framework.
  - Privacy-First: Complete offline mode with local AI and data storage
  - Optional cloud synchronization via Supabase
  - Private Mode
    - Local AI: Powered by Ollama (embedded or system-installed)
    - Local Storage: SQLite for structured data
    - Vector Search: LanceDB for semantic search (planned)
  - Cloud Mode
    - File Storage: Supabase Storage for documents and attachments
    - Data Sync: PostgreSQL database with real-time updates
    - Authentication: Google OAuth and email/password via Supabase
  - üç¥ forks
  - https://github.com/EclipseFever/Klee /202504
    - electron-build-ci: https://github.com/EclipseFever/Klee/blob/main/.github/workflows/build.yml
  - [I built and open sourced a electron app to run LLMs locally with built-in RAG knowledge base and note-taking capabilities. : r/electronjs _202503](https://www.reddit.com/r/electronjs/comments/1j43om9/i_built_and_open_sourced_a_electron_app_to_run/)
    - Would the users need to download Ollama separately or is it somehow embedded?
      - It is embedded, but if you already have ollama running, we will use your ollama instance.
    - We start with SwiftUI but switch to Electron after 3 weeks
  - [I built and open sourced a desktop app to run LLMs locally with built-in RAG knowledge base and note-taking capabilities. : r/LocalLLM _202503](https://www.reddit.com/r/LocalLLM/comments/1j4wvsu)
  - [I open-sourced Klee today, a desktop app designed to run LLMs locally with ZERO data collection. It also includes built-in RAG knowledge base and note-taking capabilities. : r/LocalLLaMA _202503](https://www.reddit.com/r/LocalLLaMA/comments/1j2j7su/i_opensourced_klee_today_a_desktop_app_designed/)
    - I see you were inspired by Slack's UI
    - Can I just point it at the folder with my existing models?
      - If you use windows, look up junctions and symbolic links: mklink /J C:\LinkDirectory D:\TargetDirectory
    - When I've used Ollama I find it's not just the file location; it requires turning the GGUF models into some hashed 'model file', which is exactly why I quit using Ollama.
    - Does this force Ollama? Or can I use llama.cpp as a backend?
      - backend and front end are in different repo, you can use llamacpp as backend
    - What key features differentiate this from those options like lmstudio/koboldai?
      - Nothing. Just yet another wrapper over ollama.
    - What does Klee use for embeddings for the RAG? does it support directory/folder upload or just individual file upload?
      - individual file, multiple files and folder
    - Would also like to know if it allows users to connect to an existing ollama instance over LAN? Would it allow me to connect to my ollama API on my network? So I can use this on my laptop and connect to my AI server in the basement? Can you put a custom IP/port? 
      - this is the most requested feature 

- https://github.com/OpenDCAI/Paper2Any /1.2kStar/apache2/202601/python/ts
  - http://dcai-paper2any.nas.cpolar.cn/
  - From paper PDFs / images / text to editable scientific figures, slide decks, video scripts, academic posters, and other multimodal content in one click.
  - Paper2Figure - Editable Scientific Figures: One-click generation of model architecture diagrams, technical roadmaps (PPT + SVG), and experimental plots. Supports multiple input sources and outputs editable PPTX.
  - Paper2PPT - Editable Slide Decks: Generate PPTs in arbitrary styles, support ultra-long document processing, and include built-in table extraction and chart parsing.
  - PDF2PPT - Layout-Preserving Conversion: Intelligent cutout and layout analysis to accurately convert PDFs into editable PPTX.
  - MinerU (PDF Parsing)
  - OCR (PaddleOCR)
  - SAM (Segment Anything Model)
# chat-excel
- https://github.com/huggingface/aisheets /1.5kStar/apache2/202510/python/ts
  - https://huggingface.co/spaces/aisheets/sheets
  - an open-source tool for building, enriching, and transforming datasets using AI models with no code. 
  - The tool can be deployed locally or on the Hub. 
  - By default, AI Sheets is configured to use the Huggingface Inference Providers API to run inference on the latest open-source models. However, you can also run Sheets with own custom LLMs, such as those hosted on your own infrastructure or other cloud providers. The only requirement is that your LLMs must support the OpenAI API specification.
  - This app has a minimal Expressjs server implementation

- https://github.com/weijunext/smart-excel-ai /MIT/202312/ts
  - https://smartexcel.cc/
  - Generate the Excel formulas you need in seconds using ChatGPT
  - https://twitter.com/weijunext/status/1744225202228089173
    - ËøôÊòØ‰∏Ä‰∏™Ë∂≥Â§üÁÆÄÂçïÔºàË∞ÉÁî®ChatGPTÁöÑAPIÔºâÂç¥ÂèàÂäüËÉΩ‰ø±ÂÖ®ÔºàÊúâÁôªÂΩïÂíåÊîØ‰ªòÔºâÁöÑdemoÁ∫ß‰∫ßÂìÅ„ÄÇ
    - ÂâçÂêéÁ´ØÔºöNext.js+Tailwind+Prisma
    - ÁôªÂΩïÔºöNext-Auth
    - ÊîØ‰ªòÔºöLemon Squeezy
    - ÈÉ®ÁΩ≤ÔºöVercel
# chat-pdf
- https://github.com/weiwill88/Local_Pdf_Chat_RAG /202510/python/inactive
  - Êú¨Âú∞ÂåñÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªü (FAISSÁâà)
  - Ê∑∑ÂêàÊ£ÄÁ¥¢ÔºöÁªìÂêàFAISSËøõË°åËØ≠‰πâÊ£ÄÁ¥¢ÂíåBM25ËøõË°åÂÖ≥ÈîÆËØçÊ£ÄÁ¥¢ÔºåÊèêÈ´òÊ£ÄÁ¥¢Âè¨ÂõûÁéáÂíåÂáÜÁ°ÆÊÄß
  - ÁªìÊûúÈáçÊéíÂ∫èÔºöÊîØÊåÅ‰∫§ÂèâÁºñÁ†ÅÂô®ÔºàCrossEncoderÔºâÂíåLLMÂØπÊ£ÄÁ¥¢ÁªìÊûúËøõË°åÈáçÊéíÂ∫èÔºå‰ºòÂåñÁõ∏ÂÖ≥ÊÄß
  - ÂèØÈÄâÊã©‰ΩøÁî®Êú¨Âú∞OllamaÂ§ßÊ®°ÂûãÔºàÂ¶ÇDeepSeek-R1Á≥ªÂàóÔºâÊàñ‰∫ëÁ´ØSiliconFlow APIËøõË°åÊé®ÁêÜ
  - Âü∫‰∫éGradioÊûÑÂª∫‰∫§‰∫íÂºèWebÁïåÈù¢ÔºåÊîØÊåÅÂ§öÁßç‰∏ªÈ¢òÁïåÈù¢

- https://github.com/shibing624/ChatPDF /801Star/apache2/202409/python/inactive
  - Á∫ØÂéüÁîüÂÆûÁé∞RAGÂäüËÉΩÔºåÂü∫‰∫éÊú¨Âú∞LLM„ÄÅembeddingÊ®°Âûã„ÄÅrerankerÊ®°ÂûãÂÆûÁé∞ÔºåÊîØÊåÅGraphRAGÔºåÊó†È°ªÂÆâË£Ö‰ªª‰ΩïÁ¨¨‰∏âÊñπagentÂ∫ì„ÄÇ
  - Êú¨È°πÁõÆÂÆûÁé∞‰∫ÜËΩªÈáèÁâàÁöÑGraphRAG
  - ÊîØÊåÅlocalÊ®°ÂºèÁöÑÂÖ≥Á≥ªÂõæÊ£ÄÁ¥¢ÁöÑÊñáÊ°£ÈóÆÁ≠î
  - ÊîØÊåÅOpenai API, Deepseek API, Ollama APIÁ≠âÔºåÂèØËá™Ë°åÊâ©Â±ïÊîØÊåÅÊõ¥Â§öLLM
  - ÊîØÊåÅopenai embedding„ÄÅÊú¨Âú∞ text2vec embedding„ÄÅhuggingface embedding„ÄÅsentence-transformers embeddingÁ≠â
  - Êú¨È°πÁõÆÊîØÊåÅÂ§öÁßçÊñá‰ª∂Ê†ºÂºèÔºåÂåÖÊã¨PDF„ÄÅdocx„ÄÅmarkdown„ÄÅtxtÁ≠â
  - ‰ºòÂåñ‰∫ÜRAGÂáÜÁ°ÆÁéá: Chinese chunkÂàáÂàÜ‰ºòÂåñÔºåÈÄÇÈÖç‰∏≠Ëã±ÊñáÊ∑∑ÂêàÊñáÊ°£
  - Êú¨È°πÁõÆÂü∫‰∫é`gradio`ÂºÄÂèë‰∫ÜRAGÂØπËØùÈ°µÈù¢ÔºåÊîØÊåÅÊµÅÂºèÂØπËØù

- https://github.com/mayooear/ai-pdf-chatbot-langchain /15.8kStar/MIT/202502/ts/inactive
  - PDF chatbot agent built with LangChain & LangGraph
  - This monorepo is a customizable template example of an AI chatbot agent that "ingests" PDF documents, stores embeddings in a vector database (Supabase), and then answers user queries using OpenAI (or another LLM provider) utilising LangChain and LangGraph as orchestration frameworks.
  - This template is also an accompanying example to the book Learning LangChain (O'Reilly): Building AI and LLM applications with LangChain and LangGraph.
  - [Support for BIG PDFs?(like 500pages) ](https://github.com/mayooear/ai-pdf-chatbot-langchain/issues/409)
    - you must be mindful of the pinecone.io free tier's pod capacity, which may be exceeded
  - https://github.com/mayooear/ai-pdf-chatbot-langchain/tree/feat/chroma

- https://github.com/williamcaseylucas/quill-pdf-chat /202401/ts
  - ‰æùËµñprisma, shadcn-ui, react-pdf, AWS S3 buckets
  - https://github.com/khaledmk20/quill
    - Quill redefines PDF interaction with intelligent conversations.

- https://github.com/Byaidu/PDFMathTranslate /29.8kStar/AGPLv3/202511/python 
  - https://pdf2zh.com/
  - Âü∫‰∫é AI ÂÆåÊï¥‰øùÁïôÊéíÁâàÁöÑ PDF ÊñáÊ°£ÂÖ®ÊñáÂèåËØ≠ÁøªËØëÔºåÊîØÊåÅ Google/DeepL/Ollama/OpenAI Á≠âÊúçÂä°ÔºåÊèê‰æõ CLI/GUI/MCP/Docker/Zotero
  - Preserve formulas, charts, table of contents, and annotations
  - Support multiple languages, and diverse translation services.
  - https://github.com/PDFMathTranslate/PDFMathTranslate-next /AGPLv3
    - pdf2zh 2.0 does not currently provide an online demo

- https://github.com/bhaskatripathi/pdfGPT /7.2kStar/MIT/202306/python/‰ª£Á†ÅÂ∞ë/inactive
  - chat with the contents of your PDF file by using GPT capabilities. 
  - When you pass a large text to Open AI, it suffers from a 4K token limit. It cannot take an entire pdf file as an input
  - The application intelligently breaks the document into smaller chunks and employs a powerful Deep Averaging Network Encoder to generate embeddings.
  - A semantic search is first performed on your pdf content and the most relevant embeddings are passed to the Open AI.
  - https://github.com/tuxxon/PDFGPT /202408/inactive
    - I rebuilt it because I thought this repository was no longer being maintained.
# chat-workspace/notebooklm
- https://github.com/lfnovo/open-notebook /13kStar/MIT/202512/python/ts/Êèê‰∫§Â∞ë
  - https://www.open-notebook.ai/
  - Open Source implementation of Notebook LM with more flexibility and features
  - A private, multi-model, 100% local, full-featured alternative to Notebook LM
  - Choose your AI models - Support for 16+ providers including OpenAI, Anthropic, Ollama, LM Studio, and more
  - ‰æùËµñSurrealDB„ÄÅfatspi„ÄÅnextjs
  - Organize multi-modal content - PDFs, videos, audio, web pages, and more
  - üêõ: no citation, no connector
  - [Our target for starting strong in 2026 _202601](https://github.com/lfnovo/open-notebook/discussions/375)
    - Here's what's on our radar:
    - RAG & Context Mechanism: We inject full source content into conversations. This overloads models and produces subpar responses
    - SurrealDB: Transaction conflicts under load, production readiness questions.Our decision: We're staying with SurrealDB. It gives us document storage, graph relationships, vector embeddings, and background jobs ‚Äî all in one. The alternative (Postgres + Redis + Celery + vector DB) doesn't align with "easy self-hosting."
  - https://github.com/lfnovo/esperanto
    - Python library that provides a unified interface for interacting with various Large Language Model (LLM) providers.
    - All providers communicate directly via HTTP APIs using httpx - no bulky vendor SDKs required

- https://github.com/eclaire-labs/eclaire /728Star/MIT/202512/ts
  - https://eclaire.co/
  - Local-first, open-source AI assistant for your data. Unify tasks, notes, docs, photos, and bookmarks. 
  - Private by default: By default all AI models run locally, all data is stored locally.
  - Layered architecture: frontend, backend, and workers are separate services. Run only the backend for API-only/data-processing use cases
    - Backend for AI assistant (eg. Qwen3 model), 
    - Workers for image and document processing (eg. Gemma3 multi-modal). 
    - Docling for processing some of the document formats.
  - Model backends: works with llama.cpp, vLLM, mlx-lm/mlx-vlm, LM Studio, Ollama, and more via the standard OpenAI-compatible API.
  - Storage: all assets (uploaded or generated) live in Postgres or file/object storage.
  - Documents: PDF, DOC/DOCX, PPT/PPTX, XLS/XLSX, ODT/ODP/ODS, MD, TXT, RTF, Pages, Numbers, Keynote, HTML, CSV, and more.
  - [AI Model Configuration](https://github.com/eclaire-labs/eclaire/blob/main/docs/ai-models.md)
    - ‰æùËµñÂ§ñÈÉ®backendÊù•ÊúüÂæÖllm api

- https://github.com/rmusser01/tldw_server /1.1kStar/GPL/202511/python
  - https://tldwproject.com/
  - Too Long; Didn't Watch - API-first media analysis & research platform
  - tldw_server is an open-source research multi-tool / backend for ingesting, transcribing, analyzing, and retrieving knowledge from video, audio, documents, websites, and more.
  - It consists of a FastAPI API-first server architecture backed by SQLite or Postgres depending on user choice, with OpenAI-compatible Chat and Audio APIs, a unified RAG pipeline, knowledge management, and integrations with local or hosted LLM providers (with cost/usage tracking).
  - https://github.com/the-crypt-keeper/tldw /inactive

- https://github.com/souzatharsis/podcastfy /5.6kStar/apache2/202510/python
  - Open Source Python alternative to NotebookLM's podcast feature: Transforming Multimodal Content into Captivating Multilingual Audio Conversations with GenAI
  - https://github.com/gabrielchua/open-notebooklm
    - Convert any PDF into a podcast episode

- https://github.com/run-llama/notebookllama /1.6kStar/MIT/202508/python/inactive
  - open-source, LlamaCloud-backed alternative to NotebookLM
  - [NotebookLlama: An open source version of NotebookLM | Hacker News _202410](https://news.ycombinator.com/item?id=41964980)

- https://github.com/xynehq/xyne /635Star/apache2/202511/ts
  - https://xynehq.com/
  - AI-first Search & Answer Engine for work. 
  - Open-source alternative to Glean, Gemini and MS Copilot
  - Your work information has become fragmented ‚Äî across so many SaaS apps, docs, files, repos
  - Xyne connects to your applications (Google Workspace, Atlassian suite, Slack, Github, etc), securely indexes your data, and maps a graph of relationships.
  - üêõ ÂøÖÈ°ª‰ΩøÁî®googleË¥¶Êà∑ÁôªÂΩïÊâçËÉΩ‰ΩøÁî®?
    - ‰ºº‰πé‰∏çÊîØÊåÅ‰∏ä‰º†Êú¨Âú∞pdfÊñá‰ª∂
  - Model Agnostic: Plugs into any LLM of your choice. You can even point it to a local Deepseek via ollama.
  - ‰ΩøÁî®Ëá™Á†îaiÊ°ÜÊû∂ jaf  ÁªìÂêà aisdk
  - https://github.com/xynehq/jaf
    - functional agent framework built on immutable state, type safety, and composable policies.

- https://github.com/deta/surf /2.7kStar/apache2/202511/rust/ts/svelte/inactive
  - https://deta.surf/
  - Personal AI Notebooks. Organize files & webpages and generate notes from them. 
  - an AI notebook that brings all your files and the web directly into your stream of thought.
  - built in Svelte, TypeScript and Rust, runs on MacOS, Windows & Linux, stores data locally in open formats 
  - PDF Notes: open a PDF and ask a question
  - Create an applet: use the "app generation" tool and ask for an app
  - Êï¥‰Ωì‰ΩìÈ™åÊòØ Notion + AI
  - üì° [Optimized Embeddings CPU Usage _202510](https://github.com/deta/surf/pull/43)
    - Implemented Lazy Embeddings for Large Document Types
    - Embeddings will then be generated on-demand when documents are accessed in chat/search
    - Larger chunks (2000 ‚Üí 2500): fewer embeddings to generate and store
  - [[FEATURE] Migrate to a better browser engine _202510](https://github.com/deta/surf/issues/27)
    - We understand the pros and cons of using Electron, and for the moment the pros outweigh the cons, specially for a small team.
  - [Unable to drag window on Mac ](https://github.com/deta/surf/issues/94)
    - ÂøÖÈ°ªÂ∞ÜÂÖâÊ†áÁßªÂà∞È°∂ÈÉ®resizeÊîπÂèòÁ™óÂè£È´òÂ∫¶Êó∂ÔºåÊâçÂèØ‰ª•ÁßªÂä®Á™óÂè£
  - [Show HN: Deta Surf ‚Äì An open source and local-first AI notebook | Hacker News _202510](https://news.ycombinator.com/item?id=45680937)
    - We took inspiration from analog notebooks as a tool for thought, but wanted something for multi-media. We also see NotebookLM as the closest mainstream product to Surf.
    - üÜöüìù The big difference UX wise between chatbots and Surf is that Surf is built entirely on editable documents that you can mold / craft into an output (vs chat).
      - We actually had a chatbot, but our explorations showed that notes were a more effective in many cases!
    - Is this an open source equivalent to google‚Äôs NotebookLM? I can tell. How does it stack up features wise?
      - Surf is built entirely on editable WYSIWYG documents, NotebookLM's main AI is built on chat. Surf is built to be a bit more open, NotebookLM was a bit locked down for our taste.
      - An example I'd highlight is taking notes against a PDF.
      - NotebookLM will convert the PDF to simple text, and the chat responses are read only. NotebookLM also has a lot of strict walls between chat, artifacts & sources. You have to "save" responses as (read only) notes, and move notes to sources.
      - With Surf you can generate notes that deep link to specific pages in the PDF, and Surf will open those pages in the original PDF. You can remove the fluff you don't want in your notes. The intention is to be a little more open -- all notes are sources from the get go, you don't have to save or migrate anything.

- https://github.com/ItzCrazyKns/Perplexica /27.3kStar/MIT/202511/ts
  - Open source alternative to Perplexity AI
  - a privacy-focused AI answering engine that runs entirely on your own hardware. It combines knowledge from the vast internet with support for local LLMs (Ollama) and cloud providers (OpenAI, Claude, Groq), delivering accurate answers with cited sources while keeping your searches completely private
  - Web search powered by SearxNG - Support for Tavily and Exa coming soon
  - Upload documents and ask questions about them. PDFs, text files, images - Perplexica understands them all.
    - Êú™‰ΩøÁî®docling/libreoffice
  - Get intelligent search suggestions as you type, helping you formulate better queries.
  - Perplexica also provides an API for developers looking to integrate its powerful search engine into their own applications.
  - Perplexica runs on Next.js and handles all API requests.
  - üêõ ÂÆòÊñπÊú™Êèê‰æõÂ§ñÈÉ®Êï∞ÊçÆÊ∫êÁöÑÈõÜÊàêÔºåÂ¶Çslack/notion/gmail
    - [Add private search connectors _202508](https://github.com/ItzCrazyKns/Perplexica/issues/856)
  - [fix(uploads): Resolve the bug of large document attachment upload failure](https://github.com/ItzCrazyKns/Perplexica/pull/675)
    - Resolve the bug where large document uploads prompt an "exceeded maximum allowed batch size" error. The error message is as follows: error: Error in uploading file results: 413 input batch size 512 > maximum allowed batch size 32 (request id: 2025031909320299661815003514673)

- https://github.com/zaidmukaddam/scira /11.4kStar/AGPL/202511/ts
  - https://scira.ai/
  - Scira (Formerly MiniPerplx) is a minimalistic AI-powered search engine that helps you find information on the internet and cites it too. 
  - Powered by Vercel AI SDK
  - using multiple AI models including xAI's Grok, Anthropic's Claude, Google's Gemini, and OpenAI's GPT models
  - Search the web using Exa's API with support for multiple queries, search depths, and topics
  - Extract and analyze content from any URL using Exa AI with live crawling capabilities
    - Search Reddit content with time range filtering using Tavily API
    - X (Twitter) search: Search X posts with date ranges and specific handle filtering using xAI Live Search
  - Advanced multi-step search capability for complex queries
  - Academic search: Search for academic papers and research using Exa AI with abstracts and summaries
  - YouTube search: Find YouTube videos with detailed information, captions, and timestamps powered by Exa AI

- https://github.com/H0NEYP0T-466/Pen2PDF /MIT/202511/js
  - https://pen2-pdf.vercel.app/
  - a modern web application that offers six powerful productivity tools: AI-powered text extraction and PDF conversion, intelligent timetable management with Excel/CSV import, comprehensive todo list management with subtasks, smart notes generation with a searchable library, a full-featured digital whiteboard, and an AI assistant (Bella) for intelligent help - all designed to streamline your academic and professional workflow.

- https://github.com/CaviraOSS/PageLM /1.3kStar/NonCommercial/202512/ts/inactive
  - https://pagelm.spotit.dev/
  - a community driven version of NotebookLM & a education platform that transforms study materials into interactive resources like quizzes, flashcards, notes, and podcasts.

- https://github.com/zstmfhy/zlibrary-to-notebooklm /MIT/202601/python
  - ‰∏ÄÈîÆÂ∞Ü Z-Library ‰π¶Á±çËá™Âä®‰∏ãËΩΩÂπ∂‰∏ä‰º†Âà∞ Google NotebookLM
# citation/sourcing
- https://github.com/preprocess-co/rag-document-viewer /MIT/202509/python/js
  - https://preprocess.co/rag-document-viewer
  - https://preprocess-rag-dv-demo.hf.space/
  - open-source library that generates high-fidelity file previews for seamless integration into your applications. 
  - It provides desktop-level file viewing capabilities for a wide range of document formats
  - The library converts these files into interactive HTML-based previews that can be easily embedded into web applications
  - You can now embed the viewer in your application with just an `<iframe>`.
  - ‰æùËµñLibreOffice„ÄÅpdf2htmlEX, These tools are not bundled with the rag-document-viewer package; they must be installed on the host system where viewers are generated. 
  - ‰ºº‰πé‰∏çÊîØÊåÅmacÊûÑÂª∫
  - Runs 100 % in-browser. Files are served directly from your backend under your auth logic, no external servers.
  - üîó Precise Highlights: Pass bounding-box coordinates from your RAG chunks; the viewer auto-scrolls and spotlights them.
    - You can get chunk coordinates from chunking providers like Preprocess.co (which supports paragraphs, layout items, multi-column layouts, slides, and more) or Unstructured.io (which offers PDF-only item-level support).
    - Chunks' coordinates should be stored in a list. When storing and then accessing a chunk, you should use the list index to reference the correct chunk.
    - If no chunk information is provided when generating the viewer, the following features will be disabled: Chunk highlighting and navigation

- https://github.com/zt6453928/ailat-translation /MIT/202601/python/js
  - AI-Powered Document Translation Tool
  - A web-based PDF document parsing and translation tool that supports intelligent document parsing, multi-language translation, and various export formats.
  - Supports PDF, DOCX and other document formats, extracts text, images, tables, and formulas
  - Export to PDF, Markdown, HTML, DOCX, JSON, LaTeX
  - Multiple Translation Engines: Supports DeepLX and OpenAI-compatible APIs
  - Real-time Preview: Side-by-side comparison of original and translated content
  - [ÂºÄÊ∫ê‰∏Ä‰∏™ÊîØÊåÅÂ§öÊ†ºÂºèÊñáÊ°£ÁøªËØëÂ∫îÁî® ](https://linux.do/t/topic/1511535)
    - Âá∫‰∫éË∫´ËæπÂêåÂ≠¶ÂØπËÆ∫ÊñáÁøªËØëÁöÑÈúÄÊ±ÇÔºåÊâÄ‰ª•ÂºÄÂèë‰∫ÜËøô‰∏™È°πÁõÆÔºåÈô§‰∫ÜËÆ∫ÊñáÊàë‰πüÁî®Êù•ÂØπ‰∏Ä‰∫õAIÊïôÁ®ãÊñáÊ°£ËøõË°åÁøªËØë
    - ÁÅµÊÑüÊù•Ëá™‰∫éMinerU, PDFÊ†ºÂºèËøòÊîØÊåÅÂØπÁÖßÁøªËØë

- https://github.com/stanford-oval/WikiChat /1.5kStar/apache2/202504/python/inactive
  - https://wikichat.genie.stanford.edu/
  - WikiChat is an improved RAG. It stops the hallucination of large language models by retrieving data from a corpus.
  - WikiChat uses Wikipedia and the following 7-stage pipeline to makes sure its responses are factual. Each numbered stage involves one or more LLM calls.
  - [WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia _202401](https://www.reddit.com/r/LocalLLaMA/comments/1920hho/wikichat_stopping_the_hallucination_of_large/)
  - [Is Wikipedia RAG possible entirely locally with a gaming machine? : r/ollama _202502](https://www.reddit.com/r/ollama/comments/1ihibp9/is_wikipedia_rag_possible_entirely_locally_with_a/)

- https://github.com/AdyTech99/volo /GPL/202501/python/inactive
  - combining AI with Wikipedia knowledge via a RAG pipeline
  - It utilizes an offline database of Wikipedia created by Kiwix, ensuring fast and reliable access to information without requiring constant internet connectivity.
  - Volo uses a tiny model (Qwen2.5:3b) and gives it the knowledge of nearly 7 million Wikipedia articles, making it a more reliable source of information than giant closed-source models like OpenAI's GPT4o and Anthropic's Claude 3.5 Sonnet, which are prone to hallucinations.
  - Offline Wikipedia Database: Leverages a `.zim` file from Kiwix, offering a snapshot of Wikipedia for offline access.
  - OpenAI-Compatible REST APIs: Use Volo with interfaces like Open WebUI or your own API client.
  - [Volo: An easy and local way to RAG with Wikipedia! : r/LocalLLaMA _202501](https://www.reddit.com/r/LocalLLaMA/comments/1hzsvkz/volo_an_easy_and_local_way_to_rag_with_wikipedia/)

- https://github.com/moodlehq/wiki-rag /BSD/202512/python
  - a project that leverages Mediawiki as a source for augmenting text generation tasks, enhancing the quality and relevance of generated content
  - RAG system specialised in ingesting MediaWiki sites via their API and providing an OpenAI API interface to interact with them.
  - Milvus 2.6.2 or later (for vector similarity search). Standalone or Distributed deployments are supported. Lite deployments are not supported.
  - Uses `LangGraph` for orchestration of the whole LLM pipeline.
  - Accepts virtually any Mediawiki site as a knowledge base (KB).
  - Uses Mediawiki API to extract content and metadata (not web crawling).
  - The whole KB is stored in a (dated) JSON file for later use, access and analysis.
  - Loads the KB into a vector database (Milvus) for fast retrieval.
  - Hybrid retrieval using both vector and keyword search with fusion reranking.
  - Exposed as a model using standard OpenAI API endpoints (v1/models and v1/chat/completions). Optionally protected with local or remote bearer tokens. Supports streaming responses.

- https://github.com/0nspaceshipearth/Hermit-AI /AGPL/202601/python
  - a privacy-first, 100% offline ai chatbot that lets you chat with wikipedia or any other `.zim` archive.
  - no cloud, no api keys, no tracking. everything stays on your machine.
  - [Hermit-AI: Chat with 100GB+ of Wikipedia/Docs offline using a Multi-Joint RAG pipeline : r/LocalLLM _202601](https://www.reddit.com/r/LocalLLM/comments/1q8mdwt/hermitai_chat_with_100gb_of_wikipediadocs_offline/)
    - I wanted to use Local AI along side my collection of ZIM files (Wikipedia, StackExchange, etc.) entirely offline. 
    - So I built a "Multi-Joint" Reasoning Pipeline. Instead of just doing one big search and hoping for the best, Hermit breaks the process down
    - Joint 1 (Extraction): It stops to ask "Who/What specifically is this user asking about?" before touching the database.
    - Joint 2 (JIT Indexing): It builds a tiny, ephemeral search index just for that query on the fly. This keeps it fast and accurate without needing 64GB of RAM.
    - Joint 3 (Verification): This is the cool part. It has a specific "Fact-Check" stage that reads the retrieved text and effectively says, "Wait, does this text actually support what the user is claiming?" If not, it corrects you.
    - zim is optimized for read-heavy content you rarely change.

- https://github.com/abgulati/LARS /622Star/AGPL/202410/python/inactive
  - LARS aims to be the ultimate open-source RAG-centric LLM application. Towards this end, LARS takes the concept of RAG much further by adding detailed citations to every response, supplying you with specific document names, page numbers, text-highlighting, and images relevant to your question
  - [Made a RAG-centric, Open-Source UI based on llama.cpp - With Advanced Source Citations & Referencing _202406](https://github.com/ggml-org/llama.cpp/discussions/7928)
  - [RAG for Documents with Advanced Source Citations & Referencing _202406](https://www.reddit.com/r/LocalLLaMA/comments/1db98el/rag_for_documents_with_advanced_source_citations/)

- https://github.com/upstash/wikipedia-semantic-search /MIT/202504/ts
  - https://wikipedia-semantic-search.vercel.app/
  - [Indexing Millions of Wikipedia Articles With Upstash Vector | Upstash Blog _202408](https://upstash.com/blog/indexing-wikipedia)
  - We've created a semantic search engine and Upstash RAG Chat SDK using Wikipedia data to demonstrate the capabilities of Upstash Vector and RAG Chat SDK.
  - Indexed over 144 million vectors from Wikipedia articles in 11 languages
  - Used BGE-M3 embedding model for multilingual support

- https://github.com/ollmer/wikichat /202401/python/inactive
  - Talk to Wikipedia offline using llama.cpp models and semantic search over the index of the whole English Wikipedia (44 million paragraphs).

- https://github.com/imDelivered/KiwixRAG /202512/python
  - A powerful offline-capable chatbot with Retrieval-Augmented Generation (RAG) that lets you chat with AI using local knowledge bases like Wikipedia, Python documentation, or any ZIM file archive.
  - Note: This software is currently only available for Linux. Windows and macOS support may be added in the future.
  - Download a ZIM file (e.g., from Kiwix), The chatbot will automatically detect and use it
  - Beautiful GUI built with tkinter
  - Hybrid Search: Semantic (FAISS) + keyword (BM25) search  
  - Just-In-Time Indexing: Auto-indexes articles on-the-fly
  - Multiple Models: Switch between any Ollama model
  - KiwixRAG uses a multi-joint architecture where three small AI reasoning models work together to prevent hallucinations and ensure accurate responses.
  - For faster retrieval on large ZIM files, you can pre-build an index
  - [Kiwix RAG: Terminal Chat Interface with Local Kiwix Content Integration : r/Kiwix _202512](https://www.reddit.com/r/Kiwix/comments/1pdio0i/kiwix_rag_terminal_chat_interface_with_local/)
  - [I built an offline AI chat app that automatically pulls Wikipedia articles for factual answers - runs completely locally with Ollama : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1pd2x8u/i_built_an_offline_ai_chat_app_that_automatically/)

- https://github.com/rouralberto/zim-llm /202508/python/inactive
  - [Building an Offline Knowledge Base with Kiwix Zim and Docker Model Runner - Alberto Roura](https://albertoroura.com/building-an-offline-knowledge-base-with-zim-and-docker-model-runner/)
  - This project provides a comprehensive system for processing ZIM files (compressed Wikipedia/offline content databases) and creating a vector database for Retrieval-Augmented Generation (RAG) with Large Language Models, effectively having an offline knowledge base.
  - Dynamic ZIM Library: Automatically discovers and processes multiple ZIM files from a dedicated library directory
  - Multi-format ZIM Support: Supports libzim, zimply, and CLI-based ZIM file reading
  - Flexible Vector Databases: Choose between ChromaDB and FAISS for storage
  - Local LLM Support: Docker Model Runner for offline LLM capabilities
  - Export articles from multiple ZIM files to JSON

- https://github.com/mozanunal/llm-tools-kiwix /202506/python/inactive
  - [I made an LLM tool to let you search offline Wikipedia/StackExchange/DevDocs ZIM files (llm-tools-kiwix, works with Python & LLM cli) : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1l3fdv3/i_made_an_llm_tool_to_let_you_search_offline/)

- https://github.com/tylertitsworth/multi-mediawiki-rag /202408/archived
  - A simple RAG chatbot that can retrieve from a mediawiki data dump
  - Mediawikis hosted by Fandom usually allow you to download an XML dump of the entire wiki as it currently exists
  - Your XML data needs to be loaded and transformed into embeddings to create a Chroma VectorDB.

- https://github.com/MauroAndretta/WikiRag /apache2/202408/python/inactive
  - RAG system designed for question answering, it reduces hallucination thanks to the RAG architecture. 
  - It leverages Wikipedia content as a knowledge base.
  - It integrates various components like Ollama, HuggingFaceEmbeddings, and Qdrant to create a powerful system

- https://github.com/zeyu-chen/Wikipedia-KG-RAG /202411/python/inactive
  - An intelligent question-answering system powered by knowledge graphs, combining Neo4j database and Wikipedia data for efficient Retrieval-Augmented Generation (RAG).
  - Hybrid vector and graph-based search
  - Real-time Updates - Dynamic Wikipedia data synchronization

- https://github.com/BurnyCoder/wikipedia-ai-agent /202501/inactive
  - AI agent research assistant that uses Wikipedia's vast knowledge base to deliver comprehensive, well-researched answers to your questions.
  - Built on a modern stack including LangChain's LangGraph's ReAct agent, Wikipedia API, FAISS vector storage, Microsoft's GraphRAG, support for leading LLM providers (OpenAI, Anthropic, Google), and Streamlit frontend.

- https://github.com/spring-projects-2024/wiki-savvy-rag /202411/inactive
  - build a Wikipedia-backed RAG system expert about STEM subjects. Features a streamlit demo.
  - We downloaded, filtered and cleaned the English Wikipedia (~100GB) and built a vector database of semantic embeddings based on all STEM articles, using the FAISS library for efficient retrieval of embeddings and SQLite for accessing text chunks from disk

- https://github.com/RUC-NLPIR/FlashRAG /MIT/202511/python
  - A Python Toolkit for Efficient RAG Research (WWW2025 Resource)
  - Features 23 advancing RAG algorithms with reported results, based on our framework.
  - [If you want to use Wikipedia as your corpus, you can download the Wikipedia dump you require in XML format](https://github.com/RUC-NLPIR/FlashRAG/blob/main/docs/original_docs/process-wiki.md)
  - https://github.com/RUC-NLPIR/FlashRAG-Paddle
    - built on the PaddlePaddle framework and PaddleNLP, which is optimized for Chinese-developed chips and computing platforms.

- https://github.com/TIGER-AI-Lab/LongRAG /202408/inactive
  - code for "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs"
  - LongRAG, consisting of a "long retriever" and a "long reader". Our framework use a 4K-token retrieval unit, which is 30x longer than before. 
  - We leverage open-sourced dense retrieval toolkit, Tevatron. 

- https://github.com/neuml/txtchat /apache2/202505/python/inactive
  - txtchat builds autonomous agents, retrieval augmented generation (RAG) processes and language model powered chat applications.
  - Wikitalk: Retrieval Augmented Generation (RAG) with Wikipedia

- https://github.com/neuml/txtai/tree/master/examples
  - [Local RAG Streamlit Apps for querying Wikipedia and ArXiv : r/LocalLLaMA _202406](https://www.reddit.com/r/LocalLLaMA/comments/1dq6kem/local_rag_streamlit_apps_for_querying_wikipedia/)
  - The data source is self contained. There is no direct querying of Wikipedia.

- https://github.com/neuml/ragdata /apache2/202507/python/inactive
  - ragdata builds knowledge bases for Retrieval Augmented Generation (RAG).
  - The currently supported datasets are: ArXiv, Wikipedia
  - This project has processes to build txtai embeddings databases for common datasets.
- https://github.com/myscale/ChatData /MIT/202406/python/inactive
  - a robust chat-with-documents application designed to extract information and provide answers
  - The knowledge base is a snapshot on 2022-12.

- https://github.com/achyuthkumarmiryala/Wikipedia-RAG-QA /MIT/202505/python
  - RAG system that answers natural-language questions on any Wikipedia topic.
  - It combines dense vector retrieval (FAISS + Sentence Transformers) with a Hugging Face question-answering model, wrapped in a friendly Gradio interface.
  - Real-time Wikipedia retrieval ‚Äì pulls the latest content directly from Wikipedia‚Äôs API.
  - Semantic search ‚Äì encodes text with all-mpnet-base-v2 and finds the most relevant passages using FAISS.
  - run locally or publish to Hugging Face Spaces with the same app.py.

- https://github.com/exowanderer/WikidataChat /202411/inactive
  - RAG with for answering question with the Wikidata REST API
  - Search and Download: Utilizes Google's Serapi search pipeline and Wikidata's REST API to fetch relevant JSON data.
  - RAG Pipeline: Merges Wikidata string statements with user-provided questions to generate informed and accurate answers through an LLM.

- https://github.com/ahmedo42/wikiqa /202409/inactive
  - RAG pipeline built for answering questions by retrieving the most relevant wikipedia articles using the Wikipedia API and the Google Search API.

- https://github.com/datastaxdevs/workshop-wikipedia-qa /202311/inactive
  - Real-time document Q&A using Pulsar, Cassandra, LangChain, and open-source language models.
  - This workshop code runs a Retrieval Augmented Generation (RAG) application stack that takes data from Wikipedia, stores it in a vector database (Astra DB), and provides a chat interface for asking questions about the Wikipedia documents.

- https://github.com/Shreyjain203/Wikipedia-Continual-Learning-RAG /202404/python/inactive
  - When a user asks a question, the model provides an answer. If the model is unable to answer, it uses get_data.py to fetch more information and fine-tunes itself before providing an appropriate answer.

- https://github.com/wikimedia/wikipedia-preview /MIT/202512/js
  - https://wikimedia.github.io/wikipedia-preview/main
  - https://wikimedia.github.io/wikipedia-preview/main/storybook
  - a JavaScript component that allows you to provide context from Wikipedia about words or phrases on any website. 
  - It lets you show a popup card with a short summary from Wikipedia when a reader hovers over a link: Read full article on Wikipedia 
  - Works with any link that has an article on Wikipedia

- https://github.com/rahulanand1103/rag-citation /MIT/202411/python/inactive
  - RAG Citation enhances Retrieval-Augmented Generation (RAG) by automatically generating relevant citations for AI-generated content

- https://github.com/thomassbooth/document-citation-rag /202410/python/ts
  - This project implements a Retrieval-Augmented Generation (RAG) system using Python and a frontend built with Nextjs - TypeScript, and advanced retrieval strategies: Multi-Query, RAG Fusion 
  - It leverages Qdrant and uses streaming responses to delivery queries to the frontend. 

- citation-examples
  - [Bluebook Citation Generator - Instant Legal Citations & OCR](https://bluebookcitationgenerator.com/)
# examples/apps
- https://github.com/dmayboroda/minima /MPL/202512/python/ts
  - On-premises conversational RAG with configurable containers
  - open source RAG on-premises containers, with ability to integrate with ChatGPT and MCP. 
  - Minima can also be used as a fully local RAG or with your own deployed LLM.
  - Minima currently supports 3 modes:
    - Isolated installation (Ollama) ‚Äì Operate fully on-premises with containers. All neural networks (LLM, reranker, embedding) run on your cloud or PC 
    - Custom LLM (OpenAI-compatible API) ‚Äì Use your own deployed LLM with OpenAI-compatible API (vLLM, Ollama server, TGI, etc.). The indexer runs locally while the LLM can be on your server, cloud, or local machine.
    - Custom GPT ‚Äì Query your local documents using ChatGPT app or web with custom GPTs. The indexer runs on your cloud or local PC, while the primary LLM remains ChatGPT.

- https://github.com/HKUDS/DeepTutor /9.3kStar/AGPL/202601/python/ts
  - https://hkuds.github.io/DeepTutor
  - AI-Powered Personalized Learning Assistant
  - Smart Knowledge Base: Upload textbooks, research papers, technical manuals, and domain-specific documents. 
  - Multi-Agent Problem Solving: Dual-loop reasoning architecture with RAG, web search, and code execution -- delivering step-by-step solutions with precise citations.
# data-rag
- https://github.com/statespace-tech/toolfront /801Star/MIT/202512/python
  - https://docs.toolfront.ai/
  - Turn your data into shareable RAG apps in minutes. All in pure Markdown.
  - [I built an MCP that finally makes your local AI models shine with SQL : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1ll3qej/i_built_an_mcp_that_finally_makes_your_local_ai/)
  - ToolFront equips AI models with a set of read-only database tools
# benchmark-rag ‚ö°Ô∏è
- https://github.com/Aaryanverma/trustifai /MIT/202601/python
  - a Python-based observability engine designed to evaluate the trustworthiness of LLM responses and Retrieval-Augmented Generation (RAG) systems. 
  - Unlike simple evaluation frameworks that rely on a single "correctness" score, TrustifAI computes a multi-dimensional Trust Score based on grounding, consistency, alignment, and diversity.
  - It also includes visualizations to help showcase why a model output was deemed unreliable.
  - [A framework to evaluate RAG answers in production : r/Rag](https://www.reddit.com/r/Rag/comments/1qpla70/a_framework_to_evaluate_rag_answers_in_production/)

- https://huggingface.co/datasets/G4KMU/t2-ragbench
  - [T2-RAGBench - Benchmark for RAG in Finance (10K Downloads on HF) : r/Rag](https://www.reddit.com/r/Rag/comments/1pde2au/t2ragbench_benchmark_for_rag_in_finance_10k/)
# more
- https://github.com/teng-lin/notebooklm-py /MIT/202601/python
  - The missing API for Google NotebookLM. 
  - [I mapped Google NotebookLM's internal RPC protocol to build a Python Library : r/Python _202601](https://www.reddit.com/r/Python/comments/1qbtgws/i_mapped_google_notebooklms_internal_rpc_protocol/)

- https://github.com/tensorlakeai/tensorlake /817Star/apache2/202512/python
  - https://tensorlake.ai/
  - Tensorlake is a Document Ingestion API and a serverless platform for building data processing and orchestration APIs
  - Parse documents (PDFs, DOCX, spreadsheets, presentations, images, and raw text) to markdown or extract structured data with schemas. This is powered by Tensorlake's state of the art layout detection and table recognition models.
  - Deploy Agentic Applications and AI Workflows using durable functions, with sandboxed and managed compute infrastructure that scales your agents with usage.
  - Sign up at cloud.tensorlake.ai and get your API key.

- [LanceDB Demos - Wikipedia 41M Hybrid Search](https://lancedb-demos.vercel.app/demo/wikipedia-search)
  - https://x.com/lancedb/status/2001327573968572579
  - We built WikiSearch, a search engine that stores and searches through real Wikipedia entries. 
  - The demo showcases how to use LanceDB‚Äôs full-text and hybrid search features to quickly find relevant information in a large dataset like Wikipedia.
  - [LanceDB WikiSearch: Native Full-Text Search on 41M Wikipedia Docs _202508](https://lancedb.com/blog/feature-full-text-search/)

- [One of our engineers wrote a 3-part series on building a RAG server with PostgreSQL : r/Rag _202512](https://www.reddit.com/r/Rag/comments/1pq79v4/one_of_our_engineers_wrote_a_3part_series_on/)
  - [Building a RAG Server with PostgreSQL - Part 1: Loading Your Content](https://www.pgedge.com/blog/building-a-rag-server-with-postgresql-part-1-loading-your-content)
