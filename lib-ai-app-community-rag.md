---
title: lib-ai-app-community-rag
tags: [ai, community, rag]
created: 2024-09-08T20:08:04.573Z
modified: 2024-09-08T20:08:16.088Z
---

# lib-ai-app-community-rag

# guide

- pros-rag
  - solutions rich
  - save cost

- cons-rag
  - extra infra like vectordb/retrive-api

- who is using #PGVector
  - SurfSense
  - colanode

- usecases-rag
  - long-docs/pdf
  - chat-history
  - ragçš„å®ç°è¿˜å¯ä»¥å‚è€ƒai-codingä¸­çš„å®è·µå’Œæ¨¡å¼
  - é’ˆå¯¹è¡¨æ ¼excel/è„‘å›¾çš„rag
  - â³ å¤šç‰ˆæœ¬æ–‡ä»¶çš„rag

- leaderboard-rag
  - [MTEB Leaderboard - a Hugging Face Space by mteb](https://huggingface.co/spaces/mteb/leaderboard)
    - https://github.com/embeddings-benchmark/mteb
    - top202509: gemini-embedding-001, embeddinggemma-300m, Qwen3-Embedding-8B/4B/0.6B, linq-mistral, jina, granite, nomic
    - ğŸ‘·å®æµ‹, embeddinggemmaåœ¨Ollamaç»“æœå·®, åœ¨LM Studioç»“æœè¿˜è¡Œ ~~åœ¨ similaritySearch æ—¶å¾ˆå·®~~
      - è·Ÿæ¨èqwen/granite

- resources
  - [RAG+AIå·¥ä½œæµ+Agentï¼šLLMæ¡†æ¶è¯¥å¦‚ä½•é€‰æ‹©ï¼Œå…¨é¢å¯¹æ¯”MaxKBã€Difyã€FastGPTã€RagFlowã€Anything-LLM, ä»¥åŠæ›´å¤šæ¨è - çŸ¥ä¹ _202407](https://zhuanlan.zhihu.com/p/711761781)
  - [[RAGä¼˜åŒ–] æ”¯æŒä¸­æ–‡çš„å¼€æº embedding models ç»¼è¿° - çŸ¥ä¹](https://zhuanlan. zhihu.com/p/1888778722225681562)
# draft
- rag as a service(ç»†åˆ†å¸‚åœº)
  - doc-search as a service, for static-sites/note-taking/papers
  - code-search
# dev-xp
- å®æµ‹Ollamaçš„embeddingçš„è¿‡ç¨‹å¯¹å†…å­˜æ¶ˆè€—å°, å¤§æ–‡æ¡£chunkå’Œembeddingè€—æ—¶å¯èƒ½è¾ƒé•¿
# discuss-stars
- ## 

- ## 

- ## 

- ## ğŸ’¥ i'm building a local rag webapp that supports to upload large pdfs, then convert to text , and doing chunking/embedding, then save to a vector db for retrieval with local llm.
  - i cannot find a solution that supports very large pdf, the memory is often overflowed, even causing system crash
  - is there any existing solution or better idea to do streaming pdf rag? I think streaming reading can lower the memory usage and achieve better performance.

- goals: streaming, lazy
- å¯å°è¯•åœ¨ç°æœ‰æ–¹æ¡ˆä¸­æ‰¾å®ç°
  - åŸºäºvlmçš„è¯†åˆ«æ–¹æ¡ˆï¼Œæœ¬èº«å°±æ˜¯æµå¼è¾“å‡ºå†…å®¹

- LlamaIndex with Streaming
  - Supports incremental ingestion with SimpleDirectoryReader
  - Can process PDFs in chunks using file_extractor parameter
  - Works with vector stores that support batch insertion

- Unstructured.io
  - Designed specifically for large document processing
  - Supports streaming mode for PDFs

- LangChain with PyMuPDF/PyPDF
  - `PyMuPDFLoader` can load pages incrementally

- Implementation Strategies
  - Strategy 1: Page-by-Page Streaming
  - Strategy 2: Batch Processing with Memory Limits
    - Process N pages at a time, Flush to vector DB after each batch
  - Strategy 3: Use Memory-Mapped Files
    - Libraries like `pymupdf` support memory-mapped reading
    - OS handles paging automatically
    - Good for very large PDFs (GB+)

- 
- 
- 
- 
- 
- 

- ## ğŸ–¼ï¸ğŸ“„ğŸ“ˆ [Multi-modal RAG at scale: Processing 200K+ documents (pharma/finance/aerospace). What works with tables/Excel/charts, what breaks, and why it costs way more than you think : r/LLMDevs _202510](https://www.reddit.com/r/LLMDevs/comments/1o5oaas/multimodal_rag_at_scale_processing_200k_documents/)
  - TL; DR: Built RAG systems for 10+ enterprise clients where 40-60% of critical information was locked in tables, Excel files, and diagrams. Standard text-based RAG completely misses this. 
  - This covers what actually works, when to use vision models vs traditional parsing, and the production issues nobody warns you about.
- Here's what nobody tells you: most enterprise knowledge isn't in clean text. It's in Excel spreadsheets with 50 linked sheets, tables buried in 200-page PDFs, and charts where the visual layout matters more than any text.
- Why Text-Only RAG Fails
- Quick context: pharmaceutical client had 50K+ documents where critical dosage data lived in tables. Banks had financial models spanning 50+ Excel sheets. Aerospace client's rocket schematics contained engineering specs that text extraction would completely mangle.
  - When a researcher asks "what were cardiovascular safety signals in Phase III trials?" and the answer is in Table 4 of document 8, 432, text-based RAG returns nothing useful.
- The Three Categories (and different approaches for each)
- ğŸ“Œ Simple Tables
  - Standard tables with clear headers. Financial reports, clinical trial demographics, product specifications.
  - What works: Traditional parsing with `pymupdf or pdfplumber` , extract to CSV or JSON, then embed both the structured data AND a text description. Store the table data, but also generate something like "Table showing cardiovascular adverse events by age group, n=2, 847 patients." Queries can match either.
  - Production issue: PDFs don't mark where tables start or end. Used heuristics like consistent spacing and grid patterns, but false positives were constant. Built quality scoring - if table extraction looked weird, flag for manual review.
- ğŸ“Œ Complex Visual Content
  - Rocket schematics, combustion chamber diagrams, financial charts where information IS the visual layout.
  - Traditional OCR extracts gibberish. What works: Vision language models. Used Qwen2.5-VL-32b for aerospace, GPT-4o for financial charts, Claude 3.5 Sonnet for complex layouts.
  - The process: Extract images at high resolution, use vision model to generate descriptions, embed the description plus preserve image reference. During retrieval, return both description and original image so users can verify.
  - The catch: Vision models are SLOW and EXPENSIVE. Processing 125K documents with image extraction plus VLM descriptions took 200+ GPU hours.
- ğŸ“Œ Excel Files (the special circle of hell)
  - Not just tables - formulas, multiple sheets, cross-sheet references, embedded charts, conditional formatting that carries meaning.
  - Financial models with 50+ linked sheets where summary depends on 12 others. Excel files where cell color indicates status. Files with millions of rows.
  - ğŸ’¡ For simple Excel use pandas. 
  - For complex Excel use `openpyxl` to preserve formulas, build a dependency graph showing which sheets feed into others. 
  - For massive files, process in chunks with metadata, use filtering to find right section before pulling actual data.
  - ğŸ’¡ Excel files with external links to other workbooks. Parser would crash. Solution: detect external references during preprocessing, flag for manual handling.
  - Vision model trick: For sheets with complex visual layouts like dashboards, screenshot the sheet and use vision model to understand layout, then combine with structured data extraction. Sounds crazy but worked better than pure parsing.

- When to Use What
  - Use traditional parsing when: clear grid structure, cleanly embedded text, you need exact values, high volume where cost matters.
  - Use vision models when: scanned documents, information IS the visual layout, spatial relationships matter, traditional parsers fail, you need conceptual understanding not just data extraction.
  - Use hybrid when: tables span multiple pages, mixed content on same page, you need both precise data AND contextual understanding.
  - Real example: Page has both detailed schematic (vision model) and data table with test results (traditional parsing). Process twice, combine results. 
  - ğŸ’¡ Vision model explains schematic, parser extracts exact values.

- Production Issues Nobody Warns You About
  - Tables spanning multiple pages: My hacky solution detects when table ends at page boundary, checks if next page starts with similar structure, attempts to stitch. Works maybe 70% of the time.
  - Image quality degradation: Client uploads scanned PDF photocopied three times. Vision models hallucinate. 
    - ğŸ’¡ Solution: document quality scoring during ingestion, flag low-quality docs, warn users results may be unreliable.
  - Memory explosions: Processing 300-page PDF with 50 embedded charts at high resolution ate 10GB+ RAM and crashed the server. 
    - ğŸ’¡ Solution: lazy loading, process pages incrementally, aggressive caching.
  - Vision model hallucinations: This almost destroyed client trust. Bank client had a chart, GPT-4o returned revenue numbers that were close but WRONG. Dangerous for financial data. 
    - ğŸ’¡ Solution: Always show original images alongside AI descriptions. For critical data, require human verification. Make it clear what's AI-generated vs extracted.

- The Metadata Architecture
  - This is where most implementations fail. You can't just embed a table and hope semantic search finds it.
  - For tables I tag content_type, column_headers, section, what data it contains, parent document, page number. 
  - For charts I tag visual description, diagram type, system, components. 
  - For Excel I tag sheet name, parent workbook, what sheets it depends on, data types.
  - Why this matters: When someone asks "what were Q3 revenue projections, " metadata filtering finds the right Excel sheet BEFORE semantic search runs. Without this, you're searching through every table in 50K documents.

- Cost Reality Check
  - Multi-modal processing is EXPENSIVE. For 50K documents with average 5 images each, that's 250K images. At roughly one cent per image with GPT-4o, that's around $2, 500 just for initial processing. Doesn't include re-processing or experimentation.
  - Self-hosted vision models like from Qwen need around 80GB VRAM. Processing 250K images takes 139-347 hours of compute. Way slower but cheaper long-term for high volume.
  - My approach: Self-hosted models for bulk processing, API calls for real-time complex cases, aggressive caching, filter by relevance before processing everything.

- What I'd Do Differently
  - Start with document quality assessment - don't build one pipeline for everything. 
  - Build the metadata schema first - spent weeks debugging retrieval issues that were actually metadata problems. Always show the source visual alongside AI descriptions. 
  - Test on garbage data early - production documents are never clean. Set expectations around accuracy - vision models aren't perfect.

- Multi-modal RAG pays off when critical information lives in tables and charts, document volumes are high, users waste hours manually searching, and you can handle the complexity and cost.
  - Skip it when most information is clean text, small document sets work with manual search, budget is tight and traditional RAG solves 80% of problems. 
  - Real ROI: Pharma client's researchers spent 10-15 hours per week finding trial data in tables. System reduced that to 1-2 hours. Paid for itself in three months.
- Multi-modal RAG is messy, expensive, and frustrating. But when 40-60% of your client's critical information is locked in tables, charts, and Excel files, you don't have a choice. The tech is getting better, but production challenges remain.

- My questions would be as a provider/consultant, how do provide support and maintenance moving forward? Long term, how do you ensure the old way of them putting data in tables is easily digested into the new RAG system automatically or that someone owns the knowledge base? The â€œdata cleaningâ€ and organization seems like a full time contract just by itself. Then, how do you test for accuracy and showcase those results to stakeholders for renewal or as case studies for other businesses?
  - support and maintenance was something i completely underestimated early on. did the typical "build it and hand off" approach with my first few clients. total disaster. systems would work great for 2-3 months then drift, new document types would break the pipeline, accuracy would drop. now i do hybrid - initial build plus ongoing support contract. usually quarterly check-ins, pipeline updates when they add new document types, on-call for critical issues.
  - for data cleaning i train someone on their team to own document quality. they become the gatekeeper, i provide the tooling. the alternative where i do it forever doesn't scale and they know their domain way better anyway. built quality detection into the ingestion pipeline that flags problematic docs automatically but someone still needs to make decisions about what to do with garbage scans from the 90s.
  - testing is golden test sets with domain experts - 100-200 real questions with known answers. run these quarterly but focus stakeholder reports on business metrics they actually care about like "researchers spend 2 hours per week searching vs 15 hours before" or "regulatory response time dropped from 5 days to 6 hours."
  - initial builds were $50k+ (currently 100k+) but support contracts run $3k-5k monthly. way more profitable long-term plus you learn edge cases that feed back into the product and i'm full-time on this, but trying to build something in the rag space as well.

- iâ€™ve battled the same multiâ€‘modal rag pain at scale in pharma/finance/aerospace. if you want this to survive production, bake in lineage, governance, and observability from day one, not after it drifts.
  - table-first retrieval: tag content_type, column_headers, section, page, and parent doc; prefilter by metadata before semantic search so â€œq3 revenue projectionsâ€ lands on the right sheet/table.
  - excel lineage and verification: parse formulas with openpyxl, build a crossâ€‘sheet dependency graph, bind screenshots of dashboards to underlying cells when layout matters, and require human verification for any financial figures the vlm â€œreads.â€
  - vlm governance and cost controls: quality-score every image/page, batch and cache captions, separate â€œconceptualâ€ descriptions from â€œnumericâ€ extraction, and run a quarterly golden test set that mixes tables, scanned PDFs, and charts.
  - production observability: trace every ingestion step (parse, stitch, caption, embed), cap memory per page, add fallbacks for multiâ€‘page table stitching, and alert on eval drift rather than waiting for user complaints.
- maxim ai helps here with endâ€‘toâ€‘end evaluation, simulation, experimentation, and observability for agents and rag pipelines: scenario-based tests at scale, prompt/version experiments, live tracing with online evaluations, ci/cd hooks, and enterprise deploy options. (builder here!)

- I'm currently working on a solution to address product cataloging issues for an ecommerce platform. Right now, analysts manually review Excel files and images, then fill out Excel templates with product attributes (title, description, features, etc.). My proposed solution would automatically extract attributes from files, populate templates, validate images, and extract attributes from images (like dimensions in cm, brand, model, and others). Also check images if this image is "main", "lateral", "left view", "right view" for an SKU and if these images contains a QR code.
  - The main challenge I'm facing is that the image processing workflow is too slow and expensive (using GPT-4o model). 
  - The goal is to scale up to processing 1, 000, 000 product reviews daily. I'm currently using AWS with Fargate jobs that launch dockerized processes for image processing, orchestrated by Step Functions.
  - Currently handling <1, 000 SKUs daily, which is far from the 1, 000, 000 target.
  - Any recommendations or advice for optimizing this process?

- If table with headers , and each table column is well aligned , in most scenarios, you can check the column alignment condition and assuming that table schema did not change to determine whether table contents flow over pages

- ## [Using LMStudio for RAG/File Search with LibreChat? Â· danny-avila/LibreChat _202506](https://github.com/danny-avila/LibreChat/discussions/7713)
  - I run LibreChat locally inside Docker, I run local models in LMStudio, and want to use LMStudio's nomic (or otherwise) embedding capabilities instead of an external provider like OpenAI.
  - (A) i want to do it for free/private locally, and (B) especially because when I tested file upload directly within LMStudio [using its RAG] it correctly pulled all text from the PDF, but when I used OpenAI cloud embedding it missed a huge chunk of the textâ€”perhaps because the API version uses a text-only embedding model without vision
- UPDATE / SOLUTION:
  - Making a copy of the config.py file in my LibreChat root folder and pointing the compose-override to it did solve the problem and allowed me to use the openai embeddings with LMStudio. I seconded the feature request to make this an easy option for people to opt into

- been down that rabbit hole too, and yep: this kind of embedding mismatch + RAG retrieval chaos is super common when things try to speak in different tensor dialects.
  - the error from LMStudio (input field must be a string or array of strings) usually hits when the upstream retrieval returns a raw int list or malformed embedding body. 
  - i eventually got tired of patching these one by one and built a semantic rescue engine that stabilizes the entire pipeline.

- ## ğŸ¤” [RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ä¼šä¸ä¼šæ¶ˆäº¡å‘¢ï¼Ÿä¸€æ—¦å¤§æ¨¡å‹çš„Context Lengthå˜å¤§ï¼ŒRAGè¿˜æœ‰å­˜æ´»çš„å¿…è¦å—ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/637421964)
- RAG çš„åŸºæœ¬åŸç†å¹¶ä¸å¤æ‚ï¼š
  1.  å°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆå¤šä¸ª **chunk**ï¼›  
  2.  é€šè¿‡å‘é‡åŒ–æ¨¡å‹ï¼ˆå¦‚ [OpenAI Embeddings] ã€Sentence-Transformerï¼‰ä¸ºæ¯ä¸ª chunk ç”Ÿæˆå‘é‡ï¼Œå¹¶å­˜å…¥æ•°æ®åº“ï¼›  
  3.  åœ¨ç”¨æˆ·æé—®æ—¶ï¼Œæ£€ç´¢å‡º **Top-K ç›¸ä¼¼æ–‡æ¡£**ï¼›  
  4.  æŠŠè¿™äº›æ–‡æ¡£ä¸é—®é¢˜ä¸€èµ·è¾“å…¥å¤§æ¨¡å‹ï¼Œä½œä¸ºé¢å¤–ä¸Šä¸‹æ–‡ï¼›  
  5.  å¤§æ¨¡å‹åŸºäºé—®é¢˜ + æ£€ç´¢ç»“æœï¼Œç”Ÿæˆæ›´å‡†ç¡®çš„å›ç­”ã€‚
- ç®€å•æ¥è¯´ï¼ŒRAG å°±åƒæ˜¯ç»™å¤§æ¨¡å‹è£…ä¸Šäº†ä¸€ä¸ªâ€œå¤–æŒ‚æœç´¢å¼•æ“â€ã€‚
  - ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¼ ç»Ÿçš„ å¾®è°ƒï¼ˆFine-tuningï¼‰ æ–¹å¼è™½ç„¶èƒ½æŠŠæ–°çŸ¥è¯†å†™è¿›æ¨¡å‹ï¼Œä½†å¾€å¾€æˆæœ¬é«˜æ˜‚ã€ä¸å¯é€†ï¼Œè¿˜å¯èƒ½â€œç‰ºç‰²â€åŸæœ‰èƒ½åŠ›ã€‚è€Œ RAG çš„ä¼˜åŠ¿åœ¨äºï¼šè½»é‡ã€çµæ´»ã€éšå–éšç”¨
- ç›®å‰å¤§å¤šæ•° RAG ç³»ç»Ÿå…¶å®æ˜¯â€œFrozen RAGâ€â€”â€”Retriever ä¸ LLM æ˜¯åˆ†å¼€çš„ï¼ŒRetriever å›ºå®šä¸å˜ï¼ŒLLM ä¹Ÿä¸æ›´æ–°ã€‚å®ƒä»¬æ‹¼åœ¨ä¸€èµ·èƒ½è·‘ï¼Œä½†æ¨¡å—ä¹‹é—´å¹¶æ²¡æœ‰æ·±åº¦ååŒã€‚
- RAG 2.0 çš„æ ¸å¿ƒç›®æ ‡å°±æ˜¯ï¼š è®©æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨æˆä¸ºä¸€ä¸ªâ€œå¯å…±åŒè®­ç»ƒâ€çš„æ•´ä½“ï¼Œä»è€Œè¾¾åˆ°ç«¯åˆ°ç«¯çš„ä¼˜åŒ–ã€‚

- ä»çº¯ä¸šåŠ¡ç«¯çš„è§’åº¦æ¥è¯´ï¼Œä¸ä½†ä¸ä¼šæ¶ˆäº¡ï¼Œè¿˜æœ‰å¯èƒ½åœ¨æœªæ¥çš„3-5å¹´é‡Œå¤§è¡Œå…¶é“ã€‚
  - è¡¨é¢åŸå› éå¸¸ç®€å•ï¼šRAGå‡ ä¹æ˜¯ç›®å‰å”¯ä¸€ä¸€ç§å…¼å…·é—¨æ§›ä½+å¯è§‚å¯Ÿ+æ§åˆ¶å‡†+è§æ•ˆå¿«çš„è·¯çº¿ã€‚
  - æœ€åæä¸€å˜´æ·±å±‚åŸå› ï¼šRAGçš„æŠ€æœ¯è·¯çº¿æå…¶ç¬¦åˆå›½æƒ…ï¼Œå…¶æœ¬è´¨æ˜¯çœŸç†çŒé¡¶ä¸”å¯ä»¥éšæ—¶æ›¿æ¢ï¼Œæ¯”å¤§ç‚¼æ¨¡å‹å¾—åˆ°çš„é†‰æ‹³å¹»è§‰å¯æ§å¾—å¤šã€‚
  - To Bå’ŒTo Géƒ½æ˜¯æ•°æ®å¯†é›†å‹å¯¹è±¡ï¼Œè‡ªèº«æ•°æ®å°±æ˜¯æœ€å¤§çš„ç­¹ç ï¼Œä¸è¦æŒ‡æœ›äººå®¶ä¼šæŠŠèº«å®¶æ€§å‘½äº¤åˆ°AIå‚å•†æ‰‹é‡Œï¼Œå»æ‰“é€ ä¸€ä¸ªé¥é¥é¢†å…ˆçš„å¤§æ¨¡å‹ã€‚
- å¤§æ¨¡å‹ç›®å‰æ²¡çœ‹åˆ°ä¼ä¸šå†…å¥½çš„è½åœ°åœºæ™¯ã€‚ragå¤§éƒ¨åˆ†æ˜¯ç”¨äºçŸ¥è¯†åº“ï¼Œä½†æ˜¯ä¹Ÿå°±åŸºæœ¬èƒ½ç”¨ï¼Œå¦‚æœæé—®ä¸èƒ½ä»å…³é”®è¯å’Œå‘é‡ä¸ŠåŒ¹é…ï¼Œé‚£ä½ ç»™å¤§æ¨¡å‹çš„å†…å®¹å°±å’Œé—®é¢˜æ— å…³ï¼Œè‡ªç„¶ç­”æ¡ˆå°±ä¸å¯¹ã€‚å¹¶ä¸”æ–‡æ¡£æ‹†åˆ†åï¼Œé€»è¾‘ä¸Šå°±è¢«æ‹†åˆ†äº†ï¼Œå¾ˆå¤šé—®é¢˜æ˜¯è¦ä»å„ä¸ªæ–‡æ¡£é€»è¾‘ä¸Šåˆ†æï¼Œä¸æ˜¯ç®€å•çš„æ£€ç´¢ã€‚

- å›é¡¾ä¸‹å½“å‰å¸‚åœºä¸Šä¼ä¸š RAG çŸ¥è¯†åº“è½åœ°çš„ä¸‰ç§ä¸»æµè·¯å¾„ã€‚
- ç›´æ¥ä½¿ç”¨é«˜å±‚çº§å¼€æºæ¡†æ¶
  - è¿™ç±»æ¡†æ¶å¦‚ RAGFlow, Dify, FastGPT ç­‰ï¼Œä¸»è¦ç‰¹ç‚¹æ˜¯æä¾›ç›¸å¯¹å®Œæ•´ã€å¼€ç®±å³ç”¨çš„ RAG å·¥ä½œæµï¼Œ
  - ç›®æ ‡æ˜¯ç®€åŒ– RAG åº”ç”¨çš„æ­å»ºè¿‡ç¨‹ï¼Œé™ä½å¼€å‘é—¨æ§›ã€‚
  - åä¹‹ï¼ŒåŠ£åŠ¿ä¸»è¦æ˜¯å®šåˆ¶åŒ–å’Œçµæ´»æ€§å—é™ï¼Œæ·±åº¦ä¼˜åŒ–å’Œé›†æˆç‰¹å®šç»„ä»¶æ—¶å¤æ‚åº¦è¾ƒé«˜ã€‚
- åº•å±‚å¼€å‘æ¡†æ¶è‡ªä¸»å¼€å‘
  - è¿™ç±»æ¡†æ¶åŒ…æ‹¬ LangChain, LlamaIndex, Haystack ç­‰ï¼Œéƒ½æä¾›äº†ä¸€ç³»åˆ—æ¨¡å—åŒ–çš„æ„å»ºå—ã€å·¥å…·å’Œæ¥å£ã€‚å¼€å‘è€…å¯ä»¥æ ¹æ®éœ€æ±‚çµæ´»ç»„åˆå’Œç¼–æ’ RAG æµç¨‹çš„å„ä¸ªç¯èŠ‚ï¼ˆå¦‚æ•°æ®åŠ è½½ã€æ–‡æœ¬åˆ†å‰²ã€åµŒå…¥ã€å‘é‡å­˜å‚¨ã€æ£€ç´¢ç­–ç•¥ã€LLM è°ƒç”¨ã€è®°å¿†ç®¡ç†ã€Agent æ„å»ºç­‰ï¼‰ã€‚
  - ä¼˜åŠ¿å¾ˆæ˜æ˜¾å°±æ˜¯çµåº¦çµæ´»ï¼Œå®šåˆ¶åŒ–èƒ½åŠ›å¼ºï¼Œèƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šä¸šåŠ¡åœºæ™¯è¿›è¡Œæ·±åº¦ä¼˜åŒ–å’Œé›†æˆã€‚é€‚åˆå¯¹ RAG æµç¨‹æœ‰ç²¾ç»†åŒ–æ§åˆ¶éœ€æ±‚çš„ä¼ä¸šã€‚
  - åä¹‹åŠ£åŠ¿å°±æ˜¯éœ€è¦æ›´å¤šçš„å¼€å‘å·¥ä½œé‡å’ŒæŠ€æœ¯æ·±åº¦ã€‚
- äº‘å‚å•† MaaS å¹³å°æ–¹æ¡ˆ
  - å¦‚é˜¿é‡Œäº‘ç™¾ç‚¼, ç™¾åº¦æ™ºèƒ½äº‘åƒå¸†, AWS Bedrock, Google Vertex AI Search ç­‰çš„ç§æœ‰åŒ–éƒ¨ç½²æ–¹æ¡ˆï¼Œæ— è®ºæ˜¯å›½å†…è¿˜æ˜¯å›½å¤–çš„è¿™äº›äº‘æœåŠ¡å•†ï¼Œéƒ½æ˜¯æŠŠ RAG ç›¸å…³çš„èƒ½åŠ›å°è£…æˆæœåŠ¡æˆ–æä¾›ç§æœ‰åŒ–éƒ¨ç½²åŒ…ï¼Œé€šå¸¸ä¸è‡ªå®¶çš„å¤§æ¨¡å‹ã€è®¡ç®—èµ„æºã€æ•°æ®å­˜å‚¨ç­‰æ·±åº¦é›†æˆã€‚ä¸€èˆ¬ä¹Ÿä¼šæä¾›æ¨¡å‹é€‰æ‹©ã€å¾®è°ƒã€éƒ¨ç½²ã€ç›‘æ§ç­‰ä¸€ç«™å¼æœåŠ¡ã€‚
  - é‡‡ç”¨è¿™ç§æ–¹æ¡ˆé€šå¸¸èƒ½æä¾›ç¨³å®šçš„åŸºç¡€è®¾æ–½ã€ä¾¿æ·çš„æ¨¡å‹ç®¡ç†å’Œéƒ¨ç½²ã€ä»¥åŠä¸å…¶ä»–äº‘æœåŠ¡çš„è‰¯å¥½å…¼å®¹æ€§ã€‚å¯¹äºå·²ç»æ·±åº¦ä½¿ç”¨ç‰¹å®šäº‘ç”Ÿæ€çš„ä¼ä¸šæ¥è¯´ï¼Œé›†æˆæˆæœ¬è¾ƒä½ã€‚
  - ä½†åŠ£åŠ¿å°±å¦‚åŒä½¿ç”¨å¤§æ¨¡å‹ä¸€ä½“æœºä¸€æ ·ï¼Œå¯èƒ½å­˜åœ¨å‚å•†é”å®šçš„é£é™©ï¼Œè·¨äº‘è¿ç§»æˆ–é›†æˆéè¯¥å‚å•†çš„æœåŠ¡å¯èƒ½ä¼šæ¯”è¾ƒå¤æ‚ã€‚
  - å½“ç„¶ï¼Œè´¹ç”¨å’Œçµæ´»æ€§ä¹Ÿéœ€è¦è€ƒè™‘ã€‚
- æ€»ç»“æ¥è¯´ï¼Œå®é™…ç”Ÿäº§åœºæ™¯ä¸­ä»¥ä¸Šä¸‰ç§é€‰æ‹©å¹¶éå®Œå…¨äº’æ–¥ï¼Œä¾‹å¦‚ä»¥åº•å±‚æ¡†æ¶ä¸ºåŸºç¡€ï¼Œä½†å€ŸåŠ©äº‘å‚å•†çš„éƒ¨åˆ†æ¨¡å‹æœåŠ¡æˆ–åŸºç¡€è®¾æ–½ä¹Ÿæ˜¯ä¸­ç›®å‰å¸¸è§çš„ç»„åˆã€‚

- RAGä¸»è¦å¯ä»¥è§£å†³å¤§æ¨¡å‹å¹»è§‰é—®é¢˜ï¼Œä»¥åŠé€šè¿‡å¤–æŒ‚çŸ¥è¯†ï¼Œé¿å…æ¨¡å‹é¢‘ç¹è¿›è¡ŒçŸ¥è¯†æ›´æ–°
  - å³ä½¿æ¨¡å‹å¯æ¥å—ä¸Šä¸‹æ–‡é•¿åº¦ä¸º128kï¼Œä½†æ˜¯æˆ‘çš„çŸ¥è¯†åº“é‡Œæœ‰10wç¯‡æ–‡æ¡£ï¼Œè¿œè¿œè¶…è¿‡æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæ‰€ä»¥ä½ è¿˜æ˜¯éœ€è¦æ£€ç´¢ã€‚
  - å½“ç„¶æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦å˜é•¿ï¼Œå¯¹äºæ£€ç´¢æ›´åŠ æœ‰å¥½ï¼Œç²’åº¦å˜å¤§ï¼Œå¬å›å˜é«˜ï¼Œç›¸è¾…ç›¸æˆã€‚
  - å³ä½¿ä½ å¯ä»¥æ¥å—128kçš„é•¿åº¦ï¼Œä½†æ˜¯ä½ çš„è®¾å¤‡æ¶ˆè€—ä¸èµ·ï¼Œæ›´å¤§çš„é•¿åº¦éœ€è¦æ›´å¤šçš„æ˜¾å­˜ï¼Œæ¨¡å‹éƒ¨ç½²æˆæœ¬æ›´é«˜ï¼Œé€Ÿåº¦æ›´æ…¢ã€‚
  - å¦‚æœä½ è°ƒç”¨apiï¼Œé‚£tokenæ¶ˆè€—ä¹Ÿå¤ªå¤§äº†ï¼Œæˆæœ¬è¿‡é«˜ã€‚

- Context Length å˜å¤§ â‰  ä¸éœ€è¦ RAG
  - æˆæœ¬é—®é¢˜: ä¸Šä¸‹æ–‡è¶Šé•¿ï¼Œæ¨ç†æˆæœ¬ï¼ˆæ˜¾å­˜ã€æ—¶é—´ã€ç®—åŠ›ï¼‰ä¼šçº¿æ€§ç”šè‡³è¶…çº¿æ€§å¢åŠ ã€‚ æ¯”å¦‚ï¼Œ100k tokens å¤„ç†ä¸€æ¬¡å°±å¾ˆçƒ§é’±ï¼Œè€Œ RAG å¯ä»¥ç”¨è¾ƒçŸ­ä¸Šä¸‹æ–‡åå¤è°ƒç”¨ã€‚
  - ä¿¡æ¯æ£€ç´¢æ•ˆç‡: å¦‚æœä½ æœ‰ 1TB çš„çŸ¥è¯†åº“ï¼ŒæŠŠå®ƒå…¨å¡è¿›ä¸Šä¸‹æ–‡æ˜¯ä¸å¯èƒ½çš„ã€‚ RAG å…ˆæ£€ç´¢ï¼Œå†é€è¿›ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥è®©æ¨¡å‹ä¸“æ³¨äºç›¸å…³ä¿¡æ¯ã€‚
  - å™ªéŸ³ä¸å¹²æ‰°: å¤§ä¸Šä¸‹æ–‡å¹¶ä¸ä»£è¡¨æ¨¡å‹ä¼šå¿½ç•¥æ— å…³ä¿¡æ¯ï¼Œç›¸åå¤ªå¤šæ— å…³å†…å®¹ä¼šç¨€é‡Šå…³é”®ä¿¡æ¯ï¼Œå¯¼è‡´æ¨ç†æ•ˆæœä¸‹é™ã€‚RAG çš„â€œæ£€ç´¢+è¿‡æ»¤â€èƒ½æé«˜ä¿¡å™ªæ¯”ã€‚
- Context å˜å¤§åï¼ŒRAG çš„å½¢æ€ä¼šæ¼”åŒ–
  - ä»â€œæŸ¥æ‰¾èµ„æ–™â€åˆ°â€œåŠ¨æ€çŸ¥è¯†ç»„ç»‡â€: æœªæ¥çš„ RAG å¯èƒ½ä¸åªæ˜¯æ£€ç´¢ï¼Œè€Œæ˜¯è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªä¸´æ—¶çš„çŸ¥è¯†æ‘˜è¦/çŸ¥è¯†å›¾è°±å†äº¤ç»™å¤§æ¨¡å‹
  - å¤šé˜¶æ®µæ¨ç†ï¼ˆMulti-turn Reasoningï¼‰: å¤§ä¸Šä¸‹æ–‡å…è®¸ä¸€æ¬¡æ€§æ”¾æ›´å¤šä¿¡æ¯ï¼Œä½† RAG å¯ä»¥åœ¨æ¯ä¸€æ­¥æ¨ç†æ—¶æ›´æ–°ä¸Šä¸‹æ–‡ï¼Œå½¢æˆ äº¤äº’å¼æ¨ç†
  - Hybrid RAGï¼ˆæ··åˆæ¨¡å¼ï¼‰: æŠŠé™æ€çŸ¥è¯†ï¼ˆå¸¸ç”¨ä¿¡æ¯ï¼‰ç›´æ¥æ”¾åœ¨é•¿ä¸Šä¸‹æ–‡é‡Œï¼ŒæŠŠåŠ¨æ€æ•°æ®ï¼ˆå®æ—¶ä¿¡æ¯ã€æœç´¢ç»“æœï¼‰ç”¨ RAG æ–¹å¼æ’å…¥ã€‚
- ä¸ºä»€ä¹ˆ RAG åœ¨é•¿ä¸Šä¸‹æ–‡æ—¶ä»£ä¾ç„¶é‡è¦
  - å¯æ›´æ–°æ€§: æ¨¡å‹è®­ç»ƒæ•°æ®æ˜¯é™æ€çš„ï¼ŒRAG å¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å¼•å…¥æœ€æ–°çŸ¥è¯†ã€‚
  - éšç§ä¸å®šåˆ¶åŒ–: ç§æœ‰ä¼ä¸šæˆ–ä¸ªäººæ•°æ®ä¸ä¼šç›´æ¥æ”¾åˆ°å¤§æ¨¡å‹é‡Œï¼Œè€Œæ˜¯é€šè¿‡ RAG åŠ¨æ€æ³¨å…¥ã€‚
  - å‡å°‘å¹»è§‰: æ¨¡å‹ç›´æ¥å¼•ç”¨æ£€ç´¢åˆ°çš„åŸæ–‡ï¼Œå¯ä»¥æ˜¾è‘—é™ä½èƒ¡ç¼–ä¹±é€ çš„æ¦‚ç‡ã€‚
  - å¤šæ¨¡æ€æ£€ç´¢: RAG æœªæ¥ä¼šä¸ä»…æ˜¯æ–‡æœ¬ï¼Œè¿˜ä¼šæ£€ç´¢å›¾ç‰‡ã€è§†é¢‘ã€éŸ³é¢‘ã€ç»“æ„åŒ–æ•°æ®ï¼Œå†ç»Ÿä¸€ç»™å¤§æ¨¡å‹å¤„ç†
- æœªæ¥ RAG çš„é‡ç‚¹ä¼šä»â€œæ‰¾æ–‡æ¡£â€å˜æˆâ€œæ„å»ºæœ€ä¼˜æ¨ç†ä¸Šä¸‹æ–‡â€ï¼Œå®ƒä¼šå˜å¾—æ›´æ™ºèƒ½ã€æ›´åŠ¨æ€ã€‚
  - éšç€ Context > 1M tokensï¼Œéƒ¨åˆ†åœºæ™¯å¯ç›´æ¥ç”¨å¤§æ¨¡å‹çš„é•¿è®°å¿†ï¼Œä½† RAG ä¼šæ¼”å˜ä¸ºâ€œæ™ºèƒ½çŸ¥è¯†ç¼–æ’å™¨â€ã€‚
  - å¦‚æœå‡ºç°æ— é™ä¸Šä¸‹æ–‡ä¸”æ¨ç†æˆæœ¬æä½çš„æ¨¡å‹ï¼ŒRAG ä¼šé€€åŒ–æˆâ€œæ•°æ®ç´¢å¼•å·¥å…·â€ï¼Œä¸»è¦ç”¨äºåŠ é€Ÿæ•°æ®ç»„ç»‡è€Œä¸æ˜¯å¿…é¡»æ­¥éª¤ã€‚

- ä¸ä¼šã€‚ä¸Šä¸‹æ–‡é•¿åº¦å˜é•¿åªä¼šè®©RAGæ›´å¥½ç”¨ã€‚ä»ç›®å‰çš„æƒ…å†µæ¥çœ‹ï¼ŒRAGçš„æˆæœ¬ä¾æ—§ä½äºå¾®è°ƒï¼Œæ•ˆæœé«˜äºå¾®è°ƒã€‚ç„¶åç›®å‰llmåº”ç”¨çš„æœ¬è´¨è¿˜æ˜¯promptï¼Œæ²¡æœ‰ä»€ä¹ˆå¤§èŠ±æ ·ï¼Œä¹Ÿè„±ç¦»ä¸äº†ç°æœ‰çš„äººç±»å­¦ç§‘ã€‚
  - å½“ç„¶ï¼Œå¦‚æœæœ‰äººå¼ºè¡Œè¯´RAGå°±æ˜¯ç”¨åµŒå…¥å‘é‡ä»€ä¹ˆçš„ï¼Œé‚£ä¹ˆæˆ‘åªèƒ½è¯´å¯èƒ½ä¼šæ¶ˆäº¡ã€‚retrieval è‚¯å®šæ˜¯åŒ…æ‹¬äº†å…¶ä»–æ–¹å¼çš„ï¼Œæˆ‘ä»¬ç”¨geminiå‘æ–‡ä»¶æˆ–è€…å›¾ç‰‡ç»™ä»–ä½•å°ä¸æ˜¯ä¸€ç§RAGå‘¢ï¼Œä¸è¿‡åªæ˜¯äººè„‘é©±åŠ¨ç½¢äº†

- ## RAG is anti pattern.
- https://x.com/pelaseyed/status/1882471632129994914
  - I donâ€™t do RAG anymore, get 10x the result just spinning up a pipeline and feed all content to Deepseek. And yes it scales to over 10K docs. 

- And you can cache it, so doesnâ€™t even cost that much

- Does deepseek have caching like Googleâ€™s media.upload? How do you otherwise avoid rebuilding the context all the time (Iâ€™ve not used the DS APIs).

- I wish I could fit the entire Internet in the prompt for @ask_pandi but it doesn't fit. RAG is still needed for this.
  - Run multiple LLMs in parallell, you will have infinite context window.

- You put everything into context?
  - Everything in context(s)

- RAG is A pattern, just not useful when your context is under 128k token and when the context is known. Pick the right tool for the job.
# discuss-solutions
- ## 

- ## 

- ## [Rebuilding RAG After It Broke at 10K Documents : r/LlamaIndex _202512](https://www.reddit.com/r/LlamaIndex/comments/1pgyedh/rebuilding_rag_after_it_broke_at_10k_documents/)
  - What Broke at 10K
- Latency Explosion: Went from 100ms to 2000ms per query.
  - Root cause: scoring 10K documents with semantic similarity is expensive.
- Memory Issues
  - 10K embeddings in memory. Python process using 4GB RAM. Getting slow.
- Semantic search on 10K documents = 10K LLM evaluations eventually = money.
- What I Rebuilt To
- Step 1: Two-Stage Retrieval
  - Stage 1: Fast keyword filtering (BM25) 
  - Stage 2: Accurate semantic ranking
- Step 2: Vector Database
  - Move embeddings to a proper vector database (not in-memory).
- Step 3: Caching
- Step 4: Metadata Filtering
- Step 5: Quality Monitoring

- What I Learned
  - Two-stage retrieval is essential - Keyword filtering + semantic ranking
  - Use a vector database - Not in-memory embeddings
  - Cache aggressively - 40% hit rate is typical
  - Monitor continuously - Catch quality degradation early
  - Use metadata - Filtering improves quality and speed
  - Test at scale - What works at 500 docs breaks at 10K

- What youâ€™re describing isnâ€™t RAG fundamentally â€œbreakingâ€ at 10K docs, itâ€™s your naive implementation breaking. 
  - Brute-force similarity in pure Python, mis-modeled costs, and in-memory everything will obviously struggle as you scale. 
  - The architecture you rebuilt (BM25 â†’ vector DB â†’ caching â†’ metadata â†’ monitoring) is solid and very standard, but 10K documents is not a meaningful scaling limit. The story is really â€œI started with a toy prototype and then replaced it with something closer to a normal production RAG stack, â€ not â€œRAG inherently breaks beyond 10K docs.â€
- This could have been limited to: I replaced my own in memory solution with a vector database.

- ## [RAGFlow is an open-source RAG engine based on OCR and document parsing | Hacker News _202404](https://news.ycombinator.com/item?id=39896923)
- Document processing is getting better and better with new tools leveraging LLMs. If anyone is interested in exploring this space, try another similar tool LLMWhisperer (https://llmwhisperer.unstract.com/). It is a part of Unstract, an open-source document processing tool (https://github.com/Zipstack/unstract)
  - Actually we've tried almost all lof existing open source models for document processing, and none of them performs well for complex documents, especially those having complicated tables, such as tables cells without borders, cells need to be combined, ..., etc. Although adopting LLMs to perform such document understanding tasks is more scalable, it requires much more data and computation power to achieve similar results. That's why we design such models start from scratch.

- I'm partly sad at the approach this and other engines take: reimplement each part (PDF parser, etc etc) in a way where they are pretty much useless except in their specific engine.
  - If instead we had a PDF() class that did what RAGFlow is doing (dealing with all the different trade-offs of the different python PDF engines such as pdfplumber), then we could easily adapt it and improve it, and it can be useful for other projects as well.
- It is open-source though. Just rip it off and make that PDF() class.

- A lot of the yolo stuff from ultralytics is AGPL3 fyi. Recommend caution depending on what code or models / model lineage are used
  - Thanks for your nice suggestion. We train the model using YOLO, but during inference, the model is converted into ONNX and we use ONNXRuntime for the model inference. As a result, YOLO itself is not included in the software package. We will open the training code in the repo soon.
  - We've used YOLOv8 as the object detection model, and use some public datasets, such as PubTable, CDLA, together with some private data to train the model. The model on Huggingface is the one trained using public dataset, and we would open this work later. We use YOLOv8 just because we want to let the document parser run without GPU, I think you could also try any other object detection models such as Detectron, and use the public datasets to train the model as well. We've not used transfomers for this task, because given limited data, it could not outperform traditional CNN based models.

- ## agentset - [Production RAG: what I learned from processing 5M+ documents | Hacker News _202510](https://news.ycombinator.com/item?id=45645349)
- I must be missing something, this says it can be self-hosted. But the first page of the self-hosting docs say you need accounts with no less than 6 (!) other third-party hosted services.
  - That was my observation as well. To be fair their business is to sell a hosted version, theyâ€™re under no obligation to release a truly self hosted version.

- The big LLM-based rerankers (e.g. Qwen3-reranker) are what you always wanted your cross-encoder to be, and I highly recommend giving them a try. Unfortunately they're also quite computationally expensive.

- Similar writeup I did about 1.5 years ago for processing millions of (technical) pages for RAG. Lots has stayed the same it seems

- > Reranking: the highest value 5 lines of code you'll add. The chunk ranking shifted a lot. More than you'd expect. Reranking can many times make up for a bad setup if you pass in enough chunks. We found the ideal reranker set-up to be 50 chunk input -> 15 output.
  - Reranking is a specialized LLM that takes the user query, and a list of candidate results, then re-sets the order based on which ones are more relevant to the query.
- What is the difference between reranking versus generating text embeddings and comparing with cosine similarity?
  - If you generate embeddings (of the query, and of the candidate documents) and compare them for similarity, you're essentially asking whether the documents "look like the question."
  - If you get an LLM to evaluate how well each candidate document follows from the query, you're asking whether the documents "look like an answer to the question."
- text similarity finds items that closely match. Reranking my select items that are less semantically "similar" but are more relevant to the query.
- Because LLMs are a lot smarter than embeddings and basic math. Think of the vector / lexical search as the first approximation.

- ## [28M Tokens Later: How I Unfucked My 4B Model with a smart distilled RAG : r/LocalLLM _202512](https://www.reddit.com/r/LocalLLM/comments/1pcwafx/28m_tokens_later_how_i_unfucked_my_4b_model_with/)
  - I've recently been playing around with making my SLM's more useful and reliable. I'd like to share some of the things I did
- TLDR
  - Use a strong model to write clean, didactic notes from source docs.
  - Distill + structure those notes with a local 8B model.
  - Load distilled notes into RAG (I love you, Qdrant).
  - Use a 4B model with low temp + strict style as the frontâ€‘end brain.
  - Let it consult RAG both for facts and for â€œwho should answer this?â€ policy.
- With a GOOD initial summary (and distillation) you can make a VERY capable little brain, that will argue quite well from first principles. Be aware, this can be a lossy pipeline...so make sure you don't GIGO yourself into stupid. IOW, trust but verify and keep both the source material AND SUMM-file.md until you're confident with the pipeline. (And of course, re-verify anything critical as needed).
  - I tested, and retested, and re-retest a lot (literally 28 million tokens on OR to make triple sure), doing a bunch of adversarial Q&A testing, side by side with GPT5, to triple check that this worked as I hoped it would.
  - The results basically showed a 9/10 for direct recall of facts, 7-8/10 for "argue based on my knowledge stack" or "extrapolate based on knowledge stack + reference to X website" and about 6/10 on "based on knowledge, give me your best guess about X adjacent topic". That's a LOT better than just YOLOing random shit into Qdrant...and orders of magnitude better than relying on pre-trained data.
- my RAG set up -
  Chunk size: 600
  Chunk o/lap: 175
  Embedding model: e5-small-v2
  Re-ranker: TinyBERT
  Top K: 6
  Top K_reranker: 4
  Relv score: 0
  BM25 weight: 0.6
  Qdrant embeds at 384 DIM. 
- TL; DR - everything is small and fast on hardware constrained rig (I7-8700, 32GB ram, 4GB Quadro P1000)

- So I've been thinking about this a lot and might embark on the same journey. Been playing with RAG pipelines a bit and am not hating it.

- Your distilled-notes-first approach is right; layer a strict retrieve-then-rerank, corpus hygiene, and automation to keep it sharp.
  - Concrete tweaks that worked for me: chunk 800-1200 tokens with small overlap and rich metadata (doc_id, section, version, date). Generate multi-query variants or HyDE to lift recall, then rerank with a local cross-encoder (bge-reranker-v2) before the 4B synthesizes. Add a confidence gate: if top reranked scores fall below threshold, return â€œinsufficient evidenceâ€ or escalate to the 8B. Use Qdrant payload filters to scope â€œbucketsâ€ and set MMR to avoid near-duplicate chunks. Hash paragraphs and re-embed only changed ones
  - Bottom line: distill first, then tight retrieve-then-rerank with guardrails, thresholds, and evals.

- ## ğŸ’¡ [I spent 2 years building privacy-first local AI. My conclusion: Ingestion is the bottleneck, not the Model. (Showcase: Ollama + Docling RAG Kit) : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pamu5t/i_spent_2_years_building_privacyfirst_local_ai_my/)
  - Iâ€™ve been working on strictly local, data-privacy-compliant AI solutions for about two years
  - The biggest lesson I learned: We spend 90% of our time debating model quantization, VRAM, and context windows. But in real-world implementations, the project usually fails long before the prompt hits the LLM. It fails at Ingestion.
  - I built a self-hosting starter kit that focuses heavily on fixing the Input Layer before worrying about the model.
  - Ingestion: Docling (v2). I chose this over PyPDF/LangChain splitters because it actually performs layout analysis. It reconstructs tables and headers into Markdown, so the LLM isn't guessing when reading a row.
  - Iâ€™d love to hear your thoughts on the "Ingestion First" approach. For me, switching from simple text-splitting to layout-aware parsing was the game changer for retrieval accuracy.

- I chose Docling primarily for Structure and Integration.
  - Structure vs. OCR: Paddle is great for raw OCR, but I needed Layout Analysis (reconstructing tables to Markdown). Paddle gives me text boxes; Docling gives me semantic context. For RAG, context is king.
  - Simplicity: Since I'm building a local desktop app, Docling felt much more 'native' to integrate into a Python backend than the Mineru stack. However, I'm pragmatic, not married to a tool.
  - I designed the ingestion pipeline to be modular. If Mineru proves to be significantly lighter or better at tables tomorrow, I can swap the engine. But right now, Docling hits the sweet spot between performance and setup pain.

- It took me a few months to understand quality of OCR matters the most, and anyone working on RAG that has used VLMs for OCR knows in their heart and soul that VLMs make the best OCR. Obviously this means, in addition to LLMs/Embedding Models there is a new expensive overhead of VLMs, unless you use higher up models, then you can do text gen + OCR from a single model.
  - How can you make an effective RAG project without decent hardware? 
  - You need 98% accurate OCR or your pipeline fails. 
  - You need a large context window so that full context of the document being retrieved can be loaded, granted, we can use chunking like we do with embeddings, but to me, this is an untested theory.
  - 8, 000 tokens is a lot of tokens, requires just a little less than 24 GB VRAM, issue is, how may people doing this locally on their potato laptop actually have 24 GB VRAM.
  - Not to mention re-rankers.
- Sounds like in-depth knowledge from my point of view ; ) VLMs produce better OCR than traditional parsers, but I think you're optimizing for the wrong bottleneck.
  - The 98% accuracy trap: You don't actually need 98% OCR accuracy if your pipeline is error-tolerant. A 90% accurate parse + a reranker + a small cleanup LLM can outperform a 98% parse that costs 10x more to run.
  - Why I optimize for 'good enough' hardware: My clients are mostly in Germany (GDPR-heavy industries). They prioritize local/privacy over raw performance. For them, the ROI calculation is simple:
  - The real question isn't 'How do I get 98% OCR?' It's: 'How do I build a system that works even when OCR fails 10% of the time, and runs 100% locally?'
  - That's why I focus on the pipeline (repair -> parse -> cleanup -> chunk -> rerank) rather than just throwing a bigger GPU at the problem. I still believe we focused on the wrong side and were blindsided vor the longterm issues

- looking at the flood of 'Why is my RAG hallucinating?' posts, it seems to be news for the 90% of developers who just joined the party via LangChain
  - And yes, even Docling struggles. That's exactly why I argue for a 'Sanitized Pipeline' (repair -> parse -> post-process) rather than just looking for a magic OCR tool. The 'pain' never goes away completely, but you can engineer around it. There is more after docling and bevor db ; )

- ## [Best open source document PARSER??!! : r/LlamaIndex](https://www.reddit.com/r/LlamaIndex/comments/1dicqkt/best_open_source_document_parser/)
  - Right now Iâ€™m using LlamaParse and it works really well. I want to know what is the best open source tool out there for parsing my PDFs before sending it to the other parts of my RAG.

- Marker is quite nice but not enough to get quality data from complex PDFs.

- I'd look into Unstructured, PyMuPDF, PyPDF, PDF.js

- Try RAGFlow https://github.com/infiniflow/ragflow which is based on deepdoc based document undertanding for better chunking results.

- i made marker+pix2tex combo and lamaParser/mathpix for complex documents

- Docling and Marker are really good open source document parsers. But obviously nothing compared to paid services. But is good enough. Docling especially works well with tabular data and layout analysis. And it also read images

- ## [How to Efficiently Parse Large PDF and DOCX Files (in GBs) for Embeddings Without Loading Fully in Memory? : r/Rag _202411](https://www.reddit.com/r/Rag/comments/1gjz2dj/how_to_efficiently_parse_large_pdf_and_docx_files/)
- Most pdf reading libraries use a .load function for each page. You can just control which pages it's called on and use garbage collectors if necessary

- pymupdf can load by page I think

- For handling massive PDFs or DOCX files, youâ€™ll want to look into libraries that support lazy loading and streaming. For PDFs, PyMuPDF (with fitz) or PDFMiner allows you to read specific pages directly without loading the whole thing into memory. For DOCX, python-docx doesnâ€™t have native streaming, but using it with smaller chunks of extracted text can work. If youâ€™re embedding, try handling these chunks individually instead of full files, as it cuts down memory use.

- Snapshot each page as an image > pixstral > markdown > chunk > use something like Antrhopics contextual retrieval to embed global context in each chunk

- this might be exactly what you need. its very fast and has buffered streaming: https://github.com/yobix-ai/extractous

- ## [I've been noticing more and more people are using GraphRAG instead of embedding and vector databases...is it really helpful or just the hype? : r/LangChain _202507](https://www.reddit.com/r/LangChain/comments/1e66e9r/graphrag/)
- Been working with graphrag on my current project. It is good, but it has its fallbacks. The good thing is that you can model you data to preserve relations. 
- The bad thing is:
  - you rely on your LLM to create queries based on your question to the answer. Lots of times the query created is pure shit, even with the most advanced models.
  - It gets really expensive. Like really expensive. Imagine that for every document you're storing in your graphDB you need to pass it to a LLM so it can extract the content and create relations so you can store it on your DB.

- If you ask me, people just follow the trends without understanding whether their application really needs something (eg GraphRAG here). 

- This is all basically more of a hype I would say. Why? Because you can use your own database that handles multiple data types and construct a knowledge graph. The knowledge graph basically shows the entity relationship between different nodes and this can be easily fed into your databases as tables and then, using retrieval methods, you can easily retrieve the most relevant chunk and the relationship score. 
  - I mean, the specialise databases like vector databases or even graph databases are really hyped, your own databases whether it is MongoDB, SingleStore, MariaDB, or even Couchbase can easily help you with creating graph knowledge and store them and then retrieve whenever required.

- how GraphRAG works with existing graphs, meaning if I already have entities and relationships, how would I load this graph in GraphRAG? 

- ## æœ‰æ²¡æœ‰æƒ³è¿‡å°†è‡ªå·±çš„ä»£ç åº“æ‰“åŒ…ç„¶åç›´æ¥å¡ç»™å¤§è¯­è¨€æ¨¡å‹æ¥å¤„ç†ï¼Ÿä¹Ÿè®¸ä½ ä¼šæƒ³åˆ°ç”¨RAGæˆ–è€…ç”¨windsurfæˆ–cursorã€‚ç°åœ¨æœ‰äº†æ›´ç®€å•çš„åŠæ³•â€”â€”Repomix
- https://x.com/karminski3/status/1881150047276138689
  - è¿™ä¸ªåº“å¯å°†æ•´ä¸ªå­˜å‚¨åº“æ‰“åŒ…åˆ°ä¸€ä¸ª AI å‹å¥½çš„æ–‡ä»¶ä¸­ï¼Œæ–¹ä¾¿ç»™å¤§è¯­è¨€æ¨¡å‹ä½¿ç”¨ã€‚

- ## ğŸ“Œ A list of software that allows searching the web with the assistance of AI.
- https://x.com/tom_doerr/status/1856778512612667838

# discuss-pdf/docs
- ## 

- ## 

- ## 

- ## 

- ## 

- ## 

- ## [Practical (online & offline) RAG Setups for Long Documents on Consumer Laptops with <16GB RAM : r/LocalLLaMA _202412](https://www.reddit.com/r/LocalLLaMA/comments/1hq36dn/practical_online_offline_rag_setups_for_long/)
  - there are a ton of different frameworks and libraries to implement a RAG system. For the purpose of my tests, I only focused on those with a UI or offering an already made RAG pipeline, because I did not yet learn how to implement RAG by myself.
  - Various offline and online LLM models: phi-3.5-mini, gemma2-2b-it, mistral, phi-4, Hermes2.5, Ghost, Qwen2.5:1.5b, Qwen2.5:7b, 
  - RAG Frontends: msty, anythingLLM, Witsy, RAGFlow, Kotaemon, khoj.dev, Dify, etc. (OpenWebUI failed to install on my machine, QwenAgent remains to be tested)
  - Backend: ollama, ChatGPT, Gemini.
  - I found only two working solutions for the multi-needles test to succeed 4 out of 4 (4/4):
  - Either without RAG using LLM models that implement infini-attention. Although the method has been published openly, currently, only the Gemini models (online, including the free Flash models) implement it, offering 1M tokens context size. 
  - Either with a RAG that somehow mimics infinite attention. 
  - Note that in all successful cases, I found that making the prompt to be iterative (which is a change I did over ggerganov's original prompt) was necessary to increase the reliability of the retrieval, otherwise some questions (up to half of the questions) failed (even with Gemini IIRC).

- ## ğŸ”¡ [MongoDBAtlasVectorSearch the PDF exceeding 100 pages cannot be processed. How can this be resolved? Â· Issue Â· langchain-ai/langchain _202409](https://github.com/langchain-ai/langchain/issues/26518)
- We tried splitting a long PDF using this code

- ## [Best practices for handling large-scale RAG implementation with thousands of lengthy PDF documents _202508](https://community.latenode.com/t/best-practices-for-handling-large-scale-rag-implementation-with-thousands-of-lengthy-pdf-documents/34950)
  - Many of these documents are quite large, with some containing up to 300 pages each. All the PDFs have been processed through OCR, so Iâ€™m dealing with plain text content.
  - My main challenge is with the document ingestion phase into the knowledge base. I initially attempted to use an open webui setup with an ollama backend, but the performance was unacceptably slow for this volume of data.

- Been there with similar document volumes - the ingestion bottleneck will absolutely kill your project if you donâ€™t handle it right.
  - Break this into parallel chunks instead of processing everything sequentially. Split those 300-page docs into smaller sections (10-20 pages each), then run multiple processing pipelines at once.
  - Donâ€™t build this manually. Coordinating document splitting, embedding generation, vector storage, and error handling gets messy fast. Youâ€™ll debug pipeline issues more than actually using your RAG system.
  - I handled 25k documents last year using automation for the entire flow. Set up parallel streams processing ~500 documents simultaneously, with automatic retry for failed chunks and smart batching for vector database writes.
  - Latenode makes complex document pipelines like this really straightforward to build and manage. Check it out at https://latenode.com

- Had the same scalability nightmare building a RAG system for legal docs last year. Game changer was ditching sequential processing for parallel with a proper queue system. Used Celery + Redis to spread the work across multiple workers - cut processing time from weeks down to 2 days for ~25k documents. Also stopped using fixed-size chunks and parsed by sections/paragraphs instead to keep the meaning intact. Switched from Chroma to Weaviate for the vector DB - handles scale way better. Heads up on memory requirements - youâ€™ll need tons of RAM or go with disk-based storage. Pro tip: test retrieval quality early because what works for hundreds of docs can fall apart at this scale.

- ## ğŸŒ° [Chatting with Large PDFs (100â€“500 Pages): Using RAG with OpenAI Embeddings (Local vs. API) _202504](https://medium.com/coxit/chatting-with-large-pdfs-100-500-pages-using-rag-with-openai-embeddings-local-vs-api-2be2a73c20d4)

- COXIT decided to invest resources to evaluate research and develop a prototype
- COXIT wanted to validate and then evaluate approaches to consciously embed large documents (100â€“500 pages) into LLMâ€™s context, how to make LLM operate, refer to, quote information from those PDFs, what document extraction and processing pipelines should look like.
- I keep intentionally avoiding Agentic Frameworks like LangChain, as they show no benefit from a long-term perspective, as they are generally poorly optimized and overbloated.

- Speaking about Appâ€™s Architecture (Figure 1): I designed it to consist of 4 microservices:
  - ChatUI â€” React-powered WebUI. I declare that the React code is notoriously badly written
    - https://github.com/v4ler11/llm-chat
  - Chat Tools Service â€” Handles completion requests, implements message history limiting, local tools execution and model validation, and files handling, including file stream uploads.
    - https://github.com/v4ler11/llm-tools-server
  - Chat Proxy Streamer â€” re-routes incoming chat completion requests to various LLM Providers e.g. OpenAI, Anthropic, Google using a standardized OpenAI Format, provides authentication, usage stats collecting.
    - https://github.com/v4ler11/llm-portal
  - Document Search MCP â€” a dedicated service that is only used to perform operations with documents: extraction, processing, storage, and vector search.
    - https://github.com/COXIT-CO/chat-with-pdf-poc

- Approach 1: OpenAI-based vector search
  - For each text-chunk or a paragraph (in case you are OK with OpenAIâ€™s chunking strategies), youâ€™d need to perform 2 requests to upload a paragraph as a file
- Approach 2: local vector search and storage, external embedding requests
  - an approach that is based on storing vectors locally and, consequently, performing vector search also locally, uses only 6* requests to fully process a given document. Thatâ€™s a 200x improvement over the previous approach. Total waiting time for processing a document resulted in 11s â€” thatâ€™s 14x improvement.
  - For 100-page document embeddings requests with batch-size = 128 were sent. For documents of other sizes, batch-size should also be adjusted to keep a minimal response time from an external embedding model. E.g., for 400-page document you might try 128 Ã— 4 = 512 batch-size.

- When this prototype is used in real-life projects, weâ€™d have to set up a more precise validation pipeline, taking into account documentsâ€™ domain. That pipeline should be centered about the same idea: measure the difference between model response in 2 cases: when the model has all context needed to answer a question e.g., provide a complete document and a 2nd case: when the model will have to use RAG calls to retrieve that context.

- ## [Best Python library for fast and accurate PDF text extraction (PyPDF2 vs alternatives) : r/LangChain _202508](https://www.reddit.com/r/LangChain/comments/1mxye53/best_python_library_for_fast_and_accurate_pdf/)
- Pymupdf is faster than docling. 10-50x

- Check out `pdfplumber` for its flexibility and ability to handle complex PDF layouts. It might improve efficiency if PyPDF2 isn't meeting your needs.

- ## [Best way to extract data from PDFs and HTML : r/Rag _202510](https://www.reddit.com/r/Rag/comments/1oavnx4/best_way_to_extract_data_from_pdfs_and_html/)
- During my experience extracting data from PDFs and HTML files for use in RAG systems, I usually follow one of the two approaches shown in this notebook I created (VLM or docling/paddleOCR)â€” I hope you find it helpful. 

- Have you looked at Pinecone Assistant? You can upload PDFs (up to 100MB) and it manages the chunking, embedding, and search for you. If you already have chat/model generation, you could use just the /context API to get search results to feed into your own model.

- At PipesHub, we use docling, pymupdf (faster than docling but need to use layout parser on top of it), ocrmupdf/Azure DI (scanned pdfs).
  - On top of docling, we have specialized logic for extracting metadata, deep understanding of document, tables, etc. 
  - We use VLM/Mulitimodal AI models for handling images, diagrams and more.
  - You can use docling (only issue it's parsing is slow) if you are looking for open source, free solution.
  - If you are looking for Higher Accuracy, Visual Citations, Cleaner UI, Direct integration with Google Drive, OneDrive, SharePoint Online, Dropbox and more. PipesHub is free and fully open source, extensible. 

- I have done two approaches so far. Docling was good, but I was not happy that it took long on larger PDFs. 
  - Another approach was converting the pdf to docx, then docx to html and finally to markdown. This approach was quite fast, but `pdf2docx` library had some trouble converting certain documents.
  - Docling to me was quite a good solution, and if you are okay with the amount of time to process larger documents.
  - Overall, a hybrid approach is always the best solution.
# discuss-rag-local
- ## 

- ## 

- ## 

- ## 

- ## 

- ## [I tested what small LLMs (1B/3B) can actually do with local RAG - Here's what I learned : r/LocalLLaMA _202410](https://www.reddit.com/r/LocalLLaMA/comments/1gdqlw7/i_tested_what_small_llms_1b3b_can_actually_do/)
  - Langchain RAG workflow
  - Nomic's embedding model
  - Chroma DB
  - Llama3.2 3B instruct
  - the smaller models (Llama3.2 3B in this case) start to break down with complex stuff. Ask it to compare year-over-year growth between different segments and explain the trends? Yeah... it start outputting nonsense.

- My use case would be finding rules for board games and tabletop RPGs when I only have a vague idea of what the rule does, or the name. I haven't found one that works too well yet- lots of false positives and no page numbers.

- While RAG systems typically search for relevant content from the source and pass it to an LLM as input for the LLM to write new content.  Maybe you just want to see the relevant content directly?  That's computationally cheap.

- 137M parameters isn't tiny for an embedding model! Embedding models are naturally much smaller than LLMs since they only convert text to vectors, rather than generating text and reasoning.
- Yes, I tested several small embedding models (under 1B parameters), including MXBai, but Nomic 1.5 performed the best. 

- ## ğŸ’¡ [Embedding With LM Studio - what am i doing wrong : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1lharbh/embedding_with_lm_studio_what_am_i_doing_wrong/)
  - "text-embedding-nomic-embed-text-v1.5" loads fine and works with Anything.
  - but text-embedding-qwen3-embedding-0.6b & 8B and any other Embed model i use i get the below error: Failed to load embedding model. Internal error: Failed to initialize the context: failed to allocate compute pp buffers

- SOLVED
  - LM Studio was defaulting to 4096 Tokens where as the model can only handle 1024. Same goes for AnythingLLM , that defaults to 8192 tokens. set everything to 1024 and now its working

- Your "SOLVED" update is wrong. I ended up on this post after also having memory allocation issues with Qwen3 embedding models. 
  - The "1024" value you are quoting is the number of output dimensions of the model that clients would use to interpret results. You don't set that anywhere in LM Studio.
  - all you did by lowering the context length to 1024 tokens is reduce memory usage enough to fix your issue. You also made the output quality worse. Your clients will have to make more requests for more chunks for indexing since the model only accepts 1024 tokens at a time. This will in turn increase the number of vector entries that need to be stored.

- I had to "Override Domain Type" to Text Embedding to get it to work my computer (Models -> settings gear -> bottom setting). Not sure what else to try if you've already done that.
  - yep, did forget to mention that i tried both configurations

- try using any model you have as an embedder. It works just fine.

- ## [What Embedding Models Are You Using For RAG? : r/LocalLLaMA _202312](https://www.reddit.com/r/LocalLLaMA/comments/18j39qt/what_embedding_models_are_you_using_for_rag/)
- For RAG, I have been using multilingual minilm model from HF hub (I have multiligual requirement), and qdrant vector database. Happy with both. Specially, minilm embeddings are 368 dims, so some space saving there as well.

- Chinese embedding models and llms are now the top performers. Qwen-1.5B Yi-34B gte-large bge-large

- I chose the most lightweight stack possibleâ€” using all-MiniLM-L6-v2 for embeddings and HyperDB for local storage/retrieval (this is super fast for <100k documents)

- I am currently using all-MiniLM-L6-v2 but responses are generally not that good my data size is almost 600mb and it is giving me not that good results, what kind of embedding should i choose considering the size of data and database of pgvector postgres

- What Embedding model is best for Text-To-SQL for Financial data? Currently I'm using OpenAIEmbeddings
  - Text-to-SQL is a sequence to sequence task. My very first thought was "zero shot learning" because I have done this successfully in other domains. 

- I use whatever models are on top 1-5 on the MTEB leaderboard and run my custom evaluation 
# discuss-rag-tips/tricks
- ## 

- ## 

- ## 

- ## 

- ## 

- ## [I rewrote hybrid search four times - here's what actually matters : r/Rag](https://www.reddit.com/r/Rag/comments/1pd7tao/i_rewrote_hybrid_search_four_times_heres_what/)
  - The actual problem Most resources tell you "combine vector search with keyword search" and show you a 0.5/0.5 weight split. That's it. But when you actually build it with real product data, you hit these issues: SKU codes like "MBP-M3MAX-32-1TB" return garbage from vector search; Score ranges don't match (vectors give you 0.3-0.4, BM25 gives you 15-50)
  - How I approached it
  - Each product has multiple fields - not just the description. This matters for multi-field indexing.
  - You can't just add vector scores (0.3-0.4 range) to BM25 scores (15-50 range). I implemented three normalization methods in the example
  - Query patterns should determine your weights. Instead of hardcoding 0.5/0.5, I built pattern detection
  - Multi-field indexing is critical. For product search you need to index multiple fields separately
  - When keyword search returns zero results (user searches "Microsoft laptop" but you only sell Apple/Dell), you need a fallback
  - https://github.com/pguso/rag-from-scratch /MIT/js

- ## [Whatâ€™s your go-to combo of LLM + embedding model for RAG? : r/Rag _202512](https://www.reddit.com/r/Rag/comments/1pba2lt/whats_your_goto_combo_of_llm_embedding_model_for/)
- Qwen3 8B is the current SOTA for embeddings with a 32k context size, which gives you much more flexibility when chunking text.
  - Gemini 2.5 Flash has very good performance and is really cheap.
  - Once you factor in maintenance costs, potential GPU issues, downtime, etc., itâ€™s almost always better to just use Googleâ€™s service â€” it saves you a ton of headaches and is always online.

- ## ğŸ’¡ [I built RAG for a rocket research company: 125K docs (1970s-present), vision models for rocket diagrams. Lessons from the technical challenges : r/LLMDevs _202509](https://www.reddit.com/r/LLMDevs/comments/1nr59iw/i_built_rag_for_a_rocket_research_company_125k/)
  - 125K documents from typewritten 1970s reports to modern digital standards
  - 40% weren't properly digitized - scanned PDFs that had been photocopied, faxed, and re-scanned over decades
  - Going Vision-First with Local Models
  - Dual embedding strategy
  - Fine-Tuning for Domain Knowledge

- 
- 

- ## ğŸ¤” [Companies need to stop applauding vanilla RAG : r/Rag](https://www.reddit.com/r/Rag/comments/1mj0tlh/companies_need_to_stop_applauding_vanilla_rag/)
  - I built a RAG system for internal documents pulled from a mix of formats, like PDFs and wikis. At first, the results were clean and useful.
  - But that was at the start. as the document set grew, the answers werent as reliable. Some of them werent using the most up to date policy section, or they were mixing information when it shouldnt be.
  - We had been using Jamba for generation. It worked well in most cases because it tended to preserve the phrasing from retrieved chunks, which made answers easier to trace. 
  - You have to keep your documentation up to date and/or have a more structured retrieval layer. If you want your setup to reason about the task, RAG is not enough. Itâ€™s retrieval, not orchestration, not a multi-layered workflow.

- vanilla RAG probably means your typical quick langchain setup (fixed chunk, embed, hybrid search). A step upwards would be something like hierarchical chunking and embedding, and then more advanced stuffs like Hyde, agentic, or even just rule based retrieval

- A more structured approach so that the LLM can understand the key entities of the business (companies, customers, projects, etc) and the key artifacts that make up your store of documents/knowledge (sales presentation, quotes, meeting notes, etc). And then the ability to connect the dots between those itemsâ€¦ eg this sales deck mentions this company, which matches to this record in our CRM.
  - The main difference with all these new tools we have is that you donâ€™t have to do all that definition as part of some crazy data orchestration flowâ€¦ but rather can do a lot of it at retrieval time.

- Semantic chunking is basically a way of using document context like headings as part of the chunking process and than retaining a contextual map across all chunks in json format to help have more context when you. Retrieve chunks in searches.

- I agree completely. The core issue with RAG is that it's a passive system, it can only retrieve what's already there. I saw this exact problem at my last job, and now I'm building an MVP to solve it.
  - My approach is to make a better RAG system that not only retrieves information but also identifies knowledge gaps and initiates a question workflows for employees to verify and answer, creating a truly dynamic and complete knowledge base.
- Bottleneck is the rag that's limiting the amount of queries, it'll probably be fixed if we went towards higher tier but that cost isn't justified yet.

- This is a realistic issue in practice. There can be various ways to make the system smarter. One is to use an agentic design and give agents hints to decide whether newer contents should be retrieved (some people may prefer this way to leverage "the I in AI" more). On the other hand, you may also design your database tables and refine the query, for example by adding a timestamp with each vector when it's injected, then during query, sort the top K results by timestamp, or even make a cut off by the timestamp.

- rag needs reranking folks
  - yes reranking folks is a brilliant way to phrase it, other people need to feed it in!

- ## ğŸ”¢ [How to dynamically prioritize numeric or structured fields in vector search? : r/LangChain _202510](https://www.reddit.com/r/LangChain/comments/1oclpn4/how_to_dynamically_prioritize_numeric_or/)
- This will not happen automatically. Either you have to rewrite the query or do a query classification and let this be handled by a database query.
  - You may be able to store the numerics as metadata and use the metadata to rerank, but to do even that, you need to classify the query first.
  - You can have a set of ranking attributes as metadata, and classify the query using a pretrained classification model, to find which ranking attributes to use and implement the reranking on those metada attributes.

- Yeah, this is a common puzzle when you move beyond basic semantic search. Post-processing reranking is probably your cleanest bet.
- The general idea is to do a two-step retrieval:
  - Use the vector search to get a broad set of semantically relevant candidates (e.g., the top 20-50 documents).
  - Then, in your application layer, iterate over just those candidates. You can parse the query to see if it mentions "top" or "most, " then extract the relevant numeric field (ranking, publications, etc.) from the metadata of those candidates and re-score them.
- This way you're not trying to cram structured logic into the embedding space. 
  - LlamaIndex has postprocessors for exactly this kind of thing, you can even write a custom one. It's way more flexible than trying to make the vector DB do everything.

- milvus supports hybrid search (vector + bm25) with reranking strategies. you can filter metadata pre-search using comparison operators on numeric fields, or build a custom llamaindex postprocessor that combines vector similarity with dynamic metadata scoring. retrieve top-k, then rescore as: new_score = similarity_weight * vector_score + metadata_weight * normalize(field). this avoids hardcoding thresholds. alternatively use llmrerank to let the llm judge relevance based on metadata context, but slower/costlier.

- ## [RAG over Database : r/LangChain _202407](https://www.reddit.com/r/LangChain/comments/1efnx5u/rag_over_database/)
  - I have been trying to build a RAG over a database that has mulitple tables. Often times, for a user query, the data has to be searched by joining multiple tables. I followed this approach as mentioned in Langchain documents.
  - What I am observing is that many times the query generated by LLM is not correct and the data that user wants is incorrect. 

- Try to make all the joins yourself in the form of view and provide a simple view for the LLM

- dont use joins. use views or make stored procs to call for summaries etc you want.

- I would use the traditional RAG-based approach more in cases when the knowledge base is made of documents instead of structured data.
# discuss-code-rag
- ## 

- ## 

- ## 

- ## [Whatâ€™s the best way to chunk large Java codebases for a vector store in a RAG system? : r/Rag _202512](https://www.reddit.com/r/Rag/comments/1pgliqi/whats_the_best_way_to_chunk_large_java_codebases/)
- AST-based chunking is definitely the way to go for Java codebases. Simple token or line-based splits will break methods and classes in weird places, making it harder for the model to understand context and relationships between code elements. Tree-sitter is solid for this, and there are some decent parsers out there, but honestly the tooling ecosystem for experimenting with different chunking strategies is pretty fragmented.

- Thatâ€™s not going to be particularly useful. Look at Claude code, Gemini cli, codex etc. none of them create embedding from your code

- tree-sitter is great. https://cocoindex.io/docs/examples/code_index this project does codebase indexing and supporting java & large codebase, and is open sourced.

- ## [[D] What I learned building code RAG without embeddings : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pfxl6x/d_what_i_learned_building_code_rag_without/)
  - I've been building a system to give LLMs relevant code context from any repo. The idea seemed simple: let an LLM look at the file tree + function signatures and pick which files to include. No embeddings, no vector DB.
- File paths arenâ€™t enough. I added AST-extracted symbols
  - Retrieval quality jumped noticeably (NDCG went from ~0.85 to ~0.92). The model doesnâ€™t need to read the full file to know â€œthis smells like auth.â€
- Generic eval criteria reward BS
  - Anchoring eval on specific symbols/files made it much easier to spot hand-wavy nonsense.
- So the â€œLLM looks at the tree + symbols and picks filesâ€ setup landed roughly on par with embeddings on answer quality, without the indexing infrastructure. Good enough for me to keep using it.

- ## [How vscode team is making copilot smarter with â€œlessâ€ tools : r/vscode _202511](https://www.reddit.com/r/vscode/comments/1p4ucl5/how_vscode_team_is_making_copilot_smarter_with/)
  - how RAG came back into fashion after being declared â€œdead, â€ and how that revival gives new life to another technology people are prematurely burying â€” tool calling, or by its full name: MCP.
  - All the tools that make an agent actually agentic - searching, editing, creating files, and more - are built directly into agents and exposed through classic tool calling. Itâ€™s very similar to MCP, and it brings the same problem with it: every tool consumes context in every single request.
  - So what did the Visual Studio Code team do?
  - They combined two approaches:
  - Adaptive Tool Clustering - They use embeddings to compute a vector for each tool, then cluster similar tools based on cosine similarity. This creates â€œvirtual groupsâ€ of tools that belong together.
  - Embedding-Guided Tool Routing - When the user asks something, the embedding of the query is compared to the group vectors, routing the agent straight to the relevant group without scanning all of them. Less trial-and-error, less wasted context.
  - What does that give us? A real reduction: from 40 tools the agent could call - down to 13 smart clusters.
  - And whatâ€™s next? Theyâ€™re aiming for long-term memory - the ability to remember which clusters and tools worked for similar tasks, prioritize them, and understand how to use each tool optimally.
  - And what about MCP? If this mechanism is expanded into the MCP client layer (not just the built-in Copilot tools), it could also solve the well-known context overload issue in MCP.
  - The interesting question: will this become part of the official spec, or remain in userland where each client implements it differently?
  - In the meantime, sub-agents with clearly defined tool scopes are a solid workaround - which, honestly, is exactly what weâ€™d want the agent to learn to do by itself.

- The first thing I do is to disable all this crap from VS.

- ## ğŸ†š [Does grep perform better than vector DB + embeddings in large code bases? : r/ClaudeCode _202511](https://www.reddit.com/r/ClaudeCode/comments/1osyrpu/does_grep_perform_better_than_vector_db/)
  - Unlike Cursor or Github Copilot, I see that Claude Code seems to leave it up to the user to either do the indexing or not. Is there a reason? Does it perform better?
- Short answer: grep â‰  BM25 â‰  vectors. They each win on different axes.
  - grep/ripgrep â€” exact string/regex scan over files. Zero indexing, blazing on rare tokens and precise patterns (â€œdef foo(|GUIDs|error codesâ€). Great for â€œI know the string.â€
  - BM25 (inverted index) â€” lexical retrieval with ranking. It tokenizes code/text and returns files that share the same terms, weighted by `tf-idf` . Faster than grep on huge repos (no full scan) and returns a ranked list, but itâ€™s still keyword-based (no synonym/semantics unless you add query expansion). Think Zoekt/Sourcegraph style code search.
  - Embeddings (vector DB) â€” semantic retrieval. Finds conceptually similar code/comments (e.g., â€œexponential backoff retryâ€ locating retry_with_jitter() in another lang with no â€œbackoffâ€ keyword). Costs an index build + memory, but best when you donâ€™t know exact strings.
- Trade-offs:
  - Speed: grep (no index) < BM25 (indexed) < vectors (indexed + compute)
  - Recall: grep (exact) < BM25 (lexical fuzzy) < vectors (semantic)
  - Precision out-of-the-box: grep high for exact needles; BM25 good for term overlap; vectors need a reranker to avoid drift.
- Best practice in large codebases:
  - Hybrid: BM25 (or Zoekt) for lexical + a small vector index for semantics.
  - Fuse results (Reciprocal Rank Fusion or LLM rerank) so you get both â€œknown stringâ€ hits and conceptual matches.
  - Keep grep/ripgrep handy for one-off precise hunts; use the indexes when scale/recall matter.
- So â€œdoes grep perform better?â€ 
  - For exact, known strings on your machine, often yes. 
  - For concept queries across languages/renames, vectors win. 
  - For day-to-day, hybrid > either alone. Edit: reworded

- People @Cursor have written a bunch about their belief that strong semantic embeddings are way better for coding tasks perf  [Improving agent with semantic search Â· Cursor _202511](https://cursor.com/blog/semsearch)
  - Closed benchmark, proprietary embeddings model... Even if it works  (the improvements aren't huge to begin with ) there is no way to reproduce their setup. 

- ## [Cline doesn't index your codebase. No RAG, no embeddings, no vector databases. : r/CLine _202505](https://www.reddit.com/r/CLine/comments/1kwhcz0/cline_doesnt_index_your_codebase_no_rag_no/)
- Does Roo Code also do that or is this just a Cline thing?
  - Roo recently added it for code search. It works with local models
  - They also added prompt caching for Gemini flash.

- Exceedingly large context windows don't just result in extreme costs but it also will slow down your every operation.
  - While prompt caching reduce costs it is not entirely free, nor does every model or provider support it. It's also like treating a symptom rather than the disease.

- RAG can be good. See how fast and precise is AUgment Code search
  - ive found Augment to miss obvious things at times

- cursor has code indexing

- RAG shines when you donâ€™t know the exact function / class names in a repo. ripgrep file search that Cline uses for context search is awesomeâ€”if you already have the right keywords. Thatâ€™s easy on your own project, but on a massive, unfamiliar codebase ripgrep can stall while RAG keeps rolling (although it can return the wrong chunks).

- Privacy issue first. Never is sending low quality windows off the code base to the LLM is bad for quality.

- ## ğŸ¤¼ Cline doesn't index your codebase. No RAG, no embeddings, no vector databases. _202505
- https://x.com/cline/status/1927226680206131530
  - [Why Cline Doesn't Index Your Codebase (And Why That's a Good Thing) - Cline Blog _202505](https://cline.bot/blog/why-cline-doesnt-index-your-codebase-and-why-thats-a-good-thing)
  - This isn't a limitation -- it's a deliberate design choice. 
  - As context windows increase, this approach enhances Cline's ability to understand your code.
  - The industry default: chunk your codebase, create embeddings, store in vector databases, retrieve "relevant" pieces.
  - But code doesn't work in chunks. A function call in chunk 47, its definition in chunk 892, the context that explains why? Scattered everywhere.
  - We believe in the agentic power of the modesl, and with Claude's 200K+ context window, we don't need clever retrieval. We need intelligent exploration.
  - So Cline reads code the way you do -- following imports, tracing dependencies, building connected understanding.
  - ğŸ’¡ Instead of indexing, Cline starts with structure. Using ASTs, it maps your codebase architecture -- classes, functions, relationships.
  - Then it explores. Need to add error handling? It traces from your function to your error utilities to similar patterns. Connected comprehension.
- The problems with RAG for code run deeper:
  - Chunking breaks the logical connections between functions and their dependencies
  - Indexes become stale the moment you push a commit
  - Your IP gets duplicated in vector embeddings (security nightmare)

- ## [Unpopular opinion: RAG is actively hurting your coding agents : r/ChatGPTCoding _202505](https://www.reddit.com/r/ChatGPTCoding/comments/1ktt4ab/unpopular_opinion_rag_is_actively_hurting_your/)
- I've been saying this since RAG first became the term used to describe the method. And you are exactly right, the whole reason it became a thing was because, back when context windows were 4k or 8k max, it was out of necessity. Now, in the age where context windows are 1M or 10M tokens, it only makes sense in specific enterprise cases where you have vast datasets to query for specific, isolated information.

- RAG isnâ€™t just calling vector stores, itâ€™s also prompt priming before generation using various sources. Dynamically priming the prompt with relevant information before the LLM generates a response. 
  - A lot of the large context models drop off in accuracy after 100k tokens, anyway.

- frontier models have large context windows, but they all equate to a token, which equals a cost
  - Knowledge Graphs + RAG. An syntax tree is constructed of the code , where each class, function , method etc become nodes in the graph. The LLM can then traverse the graph to get only what it needs.

- One thing Aider has that Roo and Cline don't, is what they call a "RepoMap" (and it ties aider to git). 
  - But the advantage of it is, given a class, it can easily determine all the related classes, so it doesn't have to go digging through folders, trying to figure out which file is actually relevant, it knows, because the RepoMap shows which classes use which classes.
  - I pulled the RepoMap class out of aider and eventually managed to remove all the aider dependencies and got running as a command-line app. I might try making an MCP with it and giving that to Cline and see if it can improve its ability to understand the app structure.
- I thought cline already had a tree sitter which did the same thing.
  - it does use tree-sitter, but not in the same way and not to the same effect. 
  - How Aider understands you program vs how Cline understands it is very apparent in working with them. Aider has a much better understanding of which bits of code are related to which other bits, whereas Cline is frequently guessing based on the filenames.
  - Using tree-sitter isn't enough. You need to actually map out the relationships and Cline doesn't seem to do this, at least not nearly as effectively as Aider does.
- using the LSP if available surley is the best approach?
  - No, beyond getting LSP diagnostic, which you can get somewhere else anyway. Using LSP server requires counting rows and columns, and LLMs are bad at it.
- but you don't have to use the interface literally
  - Then I'm not using LSP, I'm using something that uses LSP under the hood to point that is an implementation detail. Take a look at your favorite LSP server, tell me what is useful there to LLM agent besides diagnostics?
  - I can see how you give LLM diagnostics after it update the file is useful, but giving LLM access to LSP overall is IMO utterly useless.

- I was watching an interview video with Claude Code engineers and they mentioned along the lines of not using RAG or GraphRAG because they found that Claude Code performed better without it and just relying on tools, memory and agentic search.

- RAG still makes sense for latency sensitive use cases like real-time chat, although prompt caching helps with this. 

- ## [Lessons learned from implementing RAG for code generation : r/LLMDevs _202501](https://www.reddit.com/r/LLMDevs/comments/1hw1n5o/lessons_learned_from_implementing_rag_for_code/)
  - ğŸ“ [A Recipe for a Better AI-based Code Generator | Pulumi Blog _202501](https://www.pulumi.com/blog/codegen-learnings/)
  - We wrote a blog post documenting how we do retrieval augmented generation (RAG) for code generation in our AI assistant, Pulumi Copilot. 
  - Measure and tune recall (how many relevant documents are retrieved out of all relevant documents) and precision (how many of the retrieved documents are relevant)
  - Implement end-to-end testing and monitoring across development and production
  - Create self-debugging capabilities to handle common issues like type checking errors

- when I need understanding a medium sized code base I have dumped the whole thing into gemini in Google AI workbench. It can do 2 million tokens, though it is SLOOW.

- I wonder if you've considered feeding more tokens from your retrieval result into your code gen step? Why 20k? Is that always enough? How would you even know if it weren't?
  - You've got part of the answer right there: SLOOW :-) Also expensive!
  - Seriously though, you want to optimize for both recall and precision. Too much irrelevant data (poor precision) can confuse the LLM leading to hallucinations.
  - 20k is based on "it feels right" and empirical data but honestly we have not done enough analysis to conclude that it is the perfect number for all scenarios - we will continue to measure and adjust.

- Did you experiment with other retrieval methods besides or in addition to semantic similarity? I've done some work using different techniques, like parsing dependency trees out of the current file, with promising results for code RAG.
  - We did look into BM25 for FT search but did not see measurable benefits for our use cases. Our approach relies on getting a lot of documents first and then pruning - it would be better to get just what's needed in the first place, I still hope BM25 can help there. Worth another look!
# discuss-rag-graph
- ## 

- ## 

- ## 

- ## 

- ## [Why do GraphRAGs perform worser than standard vector-based RAGs? : r/Rag](https://www.reddit.com/r/Rag/comments/1pi2q68/why_do_graphrags_perform_worser_than_standard/)
  - You'd expect GraphRAG to win, right? Graphs capture relationships. Relationships are context. More context should mean better answers.
  - that's not what they found. In several tests, GraphRAG actually degraded retrieval quality compared to plain old vector search.
  - Because I've also seen production systems where knowledge graphs and graph neural networks massively improve retrieval. We're talking significant gains in precision and recall, with measurably fewer hallucinations.

- In my experience, the issue isn't that graphs don't help - it's exactly what you said about the implementation approach. I've seen GraphRAG work incredibly well, but only when teams treat the graph construction as an ML problem that needs iteration and measurement.

- Vector RAGs are based on Similarity. Graph RAGs are based on relationships. As such at the best they can compete with relational databases only

- They model the statistical dependencies between data. Standard vector-based RAG assumes all day are independent and identically distributed (i.i.d.)
# discuss-rag-vector-db
- ## 

- ## 

- ## 

- ## 

- ## 

- ## [Built a deterministic RAG database - same query, same context, every time (Rust, local embeddings, $0 API cost) : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1pisow5/built_a_deterministic_rag_database_same_query/)
  - Built AvocadoDB to fix it
  - Local embeddings via fastembed (6x faster than OpenAI)

- In what situations is the same query giving different retrieved results ? If you have the literal exact query, why not cache the LLM response too? That is the more time consuming part and does give meaningful different results even with a temperature of 0 through providers.
- Why Same Query Can Give Different Results in Traditional RAG
  - Traditional vector databases (Qdrant, Pinecone, Weaviate, etc.) return non-deterministic results because:
  - Approximate Nearest Neighbor (ANN): HNSW and similar algorithms trade exactness for speed. The search path through the graph can vary, especially with concurrent queries or after index updates. Floating point non-determinism: Different execution orders (parallelism, SIMD) can produce slightly different similarity scores, changing ranking.
  - Index mutations: Adding/removing documents changes the HNSW graph structure, affecting which neighbors are found even for unchanged documents.
  - Tie-breaking: When multiple chunks have identical/near-identical scores, the order is arbitrary.
  - Embedding API variability: Some embedding providers return slightly different vectors for the same text across calls.
- You're right that caching LLM responses is the logical next step - retrieval determinism is really just the foundation for response caching. Once you guarantee the same query produces the same context, you can cache the full response:
  - cache_key = hash(query + context_hash + model + temperature + system_prompt)
  - So the answer to "why not just cache LLM responses?" is: you can't safely cache responses if your retrieval is non-deterministic. You'd return cached answers that were generated from different context than what the current retrieval would produce.
  - Consider an AI coding assistant exploring a large codebase. Without deterministic retrieval:
    - User: "How does authentication work?"
    - First ask - LLM reads 15 files, 4000 tokens of context
    - Second ask (same question) - different retrieval, reads 12 different files
    - LLM has to re-process everything from scratch
  - With deterministic retrieval + caching: The LLM doesn't need to read entire files - it gets precise line-number citations (e.g., src/auth.rs:45-78) with just the relevant spans. 

- How does this tool maintain contextual or metadata relationships between chunks? Can it maintain distinction between multiple documents on a similar topic, and identify which source makes which claim?
  - Yes - this is core to how AvocadoDB works:
  - Span-level tracking: Every chunk (span) is tied to its source file with exact line numbers. When you compile context, each span includes [1] docs/auth.md Lines 1-23 so you know exactly where every claim comes from. Citation in output: The compiled context includes a citations array mapping each span to its artifact (file), start/end lines, and relevance score. Your LLM can reference these directly. Cross-document deduplication: Hybrid retrieval (semantic + lexical) combined with MMR diversification ensures you get diverse sources, not 5 chunks from the same file saying the same thing.
  - Metadata preservation: Each span stores the parent artifact ID, so you can always trace back which claim came from api-docs.md versus security-policy.md.
  - The deterministic sort ensures the same sources appear in the same order every time, so you can reliably say source 1 said X, source 2 said Y.

- Deterministic RAG is the right call; debugging and evals donâ€™t work if the context shifts.
  - Iâ€™ve run similar stacks with Qdrant and Tantivy; DreamFactory helped expose a read-only REST layer so agents hit stable endpoints, not raw DBs.
  - Bottom line: end-to-end determinism plus explainable retrieval is the win.

- Would be cool to have optional Postgres backend

- ## [Best Vector Database for RAG : r/vectordatabase _202501](https://www.reddit.com/r/vectordatabase/comments/1hzovpy/best_vector_database_for_rag/)
- All vector databases work well for RAG. The selection of a vector database usually depends on your preference: cloud based vs self-hosted, open source vs private, programming languages / clients, API access, already part of SQL etc.
- Some common ones are (not in any order, I'm not affiliated with any):
  - Pinecone: private, cloud based, very popular
  - pgvector: Postres vector search extension, useful if you have already data in Postgres.
  - Faiss: A library for efficient similarity search and clustering of dense vectors.
  - Qdrant: Open source vector search engine written in Rust.
  - Chroma: Yet another vector database
  - Milvus: An open-source similarity search engine for embedding vectors.
  - Weaviate: Open source vector database written in Go.

- Pgvector because it is part of Postgres. In the end Vectors are just a data type in a databases. If you only need vectoring and not SQL then it is better to look search engine that does that I think.
  - PGVector, is like PostGIS for geospatial ops, an extension for Postgre to support vector search, it is not a true vector-native DB, so it adds operational overhead and lacks advanced features that a vector DB provides. 

- Quadrant, it's a vector DB designed specifically for RAG/AI workflows. It offers advanced filtering features (multi-tenancy) that are natively baked-in into its core architecture. These features would require more legwork using other data stores like Redis etc...
- FAISS: great for vector search is not really a DB, it's a lib and not a true DB

- ## ğŸ†š [Vector db comparison : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ph7njc/vector_db_comparison/)
  - Key findings:
  - RAG systems under ~10M vectors, standard HNSW is fine. Above that, you'll need to choose a different index.
  - Large dataset + cost-sensitive*:* Turbopuffer. Object storage makes it cheap at scale.
  - pgvector is good for small scale and local experiments. Specialized vector dbs perform better at scale.
  - Chroma - Lightweight, good for running in notebooks or small servers
  - [Best Vector Databases for RAG - Agentset](https://agentset.ai/blog/best-vector-db-for-rag)

- My decision tree looks like this: Use pgvector until I have a very specific reason not to.

- Missing from this is Vespa. But everything else is spot on. I think it goes into teh last column along with Qdrant, Milvus, Weaviate etc.
  - For me Vespa is on another level. It is a production ready and very capable of "regular search" (textual). SO you can do very good hybrid serachs. For me is even leaps ahead of ElasticSearch. We migrate a medium workload(5 nodes) from ES to Vespa 4 years ago and was the best decision we ever made.
- Agree with this assessment. But I think overall it's a lot more complex than others here too. It's a very steep hill to climb but once you do the power is there.

- Our rag team (afaik) uses elastic / weaviate because of hybrid search, we have lots of cases where search could be about some named entity (like people = name + surname), so hybrid is a must. IDK on which basis they chose which one to use for cases. Also, Qdrant has bm42 hybrid search, by any chance you know anything about how it performs compared to other solutions?
  - Hybrid search is incredible. But in my experience it's better to do parallel queries for semantic and keyword and then put all the results in a reranker

- Or just use SQLite and don't overcomplicate things
  - You need a vector search extension for it. And there aren't any particularly good ones that I know of.

- Definitely Elasticsearch if you need extreme levels of horizontal scalability.

- S3 vector is now a thing .
# discuss-chunking
- ## 

- ## 

- ## 

- ## 
# discuss-embedding
- ## 

- ## 

- ## 

- ## 

- ## [Show HN: We cut RAG latency ~2Ã— by switching embedding model | Hacker News _202511](https://news.ycombinator.com/item?id=46043354)
- I always feel that the choice of embedding model is quite important, but it's seldom mentioned. Most tutorials about RAG just tell you to use a common model like OpenAI's text embedding, making it seem as though it's okay to use anything else. But even though I'm somewhat aware of this, I lack the knowledge and methods to determine which model is best suited for my scenario.

- The biggest latency improvement I saw was switching off OpenAI's API that would have a latency anywhere between 0.3 - 6 seconds(!) for the same two word search embedding...

- ## ğŸ¤” [[Discussion] Anyone else doing â€œsummary-only embeddings + full-text contextâ€ for RAG? : r/Rag _202511](https://www.reddit.com/r/Rag/comments/1p05u0f/discussion_anyone_else_doing_summaryonly/)
  - 1) For each doc/section: I generate a tiny synthetic text (title + LLM summary). â†’ This is the ONLY thing I embed.
  - 2) At query time: I search over those short summaries.
  - 3) For answering: I take all retrieved sections and feed the full original text straight into the LLM. No reranking, no chunk scoring, nothing fancy.
  - Because chunking was slow, expensive, and honestly ruined the semantic boundaries of my data. Summary vectors are way cleaner, and long-context LLMs can handle the raw text way better.
  - Anyone else trying this â€œlightweight retriever + heavy context inputâ€ style?

- I wouldnâ€™t blindly trust the summarizer though. The summaries are lossy by design but the good news is thereâ€™s models dedicated to this task.
  - Adding a sumeval model of some variety will give you much greater debugability and interpretability. It can help you surface where the summarizer is struggling or even better serve as the backbone for an RL loop.

- This is very similar to the contextual chunking anthropic proposed. Yes, this is standard now. Problem is still going to be latency and cost with the LLMs

- The idea works because one of the techniques used in chunking enrichment is adding keywords and summaries to the chunks. However, relying only on the summaries may work in some contexts but can still be less accurate, as you might lose information. Imagine 20-page sections where you have, for example, 5 chunks; in this case, they would be compressed into a single summary, which could not be enough.

- You are doing what industry needed but still their are room to improve the latency by implementing cache and have more meta data for the docs to get better context before going to LLM

- I've tried this- performed worse in my case. I would run an eval and decide for myself.

- I did follow the same approach to feed more context to MCP client hosted in IDEs through RAG. Vectorize the only metadata and source is returned as a result which contains the full document to feed to the LLM

- I didn't test it myself but I read about 'voyage-context-3' which seems to address the issue of global context without the need for an extra summarization step.

- This is almost the same as what the RAPTOR paper from last year suggested: https://arxiv.org/abs/2401.18059
  - Create summaries, cluster based on summary, then create a summary of those, etc. 
  - Itâ€™s a solid approach, especially since, when summarizing, you can ensure the LLM uses a consistent style.

- ## ğŸ†š [Embedding models have converged : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1ozf9al/embedding_models_have_converged/)
  - I ran 13 models on 8 datasets and checked latency, accuracy, and an LLM-judged ELO score. Honestly, the results were not what I expected - most models ended up clustered pretty tightly.
  - rank 1 â†’ 10 is around a 3% gap

- This is a case of saturated benchmarks not a case of the quality being the same
  - 100% this. It's like saying LLMs have converged because they all ace SuperGLUE.

- Why use LLM as a judge for this? You have a query and a matching document. You know which document you want for which query. What matters is the rank of that document for that particular query. I'm not sure I get what do the LLM's have to judge here.
  - Because a lot of the datasets in real life donâ€™t have a single correct doc. And LLM judge evaluates the quality of the retrieved list, not just "did u hit the label".
  - We used 2 private datasets and 6 public datasets. Publc datasets that do have perfect ground truth also have ncdg and recall for accuracy. So we calculated both elo score and classical metrics.

- Did you try with different judge models and slightly different judge prompts, just to see if the judge influence on the results might be larger than the actual score differences?
  - great suggestion, will try that!

- Interesting - do we have a similar subset of benchmarks for rerankers?
  - yess! before embedding models, I also ran rerankers on multiple datasets and pretty much used the same elo scoring. check this out: https://agentset.ai/rerankers

- I've found embedding model benchmarks... Useless as of late? Qwen 3 models especially, they are killer on paper, but I found them near useless after a top_k of 5 in my semantic search engine use case. The first couple of results are great, but seemed almost random after about 5 results, which is a no-go for our search pages returning ~40 items per page. We found "worse" models have much more consistent results, we went with e5-large-v2
  - yes, agree! using ELO actually helps with that, it basically checks how consistent a model(looks at the win rate) is across all datasets, not just the first few hits

- ## [what free model should i use for codebase indexing with speed indexing : r/RooCode](https://www.reddit.com/r/RooCode/comments/1oiw4w7/what_free_model_should_i_use_for_codebase/)
- Use text-embedding-004 model from google (fast and free)

- ## [Open-source embedding models: which one's the best? : r/Rag _202509](https://www.reddit.com/r/Rag/comments/1nro65j/opensource_embedding_models_which_ones_the_best/)
- tried embeddinggemma:300m and qwen3-embedding-0.6b and 4b

- [Open-source embedding models: which one's the best? : r/ollama](https://www.reddit.com/r/ollama/comments/1nro30i/opensource_embedding_models_which_ones_the_best/)
- I find the latest Qwen embeddings and reranker models the best out there today even when comparing with offerings from OpenAI and VoyageAI
  - I tried BGE, Nomic, and Qwen using ChunkHound I don't care so much about the raw numbers but about the end to end result. You can easily see the difference in results by running the chunkhound search command

- [Open-source embedding models: which one to use? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1nrgklt/opensource_embedding_models_which_one_to_use/)
- I've had my best results with bge-m3 or qwen3-embedding

- ## [Is there a classification of worst vs best ai models for RAG : r/Rag _202509](https://www.reddit.com/r/Rag/comments/1nk2rza/is_there_a_classification_of_worst_vs_best_ai/)
- For small models, most of our users have converged around: Gemma3, Qwen3, and DeepSeek
  - I always use for demos Mistral Medium 3.1 (mistral-medium-2508), which is a very good middle ground.
  - For the "large" LLMs, Gemini Flash variants and Claude Sonnet or Haiku. OpenAI models have *never* been good at this use case. The GPT-OSS models have been abysmally bad in testing.

- For an embeddings model, we've been using all-MiniLM-L6-v2 since we released TrustGraph, and haven't really seen any need to change. The platform allows you to choose any embeddings model from HF, but all-MiniLM-L6-v2 seems to do just fine in most use cases. If you want be able to try out all these model combinations, you can give them a try with TrustGraph (open source).

- gemma3 worked best in my case(turkish docs)
  - but rag mostly depends on understanding and memorization abilities

- I have tried several OpenAI models for our RAG system, and got great results using gpt-4.1-mini.
  - Via the API, it is the OpenAI model that has the largest context window right now.

- ## [embeddinggemma has higher memory footprint than qwen3:0.6b? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nbbi1c/embeddinggemma_has_higher_memory_footprint_than/)
- For whatever reason, Gemma models have a larger vocab size of 256K whereas most models have a vocab size of around 120k. This adds to the size

- confirmed, i use ollama just for embedding, download it from official ollama library. The size is 621MB on command ollama list. But getting bigger like 2.8GB on ollama ps
  - and it very slow compared to qwen embedding 4b.

- ## [Real life experience with Qwen3 embeddings? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nezmfi/real_life_experience_with_qwen3_embeddings/)
  - I need to decide on an embedding model for our new vector store and Iâ€™m torn between Qwen3 0.6b and OpenAI v3 small.
  - The qwen3 embeddings top the MTEB leaderboards scoring even higher than the new Gemini embeddings. Qwen3 has been killing it, but embeddings can be a fragile thing.

- I don't think Qwen3-Embedding-0.6B performs better than previous encoder models of similar size (e.g., bge-m3); 
  - its main advantage is long-context support. 
  - Overall, it's only a little bit better than other prior state-of-the-art LLM-based embedding models (e.g., Kalm-v2), with advantages mainly comes from instruction tuning on the query side, which improves adaptability.
- Qwen3-Embedding-4B is good. It outperforms bge by 2â€“3 points (on my own dataset, using NDCG@10), and maintains strong retrieval performance at 2â€“4k tokens per chunk. 
  - However, the GGUF version of this model seems inconsistent with the original checkpointâ€”this discrepancy is unclear (I suspect it may be related to the pooling configuration).
- Qwen3-Embedding-8B might indeed be a SOTA model, but it costs too much.

- I never tested that quant. There are so many mistakes you can make with embeddings (omitting required eot tokens, missing instructions, wrong padding alignment etc.) 

- I always include something like an embedding version so it is always possible to change embedding algo without reencoding old data so long as you are willing to do a search per algo and re-rank them.

- I've used 8B and 4B as GGUF at Q4_K_M and never had issues some are pointing.
  - Found the 4B most efficient as the difference between the 8B is small for such resource difference.
  - Been using for code bases, currently over 380 files with code. No issues.

- ## [Are Qwen3 Embedding GGUF faulty? : r/LocalLLaMA _202507](https://www.reddit.com/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/)
  - Qwen3 Embedding has great retrieval results on MTEB.
  - However, I tried it in llama.cpp. The results were much worse than competitors.

- You have to add the EOS Token manually "<|endoftext|>" as of here: https://github.com/ggml-org/llama.cpp/issues/14234

- In multilingual, I was very disappointed with qwen3 embedding compared to jinaai/jina-embeddings-v3 which remains my favorite for the moment
- v4 is out btw: https://huggingface.co/jinaai/jina-embeddings-v4
  - It does work much better, getting 48.11% on the same benchmark.
  - The official JINA API is very slow though. Half a minute for a batch of 32.

- Would you believe I was just trying it out today and it was all messed up. Swapped from Q3 4B and 0.6B to granite 278m and all my problems went away.

- Yes, though if I tried generating the embeddings through the SentenceTransformers module instead, I got the state-of-the-art results I was hoping for on my benchmark. A code snippet for how to do so is listed on their HF page.
# discuss-rag-elements
- ## 

- ## 

- ## [How to Intelligently Chunk Document with Charts, Tables, Graphs etc? : r/LangChain _202510](https://www.reddit.com/r/LangChain/comments/1oe4wh4/how_to_intelligently_chunk_document_with_charts/)
- when i was working on extraction, i faced the same issue with tables. a simple parsing tool was not enough, so we added a prompt before processing the document by saying that whenever you encounter a table, first mark it by saying #Table Start# and end it with a #Table End# and take a screenshot of the whole table , feed it to the llm for ocr operation and get a parseable text based table. then during chunking, we made sure that a separate logic was being used for cases when we encounter the #Table Start# and #Table End# cause we would want to keep the whole table as one big chunk else it would lose context for the other half of the table since it would be starting with just some numbers and no context even with the overlaps. 
  - Other than this you can use MarkdownHeaderTextSplitter since it helps with the other part of documents aswell

- It is really simple. First split each pdf into multiple files, one page = one file.
  - Then, write a program that detects pages with tables, graphs etc and move them into a separate folder. When you move, assign a meaningful filename.
  - Then chunk all other pages. Only meaningful chunking is hierarchical.while chunking, build a graph or a tree view, and visualise this. This verifies that you have preserved the hierarchical integrity of the document.
  - Now, using an LLM, process each visual page to provide summary. Insert these chunks in the tree view at their correct location. The image page should be available as a link in metadata.
  - Now design a context template. It should have document id, current chunk, prev chunk, subtitle summary, title and book title.
  - Embed the chunk with this template into a vectordb. Then also embed a sparse vector.
  - While querying use both vectors and rerank them.

- For complex documents (with tables, charts, and images), RAGFlow works surprisingly well â€” it can intelligently recognize and preserve layouts like tables and embedded figures during parsing.
# discuss
- ## 

- ## 

- ## 

- ## 

- ## [RAG beginner - Help me understand the "Why" of RAG. : r/Rag _202510](https://www.reddit.com/r/Rag/comments/1phkzw6/rag_beginner_help_me_understand_the_why_of_rag/)
  - Is RAG even necessary for this usecase? Now LLM models have become so good that RAG is not required for tasks like this. (Evaluator asked me this question)

- For evaluation I would treat â€œwith RAGâ€ and â€œno RAGâ€ as two systems. Build a small gold set of quiz questions and grading decisions from a human teacher, then
  - That gives you a concrete argument: RAG is not â€œphilosophically nicerâ€, it either improves faithfulness and coverage on real data or it does not.

- Where RAG becomes valuable is when correctness and grounding matterâ€”ensuring questions and answers come only from the uploaded content and not from the modelâ€™s prior knowledge or hallucinations. That distinction is especially important in educational assessment.

- ## [Why RAG Fails on Tables, Graphs, and Structured Data : r/Rag _202512](https://www.reddit.com/r/Rag/comments/1pfuxis/why_rag_fails_on_tables_graphs_and_structured_data/)
  - Most RAG pipelines are built for unstructured text, not for structured data.
  - Tables donâ€™t fit semantic embeddings well
  - Graph-shaped knowledge gets crushed into linear chunks
  - SQL-shaped questions donâ€™t want vectors
- The real fix is multi-engine retrieval, not â€œbetter vectorsâ€
  - Vectors for semantic meaning and fuzzy matches
  - Sparse / keyword search for exact terms, IDs, codes, SKUs, citations
  - SQL for structured fields, filters, and aggregations
  - Graph queries for multi-hop and relationship-heavy questions
  - Layout- or table-aware parsers for preserving structure in complex docs

- Setting the questionable AI slop style aside, while the diagnosis is correct, the proposed solution is pretty generic and simplistic. Sure, tables belong into a relational DB. So, hybrid search. But if your docs have no unified table structure, well, thatâ€™s a problem this post fails to address. And thatâ€™s a very common situation.

- At iGPT, we handle this by treating different data types as first-class citizens from ingestion onward. Emails get thread reconstruction and role detection, tables get parsed with column-header preservation, and attachments trigger format-specific pipelines before anything hits the vector layer.
  - The retrieval stack uses hybrid search (semantic + keyword + metadata filters) with post-retrieval rescoring, so structured queries don't get drowned out by "relevant-ish" prose.

- ## [My Experience with Table Extraction and Data Extraction Tools for complex documents. : r/Rag _202512](https://www.reddit.com/r/Rag/comments/1pfwfvy/my_experience_with_table_extraction_and_data/)
- Tables:
  - For documents with simple tables I mostly use Camelot. Other options are pdfplumber, pymupdf (AGPL license), tabula.
  - For scanned documents or images I try using paddleocr or easyocr but recreating the table structure is often not simple. For straightforward tables it works but not for complex tables.
  - Then when the above mentioned option does not work I use APIs like ParseExtract, MistralOCR.
  - When Conversion of Tables to CSV/Excel is required I use ParseExtract and when I only need Parsing/OCR then I use either ParseExtract or MistralOCR. ExtractTable is also a good option for csv/excel conversion. 
  - Apart from the above two options, other options are either costly for similar accuracy or subscription based.
  - Google Document AI is also a good pay-as-you-go option but I first use ParseExtract then MistralOCR for table OCR requirement & ParseExtract then ExtractTable for CSV/Excel conversion.
  - I have used open source options like Docling, DeepSeek-OCR, dotsOCR, NanonetsOCR, MinerU, PaddleOCR-VL etc. for clients that are willing to invest in GPU for privacy reasons. I will later share a separate post to compare them for table extraction.
- Data Extraction:
  - I have worked for use cases like data extraction from invoice, financial documents, images and general data extraction as this is one area where AI tools have been very useful.
  - If document structure is fixed then I try using regex or string manipulations, getting text from OCR tools like paddleocr, easyocr, pymupdf, pdfplumber. But most documents are complex and come with varying structure.
  - First I try using various LLMs directly for data extraction then use ParseExtract APIs due to its good accuracy and pricing. Another good option is LlamaExtract but it becomes costly for higher volume.
  - ParseExtract do not provide direct solutions for multi page data extraction for which they provide custom solutions. I have connected with ParseExtract for such a multipage solution and they provided such a solution for good pay-as-you-go pricing. Llamaextract has multi page support but if you can wait for a few days then ParseExtract works with better pricing.

- https://github.com/camelot-dev/camelot /MIT/python
  - A Python library to extract tabular data from PDFs

- I havenâ€™t found a single thing, other than custom vision based LLM workflows, that is good at extracting tables with complex structures (spanning multiple pages, merger cells, hierarchical rows/column headings, etcâ€¦). But that is slow and expensive.

- ## [Struggling with finding good RAG LLM : r/LocalLLaMA _202504](https://www.reddit.com/r/LocalLLaMA/comments/1jwtn6j/struggling_with_finding_good_rag_llm/)
- Would strongly recommend reading up on the basic RAG process and the systems around it. The LLM itself typically isn't involved in the data lookup/search unless it's been extended to work as an agent. The quality of the sources will heavily influence the answer quality : garbage in/ garbage out sort of deal. Beyond that, check out mistral small 3.1, a bunch of the qwen models, and command-r series.
- Your RAG has to return good results. So you should be querying the rag with the embedding model directly and looking at the top 10 results. Are they what you expected? If not, you need to change your embedding model, change the way you format/chunk the input data, increase metadata, etc.
  - The info from the RAG results will be fed into the LLM for inference.

- I think this is more to do with how RAG is implemented rather than the model itself, but of course you want to use a reliable model.

- You need two models: one for embeddings and another for chat. For embeddings Llama-3.2-1B will be a good choice: reasonable fast and capable to understand anything in any language. And you will not use it to speak or think at all.
  - Next use any strong model to chat and to ask questions to rag.

- ## [Successful RAGFlow in Prod : r/Rag _202512](https://www.reddit.com/r/Rag/comments/1pawrzj/successful_ragflow_in_prod/)
- TL; DR: RAGFlow is powerful for complex documents but resource-hungry.
  - Stress Testing: It is heavier than "vanilla" RAG because of the deep document understanding (OCR/Layout analysis). Expect higher ingestion latency. For query latency, it depends entirely on your backing vector database (e.g., Elasticsearch/Infinity). You must scale your parsing workers separately from your query services.
  - Maintenance: The visual graph builder makes logic errors easy to spot, but debugging specific parsing failures (e.g., a specific weird PDF format) requires digging into container logs.
  - Performance: Retrieval precision is significantly better than standard recursive chunking because it respects document structure (tables, headers, sections).
  - The Killer Feature: Table Parsing/Layout Analysis. It handles complex tables in PDFs where other frameworks (like standard LangChain) produce garbage.
  - Must-Know Tip: Don't rely on vector search alone. Configure Hybrid Search (Keyword + Vector) and Re-ranking from day one. It dramatically improves accuracy for technical jargon and specific IDs. 

- ## [How to make a RAG pipeline near real-time : r/LangChain](https://www.reddit.com/r/LangChain/comments/1p623zt/how_to_make_a_rag_pipeline_near_realtime/)
- There are specific models that exceeds at this. I know AWSâ€™s Nova Sonic can do this well. Although for RAG 3-4 seconds ainâ€™t bad especially for a voice bot.

- RAG latency is usually a combination of retrieval tuning and orchestration choices, so try reducing candidate counts, sharding vectors for faster nearest neighbor lookups, precomputing and caching top-k results for common queries, and parallelizing embedding and retrieval steps so STT and retrieval overlap.
  - Also check chunk sizes and embedding model choice because those affect recall and speed, and consider a lightweight semantic cache to answer repeated context queries instantly.
  - People mix FAISS tuning or an alternative vector store with a caching layer and async streaming to get sub-second responses, and teams that need observability and easy orchestration prototype these flows in visual builders or frameworks like LlmFlowDesigner, while others tune Milvus or FAISS directly.

- ## ğŸ¤” [Which model is good for making a highly efficient RAG? : r/LocalLLM _202506](https://www.reddit.com/r/LocalLLM/comments/1l0pncp/which_model_is_good_for_making_a_highly_efficient/)
- Founder of agentset here. I'd say the quality of the embedding model + vector db caries a lot more weight than the generation model. We generally found any non trivially small model to be able to answer questions as long as the context is short and concise.
- What embeddings approach would you recommend?
  - Most of the working is in the parsing and chunking strategy. Embedding just comes down to choosing a model. If you're doing multi-lingual or technical work, you should go with a big embedding model like text-large-3. If you're doing english only there are plenty of cheaper and lighter weight models.

- Qwen3 14B and Qwen3 32B (crazy good, they fetch, think then provide a comprehensive answer) and those boys are not afraid of follow up questions either.. ask away!
  - 32B uses citations functions following every statement he says. 14B does not for some reason.. but that does not mean it's bad or anything. Still a very decent RAG AI.

- I found Qwen 3 and Gemma 3 work the best.

- I use Linq-Embed-Mistral because it's high on MTEB. But I haven't compared it with other models.

- [What's the best model for RAG with docs? : r/ollama _202506](https://www.reddit.com/r/ollama/comments/1l9wvnr/whats_the_best_model_for_rag_with_docs/)
- I've had good success with simple models such as Qwen3 4B using Open WebUI and Ollama. There are probably better RAG backends though (planning to try LightRag).
  - Basically you need a good embedding engine (try bge-m3, which is actually available through Ollama if that is your poison).
  - Then you also need a good content extraction engine, to suck out the right information out of your specific type of document. Try Tika, which is working great.
  - Then you'll need a good re-ranking model to do some magic on the proposal the RAG comes up with. Try BAAI/bge-reranker-v2-m3.
  - Those are the main things you need.
  - Then there are some settings of course that you can toy around with (temperature, top k, min probability, for example), but with those main components you'll be able to ask detailed questions.
  - Then .. depending on your knowledge base, you should structure the data (documents, contents) in a smart way. But if you just want to ask questions on a pdf-specification or such then you'll fine with the above components.
  - That works for me at least, feeding lots of specifications, requirements, manuals etc. into my system.
  - Try it out in Open-Webui (frontend and backend solution). It works, but you'll probably want a separate backend solution going forward (with GPU re-ranking support for instance, since it's slow).

- Try out granite3.3:8b - its designed specifically for enterprise tool use & rag. Really slept on & everyone else defaults to the text gen llms, but youll be pleasantly surprised especially since its so efficient your video card would be overkill.

- ## ğŸ’¥ [Best RAG Architecture & Stack for 10M+ Text Files? (Semantic Search Assistant) : r/LangChain _202511](https://www.reddit.com/r/LangChain/comments/1p2klni/best_rag_architecture_stack_for_10m_text_files/)
  - I am building an AI assistant for a dataset of 10 million text documents (PostgreSQL). The goal is to enable deep semantic search and chat capabilities over this data.
  - Scale: The system must handle 10M files efficiently (likely resulting in 100M+ vectors).
  - Updates: I need to easily add/remove documents monthly without re-indexing the whole database.

- Most RAG pipelines suck at two main points processing unstructured data while splitting and context aware retrieval

- [Best RAG Architecture & Stack for 10M+ Text Files? (Semantic Search Assistant) : r/Rag](https://www.reddit.com/r/Rag/comments/1p2kkua/best_rag_architecture_stack_for_10m_text_files/)
- To get any decent results, you need to build a distributed RAG, if there is such a thing.
  First create a taxonomy of the assets. This is like Table contents for the entire collection. You can keep this in a graph db.
  Then build a classifier with positive and negative targets. Use this to classify the query first to determine which groups/clusters of the documents are the best to use. Once you know the target collections you can focus only on those.
  You can perhaps supply the classifier as an MCP service to the LLM.
  For each asset, create a chunking model. Keep in mind, not all documents would fit the same model. Store chunks in a vectordb.
  Along with chunks, extract entities. Build a BM25S database (relational) with these keywords.
  Then extract entities and relationships from the documents and build a ER knowledge graph for each of the documents.
  All databases should have the same metadata to enable reranking or hybridization.
  How to manage the CDC on the document mountain is another post.

- If you are generating vectors, you can track signatures of your text chunks and reuse vectors if the text chunk didn't change. That saves you cost of new vectors when you reindex.

- ## [Stop fine-tuning your model for every little thing. You're probably wasting your time. : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ov7ogq/stop_finetuning_your_model_for_every_little_thing/)
  - confession time. I just wasted three weeks and a chunk of my compute budget trying to fine-tune a model to answer questions about our internal API. The results were... mediocre(å¹³åº¸çš„ï¼›æ™®é€šçš„ï¼›ä¸å¤Ÿå¥½çš„) at best. It kinda knew the stuff, but it also started hallucinating in new and creative ways, and forgot how to do basic things it was good at before.
  - I feel like "fine-tuning" has become this default magic wand people wave when an LLM isn't perfect. But 80% of the time, what you actually need is RAG
  - RAG is like giving your AI a cheat sheet. You've got a mountain of internal docs, PDFs, or knowledge that the model wasn't trained on? Don't shove it down the model's throat and hope it digests it. Just keep it in a database (a "vector store, " if we're being fancy) and teach the AI to look things up before it answers. 
  - It's faster, cheaper, and the AI can't "forget" or misremember the source material. Fine-tuning is for changing the AI's personality or teaching it a new skill. This is when you need the model to fundamentally write or reason differently. 
  - So, the dumb-simple rule I go by now: Â· Problem:- "The AI doesn't know about X." -> Use RAG. "The AI doesn't act or sound the way I want." -> Consider Fine-Tuning.

- RAG is for knowledge
  - Finetuning is best for style
  - Don't mix up the two

- I thought you can't SFT modern reasoning LLMs anymore? all this does is fuck up their complex post train since GRPO/RLHF happens after SFT and you can't replicate that with a simple pipe.
  - I think it depends on the model? Like, Qwen3 models are pretty good for SFT training.

- ## [After Building Multiple Production RAGs, I Realized â€” No One Really Wants "Just a RAG" : r/Rag _202511](https://www.reddit.com/r/Rag/comments/1oly866/after_building_multiple_production_rags_i/)
- I get asked to RAGify :) company documents, but just RAG isn't enough. It needs to be able to analyze code without a single click, able to produce similar documents similar to provided document, able to chat with documents, and needs to completely run offline, no data outside. More requirements keep coming as the week go on.

- thanks mr chatgpt

- ## ğŸ†š [I tested local models on 100+ real RAG tasks. Here are the best 1B model picks : r/Rag _202510](https://www.reddit.com/r/Rag/comments/1o60ib6/i_tested_local_models_on_100_real_rag_tasks_here/)
- Best model by real-life file QA tasks (Tested on 16GB Macbook Air M2)
  - Iâ€™m building this local file agent for RAG - Hyperlink. The idea of this test is to really understand how models perform in privacy-concerned real-life tasks
- tests result
  - A â€” Find facts + cite sources â†’ Qwen3â€“1.7B-MLX-8bit
  - B â€” Compare evidence across files â†’ LMF2â€“1.2B-MLX
  - C â€” Build timelines â†’ LMF2â€“1.2B-MLX
  - D â€” Summarize documents â†’ Qwen3â€“1.7B-MLX-8bit & LMF2â€“1.2B-MLX
  - E â€” Organize themed collections â†’ stronger models needed
- Tasks Types (High Frequency, Low NPS file RAG scenarios)
  - Find facts + cite sources â€” 10 PDFs consisting of project management documents
  - Compare evidence across documents â€” 12 PDFs of contract and pricing review documents
  - Build timelines â€” 13 deposition transcripts in PDF format
  - Summarize documents â€” 13 deposition transcripts in PDF format.
  - Organize themed collections â€” 1158 MD files of an Obsidian note-taking user.

- ## [The â€œnew RAG every weekâ€ problem â€” do we really need to switch frameworks this often? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o764lj/the_new_rag_every_week_problem_do_we_really_need/)
  - It feels like every couple of weeks, thereâ€™s a brand-new â€œnext-generationâ€ RAG framework popping up â€” R2R, LightRAG, RAGFlow, and now a few others that just launched on GitHub.
  - But when you look closer, most of them arenâ€™t new paradigms at all.
  - Theyâ€™re slight variations of existing pipelines â€” same retrieval backbone, same chunking strategies â€” maybe with a bit of optimization in one step (say, hybrid retriever logic, graph-based context linking, or compression on long contexts).

- AI is a rapidly growing tech space. There will be 10 technologies that mildly differ each other and then there will be that 1 that will make some kind of a breakthrough. That's how it is. It's far too early for any real standards yet, and honestly that's the exciting part of following the AI space.

- ## ğŸ†š [Tested 9 RAG query transformation techniques â€“ HydE is absurdly underrated : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o6s89n/tested_9_rag_query_transformation_techniques_hyde/)
  - Your RAG system isn't bad. Your queries are.
- I just tested 9 query transformation techniques. Here's what actually moved the needle
- Top 3:
  - HydE â€“ Generate a hypothetical answer, search for docs similar to that. Sounds dumb, works incredibly well. Solves the semantic gap problem.
  - RAG-Fusion â€“ Multi-query + reranking. Simple, effective, production-ready.
  - Step-Back â€“ Ask abstract questions first. "What is photosynthesis?" before "How do C4 plants fix carbon?"
- Meh tier:
  - Multi-Query: Good baseline, nothing special
  - Decomposition: Works but adds complexity
  - Recursive: Slow, minimal quality gain for simple queries
- Key insight: You're spending time optimizing embeddings when your query formulation is the actual bottleneck.

- Many larger RAG platforms I've worked on or seen in use are making embeddings from the text to be retrieved and also generating a couple of questions that can be answered by the same chunk, saving those embeddings as well (so you have several embeddings pointing at the same chunk).
  - This performs a lot like HydE, but shifting the extra compute (generation step) to the ingestion stages instead of query time for better latency/performance in exchange for a larger index to store and query, which is usually the desired tradeoff for interactive systems.

- ## ğŸ¤” [Is RAG system actually slow because of tool calling protocol? : r/Rag _202509](https://www.reddit.com/r/Rag/comments/1ntc1ky/is_rag_system_actually_slow_because_of_tool/)
  - Just came across few wild comparison between MCP and UTCP protocols and honestly... my mind is blown.
  - For RAG systems where every millisecond counts when we are retrieving documents. UTCP is 30-40% faster performance than MCP. that's HUGE.

- Why would you use mcp in RAG to begin with?
  - The consumer of a RAG might use tools, but â€˜theâ€™ RAG components do not need tools.

- Find it funny everyone so focused on speed, but go ask gpt and wait 15min is fine

- ## ğŸ†š [Does anyone know how much of a performance difference between knowledge graphs and vector based searches? : r/LangChain _202408](https://www.reddit.com/r/LangChain/comments/1eragqk/does_anyone_know_how_much_of_a_performance/)
  - I made a pretty simple vector based RAG search, and it performs "okay" and doesn't always generate the expected results. I have the pieces for a knowledge graph, but I was wondering if people knew the expected improvements that I should expect to see by moving to knowledge graphs?

- Vector search alone is usually faster. Without any bells and whistles like metadata filtering or stepback, it's just an optimized KNN. 
  - Graph based RAG involves traversing a graph in addition to searching for the appropriate starting node, which often times is a done via vector search. 
  - The graph structure usually leads to good results though.

- ## LobeChat v1.85.0 ä¼˜åŒ–äº†å¯¹è¯ä¸Šä¼ æ–‡ä»¶çš„é€»è¾‘ï¼ŒæŠŠå¤§å®¶å‘¼å£°æ¯”è¾ƒé«˜çš„å…¨æ–‡ä¸Šä¼ çš„èƒ½åŠ›åšå¥½äº†ï¼Œä¸å†èµ° åˆ†å— + embedding çš„æœ´ç´  RAG ã€‚å¸¦æ¥çš„å¥½å¤„æ˜¯é€Ÿåº¦å˜å¿«äº†ï¼Œæ•ˆæœä¹Ÿæ›´åŠ ç†æƒ³äº†ã€‚
- https://x.com/arvin17x/status/1921101310004179082
  - æ„Ÿè§‰å¤§æ¨¡å‹å†è¿­ä»£ä¸ªä¸€å¹´ï¼Œæœ´ç´  RAG å¯èƒ½å°±è¦é€€å‡ºå†å²èˆå°äº†
  - æ¯”å¦‚ä¸€ä¸ªæ–‡ä»¶ 10w å­—çš„å†…å®¹ï¼Œå…¨æ–‡ä¸Šä¼ å°±æ˜¯10wå­—å…¨æ”¾ä¸Šä¸‹æ–‡é‡Œã€‚ RAG æ˜¯æ‰¾å‡ºæœ€ç›¸å…³çš„ N ä¸ªå†…å®¹ç‰‡æ®µï¼ˆå‡è®¾ä¸€ä¸ªç‰‡æ®µ1kï¼ŒåŠ èµ·æ¥ N kä¸ªå­—ï¼‰æ”¾ä¸Šä¸‹æ–‡é‡Œã€‚ å…¨æ–‡ä¸Šä¼ çš„æ•ˆæœåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¸€å®šæ˜¯æ¯” RAG è¦å¥½çš„ã€‚
  - ç°åœ¨é»˜è®¤ä¸Šä¼ å°±æ˜¯å…¨æ–‡ã€‚ä¸è¿‡çŸ¥è¯†åº“è¯»å–è¿˜æ˜¯åŸæ¥çš„
- ä¸ç”¨ RAGï¼Œç›´æ¥ä¼ ç»™ LLMï¼Œé’±åŒ…é¡¶ä¸ä½å•Š
- å…¨æ–‡ä¸Šä¼ ï¼Œåªæœ‰Googleå®¶æ‰æœ‰å¾ˆå¤§çš„ä¸Šä¸‹æ–‡å§ï¼Ÿå…¶ä»–å®¶æ•ˆæœå°±ä¸å¥½äº†
  - ä¸å•Šï¼Œç°åœ¨gpt4.1miniéƒ½æœ‰1Mä¸Šä¸‹æ–‡äº†ï¼Œä¹Ÿä¾¿å®œã€‚æµ‹ä¸‹æ¥æ•ˆæœéå¸¸å¥½

- ## å¼€æºçš„ llm ç³»ç»Ÿé‡Œï¼Œå¸¦çŸ¥è¯†åº“åŠŸèƒ½çš„ï¼Œç›®å‰è¿˜æ²¡æ‰¾åˆ°ä¸€ä¸ªåšçš„å¥½çš„ã€‚
- https://x.com/wwwgoubuli/status/1830227047173751206
  - ä¸è¿‡è¿™ä¹ˆè¯´å€’ä¹Ÿä¸æ˜¯è¦è´¬ä½ä»€ä¹ˆï¼Œæˆ‘è‡ªå·±å¸®å®¢æˆ·å®šåˆ¶è¿‡çš„çŸ¥è¯†åº“ä¸­ï¼Œåšçš„ç¨å¾®å¥½ä¸€ç‚¹çš„ï¼Œä¹ŸæŠ•å…¥äº†å¤§é‡çš„ç²¾åŠ›å’Œæˆæœ¬æ¥å®ç°æ•°æ®é¢„å¤„ç†ï¼Œå•å•ä¸€ä¸ªæ–‡æœ¬åˆ†æ®µéƒ½æ‰¾ä¸åˆ°é€šç”¨èŒƒå¼ï¼Œå¾—å¤§é‡å®šåˆ¶ã€‚

- é—®é¢˜å°±æ˜¯ to b é‡Œã€‚ to c é‚£ä¸€ä¸ªç”¨æˆ·ä¸¢ä¸ª pdf ç®—ä¸ä¸Šä»€ä¹ˆçŸ¥è¯†åº“ã€‚ åˆ°äº† to b  é¢†åŸŸï¼Œä½ çœ‹ç€ä¸€å † excel ï¼Œpdfï¼Œ ä½ çš„æ¢¦é­‡æ‰åˆšå¼€å§‹

- æˆ‘ä»¬è‡ªå·±åšå”®åå®¢æœæœºå™¨äººï¼Œèµ·æ­¥æ˜¯æ‰¾å®ä¹ ç”Ÿæ’¸äº†1ä¸‡æ¡é—®ç­”å¯¹ï¼Œä¸Šçº¿åæ¯å‘¨æ ¹æ®bad caseè¿­ä»£ç»§ç»­åšæ•°æ®ã€‚è¿™ç©æ„æ•ˆæœå°±æ˜¯çœ‹æ ‡æ³¨æ•°æ®ï¼Œæƒ³è¦å¥½çš„æ•ˆæœï¼Œå°±å¥½å¥½åšæ•°æ®ã€‚

- çŸ¥è¯†åº“å¦‚æœåƒç¼–ç¨‹è¯­æ³•æ‰‹å†Œä¸€æ ·ï¼Œé‚£ragæ•ˆæœä¼šéå¸¸å¥½ï¼Œå¦‚æœåƒæ•£æ–‡è¯—ï¼Œragæ•ˆæœå°±ä¼šå¾ˆå·®ã€‚æˆ‘ä»¬æ‹¿åˆ°æ‰‹çš„æ–‡æ¡£ä¸€èˆ¬ä»‹äºä¸¤è€…ä¹‹é—´ï¼Œæ‰€ä»¥æƒ³æé«˜ragæ•ˆæœï¼Œåªå¥½æŠŠæ–‡æ¡£æœ¬èº«å¾€ç¼–ç¨‹è¯­æ³•æ‰‹å†Œæ–¹å‘æ”¹

- æˆ‘ç°åœ¨æœ€å¤´ç–¼çš„æ˜¯pdfæ–‡æ¡£ï¼Œé‡Œé¢å…¨æ˜¯å›¾ç‰‡çš„é‚£ç§ã€‚ï¼Œ
  - è®©ç”²æ–¹åŠ é’±ï¼Œå›¾ç‰‡åˆ°æ–‡å­—çš„è½¬æ¢ä¸ä»…éœ€è¦å·¥å…·ï¼Œè¿˜éœ€è¦äººå·¥å®¡é˜…ï¼Œä¸åŠ é’±æ²¡æ³•å¹²
