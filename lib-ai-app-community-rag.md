---
title: lib-ai-app-community-rag
tags: [ai, community, rag]
created: 2024-09-08T20:08:04.573Z
modified: 2024-09-08T20:08:16.088Z
---

# lib-ai-app-community-rag

# guide

- pros-rag
  - solutions rich
  - save cost

- cons
  - extra infra like vectordb/retrive-api

- usecases-rag
  - long-docs/pdf
  - chat-history
  - ragÁöÑÂÆûÁé∞ËøòÂèØ‰ª•ÂèÇËÄÉai-coding‰∏≠ÁöÑÂÆûË∑µÂíåÊ®°Âºè
  - ÈíàÂØπË°®Ê†ºexcel/ËÑëÂõæÁöÑrag
  - ‚è≥ Â§öÁâàÊú¨Êñá‰ª∂ÁöÑrag

- leaderboard-rag
  - [MTEB Leaderboard - a Hugging Face Space by mteb](https://huggingface.co/spaces/mteb/leaderboard)
    - https://github.com/embeddings-benchmark/mteb
    - top202509: gemini-embedding-001, embeddinggemma-300m, Qwen3-Embedding-8B/4B/0.6B, linq-mistral, jina, granite, nomic
    - üë∑ÂÆûÊµã, embeddinggemmaÂú®OllamaÁªìÊûúÂ∑Æ, Âú®LM StudioÁªìÊûúËøòË°å ~~Âú® similaritySearch Êó∂ÂæàÂ∑Æ~~
      - Ë∑üÊé®Ëçêqwen/granite
- resources
  - [RAG+AIÂ∑•‰ΩúÊµÅ+AgentÔºöLLMÊ°ÜÊû∂ËØ•Â¶Ç‰ΩïÈÄâÊã©ÔºåÂÖ®Èù¢ÂØπÊØîMaxKB„ÄÅDify„ÄÅFastGPT„ÄÅRagFlow„ÄÅAnything-LLM, ‰ª•ÂèäÊõ¥Â§öÊé®Ëçê - Áü•‰πé _202407](https://zhuanlan.zhihu.com/p/711761781)
# discuss-stars
- ## 

- ## 

- ## 

- ## 

- ## üñºÔ∏èüìÑüìà [Multi-modal RAG at scale: Processing 200K+ documents (pharma/finance/aerospace). What works with tables/Excel/charts, what breaks, and why it costs way more than you think : r/LLMDevs _202510](https://www.reddit.com/r/LLMDevs/comments/1o5oaas/multimodal_rag_at_scale_processing_200k_documents/)
  - TL; DR: Built RAG systems for 10+ enterprise clients where 40-60% of critical information was locked in tables, Excel files, and diagrams. Standard text-based RAG completely misses this. 
  - This covers what actually works, when to use vision models vs traditional parsing, and the production issues nobody warns you about.
- Here's what nobody tells you: most enterprise knowledge isn't in clean text. It's in Excel spreadsheets with 50 linked sheets, tables buried in 200-page PDFs, and charts where the visual layout matters more than any text.
- Why Text-Only RAG Fails
- Quick context: pharmaceutical client had 50K+ documents where critical dosage data lived in tables. Banks had financial models spanning 50+ Excel sheets. Aerospace client's rocket schematics contained engineering specs that text extraction would completely mangle.
  - When a researcher asks "what were cardiovascular safety signals in Phase III trials?" and the answer is in Table 4 of document 8, 432, text-based RAG returns nothing useful.
- The Three Categories (and different approaches for each)
- üìå Simple Tables
  - Standard tables with clear headers. Financial reports, clinical trial demographics, product specifications.
  - What works: Traditional parsing with `pymupdf or pdfplumber` , extract to CSV or JSON, then embed both the structured data AND a text description. Store the table data, but also generate something like "Table showing cardiovascular adverse events by age group, n=2, 847 patients." Queries can match either.
  - Production issue: PDFs don't mark where tables start or end. Used heuristics like consistent spacing and grid patterns, but false positives were constant. Built quality scoring - if table extraction looked weird, flag for manual review.
- üìå Complex Visual Content
  - Rocket schematics, combustion chamber diagrams, financial charts where information IS the visual layout.
  - Traditional OCR extracts gibberish. What works: Vision language models. Used Qwen2.5-VL-32b for aerospace, GPT-4o for financial charts, Claude 3.5 Sonnet for complex layouts.
  - The process: Extract images at high resolution, use vision model to generate descriptions, embed the description plus preserve image reference. During retrieval, return both description and original image so users can verify.
  - The catch: Vision models are SLOW and EXPENSIVE. Processing 125K documents with image extraction plus VLM descriptions took 200+ GPU hours.
- üìå Excel Files (the special circle of hell)
  - Not just tables - formulas, multiple sheets, cross-sheet references, embedded charts, conditional formatting that carries meaning.
  - Financial models with 50+ linked sheets where summary depends on 12 others. Excel files where cell color indicates status. Files with millions of rows.
  - üí° For simple Excel use pandas. 
  - For complex Excel use `openpyxl` to preserve formulas, build a dependency graph showing which sheets feed into others. 
  - For massive files, process in chunks with metadata, use filtering to find right section before pulling actual data.
  - üí° Excel files with external links to other workbooks. Parser would crash. Solution: detect external references during preprocessing, flag for manual handling.
  - Vision model trick: For sheets with complex visual layouts like dashboards, screenshot the sheet and use vision model to understand layout, then combine with structured data extraction. Sounds crazy but worked better than pure parsing.

- When to Use What
  - Use traditional parsing when: clear grid structure, cleanly embedded text, you need exact values, high volume where cost matters.
  - Use vision models when: scanned documents, information IS the visual layout, spatial relationships matter, traditional parsers fail, you need conceptual understanding not just data extraction.
  - Use hybrid when: tables span multiple pages, mixed content on same page, you need both precise data AND contextual understanding.
  - Real example: Page has both detailed schematic (vision model) and data table with test results (traditional parsing). Process twice, combine results. 
  - üí° Vision model explains schematic, parser extracts exact values.

- Production Issues Nobody Warns You About
  - Tables spanning multiple pages: My hacky solution detects when table ends at page boundary, checks if next page starts with similar structure, attempts to stitch. Works maybe 70% of the time.
  - Image quality degradation: Client uploads scanned PDF photocopied three times. Vision models hallucinate. 
    - üí° Solution: document quality scoring during ingestion, flag low-quality docs, warn users results may be unreliable.
  - Memory explosions: Processing 300-page PDF with 50 embedded charts at high resolution ate 10GB+ RAM and crashed the server. 
    - üí° Solution: lazy loading, process pages incrementally, aggressive caching.
  - Vision model hallucinations: This almost destroyed client trust. Bank client had a chart, GPT-4o returned revenue numbers that were close but WRONG. Dangerous for financial data. 
    - üí° Solution: Always show original images alongside AI descriptions. For critical data, require human verification. Make it clear what's AI-generated vs extracted.

- The Metadata Architecture
  - This is where most implementations fail. You can't just embed a table and hope semantic search finds it.
  - For tables I tag content_type, column_headers, section, what data it contains, parent document, page number. 
  - For charts I tag visual description, diagram type, system, components. 
  - For Excel I tag sheet name, parent workbook, what sheets it depends on, data types.
  - Why this matters: When someone asks "what were Q3 revenue projections, " metadata filtering finds the right Excel sheet BEFORE semantic search runs. Without this, you're searching through every table in 50K documents.

- Cost Reality Check
  - Multi-modal processing is EXPENSIVE. For 50K documents with average 5 images each, that's 250K images. At roughly one cent per image with GPT-4o, that's around $2, 500 just for initial processing. Doesn't include re-processing or experimentation.
  - Self-hosted vision models like from Qwen need around 80GB VRAM. Processing 250K images takes 139-347 hours of compute. Way slower but cheaper long-term for high volume.
  - My approach: Self-hosted models for bulk processing, API calls for real-time complex cases, aggressive caching, filter by relevance before processing everything.

- What I'd Do Differently
  - Start with document quality assessment - don't build one pipeline for everything. 
  - Build the metadata schema first - spent weeks debugging retrieval issues that were actually metadata problems. Always show the source visual alongside AI descriptions. 
  - Test on garbage data early - production documents are never clean. Set expectations around accuracy - vision models aren't perfect.

- Multi-modal RAG pays off when critical information lives in tables and charts, document volumes are high, users waste hours manually searching, and you can handle the complexity and cost.
  - Skip it when most information is clean text, small document sets work with manual search, budget is tight and traditional RAG solves 80% of problems. 
  - Real ROI: Pharma client's researchers spent 10-15 hours per week finding trial data in tables. System reduced that to 1-2 hours. Paid for itself in three months.
- Multi-modal RAG is messy, expensive, and frustrating. But when 40-60% of your client's critical information is locked in tables, charts, and Excel files, you don't have a choice. The tech is getting better, but production challenges remain.

- My questions would be as a provider/consultant, how do provide support and maintenance moving forward? Long term, how do you ensure the old way of them putting data in tables is easily digested into the new RAG system automatically or that someone owns the knowledge base? The ‚Äúdata cleaning‚Äù and organization seems like a full time contract just by itself. Then, how do you test for accuracy and showcase those results to stakeholders for renewal or as case studies for other businesses?
  - support and maintenance was something i completely underestimated early on. did the typical "build it and hand off" approach with my first few clients. total disaster. systems would work great for 2-3 months then drift, new document types would break the pipeline, accuracy would drop. now i do hybrid - initial build plus ongoing support contract. usually quarterly check-ins, pipeline updates when they add new document types, on-call for critical issues.
  - for data cleaning i train someone on their team to own document quality. they become the gatekeeper, i provide the tooling. the alternative where i do it forever doesn't scale and they know their domain way better anyway. built quality detection into the ingestion pipeline that flags problematic docs automatically but someone still needs to make decisions about what to do with garbage scans from the 90s.
  - testing is golden test sets with domain experts - 100-200 real questions with known answers. run these quarterly but focus stakeholder reports on business metrics they actually care about like "researchers spend 2 hours per week searching vs 15 hours before" or "regulatory response time dropped from 5 days to 6 hours."
  - initial builds were $50k+ (currently 100k+) but support contracts run $3k-5k monthly. way more profitable long-term plus you learn edge cases that feed back into the product and i'm full-time on this, but trying to build something in the rag space as well.

- i‚Äôve battled the same multi‚Äëmodal rag pain at scale in pharma/finance/aerospace. if you want this to survive production, bake in lineage, governance, and observability from day one, not after it drifts.
  - table-first retrieval: tag content_type, column_headers, section, page, and parent doc; prefilter by metadata before semantic search so ‚Äúq3 revenue projections‚Äù lands on the right sheet/table.
  - excel lineage and verification: parse formulas with openpyxl, build a cross‚Äësheet dependency graph, bind screenshots of dashboards to underlying cells when layout matters, and require human verification for any financial figures the vlm ‚Äúreads.‚Äù
  - vlm governance and cost controls: quality-score every image/page, batch and cache captions, separate ‚Äúconceptual‚Äù descriptions from ‚Äúnumeric‚Äù extraction, and run a quarterly golden test set that mixes tables, scanned PDFs, and charts.
  - production observability: trace every ingestion step (parse, stitch, caption, embed), cap memory per page, add fallbacks for multi‚Äëpage table stitching, and alert on eval drift rather than waiting for user complaints.
- maxim ai helps here with end‚Äëto‚Äëend evaluation, simulation, experimentation, and observability for agents and rag pipelines: scenario-based tests at scale, prompt/version experiments, live tracing with online evaluations, ci/cd hooks, and enterprise deploy options. (builder here!)

- I'm currently working on a solution to address product cataloging issues for an ecommerce platform. Right now, analysts manually review Excel files and images, then fill out Excel templates with product attributes (title, description, features, etc.). My proposed solution would automatically extract attributes from files, populate templates, validate images, and extract attributes from images (like dimensions in cm, brand, model, and others). Also check images if this image is "main", "lateral", "left view", "right view" for an SKU and if these images contains a QR code.
  - The main challenge I'm facing is that the image processing workflow is too slow and expensive (using GPT-4o model). 
  - The goal is to scale up to processing 1, 000, 000 product reviews daily. I'm currently using AWS with Fargate jobs that launch dockerized processes for image processing, orchestrated by Step Functions.
  - Currently handling <1, 000 SKUs daily, which is far from the 1, 000, 000 target.
  - Any recommendations or advice for optimizing this process?

- If table with headers , and each table column is well aligned , in most scenarios, you can check the column alignment condition and assuming that table schema did not change to determine whether table contents flow over pages

- ## [Using LMStudio for RAG/File Search with LibreChat? ¬∑ danny-avila/LibreChat _202506](https://github.com/danny-avila/LibreChat/discussions/7713)
  - I run LibreChat locally inside Docker, I run local models in LMStudio, and want to use LMStudio's nomic (or otherwise) embedding capabilities instead of an external provider like OpenAI.
  - (A) i want to do it for free/private locally, and (B) especially because when I tested file upload directly within LMStudio [using its RAG] it correctly pulled all text from the PDF, but when I used OpenAI cloud embedding it missed a huge chunk of the text‚Äîperhaps because the API version uses a text-only embedding model without vision
- UPDATE / SOLUTION:
  - Making a copy of the config.py file in my LibreChat root folder and pointing the compose-override to it did solve the problem and allowed me to use the openai embeddings with LMStudio. I seconded the feature request to make this an easy option for people to opt into

- been down that rabbit hole too, and yep: this kind of embedding mismatch + RAG retrieval chaos is super common when things try to speak in different tensor dialects.
  - the error from LMStudio (input field must be a string or array of strings) usually hits when the upstream retrieval returns a raw int list or malformed embedding body. 
  - i eventually got tired of patching these one by one and built a semantic rescue engine that stabilizes the entire pipeline.

- ## ü§î [RAGÔºàÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºâ‰ºö‰∏ç‰ºöÊ∂à‰∫°Âë¢Ôºü‰∏ÄÊó¶Â§ßÊ®°ÂûãÁöÑContext LengthÂèòÂ§ßÔºåRAGËøòÊúâÂ≠òÊ¥ªÁöÑÂøÖË¶ÅÂêóÔºü - Áü•‰πé](https://www.zhihu.com/question/637421964)
- RAG ÁöÑÂü∫Êú¨ÂéüÁêÜÂπ∂‰∏çÂ§çÊùÇÔºö
  1.  Â∞ÜÈïøÊñáÊ°£ÂàáÂàÜÊàêÂ§ö‰∏™ **chunk**Ôºõ  
  2.  ÈÄöËøáÂêëÈáèÂåñÊ®°ÂûãÔºàÂ¶Ç [OpenAI Embeddings] „ÄÅSentence-TransformerÔºâ‰∏∫ÊØè‰∏™ chunk ÁîüÊàêÂêëÈáèÔºåÂπ∂Â≠òÂÖ•Êï∞ÊçÆÂ∫ìÔºõ  
  3.  Âú®Áî®Êà∑ÊèêÈóÆÊó∂ÔºåÊ£ÄÁ¥¢Âá∫ **Top-K Áõ∏‰ººÊñáÊ°£**Ôºõ  
  4.  ÊääËøô‰∫õÊñáÊ°£‰∏éÈóÆÈ¢ò‰∏ÄËµ∑ËæìÂÖ•Â§ßÊ®°ÂûãÔºå‰Ωú‰∏∫È¢ùÂ§ñ‰∏ä‰∏ãÊñáÔºõ  
  5.  Â§ßÊ®°ÂûãÂü∫‰∫éÈóÆÈ¢ò + Ê£ÄÁ¥¢ÁªìÊûúÔºåÁîüÊàêÊõ¥ÂáÜÁ°ÆÁöÑÂõûÁ≠î„ÄÇ
- ÁÆÄÂçïÊù•ËØ¥ÔºåRAG Â∞±ÂÉèÊòØÁªôÂ§ßÊ®°ÂûãË£Ö‰∏ä‰∫Ü‰∏Ä‰∏™‚ÄúÂ§ñÊåÇÊêúÁ¥¢ÂºïÊìé‚Äù„ÄÇ
  - Áõ∏ÊØî‰πã‰∏ãÔºå‰º†ÁªüÁöÑ ÂæÆË∞ÉÔºàFine-tuningÔºâ ÊñπÂºèËôΩÁÑ∂ËÉΩÊääÊñ∞Áü•ËØÜÂÜôËøõÊ®°ÂûãÔºå‰ΩÜÂæÄÂæÄÊàêÊú¨È´òÊòÇ„ÄÅ‰∏çÂèØÈÄÜÔºåËøòÂèØËÉΩ‚ÄúÁâ∫Áâ≤‚ÄùÂéüÊúâËÉΩÂäõ„ÄÇËÄå RAG ÁöÑ‰ºòÂäøÂú®‰∫éÔºöËΩªÈáè„ÄÅÁÅµÊ¥ª„ÄÅÈöèÂèñÈöèÁî®
- ÁõÆÂâçÂ§ßÂ§öÊï∞ RAG Á≥ªÁªüÂÖ∂ÂÆûÊòØ‚ÄúFrozen RAG‚Äù‚Äî‚ÄîRetriever ‰∏é LLM ÊòØÂàÜÂºÄÁöÑÔºåRetriever Âõ∫ÂÆö‰∏çÂèòÔºåLLM ‰πü‰∏çÊõ¥Êñ∞„ÄÇÂÆÉ‰ª¨ÊãºÂú®‰∏ÄËµ∑ËÉΩË∑ëÔºå‰ΩÜÊ®°Âùó‰πãÈó¥Âπ∂Ê≤°ÊúâÊ∑±Â∫¶ÂçèÂêå„ÄÇ
- RAG 2.0 ÁöÑÊ†∏ÂøÉÁõÆÊ†áÂ∞±ÊòØÔºö ËÆ©Ê£ÄÁ¥¢Âô®ÂíåÁîüÊàêÂô®Êàê‰∏∫‰∏Ä‰∏™‚ÄúÂèØÂÖ±ÂêåËÆ≠ÁªÉ‚ÄùÁöÑÊï¥‰ΩìÔºå‰ªéËÄåËææÂà∞Á´ØÂà∞Á´ØÁöÑ‰ºòÂåñ„ÄÇ

- ‰ªéÁ∫Ø‰∏öÂä°Á´ØÁöÑËßíÂ∫¶Êù•ËØ¥Ôºå‰∏ç‰ΩÜ‰∏ç‰ºöÊ∂à‰∫°ÔºåËøòÊúâÂèØËÉΩÂú®Êú™Êù•ÁöÑ3-5Âπ¥ÈáåÂ§ßË°åÂÖ∂ÈÅì„ÄÇ
  - Ë°®Èù¢ÂéüÂõ†ÈùûÂ∏∏ÁÆÄÂçïÔºöRAGÂá†‰πéÊòØÁõÆÂâçÂîØ‰∏Ä‰∏ÄÁßçÂÖºÂÖ∑Èó®Êßõ‰Ωé+ÂèØËßÇÂØü+ÊéßÂà∂ÂáÜ+ËßÅÊïàÂø´ÁöÑË∑ØÁ∫ø„ÄÇ
  - ÊúÄÂêéÊèê‰∏ÄÂò¥Ê∑±Â±ÇÂéüÂõ†ÔºöRAGÁöÑÊäÄÊúØË∑ØÁ∫øÊûÅÂÖ∂Á¨¶ÂêàÂõΩÊÉÖÔºåÂÖ∂Êú¨Ë¥®ÊòØÁúüÁêÜÁÅåÈ°∂‰∏îÂèØ‰ª•ÈöèÊó∂ÊõøÊç¢ÔºåÊØîÂ§ßÁÇºÊ®°ÂûãÂæóÂà∞ÁöÑÈÜâÊã≥ÂπªËßâÂèØÊéßÂæóÂ§ö„ÄÇ
  - To BÂíåTo GÈÉΩÊòØÊï∞ÊçÆÂØÜÈõÜÂûãÂØπË±°ÔºåËá™Ë∫´Êï∞ÊçÆÂ∞±ÊòØÊúÄÂ§ßÁöÑÁ≠πÁ†ÅÔºå‰∏çË¶ÅÊåáÊúõ‰∫∫ÂÆ∂‰ºöÊääË∫´ÂÆ∂ÊÄßÂëΩ‰∫§Âà∞AIÂéÇÂïÜÊâãÈáåÔºåÂéªÊâìÈÄ†‰∏Ä‰∏™ÈÅ•ÈÅ•È¢ÜÂÖàÁöÑÂ§ßÊ®°Âûã„ÄÇ
- Â§ßÊ®°ÂûãÁõÆÂâçÊ≤°ÁúãÂà∞‰ºÅ‰∏öÂÜÖÂ•ΩÁöÑËêΩÂú∞Âú∫ÊôØ„ÄÇragÂ§ßÈÉ®ÂàÜÊòØÁî®‰∫éÁü•ËØÜÂ∫ìÔºå‰ΩÜÊòØ‰πüÂ∞±Âü∫Êú¨ËÉΩÁî®ÔºåÂ¶ÇÊûúÊèêÈóÆ‰∏çËÉΩ‰ªéÂÖ≥ÈîÆËØçÂíåÂêëÈáè‰∏äÂåπÈÖçÔºåÈÇ£‰Ω†ÁªôÂ§ßÊ®°ÂûãÁöÑÂÜÖÂÆπÂ∞±ÂíåÈóÆÈ¢òÊó†ÂÖ≥ÔºåËá™ÁÑ∂Á≠îÊ°àÂ∞±‰∏çÂØπ„ÄÇÂπ∂‰∏îÊñáÊ°£ÊãÜÂàÜÂêéÔºåÈÄªËæë‰∏äÂ∞±Ë¢´ÊãÜÂàÜ‰∫ÜÔºåÂæàÂ§öÈóÆÈ¢òÊòØË¶Å‰ªéÂêÑ‰∏™ÊñáÊ°£ÈÄªËæë‰∏äÂàÜÊûêÔºå‰∏çÊòØÁÆÄÂçïÁöÑÊ£ÄÁ¥¢„ÄÇ

- ÂõûÈ°æ‰∏ãÂΩìÂâçÂ∏ÇÂú∫‰∏ä‰ºÅ‰∏ö RAG Áü•ËØÜÂ∫ìËêΩÂú∞ÁöÑ‰∏âÁßç‰∏ªÊµÅË∑ØÂæÑ„ÄÇ
- Áõ¥Êé•‰ΩøÁî®È´òÂ±ÇÁ∫ßÂºÄÊ∫êÊ°ÜÊû∂
  - ËøôÁ±ªÊ°ÜÊû∂Â¶Ç RAGFlow, Dify, FastGPT Á≠âÔºå‰∏ªË¶ÅÁâπÁÇπÊòØÊèê‰æõÁõ∏ÂØπÂÆåÊï¥„ÄÅÂºÄÁÆ±Âç≥Áî®ÁöÑ RAG Â∑•‰ΩúÊµÅÔºå
  - ÁõÆÊ†áÊòØÁÆÄÂåñ RAG Â∫îÁî®ÁöÑÊê≠Âª∫ËøáÁ®ãÔºåÈôç‰ΩéÂºÄÂèëÈó®Êßõ„ÄÇ
  - Âèç‰πãÔºåÂä£Âäø‰∏ªË¶ÅÊòØÂÆöÂà∂ÂåñÂíåÁÅµÊ¥ªÊÄßÂèóÈôêÔºåÊ∑±Â∫¶‰ºòÂåñÂíåÈõÜÊàêÁâπÂÆöÁªÑ‰ª∂Êó∂Â§çÊùÇÂ∫¶ËæÉÈ´ò„ÄÇ
- Â∫ïÂ±ÇÂºÄÂèëÊ°ÜÊû∂Ëá™‰∏ªÂºÄÂèë
  - ËøôÁ±ªÊ°ÜÊû∂ÂåÖÊã¨ LangChain, LlamaIndex, Haystack Á≠âÔºåÈÉΩÊèê‰æõ‰∫Ü‰∏ÄÁ≥ªÂàóÊ®°ÂùóÂåñÁöÑÊûÑÂª∫Âùó„ÄÅÂ∑•ÂÖ∑ÂíåÊé•Âè£„ÄÇÂºÄÂèëËÄÖÂèØ‰ª•Ê†πÊçÆÈúÄÊ±ÇÁÅµÊ¥ªÁªÑÂêàÂíåÁºñÊéí RAG ÊµÅÁ®ãÁöÑÂêÑ‰∏™ÁéØËäÇÔºàÂ¶ÇÊï∞ÊçÆÂä†ËΩΩ„ÄÅÊñáÊú¨ÂàÜÂâ≤„ÄÅÂµåÂÖ•„ÄÅÂêëÈáèÂ≠òÂÇ®„ÄÅÊ£ÄÁ¥¢Á≠ñÁï•„ÄÅLLM Ë∞ÉÁî®„ÄÅËÆ∞ÂøÜÁÆ°ÁêÜ„ÄÅAgent ÊûÑÂª∫Á≠âÔºâ„ÄÇ
  - ‰ºòÂäøÂæàÊòéÊòæÂ∞±ÊòØÁÅµÂ∫¶ÁÅµÊ¥ªÔºåÂÆöÂà∂ÂåñËÉΩÂäõÂº∫ÔºåËÉΩÂ§üÈíàÂØπÁâπÂÆö‰∏öÂä°Âú∫ÊôØËøõË°åÊ∑±Â∫¶‰ºòÂåñÂíåÈõÜÊàê„ÄÇÈÄÇÂêàÂØπ RAG ÊµÅÁ®ãÊúâÁ≤æÁªÜÂåñÊéßÂà∂ÈúÄÊ±ÇÁöÑ‰ºÅ‰∏ö„ÄÇ
  - Âèç‰πãÂä£ÂäøÂ∞±ÊòØÈúÄË¶ÅÊõ¥Â§öÁöÑÂºÄÂèëÂ∑•‰ΩúÈáèÂíåÊäÄÊúØÊ∑±Â∫¶„ÄÇ
- ‰∫ëÂéÇÂïÜ MaaS Âπ≥Âè∞ÊñπÊ°à
  - Â¶ÇÈòøÈáå‰∫ëÁôæÁÇº, ÁôæÂ∫¶Êô∫ËÉΩ‰∫ëÂçÉÂ∏Ü, AWS Bedrock, Google Vertex AI Search Á≠âÁöÑÁßÅÊúâÂåñÈÉ®ÁΩ≤ÊñπÊ°àÔºåÊó†ËÆ∫ÊòØÂõΩÂÜÖËøòÊòØÂõΩÂ§ñÁöÑËøô‰∫õ‰∫ëÊúçÂä°ÂïÜÔºåÈÉΩÊòØÊää RAG Áõ∏ÂÖ≥ÁöÑËÉΩÂäõÂ∞ÅË£ÖÊàêÊúçÂä°ÊàñÊèê‰æõÁßÅÊúâÂåñÈÉ®ÁΩ≤ÂåÖÔºåÈÄöÂ∏∏‰∏éËá™ÂÆ∂ÁöÑÂ§ßÊ®°Âûã„ÄÅËÆ°ÁÆóËµÑÊ∫ê„ÄÅÊï∞ÊçÆÂ≠òÂÇ®Á≠âÊ∑±Â∫¶ÈõÜÊàê„ÄÇ‰∏ÄËà¨‰πü‰ºöÊèê‰æõÊ®°ÂûãÈÄâÊã©„ÄÅÂæÆË∞É„ÄÅÈÉ®ÁΩ≤„ÄÅÁõëÊéßÁ≠â‰∏ÄÁ´ôÂºèÊúçÂä°„ÄÇ
  - ÈááÁî®ËøôÁßçÊñπÊ°àÈÄöÂ∏∏ËÉΩÊèê‰æõÁ®≥ÂÆöÁöÑÂü∫Á°ÄËÆæÊñΩ„ÄÅ‰æøÊç∑ÁöÑÊ®°ÂûãÁÆ°ÁêÜÂíåÈÉ®ÁΩ≤„ÄÅ‰ª•Âèä‰∏éÂÖ∂‰ªñ‰∫ëÊúçÂä°ÁöÑËâØÂ•ΩÂÖºÂÆπÊÄß„ÄÇÂØπ‰∫éÂ∑≤ÁªèÊ∑±Â∫¶‰ΩøÁî®ÁâπÂÆö‰∫ëÁîüÊÄÅÁöÑ‰ºÅ‰∏öÊù•ËØ¥ÔºåÈõÜÊàêÊàêÊú¨ËæÉ‰Ωé„ÄÇ
  - ‰ΩÜÂä£ÂäøÂ∞±Â¶ÇÂêå‰ΩøÁî®Â§ßÊ®°Âûã‰∏Ä‰ΩìÊú∫‰∏ÄÊ†∑ÔºåÂèØËÉΩÂ≠òÂú®ÂéÇÂïÜÈîÅÂÆöÁöÑÈ£éÈô©ÔºåË∑®‰∫ëËøÅÁßªÊàñÈõÜÊàêÈùûËØ•ÂéÇÂïÜÁöÑÊúçÂä°ÂèØËÉΩ‰ºöÊØîËæÉÂ§çÊùÇ„ÄÇ
  - ÂΩìÁÑ∂ÔºåË¥πÁî®ÂíåÁÅµÊ¥ªÊÄß‰πüÈúÄË¶ÅËÄÉËôë„ÄÇ
- ÊÄªÁªìÊù•ËØ¥ÔºåÂÆûÈôÖÁîü‰∫ßÂú∫ÊôØ‰∏≠‰ª•‰∏ä‰∏âÁßçÈÄâÊã©Âπ∂ÈùûÂÆåÂÖ®‰∫íÊñ•Ôºå‰æãÂ¶Ç‰ª•Â∫ïÂ±ÇÊ°ÜÊû∂‰∏∫Âü∫Á°ÄÔºå‰ΩÜÂÄüÂä©‰∫ëÂéÇÂïÜÁöÑÈÉ®ÂàÜÊ®°ÂûãÊúçÂä°ÊàñÂü∫Á°ÄËÆæÊñΩ‰πüÊòØ‰∏≠ÁõÆÂâçÂ∏∏ËßÅÁöÑÁªÑÂêà„ÄÇ

- RAG‰∏ªË¶ÅÂèØ‰ª•Ëß£ÂÜ≥Â§ßÊ®°ÂûãÂπªËßâÈóÆÈ¢òÔºå‰ª•ÂèäÈÄöËøáÂ§ñÊåÇÁü•ËØÜÔºåÈÅøÂÖçÊ®°ÂûãÈ¢ëÁπÅËøõË°åÁü•ËØÜÊõ¥Êñ∞
  - Âç≥‰ΩøÊ®°ÂûãÂèØÊé•Âèó‰∏ä‰∏ãÊñáÈïøÂ∫¶‰∏∫128kÔºå‰ΩÜÊòØÊàëÁöÑÁü•ËØÜÂ∫ìÈáåÊúâ10wÁØáÊñáÊ°£ÔºåËøúËøúË∂ÖËøáÊ®°ÂûãÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÔºåÊâÄ‰ª•‰Ω†ËøòÊòØÈúÄË¶ÅÊ£ÄÁ¥¢„ÄÇ
  - ÂΩìÁÑ∂Ê®°Âûã‰∏ä‰∏ãÊñáÈïøÂ∫¶ÂèòÈïøÔºåÂØπ‰∫éÊ£ÄÁ¥¢Êõ¥Âä†ÊúâÂ•ΩÔºåÁ≤íÂ∫¶ÂèòÂ§ßÔºåÂè¨ÂõûÂèòÈ´òÔºåÁõ∏ËæÖÁõ∏Êàê„ÄÇ
  - Âç≥‰Ωø‰Ω†ÂèØ‰ª•Êé•Âèó128kÁöÑÈïøÂ∫¶Ôºå‰ΩÜÊòØ‰Ω†ÁöÑËÆæÂ§áÊ∂àËÄó‰∏çËµ∑ÔºåÊõ¥Â§ßÁöÑÈïøÂ∫¶ÈúÄË¶ÅÊõ¥Â§öÁöÑÊòæÂ≠òÔºåÊ®°ÂûãÈÉ®ÁΩ≤ÊàêÊú¨Êõ¥È´òÔºåÈÄüÂ∫¶Êõ¥ÊÖ¢„ÄÇ
  - Â¶ÇÊûú‰Ω†Ë∞ÉÁî®apiÔºåÈÇ£tokenÊ∂àËÄó‰πüÂ§™Â§ß‰∫ÜÔºåÊàêÊú¨ËøáÈ´ò„ÄÇ

- Context Length ÂèòÂ§ß ‚â† ‰∏çÈúÄË¶Å RAG
  - ÊàêÊú¨ÈóÆÈ¢ò: ‰∏ä‰∏ãÊñáË∂äÈïøÔºåÊé®ÁêÜÊàêÊú¨ÔºàÊòæÂ≠ò„ÄÅÊó∂Èó¥„ÄÅÁÆóÂäõÔºâ‰ºöÁ∫øÊÄßÁîöËá≥Ë∂ÖÁ∫øÊÄßÂ¢ûÂä†„ÄÇ ÊØîÂ¶ÇÔºå100k tokens Â§ÑÁêÜ‰∏ÄÊ¨°Â∞±ÂæàÁÉßÈí±ÔºåËÄå RAG ÂèØ‰ª•Áî®ËæÉÁü≠‰∏ä‰∏ãÊñáÂèçÂ§çË∞ÉÁî®„ÄÇ
  - ‰ø°ÊÅØÊ£ÄÁ¥¢ÊïàÁéá: Â¶ÇÊûú‰Ω†Êúâ 1TB ÁöÑÁü•ËØÜÂ∫ìÔºåÊääÂÆÉÂÖ®Â°ûËøõ‰∏ä‰∏ãÊñáÊòØ‰∏çÂèØËÉΩÁöÑ„ÄÇ RAG ÂÖàÊ£ÄÁ¥¢ÔºåÂÜçÈÄÅËøõ‰∏ä‰∏ãÊñáÔºåÂèØ‰ª•ËÆ©Ê®°Âûã‰∏ìÊ≥®‰∫éÁõ∏ÂÖ≥‰ø°ÊÅØ„ÄÇ
  - Âô™Èü≥‰∏éÂπ≤Êâ∞: Â§ß‰∏ä‰∏ãÊñáÂπ∂‰∏ç‰ª£Ë°®Ê®°Âûã‰ºöÂøΩÁï•Êó†ÂÖ≥‰ø°ÊÅØÔºåÁõ∏ÂèçÂ§™Â§öÊó†ÂÖ≥ÂÜÖÂÆπ‰ºöÁ®ÄÈáäÂÖ≥ÈîÆ‰ø°ÊÅØÔºåÂØºËá¥Êé®ÁêÜÊïàÊûú‰∏ãÈôç„ÄÇRAG ÁöÑ‚ÄúÊ£ÄÁ¥¢+ËøáÊª§‚ÄùËÉΩÊèêÈ´ò‰ø°Âô™ÊØî„ÄÇ
- Context ÂèòÂ§ßÂêéÔºåRAG ÁöÑÂΩ¢ÊÄÅ‰ºöÊºîÂåñ
  - ‰ªé‚ÄúÊü•ÊâæËµÑÊñô‚ÄùÂà∞‚ÄúÂä®ÊÄÅÁü•ËØÜÁªÑÁªá‚Äù: Êú™Êù•ÁöÑ RAG ÂèØËÉΩ‰∏çÂè™ÊòØÊ£ÄÁ¥¢ÔºåËÄåÊòØËá™Âä®ÁîüÊàê‰∏Ä‰∏™‰∏¥Êó∂ÁöÑÁü•ËØÜÊëòË¶Å/Áü•ËØÜÂõæË∞±ÂÜç‰∫§ÁªôÂ§ßÊ®°Âûã
  - Â§öÈò∂ÊÆµÊé®ÁêÜÔºàMulti-turn ReasoningÔºâ: Â§ß‰∏ä‰∏ãÊñáÂÖÅËÆ∏‰∏ÄÊ¨°ÊÄßÊîæÊõ¥Â§ö‰ø°ÊÅØÔºå‰ΩÜ RAG ÂèØ‰ª•Âú®ÊØè‰∏ÄÊ≠•Êé®ÁêÜÊó∂Êõ¥Êñ∞‰∏ä‰∏ãÊñáÔºåÂΩ¢Êàê ‰∫§‰∫íÂºèÊé®ÁêÜ
  - Hybrid RAGÔºàÊ∑∑ÂêàÊ®°ÂºèÔºâ: ÊääÈùôÊÄÅÁü•ËØÜÔºàÂ∏∏Áî®‰ø°ÊÅØÔºâÁõ¥Êé•ÊîæÂú®Èïø‰∏ä‰∏ãÊñáÈáåÔºåÊääÂä®ÊÄÅÊï∞ÊçÆÔºàÂÆûÊó∂‰ø°ÊÅØ„ÄÅÊêúÁ¥¢ÁªìÊûúÔºâÁî® RAG ÊñπÂºèÊèíÂÖ•„ÄÇ
- ‰∏∫‰ªÄ‰πà RAG Âú®Èïø‰∏ä‰∏ãÊñáÊó∂‰ª£‰æùÁÑ∂ÈáçË¶Å
  - ÂèØÊõ¥Êñ∞ÊÄß: Ê®°ÂûãËÆ≠ÁªÉÊï∞ÊçÆÊòØÈùôÊÄÅÁöÑÔºåRAG ÂèØ‰ª•Âú®‰∏çÈáçÊñ∞ËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÂºïÂÖ•ÊúÄÊñ∞Áü•ËØÜ„ÄÇ
  - ÈöêÁßÅ‰∏éÂÆöÂà∂Âåñ: ÁßÅÊúâ‰ºÅ‰∏öÊàñ‰∏™‰∫∫Êï∞ÊçÆ‰∏ç‰ºöÁõ¥Êé•ÊîæÂà∞Â§ßÊ®°ÂûãÈáåÔºåËÄåÊòØÈÄöËøá RAG Âä®ÊÄÅÊ≥®ÂÖ•„ÄÇ
  - ÂáèÂ∞ëÂπªËßâ: Ê®°ÂûãÁõ¥Êé•ÂºïÁî®Ê£ÄÁ¥¢Âà∞ÁöÑÂéüÊñáÔºåÂèØ‰ª•ÊòæËëóÈôç‰ΩéËÉ°Áºñ‰π±ÈÄ†ÁöÑÊ¶ÇÁéá„ÄÇ
  - Â§öÊ®°ÊÄÅÊ£ÄÁ¥¢: RAG Êú™Êù•‰ºö‰∏ç‰ªÖÊòØÊñáÊú¨ÔºåËøò‰ºöÊ£ÄÁ¥¢ÂõæÁâá„ÄÅËßÜÈ¢ë„ÄÅÈü≥È¢ë„ÄÅÁªìÊûÑÂåñÊï∞ÊçÆÔºåÂÜçÁªü‰∏ÄÁªôÂ§ßÊ®°ÂûãÂ§ÑÁêÜ
- Êú™Êù• RAG ÁöÑÈáçÁÇπ‰ºö‰ªé‚ÄúÊâæÊñáÊ°£‚ÄùÂèòÊàê‚ÄúÊûÑÂª∫ÊúÄ‰ºòÊé®ÁêÜ‰∏ä‰∏ãÊñá‚ÄùÔºåÂÆÉ‰ºöÂèòÂæóÊõ¥Êô∫ËÉΩ„ÄÅÊõ¥Âä®ÊÄÅ„ÄÇ
  - ÈöèÁùÄ Context > 1M tokensÔºåÈÉ®ÂàÜÂú∫ÊôØÂèØÁõ¥Êé•Áî®Â§ßÊ®°ÂûãÁöÑÈïøËÆ∞ÂøÜÔºå‰ΩÜ RAG ‰ºöÊºîÂèò‰∏∫‚ÄúÊô∫ËÉΩÁü•ËØÜÁºñÊéíÂô®‚Äù„ÄÇ
  - Â¶ÇÊûúÂá∫Áé∞Êó†Èôê‰∏ä‰∏ãÊñá‰∏îÊé®ÁêÜÊàêÊú¨ÊûÅ‰ΩéÁöÑÊ®°ÂûãÔºåRAG ‰ºöÈÄÄÂåñÊàê‚ÄúÊï∞ÊçÆÁ¥¢ÂºïÂ∑•ÂÖ∑‚ÄùÔºå‰∏ªË¶ÅÁî®‰∫éÂä†ÈÄüÊï∞ÊçÆÁªÑÁªáËÄå‰∏çÊòØÂøÖÈ°ªÊ≠•È™§„ÄÇ

- ‰∏ç‰ºö„ÄÇ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÂèòÈïøÂè™‰ºöËÆ©RAGÊõ¥Â•ΩÁî®„ÄÇ‰ªéÁõÆÂâçÁöÑÊÉÖÂÜµÊù•ÁúãÔºåRAGÁöÑÊàêÊú¨‰æùÊóß‰Ωé‰∫éÂæÆË∞ÉÔºåÊïàÊûúÈ´ò‰∫éÂæÆË∞É„ÄÇÁÑ∂ÂêéÁõÆÂâçllmÂ∫îÁî®ÁöÑÊú¨Ë¥®ËøòÊòØpromptÔºåÊ≤°Êúâ‰ªÄ‰πàÂ§ßËä±Ê†∑Ôºå‰πüËÑ±Á¶ª‰∏ç‰∫ÜÁé∞ÊúâÁöÑ‰∫∫Á±ªÂ≠¶Áßë„ÄÇ
  - ÂΩìÁÑ∂ÔºåÂ¶ÇÊûúÊúâ‰∫∫Âº∫Ë°åËØ¥RAGÂ∞±ÊòØÁî®ÂµåÂÖ•ÂêëÈáè‰ªÄ‰πàÁöÑÔºåÈÇ£‰πàÊàëÂè™ËÉΩËØ¥ÂèØËÉΩ‰ºöÊ∂à‰∫°„ÄÇretrieval ËÇØÂÆöÊòØÂåÖÊã¨‰∫ÜÂÖ∂‰ªñÊñπÂºèÁöÑÔºåÊàë‰ª¨Áî®geminiÂèëÊñá‰ª∂ÊàñËÄÖÂõæÁâáÁªô‰ªñ‰ΩïÂ∞ù‰∏çÊòØ‰∏ÄÁßçRAGÂë¢Ôºå‰∏çËøáÂè™ÊòØ‰∫∫ËÑëÈ©±Âä®ÁΩ¢‰∫Ü

- ## RAG is anti pattern.
- https://x.com/pelaseyed/status/1882471632129994914
  - I don‚Äôt do RAG anymore, get 10x the result just spinning up a pipeline and feed all content to Deepseek. And yes it scales to over 10K docs. 

- And you can cache it, so doesn‚Äôt even cost that much

- Does deepseek have caching like Google‚Äôs media.upload? How do you otherwise avoid rebuilding the context all the time (I‚Äôve not used the DS APIs).

- I wish I could fit the entire Internet in the prompt for @ask_pandi but it doesn't fit. RAG is still needed for this.
  - Run multiple LLMs in parallell, you will have infinite context window.

- You put everything into context?
  - Everything in context(s)

- RAG is A pattern, just not useful when your context is under 128k token and when the context is known. Pick the right tool for the job.
# discuss-solutions
- ## 

- ## 

- ## 

- ## 

- ## [I've been noticing more and more people are using GraphRAG instead of embedding and vector databases...is it really helpful or just the hype? : r/LangChain _202507](https://www.reddit.com/r/LangChain/comments/1e66e9r/graphrag/)
- Been working with graphrag on my current project. It is good, but it has its fallbacks. The good thing is that you can model you data to preserve relations. 
- The bad thing is:
  - you rely on your LLM to create queries based on your question to the answer. Lots of times the query created is pure shit, even with the most advanced models.
  - It gets really expensive. Like really expensive. Imagine that for every document you're storing in your graphDB you need to pass it to a LLM so it can extract the content and create relations so you can store it on your DB.

- If you ask me, people just follow the trends without understanding whether their application really needs something (eg GraphRAG here). 

- This is all basically more of a hype I would say. Why? Because you can use your own database that handles multiple data types and construct a knowledge graph. The knowledge graph basically shows the entity relationship between different nodes and this can be easily fed into your databases as tables and then, using retrieval methods, you can easily retrieve the most relevant chunk and the relationship score. 
  - I mean, the specialise databases like vector databases or even graph databases are really hyped, your own databases whether it is MongoDB, SingleStore, MariaDB, or even Couchbase can easily help you with creating graph knowledge and store them and then retrieve whenever required.

- how GraphRAG works with existing graphs, meaning if I already have entities and relationships, how would I load this graph in GraphRAG? 

- ## ÊúâÊ≤°ÊúâÊÉ≥ËøáÂ∞ÜËá™Â∑±ÁöÑ‰ª£Á†ÅÂ∫ìÊâìÂåÖÁÑ∂ÂêéÁõ¥Êé•Â°ûÁªôÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊù•Â§ÑÁêÜÔºü‰πüËÆ∏‰Ω†‰ºöÊÉ≥Âà∞Áî®RAGÊàñËÄÖÁî®windsurfÊàñcursor„ÄÇÁé∞Âú®Êúâ‰∫ÜÊõ¥ÁÆÄÂçïÁöÑÂäûÊ≥ï‚Äî‚ÄîRepomix
- https://x.com/karminski3/status/1881150047276138689
  - Ëøô‰∏™Â∫ìÂèØÂ∞ÜÊï¥‰∏™Â≠òÂÇ®Â∫ìÊâìÂåÖÂà∞‰∏Ä‰∏™ AI ÂèãÂ•ΩÁöÑÊñá‰ª∂‰∏≠ÔºåÊñπ‰æøÁªôÂ§ßËØ≠Ë®ÄÊ®°Âûã‰ΩøÁî®„ÄÇ

- ## üìå A list of software that allows searching the web with the assistance of AI.
- https://x.com/tom_doerr/status/1856778512612667838

# discuss-pdf
- ## 

- ## 

- ## 

- ## 

- ## 

- ## [Best way to extract data from PDFs and HTML : r/Rag _202510](https://www.reddit.com/r/Rag/comments/1oavnx4/best_way_to_extract_data_from_pdfs_and_html/)
- During my experience extracting data from PDFs and HTML files for use in RAG systems, I usually follow one of the two approaches shown in this notebook I created (VLM or docling/paddleOCR)‚Äî I hope you find it helpful. 

- Have you looked at Pinecone Assistant? You can upload PDFs (up to 100MB) and it manages the chunking, embedding, and search for you. If you already have chat/model generation, you could use just the /context API to get search results to feed into your own model.

- At PipesHub, we use docling, pymupdf (faster than docling but need to use layout parser on top of it), ocrmupdf/Azure DI (scanned pdfs).
  - On top of docling, we have specialized logic for extracting metadata, deep understanding of document, tables, etc. 
  - We use VLM/Mulitimodal AI models for handling images, diagrams and more.
  - You can use docling (only issue it's parsing is slow) if you are looking for open source, free solution.
  - If you are looking for Higher Accuracy, Visual Citations, Cleaner UI, Direct integration with Google Drive, OneDrive, SharePoint Online, Dropbox and more. PipesHub is free and fully open source, extensible. 

- I have done two approaches so far. Docling was good, but I was not happy that it took long on larger PDFs. 
  - Another approach was converting the pdf to docx, then docx to html and finally to markdown. This approach was quite fast, but `pdf2docx` library had some trouble converting certain documents.
  - Docling to me was quite a good solution, and if you are okay with the amount of time to process larger documents.
  - Overall, a hybrid approach is always the best solution.
# discuss-rag-local
- ## 

- ## 

- ## 

- ## üí° [Embedding With LM Studio - what am i doing wrong : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1lharbh/embedding_with_lm_studio_what_am_i_doing_wrong/)
  - "text-embedding-nomic-embed-text-v1.5" loads fine and works with Anything.
  - but text-embedding-qwen3-embedding-0.6b & 8B and any other Embed model i use i get the below error: Failed to load embedding model. Internal error: Failed to initialize the context: failed to allocate compute pp buffers

- SOLVED
  - LM Studio was defaulting to 4096 Tokens where as the model can only handle 1024. Same goes for AnythingLLM , that defaults to 8192 tokens. set everything to 1024 and now its working

- Your "SOLVED" update is wrong. I ended up on this post after also having memory allocation issues with Qwen3 embedding models. 
  - The "1024" value you are quoting is the number of output dimensions of the model that clients would use to interpret results. You don't set that anywhere in LM Studio.
  - all you did by lowering the context length to 1024 tokens is reduce memory usage enough to fix your issue. You also made the output quality worse. Your clients will have to make more requests for more chunks for indexing since the model only accepts 1024 tokens at a time. This will in turn increase the number of vector entries that need to be stored.

- I had to "Override Domain Type" to Text Embedding to get it to work my computer (Models -> settings gear -> bottom setting). Not sure what else to try if you've already done that.
  - yep, did forget to mention that i tried both configurations

- try using any model you have as an embedder. It works just fine.

- ## [What Embedding Models Are You Using For RAG? : r/LocalLLaMA _202312](https://www.reddit.com/r/LocalLLaMA/comments/18j39qt/what_embedding_models_are_you_using_for_rag/)
- For RAG, I have been using multilingual minilm model from HF hub (I have multiligual requirement), and qdrant vector database. Happy with both. Specially, minilm embeddings are 368 dims, so some space saving there as well.

- Chinese embedding models and llms are now the top performers. Qwen-1.5B Yi-34B gte-large bge-large

- I chose the most lightweight stack possible‚Äî using all-MiniLM-L6-v2 for embeddings and HyperDB for local storage/retrieval (this is super fast for <100k documents)

- I am currently using all-MiniLM-L6-v2 but responses are generally not that good my data size is almost 600mb and it is giving me not that good results, what kind of embedding should i choose considering the size of data and database of pgvector postgres

- What Embedding model is best for Text-To-SQL for Financial data? Currently I'm using OpenAIEmbeddings
  - Text-to-SQL is a sequence to sequence task. My very first thought was "zero shot learning" because I have done this successfully in other domains. 

- I use whatever models are on top 1-5 on the MTEB leaderboard and run my custom evaluation 
# discuss-rag-tips/tricks
- ## 

- ## 

- ## üî¢ [How to dynamically prioritize numeric or structured fields in vector search? : r/LangChain _202510](https://www.reddit.com/r/LangChain/comments/1oclpn4/how_to_dynamically_prioritize_numeric_or/)
- This will not happen automatically. Either you have to rewrite the query or do a query classification and let this be handled by a database query.
  - You may be able to store the numerics as metadata and use the metadata to rerank, but to do even that, you need to classify the query first.
  - You can have a set of ranking attributes as metadata, and classify the query using a pretrained classification model, to find which ranking attributes to use and implement the reranking on those metada attributes.

- Yeah, this is a common puzzle when you move beyond basic semantic search. Post-processing reranking is probably your cleanest bet.
- The general idea is to do a two-step retrieval:
  - Use the vector search to get a broad set of semantically relevant candidates (e.g., the top 20-50 documents).
  - Then, in your application layer, iterate over just those candidates. You can parse the query to see if it mentions "top" or "most, " then extract the relevant numeric field (ranking, publications, etc.) from the metadata of those candidates and re-score them.
- This way you're not trying to cram structured logic into the embedding space. 
  - LlamaIndex has postprocessors for exactly this kind of thing, you can even write a custom one. It's way more flexible than trying to make the vector DB do everything.

- milvus supports hybrid search (vector + bm25) with reranking strategies. you can filter metadata pre-search using comparison operators on numeric fields, or build a custom llamaindex postprocessor that combines vector similarity with dynamic metadata scoring. retrieve top-k, then rescore as: new_score = similarity_weight * vector_score + metadata_weight * normalize(field). this avoids hardcoding thresholds. alternatively use llmrerank to let the llm judge relevance based on metadata context, but slower/costlier.

- ## [RAG over Database : r/LangChain _202407](https://www.reddit.com/r/LangChain/comments/1efnx5u/rag_over_database/)
  - I have been trying to build a RAG over a database that has mulitple tables. Often times, for a user query, the data has to be searched by joining multiple tables. I followed this approach as mentioned in Langchain documents.
  - What I am observing is that many times the query generated by LLM is not correct and the data that user wants is incorrect. 

- Try to make all the joins yourself in the form of view and provide a simple view for the LLM

- dont use joins. use views or make stored procs to call for summaries etc you want.

- I would use the traditional RAG-based approach more in cases when the knowledge base is made of documents instead of structured data.
# discuss-rag-db
- ## 

- ## 

- ## 

- ## 

- ## 

- ## 

- ## [Best Vector Database for RAG : r/vectordatabase _202501](https://www.reddit.com/r/vectordatabase/comments/1hzovpy/best_vector_database_for_rag/)
- All vector databases work well for RAG. The selection of a vector database usually depends on your preference: cloud based vs self-hosted, open source vs private, programming languages / clients, API access, already part of SQL etc.
- Some common ones are (not in any order, I'm not affiliated with any):
  - Pinecone: private, cloud based, very popular
  - pgvector: Postres vector search extension, useful if you have already data in Postgres.
  - Faiss: A library for efficient similarity search and clustering of dense vectors.
  - Qdrant: Open source vector search engine written in Rust.
  - Chroma: Yet another vector database
  - Milvus: An open-source similarity search engine for embedding vectors.
  - Weaviate: Open source vector database written in Go.

- Pgvector because it is part of Postgres. In the end Vectors are just a data type in a databases. If you only need vectoring and not SQL then it is better to look search engine that does that I think.
  - PGVector, is like PostGIS for geospatial ops, an extension for Postgre to support vector search, it is not a true vector-native DB, so it adds operational overhead and lacks advanced features that a vector DB provides. 

- Quadrant, it's a vector DB designed specifically for RAG/AI workflows. It offers advanced filtering features (multi-tenancy) that are natively baked-in into its core architecture. These features would require more legwork using other data stores like Redis etc...
- FAISS: great for vector search is not really a DB, it's a lib and not a true DB
# discuss-code-rag
- ## 

- ## 

- ## 

- ## üÜö [Does grep perform better than vector DB + embeddings in large code bases? : r/ClaudeCode _202511](https://www.reddit.com/r/ClaudeCode/comments/1osyrpu/does_grep_perform_better_than_vector_db/)
  - Unlike Cursor or Github Copilot, I see that Claude Code seems to leave it up to the user to either do the indexing or not. Is there a reason? Does it perform better?
- Short answer: grep ‚â† BM25 ‚â† vectors. They each win on different axes.
  - grep/ripgrep ‚Äî exact string/regex scan over files. Zero indexing, blazing on rare tokens and precise patterns (‚Äúdef foo(|GUIDs|error codes‚Äù). Great for ‚ÄúI know the string.‚Äù
  - BM25 (inverted index) ‚Äî lexical retrieval with ranking. It tokenizes code/text and returns files that share the same terms, weighted by `tf-idf` . Faster than grep on huge repos (no full scan) and returns a ranked list, but it‚Äôs still keyword-based (no synonym/semantics unless you add query expansion). Think Zoekt/Sourcegraph style code search.
  - Embeddings (vector DB) ‚Äî semantic retrieval. Finds conceptually similar code/comments (e.g., ‚Äúexponential backoff retry‚Äù locating retry_with_jitter() in another lang with no ‚Äúbackoff‚Äù keyword). Costs an index build + memory, but best when you don‚Äôt know exact strings.
- Trade-offs:
  - Speed: grep (no index) < BM25 (indexed) < vectors (indexed + compute)
  - Recall: grep (exact) < BM25 (lexical fuzzy) < vectors (semantic)
  - Precision out-of-the-box: grep high for exact needles; BM25 good for term overlap; vectors need a reranker to avoid drift.
- Best practice in large codebases:
  - Hybrid: BM25 (or Zoekt) for lexical + a small vector index for semantics.
  - Fuse results (Reciprocal Rank Fusion or LLM rerank) so you get both ‚Äúknown string‚Äù hits and conceptual matches.
  - Keep grep/ripgrep handy for one-off precise hunts; use the indexes when scale/recall matter.
- So ‚Äúdoes grep perform better?‚Äù 
  - For exact, known strings on your machine, often yes. 
  - For concept queries across languages/renames, vectors win. 
  - For day-to-day, hybrid > either alone. Edit: reworded

- People @Cursor have written a bunch about their belief that strong semantic embeddings are way better for coding tasks perf  [Improving agent with semantic search ¬∑ Cursor _202511](https://cursor.com/blog/semsearch)
  - Closed benchmark, proprietary embeddings model... Even if it works  (the improvements aren't huge to begin with ) there is no way to reproduce their setup. 

- ## [Cline doesn't index your codebase. No RAG, no embeddings, no vector databases. : r/CLine _202505](https://www.reddit.com/r/CLine/comments/1kwhcz0/cline_doesnt_index_your_codebase_no_rag_no/)
- Does Roo Code also do that or is this just a Cline thing?
  - Roo recently added it for code search. It works with local models
  - They also added prompt caching for Gemini flash.

- Exceedingly large context windows don't just result in extreme costs but it also will slow down your every operation.
  - While prompt caching reduce costs it is not entirely free, nor does every model or provider support it. It's also like treating a symptom rather than the disease.

- RAG can be good. See how fast and precise is AUgment Code search
  - ive found Augment to miss obvious things at times

- cursor has code indexing

- RAG shines when you don‚Äôt know the exact function / class names in a repo. ripgrep file search that Cline uses for context search is awesome‚Äîif you already have the right keywords. That‚Äôs easy on your own project, but on a massive, unfamiliar codebase ripgrep can stall while RAG keeps rolling (although it can return the wrong chunks).

- Privacy issue first. Never is sending low quality windows off the code base to the LLM is bad for quality.

- ## ü§º Cline doesn't index your codebase. No RAG, no embeddings, no vector databases. _202505
- https://x.com/cline/status/1927226680206131530
  - [Why Cline Doesn't Index Your Codebase (And Why That's a Good Thing) - Cline Blog _202505](https://cline.bot/blog/why-cline-doesnt-index-your-codebase-and-why-thats-a-good-thing)
  - This isn't a limitation -- it's a deliberate design choice. 
  - As context windows increase, this approach enhances Cline's ability to understand your code.
  - The industry default: chunk your codebase, create embeddings, store in vector databases, retrieve "relevant" pieces.
  - But code doesn't work in chunks. A function call in chunk 47, its definition in chunk 892, the context that explains why? Scattered everywhere.
  - We believe in the agentic power of the modesl, and with Claude's 200K+ context window, we don't need clever retrieval. We need intelligent exploration.
  - So Cline reads code the way you do -- following imports, tracing dependencies, building connected understanding.
  - üí° Instead of indexing, Cline starts with structure. Using ASTs, it maps your codebase architecture -- classes, functions, relationships.
  - Then it explores. Need to add error handling? It traces from your function to your error utilities to similar patterns. Connected comprehension.
- The problems with RAG for code run deeper:
  - Chunking breaks the logical connections between functions and their dependencies
  - Indexes become stale the moment you push a commit
  - Your IP gets duplicated in vector embeddings (security nightmare)

- ## [Unpopular opinion: RAG is actively hurting your coding agents : r/ChatGPTCoding _202505](https://www.reddit.com/r/ChatGPTCoding/comments/1ktt4ab/unpopular_opinion_rag_is_actively_hurting_your/)
- I've been saying this since RAG first became the term used to describe the method. And you are exactly right, the whole reason it became a thing was because, back when context windows were 4k or 8k max, it was out of necessity. Now, in the age where context windows are 1M or 10M tokens, it only makes sense in specific enterprise cases where you have vast datasets to query for specific, isolated information.

- RAG isn‚Äôt just calling vector stores, it‚Äôs also prompt priming before generation using various sources. Dynamically priming the prompt with relevant information before the LLM generates a response. 
  - A lot of the large context models drop off in accuracy after 100k tokens, anyway.

- frontier models have large context windows, but they all equate to a token, which equals a cost
  - Knowledge Graphs + RAG. An syntax tree is constructed of the code , where each class, function , method etc become nodes in the graph. The LLM can then traverse the graph to get only what it needs.

- One thing Aider has that Roo and Cline don't, is what they call a "RepoMap" (and it ties aider to git). 
  - But the advantage of it is, given a class, it can easily determine all the related classes, so it doesn't have to go digging through folders, trying to figure out which file is actually relevant, it knows, because the RepoMap shows which classes use which classes.
  - I pulled the RepoMap class out of aider and eventually managed to remove all the aider dependencies and got running as a command-line app. I might try making an MCP with it and giving that to Cline and see if it can improve its ability to understand the app structure.
- I thought cline already had a tree sitter which did the same thing.
  - it does use tree-sitter, but not in the same way and not to the same effect. 
  - How Aider understands you program vs how Cline understands it is very apparent in working with them. Aider has a much better understanding of which bits of code are related to which other bits, whereas Cline is frequently guessing based on the filenames.
  - Using tree-sitter isn't enough. You need to actually map out the relationships and Cline doesn't seem to do this, at least not nearly as effectively as Aider does.
- using the LSP if available surley is the best approach?
  - No, beyond getting LSP diagnostic, which you can get somewhere else anyway. Using LSP server requires counting rows and columns, and LLMs are bad at it.
- but you don't have to use the interface literally
  - Then I'm not using LSP, I'm using something that uses LSP under the hood to point that is an implementation detail. Take a look at your favorite LSP server, tell me what is useful there to LLM agent besides diagnostics?
  - I can see how you give LLM diagnostics after it update the file is useful, but giving LLM access to LSP overall is IMO utterly useless.

- I was watching an interview video with Claude Code engineers and they mentioned along the lines of not using RAG or GraphRAG because they found that Claude Code performed better without it and just relying on tools, memory and agentic search.

- RAG still makes sense for latency sensitive use cases like real-time chat, although prompt caching helps with this. 

- ## [Lessons learned from implementing RAG for code generation : r/LLMDevs _202501](https://www.reddit.com/r/LLMDevs/comments/1hw1n5o/lessons_learned_from_implementing_rag_for_code/)
  - üìù [A Recipe for a Better AI-based Code Generator | Pulumi Blog _202501](https://www.pulumi.com/blog/codegen-learnings/)
  - We wrote a blog post documenting how we do retrieval augmented generation (RAG) for code generation in our AI assistant, Pulumi Copilot. 
  - Measure and tune recall (how many relevant documents are retrieved out of all relevant documents) and precision (how many of the retrieved documents are relevant)
  - Implement end-to-end testing and monitoring across development and production
  - Create self-debugging capabilities to handle common issues like type checking errors

- when I need understanding a medium sized code base I have dumped the whole thing into gemini in Google AI workbench. It can do 2 million tokens, though it is SLOOW.

- I wonder if you've considered feeding more tokens from your retrieval result into your code gen step? Why 20k? Is that always enough? How would you even know if it weren't?
  - You've got part of the answer right there: SLOOW :-) Also expensive!
  - Seriously though, you want to optimize for both recall and precision. Too much irrelevant data (poor precision) can confuse the LLM leading to hallucinations.
  - 20k is based on "it feels right" and empirical data but honestly we have not done enough analysis to conclude that it is the perfect number for all scenarios - we will continue to measure and adjust.

- Did you experiment with other retrieval methods besides or in addition to semantic similarity? I've done some work using different techniques, like parsing dependency trees out of the current file, with promising results for code RAG.
  - We did look into BM25 for FT search but did not see measurable benefits for our use cases. Our approach relies on getting a lot of documents first and then pruning - it would be better to get just what's needed in the first place, I still hope BM25 can help there. Worth another look!
# discuss-embedding
- ## 

- ## 

- ## [what free model should i use for codebase indexing with speed indexing : r/RooCode](https://www.reddit.com/r/RooCode/comments/1oiw4w7/what_free_model_should_i_use_for_codebase/)
- Use text-embedding-004 model from google (fast and free)

- ## [Open-source embedding models: which one's the best? : r/Rag _202509](https://www.reddit.com/r/Rag/comments/1nro65j/opensource_embedding_models_which_ones_the_best/)
- tried embeddinggemma:300m and qwen3-embedding-0.6b and 4b

- [Open-source embedding models: which one's the best? : r/ollama](https://www.reddit.com/r/ollama/comments/1nro30i/opensource_embedding_models_which_ones_the_best/)
- I find the latest Qwen embeddings and reranker models the best out there today even when comparing with offerings from OpenAI and VoyageAI
  - I tried BGE, Nomic, and Qwen using ChunkHound I don't care so much about the raw numbers but about the end to end result. You can easily see the difference in results by running the chunkhound search command

- [Open-source embedding models: which one to use? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1nrgklt/opensource_embedding_models_which_one_to_use/)
- I've had my best results with bge-m3 or qwen3-embedding

- ## [Is there a classification of worst vs best ai models for RAG : r/Rag _202509](https://www.reddit.com/r/Rag/comments/1nk2rza/is_there_a_classification_of_worst_vs_best_ai/)
- For small models, most of our users have converged around: Gemma3, Qwen3, and DeepSeek
  - I always use for demos Mistral Medium 3.1 (mistral-medium-2508), which is a very good middle ground.
  - For the "large" LLMs, Gemini Flash variants and Claude Sonnet or Haiku. OpenAI models have *never* been good at this use case. The GPT-OSS models have been abysmally bad in testing.

- For an embeddings model, we've been using all-MiniLM-L6-v2 since we released TrustGraph, and haven't really seen any need to change. The platform allows you to choose any embeddings model from HF, but all-MiniLM-L6-v2 seems to do just fine in most use cases. If you want be able to try out all these model combinations, you can give them a try with TrustGraph (open source).

- gemma3 worked best in my case(turkish docs)
  - but rag mostly depends on understanding and memorization abilities

- I have tried several OpenAI models for our RAG system, and got great results using gpt-4.1-mini.
  - Via the API, it is the OpenAI model that has the largest context window right now.

- ## [embeddinggemma has higher memory footprint than qwen3:0.6b? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nbbi1c/embeddinggemma_has_higher_memory_footprint_than/)
- For whatever reason, Gemma models have a larger vocab size of 256K whereas most models have a vocab size of around 120k. This adds to the size

- confirmed, i use ollama just for embedding, download it from official ollama library. The size is 621MB on command ollama list. But getting bigger like 2.8GB on ollama ps
  - and it very slow compared to qwen embedding 4b.

- ## [Real life experience with Qwen3 embeddings? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nezmfi/real_life_experience_with_qwen3_embeddings/)
  - I need to decide on an embedding model for our new vector store and I‚Äôm torn between Qwen3 0.6b and OpenAI v3 small.
  - The qwen3 embeddings top the MTEB leaderboards scoring even higher than the new Gemini embeddings. Qwen3 has been killing it, but embeddings can be a fragile thing.

- I don't think Qwen3-Embedding-0.6B performs better than previous encoder models of similar size (e.g., bge-m3); 
  - its main advantage is long-context support. 
  - Overall, it's only a little bit better than other prior state-of-the-art LLM-based embedding models (e.g., Kalm-v2), with advantages mainly comes from instruction tuning on the query side, which improves adaptability.
- Qwen3-Embedding-4B is good. It outperforms bge by 2‚Äì3 points (on my own dataset, using NDCG@10), and maintains strong retrieval performance at 2‚Äì4k tokens per chunk. 
  - However, the GGUF version of this model seems inconsistent with the original checkpoint‚Äîthis discrepancy is unclear (I suspect it may be related to the pooling configuration).
- Qwen3-Embedding-8B might indeed be a SOTA model, but it costs too much.

- I never tested that quant. There are so many mistakes you can make with embeddings (omitting required eot tokens, missing instructions, wrong padding alignment etc.) 

- I always include something like an embedding version so it is always possible to change embedding algo without reencoding old data so long as you are willing to do a search per algo and re-rank them.

- I've used 8B and 4B as GGUF at Q4_K_M and never had issues some are pointing.
  - Found the 4B most efficient as the difference between the 8B is small for such resource difference.
  - Been using for code bases, currently over 380 files with code. No issues.

- ## [Are Qwen3 Embedding GGUF faulty? : r/LocalLLaMA _202507](https://www.reddit.com/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/)
  - Qwen3 Embedding has great retrieval results on MTEB.
  - However, I tried it in llama.cpp. The results were much worse than competitors.

- You have to add the EOS Token manually "<|endoftext|>" as of here: https://github.com/ggml-org/llama.cpp/issues/14234

- In multilingual, I was very disappointed with qwen3 embedding compared to jinaai/jina-embeddings-v3 which remains my favorite for the moment
- v4 is out btw: https://huggingface.co/jinaai/jina-embeddings-v4
  - It does work much better, getting 48.11% on the same benchmark.
  - The official JINA API is very slow though. Half a minute for a batch of 32.

- Would you believe I was just trying it out today and it was all messed up. Swapped from Q3 4B and 0.6B to granite 278m and all my problems went away.

- Yes, though if I tried generating the embeddings through the SentenceTransformers module instead, I got the state-of-the-art results I was hoping for on my benchmark. A code snippet for how to do so is listed on their HF page.
# discuss-rag-elements
- ## 

- ## 

- ## [How to Intelligently Chunk Document with Charts, Tables, Graphs etc? : r/LangChain _202510](https://www.reddit.com/r/LangChain/comments/1oe4wh4/how_to_intelligently_chunk_document_with_charts/)
- when i was working on extraction, i faced the same issue with tables. a simple parsing tool was not enough, so we added a prompt before processing the document by saying that whenever you encounter a table, first mark it by saying #Table Start# and end it with a #Table End# and take a screenshot of the whole table , feed it to the llm for ocr operation and get a parseable text based table. then during chunking, we made sure that a separate logic was being used for cases when we encounter the #Table Start# and #Table End# cause we would want to keep the whole table as one big chunk else it would lose context for the other half of the table since it would be starting with just some numbers and no context even with the overlaps. 
  - Other than this you can use MarkdownHeaderTextSplitter since it helps with the other part of documents aswell

- It is really simple. First split each pdf into multiple files, one page = one file.
  - Then, write a program that detects pages with tables, graphs etc and move them into a separate folder. When you move, assign a meaningful filename.
  - Then chunk all other pages. Only meaningful chunking is hierarchical.while chunking, build a graph or a tree view, and visualise this. This verifies that you have preserved the hierarchical integrity of the document.
  - Now, using an LLM, process each visual page to provide summary. Insert these chunks in the tree view at their correct location. The image page should be available as a link in metadata.
  - Now design a context template. It should have document id, current chunk, prev chunk, subtitle summary, title and book title.
  - Embed the chunk with this template into a vectordb. Then also embed a sparse vector.
  - While querying use both vectors and rerank them.

- For complex documents (with tables, charts, and images), RAGFlow works surprisingly well ‚Äî it can intelligently recognize and preserve layouts like tables and embedded figures during parsing.
# discuss
- ## 

- ## 

- ## 

- ## 

- ## [Stop fine-tuning your model for every little thing. You're probably wasting your time. : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ov7ogq/stop_finetuning_your_model_for_every_little_thing/)
  - confession time. I just wasted three weeks and a chunk of my compute budget trying to fine-tune a model to answer questions about our internal API. The results were... mediocre(Âπ≥Â∫∏ÁöÑÔºõÊôÆÈÄöÁöÑÔºõ‰∏çÂ§üÂ•ΩÁöÑ) at best. It kinda knew the stuff, but it also started hallucinating in new and creative ways, and forgot how to do basic things it was good at before.
  - I feel like "fine-tuning" has become this default magic wand people wave when an LLM isn't perfect. But 80% of the time, what you actually need is RAG
  - RAG is like giving your AI a cheat sheet. You've got a mountain of internal docs, PDFs, or knowledge that the model wasn't trained on? Don't shove it down the model's throat and hope it digests it. Just keep it in a database (a "vector store, " if we're being fancy) and teach the AI to look things up before it answers. 
  - It's faster, cheaper, and the AI can't "forget" or misremember the source material. Fine-tuning is for changing the AI's personality or teaching it a new skill. This is when you need the model to fundamentally write or reason differently. 
  - So, the dumb-simple rule I go by now: ¬∑ Problem:- "The AI doesn't know about X." -> Use RAG. "The AI doesn't act or sound the way I want." -> Consider Fine-Tuning.

- RAG is for knowledge
  - Finetuning is best for style
  - Don't mix up the two

- I thought you can't SFT modern reasoning LLMs anymore? all this does is fuck up their complex post train since GRPO/RLHF happens after SFT and you can't replicate that with a simple pipe.
  - I think it depends on the model? Like, Qwen3 models are pretty good for SFT training.

- ## [After Building Multiple Production RAGs, I Realized ‚Äî No One Really Wants "Just a RAG" : r/Rag _202511](https://www.reddit.com/r/Rag/comments/1oly866/after_building_multiple_production_rags_i/)
- I get asked to RAGify :) company documents, but just RAG isn't enough. It needs to be able to analyze code without a single click, able to produce similar documents similar to provided document, able to chat with documents, and needs to completely run offline, no data outside. More requirements keep coming as the week go on.

- thanks mr chatgpt

- ## üÜö [I tested local models on 100+ real RAG tasks. Here are the best 1B model picks : r/Rag _202510](https://www.reddit.com/r/Rag/comments/1o60ib6/i_tested_local_models_on_100_real_rag_tasks_here/)
- Best model by real-life file QA tasks (Tested on 16GB Macbook Air M2)
  - I‚Äôm building this local file agent for RAG - Hyperlink. The idea of this test is to really understand how models perform in privacy-concerned real-life tasks
- tests result
  - A ‚Äî Find facts + cite sources ‚Üí Qwen3‚Äì1.7B-MLX-8bit
  - B ‚Äî Compare evidence across files ‚Üí LMF2‚Äì1.2B-MLX
  - C ‚Äî Build timelines ‚Üí LMF2‚Äì1.2B-MLX
  - D ‚Äî Summarize documents ‚Üí Qwen3‚Äì1.7B-MLX-8bit & LMF2‚Äì1.2B-MLX
  - E ‚Äî Organize themed collections ‚Üí stronger models needed
- Tasks Types (High Frequency, Low NPS file RAG scenarios)
  - Find facts + cite sources ‚Äî 10 PDFs consisting of project management documents
  - Compare evidence across documents ‚Äî 12 PDFs of contract and pricing review documents
  - Build timelines ‚Äî 13 deposition transcripts in PDF format
  - Summarize documents ‚Äî 13 deposition transcripts in PDF format.
  - Organize themed collections ‚Äî 1158 MD files of an Obsidian note-taking user.

- ## [The ‚Äúnew RAG every week‚Äù problem ‚Äî do we really need to switch frameworks this often? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o764lj/the_new_rag_every_week_problem_do_we_really_need/)
  - It feels like every couple of weeks, there‚Äôs a brand-new ‚Äúnext-generation‚Äù RAG framework popping up ‚Äî R2R, LightRAG, RAGFlow, and now a few others that just launched on GitHub.
  - But when you look closer, most of them aren‚Äôt new paradigms at all.
  - They‚Äôre slight variations of existing pipelines ‚Äî same retrieval backbone, same chunking strategies ‚Äî maybe with a bit of optimization in one step (say, hybrid retriever logic, graph-based context linking, or compression on long contexts).

- AI is a rapidly growing tech space. There will be 10 technologies that mildly differ each other and then there will be that 1 that will make some kind of a breakthrough. That's how it is. It's far too early for any real standards yet, and honestly that's the exciting part of following the AI space.

- ## üÜö [Tested 9 RAG query transformation techniques ‚Äì HydE is absurdly underrated : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o6s89n/tested_9_rag_query_transformation_techniques_hyde/)
  - Your RAG system isn't bad. Your queries are.
- I just tested 9 query transformation techniques. Here's what actually moved the needle
- Top 3:
  - HydE ‚Äì Generate a hypothetical answer, search for docs similar to that. Sounds dumb, works incredibly well. Solves the semantic gap problem.
  - RAG-Fusion ‚Äì Multi-query + reranking. Simple, effective, production-ready.
  - Step-Back ‚Äì Ask abstract questions first. "What is photosynthesis?" before "How do C4 plants fix carbon?"
- Meh tier:
  - Multi-Query: Good baseline, nothing special
  - Decomposition: Works but adds complexity
  - Recursive: Slow, minimal quality gain for simple queries
- Key insight: You're spending time optimizing embeddings when your query formulation is the actual bottleneck.

- Many larger RAG platforms I've worked on or seen in use are making embeddings from the text to be retrieved and also generating a couple of questions that can be answered by the same chunk, saving those embeddings as well (so you have several embeddings pointing at the same chunk).
  - This performs a lot like HydE, but shifting the extra compute (generation step) to the ingestion stages instead of query time for better latency/performance in exchange for a larger index to store and query, which is usually the desired tradeoff for interactive systems.

- ## ü§î [Is RAG system actually slow because of tool calling protocol? : r/Rag _202509](https://www.reddit.com/r/Rag/comments/1ntc1ky/is_rag_system_actually_slow_because_of_tool/)
  - Just came across few wild comparison between MCP and UTCP protocols and honestly... my mind is blown.
  - For RAG systems where every millisecond counts when we are retrieving documents. UTCP is 30-40% faster performance than MCP. that's HUGE.

- Why would you use mcp in RAG to begin with?
  - The consumer of a RAG might use tools, but ‚Äòthe‚Äô RAG components do not need tools.

- Find it funny everyone so focused on speed, but go ask gpt and wait 15min is fine

- ## üÜö [Does anyone know how much of a performance difference between knowledge graphs and vector based searches? : r/LangChain _202408](https://www.reddit.com/r/LangChain/comments/1eragqk/does_anyone_know_how_much_of_a_performance/)
  - I made a pretty simple vector based RAG search, and it performs "okay" and doesn't always generate the expected results. I have the pieces for a knowledge graph, but I was wondering if people knew the expected improvements that I should expect to see by moving to knowledge graphs?

- Vector search alone is usually faster. Without any bells and whistles like metadata filtering or stepback, it's just an optimized KNN. 
  - Graph based RAG involves traversing a graph in addition to searching for the appropriate starting node, which often times is a done via vector search. 
  - The graph structure usually leads to good results though.

- ## LobeChat v1.85.0 ‰ºòÂåñ‰∫ÜÂØπËØù‰∏ä‰º†Êñá‰ª∂ÁöÑÈÄªËæëÔºåÊääÂ§ßÂÆ∂ÂëºÂ£∞ÊØîËæÉÈ´òÁöÑÂÖ®Êñá‰∏ä‰º†ÁöÑËÉΩÂäõÂÅöÂ•Ω‰∫ÜÔºå‰∏çÂÜçËµ∞ ÂàÜÂùó + embedding ÁöÑÊú¥Á¥† RAG „ÄÇÂ∏¶Êù•ÁöÑÂ•ΩÂ§ÑÊòØÈÄüÂ∫¶ÂèòÂø´‰∫ÜÔºåÊïàÊûú‰πüÊõ¥Âä†ÁêÜÊÉ≥‰∫Ü„ÄÇ
- https://x.com/arvin17x/status/1921101310004179082
  - ÊÑüËßâÂ§ßÊ®°ÂûãÂÜçËø≠‰ª£‰∏™‰∏ÄÂπ¥ÔºåÊú¥Á¥† RAG ÂèØËÉΩÂ∞±Ë¶ÅÈÄÄÂá∫ÂéÜÂè≤ËàûÂè∞‰∫Ü
  - ÊØîÂ¶Ç‰∏Ä‰∏™Êñá‰ª∂ 10w Â≠óÁöÑÂÜÖÂÆπÔºåÂÖ®Êñá‰∏ä‰º†Â∞±ÊòØ10wÂ≠óÂÖ®Êîæ‰∏ä‰∏ãÊñáÈáå„ÄÇ RAG ÊòØÊâæÂá∫ÊúÄÁõ∏ÂÖ≥ÁöÑ N ‰∏™ÂÜÖÂÆπÁâáÊÆµÔºàÂÅáËÆæ‰∏Ä‰∏™ÁâáÊÆµ1kÔºåÂä†Ëµ∑Êù• N k‰∏™Â≠óÔºâÊîæ‰∏ä‰∏ãÊñáÈáå„ÄÇ ÂÖ®Êñá‰∏ä‰º†ÁöÑÊïàÊûúÂú®Â§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ã‰∏ÄÂÆöÊòØÊØî RAG Ë¶ÅÂ•ΩÁöÑ„ÄÇ
  - Áé∞Âú®ÈªòËÆ§‰∏ä‰º†Â∞±ÊòØÂÖ®Êñá„ÄÇ‰∏çËøáÁü•ËØÜÂ∫ìËØªÂèñËøòÊòØÂéüÊù•ÁöÑ
- ‰∏çÁî® RAGÔºåÁõ¥Êé•‰º†Áªô LLMÔºåÈí±ÂåÖÈ°∂‰∏ç‰ΩèÂïä
- ÂÖ®Êñá‰∏ä‰º†ÔºåÂè™ÊúâGoogleÂÆ∂ÊâçÊúâÂæàÂ§ßÁöÑ‰∏ä‰∏ãÊñáÂêßÔºüÂÖ∂‰ªñÂÆ∂ÊïàÊûúÂ∞±‰∏çÂ•Ω‰∫Ü
  - ‰∏çÂïäÔºåÁé∞Âú®gpt4.1miniÈÉΩÊúâ1M‰∏ä‰∏ãÊñá‰∫ÜÔºå‰πü‰æøÂÆú„ÄÇÊµã‰∏ãÊù•ÊïàÊûúÈùûÂ∏∏Â•Ω

- ## ÂºÄÊ∫êÁöÑ llm Á≥ªÁªüÈáåÔºåÂ∏¶Áü•ËØÜÂ∫ìÂäüËÉΩÁöÑÔºåÁõÆÂâçËøòÊ≤°ÊâæÂà∞‰∏Ä‰∏™ÂÅöÁöÑÂ•ΩÁöÑ„ÄÇ
- https://x.com/wwwgoubuli/status/1830227047173751206
  - ‰∏çËøáËøô‰πàËØ¥ÂÄí‰πü‰∏çÊòØË¶ÅË¥¨‰Ωé‰ªÄ‰πàÔºåÊàëËá™Â∑±Â∏ÆÂÆ¢Êà∑ÂÆöÂà∂ËøáÁöÑÁü•ËØÜÂ∫ì‰∏≠ÔºåÂÅöÁöÑÁ®çÂæÆÂ•Ω‰∏ÄÁÇπÁöÑÔºå‰πüÊäïÂÖ•‰∫ÜÂ§ßÈáèÁöÑÁ≤æÂäõÂíåÊàêÊú¨Êù•ÂÆûÁé∞Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÔºåÂçïÂçï‰∏Ä‰∏™ÊñáÊú¨ÂàÜÊÆµÈÉΩÊâæ‰∏çÂà∞ÈÄöÁî®ËåÉÂºèÔºåÂæóÂ§ßÈáèÂÆöÂà∂„ÄÇ

- ÈóÆÈ¢òÂ∞±ÊòØ to b Èáå„ÄÇ to c ÈÇ£‰∏Ä‰∏™Áî®Êà∑‰∏¢‰∏™ pdf ÁÆó‰∏ç‰∏ä‰ªÄ‰πàÁü•ËØÜÂ∫ì„ÄÇ Âà∞‰∫Ü to b  È¢ÜÂüüÔºå‰Ω†ÁúãÁùÄ‰∏ÄÂ†Ü excel ÔºåpdfÔºå ‰Ω†ÁöÑÊ¢¶È≠áÊâçÂàöÂºÄÂßã

- Êàë‰ª¨Ëá™Â∑±ÂÅöÂîÆÂêéÂÆ¢ÊúçÊú∫Âô®‰∫∫ÔºåËµ∑Ê≠•ÊòØÊâæÂÆû‰π†ÁîüÊí∏‰∫Ü1‰∏áÊù°ÈóÆÁ≠îÂØπÔºå‰∏äÁ∫øÂêéÊØèÂë®Ê†πÊçÆbad caseËø≠‰ª£ÁªßÁª≠ÂÅöÊï∞ÊçÆ„ÄÇËøôÁé©ÊÑèÊïàÊûúÂ∞±ÊòØÁúãÊ†áÊ≥®Êï∞ÊçÆÔºåÊÉ≥Ë¶ÅÂ•ΩÁöÑÊïàÊûúÔºåÂ∞±Â•ΩÂ•ΩÂÅöÊï∞ÊçÆ„ÄÇ

- Áü•ËØÜÂ∫ìÂ¶ÇÊûúÂÉèÁºñÁ®ãËØ≠Ê≥ïÊâãÂÜå‰∏ÄÊ†∑ÔºåÈÇ£ragÊïàÊûú‰ºöÈùûÂ∏∏Â•ΩÔºåÂ¶ÇÊûúÂÉèÊï£ÊñáËØóÔºåragÊïàÊûúÂ∞±‰ºöÂæàÂ∑Æ„ÄÇÊàë‰ª¨ÊãøÂà∞ÊâãÁöÑÊñáÊ°£‰∏ÄËà¨‰ªã‰∫é‰∏§ËÄÖ‰πãÈó¥ÔºåÊâÄ‰ª•ÊÉ≥ÊèêÈ´òragÊïàÊûúÔºåÂè™Â•ΩÊääÊñáÊ°£Êú¨Ë∫´ÂæÄÁºñÁ®ãËØ≠Ê≥ïÊâãÂÜåÊñπÂêëÊîπ

- ÊàëÁé∞Âú®ÊúÄÂ§¥ÁñºÁöÑÊòØpdfÊñáÊ°£ÔºåÈáåÈù¢ÂÖ®ÊòØÂõæÁâáÁöÑÈÇ£Áßç„ÄÇÔºå
  - ËÆ©Áî≤ÊñπÂä†Èí±ÔºåÂõæÁâáÂà∞ÊñáÂ≠óÁöÑËΩ¨Êç¢‰∏ç‰ªÖÈúÄË¶ÅÂ∑•ÂÖ∑ÔºåËøòÈúÄË¶Å‰∫∫Â∑•ÂÆ°ÈòÖÔºå‰∏çÂä†Èí±Ê≤°Ê≥ïÂπ≤
