---
title: lib-ai-app-community-rag
tags: [ai, community, rag]
created: 2024-09-08T20:08:04.573Z
modified: 2024-09-08T20:08:16.088Z
---

# lib-ai-app-community-rag

# guide

- pros-rag
  - solutions rich
  - save cost

- cons
  - extra infra like vectordb/retrive-api

- usecases-rag
  - long-docs/pdf
  - chat-history
  - ragçš„å®žçŽ°è¿˜å¯ä»¥å‚è€ƒai-codingä¸­çš„å®žè·µå’Œæ¨¡å¼
  - é’ˆå¯¹è¡¨æ ¼excel/è„‘å›¾çš„rag
  - â³ å¤šç‰ˆæœ¬æ–‡ä»¶çš„rag

- leaderboard-rag
  - [MTEB Leaderboard - a Hugging Face Space by mteb](https://huggingface.co/spaces/mteb/leaderboard)
    - https://github.com/embeddings-benchmark/mteb
    - top202509: gemini-embedding-001, embeddinggemma-300m, Qwen3-Embedding-8B/4B/0.6B, linq-mistral, jina, granite, nomic
    - ðŸ‘·å®žæµ‹, embeddinggemmaåœ¨Ollamaç»“æžœå·®, åœ¨LM Studioç»“æžœè¿˜è¡Œ ~~åœ¨ similaritySearch æ—¶å¾ˆå·®~~
      - è·ŸæŽ¨èqwen/granite
- resources
  - [RAG+AIå·¥ä½œæµ+Agentï¼šLLMæ¡†æž¶è¯¥å¦‚ä½•é€‰æ‹©ï¼Œå…¨é¢å¯¹æ¯”MaxKBã€Difyã€FastGPTã€RagFlowã€Anything-LLM, ä»¥åŠæ›´å¤šæŽ¨è - çŸ¥ä¹Ž _202407](https://zhuanlan.zhihu.com/p/711761781)
# discuss-stars
- ## 

- ## 

- ## 

- ## 

- ## [Using LMStudio for RAG/File Search with LibreChat? Â· danny-avila/LibreChat _202506](https://github.com/danny-avila/LibreChat/discussions/7713)
  - I run LibreChat locally inside Docker, I run local models in LMStudio, and want to use LMStudio's nomic (or otherwise) embedding capabilities instead of an external provider like OpenAI.
  - (A) i want to do it for free/private locally, and B ) especially because when I tested file upload directly within LMStudio [using its RAG] it correctly pulled all text from the PDF, but when I used OpenAI cloud embedding it missed a huge chunk of the textâ€”perhaps because the API version uses a text-only embedding model without vision
- UPDATE / SOLUTION:
  - Making a copy of the config.py file in my LibreChat root folder and pointing the compose-override to it did solve the problem and allowed me to use the openai embeddings with LMStudio. I seconded the feature request to make this an easy option for people to opt into

- been down that rabbit hole too, and yep: this kind of embedding mismatch + RAG retrieval chaos is super common when things try to speak in different tensor dialects.
  - the error from LMStudio (input field must be a string or array of strings) usually hits when the upstream retrieval returns a raw int list or malformed embedding body. 
  - i eventually got tired of patching these one by one and built a semantic rescue engine that stabilizes the entire pipeline.

- ## ðŸ¤” [RAGï¼ˆæ£€ç´¢å¢žå¼ºç”Ÿæˆï¼‰ä¼šä¸ä¼šæ¶ˆäº¡å‘¢ï¼Ÿä¸€æ—¦å¤§æ¨¡åž‹çš„Context Lengthå˜å¤§ï¼ŒRAGè¿˜æœ‰å­˜æ´»çš„å¿…è¦å—ï¼Ÿ - çŸ¥ä¹Ž](https://www.zhihu.com/question/637421964)
- RAG çš„åŸºæœ¬åŽŸç†å¹¶ä¸å¤æ‚ï¼š
  1.  å°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆå¤šä¸ª **chunk**ï¼›  
  2.  é€šè¿‡å‘é‡åŒ–æ¨¡åž‹ï¼ˆå¦‚ [OpenAI Embeddings] ã€Sentence-Transformerï¼‰ä¸ºæ¯ä¸ª chunk ç”Ÿæˆå‘é‡ï¼Œå¹¶å­˜å…¥æ•°æ®åº“ï¼›  
  3.  åœ¨ç”¨æˆ·æé—®æ—¶ï¼Œæ£€ç´¢å‡º **Top-K ç›¸ä¼¼æ–‡æ¡£**ï¼›  
  4.  æŠŠè¿™äº›æ–‡æ¡£ä¸Žé—®é¢˜ä¸€èµ·è¾“å…¥å¤§æ¨¡åž‹ï¼Œä½œä¸ºé¢å¤–ä¸Šä¸‹æ–‡ï¼›  
  5.  å¤§æ¨¡åž‹åŸºäºŽé—®é¢˜ + æ£€ç´¢ç»“æžœï¼Œç”Ÿæˆæ›´å‡†ç¡®çš„å›žç­”ã€‚
- ç®€å•æ¥è¯´ï¼ŒRAG å°±åƒæ˜¯ç»™å¤§æ¨¡åž‹è£…ä¸Šäº†ä¸€ä¸ªâ€œå¤–æŒ‚æœç´¢å¼•æ“Žâ€ã€‚
  - ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¼ ç»Ÿçš„ å¾®è°ƒï¼ˆFine-tuningï¼‰ æ–¹å¼è™½ç„¶èƒ½æŠŠæ–°çŸ¥è¯†å†™è¿›æ¨¡åž‹ï¼Œä½†å¾€å¾€æˆæœ¬é«˜æ˜‚ã€ä¸å¯é€†ï¼Œè¿˜å¯èƒ½â€œç‰ºç‰²â€åŽŸæœ‰èƒ½åŠ›ã€‚è€Œ RAG çš„ä¼˜åŠ¿åœ¨äºŽï¼šè½»é‡ã€çµæ´»ã€éšå–éšç”¨
- ç›®å‰å¤§å¤šæ•° RAG ç³»ç»Ÿå…¶å®žæ˜¯â€œFrozen RAGâ€â€”â€”Retriever ä¸Ž LLM æ˜¯åˆ†å¼€çš„ï¼ŒRetriever å›ºå®šä¸å˜ï¼ŒLLM ä¹Ÿä¸æ›´æ–°ã€‚å®ƒä»¬æ‹¼åœ¨ä¸€èµ·èƒ½è·‘ï¼Œä½†æ¨¡å—ä¹‹é—´å¹¶æ²¡æœ‰æ·±åº¦ååŒã€‚
- RAG 2.0 çš„æ ¸å¿ƒç›®æ ‡å°±æ˜¯ï¼š è®©æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨æˆä¸ºä¸€ä¸ªâ€œå¯å…±åŒè®­ç»ƒâ€çš„æ•´ä½“ï¼Œä»Žè€Œè¾¾åˆ°ç«¯åˆ°ç«¯çš„ä¼˜åŒ–ã€‚

- ä»Žçº¯ä¸šåŠ¡ç«¯çš„è§’åº¦æ¥è¯´ï¼Œä¸ä½†ä¸ä¼šæ¶ˆäº¡ï¼Œè¿˜æœ‰å¯èƒ½åœ¨æœªæ¥çš„3-5å¹´é‡Œå¤§è¡Œå…¶é“ã€‚
  - è¡¨é¢åŽŸå› éžå¸¸ç®€å•ï¼šRAGå‡ ä¹Žæ˜¯ç›®å‰å”¯ä¸€ä¸€ç§å…¼å…·é—¨æ§›ä½Ž+å¯è§‚å¯Ÿ+æŽ§åˆ¶å‡†+è§æ•ˆå¿«çš„è·¯çº¿ã€‚
  - æœ€åŽæä¸€å˜´æ·±å±‚åŽŸå› ï¼šRAGçš„æŠ€æœ¯è·¯çº¿æžå…¶ç¬¦åˆå›½æƒ…ï¼Œå…¶æœ¬è´¨æ˜¯çœŸç†çŒé¡¶ä¸”å¯ä»¥éšæ—¶æ›¿æ¢ï¼Œæ¯”å¤§ç‚¼æ¨¡åž‹å¾—åˆ°çš„é†‰æ‹³å¹»è§‰å¯æŽ§å¾—å¤šã€‚
  - To Bå’ŒTo Géƒ½æ˜¯æ•°æ®å¯†é›†åž‹å¯¹è±¡ï¼Œè‡ªèº«æ•°æ®å°±æ˜¯æœ€å¤§çš„ç­¹ç ï¼Œä¸è¦æŒ‡æœ›äººå®¶ä¼šæŠŠèº«å®¶æ€§å‘½äº¤åˆ°AIåŽ‚å•†æ‰‹é‡Œï¼ŒåŽ»æ‰“é€ ä¸€ä¸ªé¥é¥é¢†å…ˆçš„å¤§æ¨¡åž‹ã€‚
- å¤§æ¨¡åž‹ç›®å‰æ²¡çœ‹åˆ°ä¼ä¸šå†…å¥½çš„è½åœ°åœºæ™¯ã€‚ragå¤§éƒ¨åˆ†æ˜¯ç”¨äºŽçŸ¥è¯†åº“ï¼Œä½†æ˜¯ä¹Ÿå°±åŸºæœ¬èƒ½ç”¨ï¼Œå¦‚æžœæé—®ä¸èƒ½ä»Žå…³é”®è¯å’Œå‘é‡ä¸ŠåŒ¹é…ï¼Œé‚£ä½ ç»™å¤§æ¨¡åž‹çš„å†…å®¹å°±å’Œé—®é¢˜æ— å…³ï¼Œè‡ªç„¶ç­”æ¡ˆå°±ä¸å¯¹ã€‚å¹¶ä¸”æ–‡æ¡£æ‹†åˆ†åŽï¼Œé€»è¾‘ä¸Šå°±è¢«æ‹†åˆ†äº†ï¼Œå¾ˆå¤šé—®é¢˜æ˜¯è¦ä»Žå„ä¸ªæ–‡æ¡£é€»è¾‘ä¸Šåˆ†æžï¼Œä¸æ˜¯ç®€å•çš„æ£€ç´¢ã€‚

- å›žé¡¾ä¸‹å½“å‰å¸‚åœºä¸Šä¼ä¸š RAG çŸ¥è¯†åº“è½åœ°çš„ä¸‰ç§ä¸»æµè·¯å¾„ã€‚
- ç›´æŽ¥ä½¿ç”¨é«˜å±‚çº§å¼€æºæ¡†æž¶
  - è¿™ç±»æ¡†æž¶å¦‚ RAGFlow, Dify, FastGPT ç­‰ï¼Œä¸»è¦ç‰¹ç‚¹æ˜¯æä¾›ç›¸å¯¹å®Œæ•´ã€å¼€ç®±å³ç”¨çš„ RAG å·¥ä½œæµï¼Œ
  - ç›®æ ‡æ˜¯ç®€åŒ– RAG åº”ç”¨çš„æ­å»ºè¿‡ç¨‹ï¼Œé™ä½Žå¼€å‘é—¨æ§›ã€‚
  - åä¹‹ï¼ŒåŠ£åŠ¿ä¸»è¦æ˜¯å®šåˆ¶åŒ–å’Œçµæ´»æ€§å—é™ï¼Œæ·±åº¦ä¼˜åŒ–å’Œé›†æˆç‰¹å®šç»„ä»¶æ—¶å¤æ‚åº¦è¾ƒé«˜ã€‚
- åº•å±‚å¼€å‘æ¡†æž¶è‡ªä¸»å¼€å‘
  - è¿™ç±»æ¡†æž¶åŒ…æ‹¬ LangChain, LlamaIndex, Haystack ç­‰ï¼Œéƒ½æä¾›äº†ä¸€ç³»åˆ—æ¨¡å—åŒ–çš„æž„å»ºå—ã€å·¥å…·å’ŒæŽ¥å£ã€‚å¼€å‘è€…å¯ä»¥æ ¹æ®éœ€æ±‚çµæ´»ç»„åˆå’Œç¼–æŽ’ RAG æµç¨‹çš„å„ä¸ªçŽ¯èŠ‚ï¼ˆå¦‚æ•°æ®åŠ è½½ã€æ–‡æœ¬åˆ†å‰²ã€åµŒå…¥ã€å‘é‡å­˜å‚¨ã€æ£€ç´¢ç­–ç•¥ã€LLM è°ƒç”¨ã€è®°å¿†ç®¡ç†ã€Agent æž„å»ºç­‰ï¼‰ã€‚
  - ä¼˜åŠ¿å¾ˆæ˜Žæ˜¾å°±æ˜¯çµåº¦çµæ´»ï¼Œå®šåˆ¶åŒ–èƒ½åŠ›å¼ºï¼Œèƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šä¸šåŠ¡åœºæ™¯è¿›è¡Œæ·±åº¦ä¼˜åŒ–å’Œé›†æˆã€‚é€‚åˆå¯¹ RAG æµç¨‹æœ‰ç²¾ç»†åŒ–æŽ§åˆ¶éœ€æ±‚çš„ä¼ä¸šã€‚
  - åä¹‹åŠ£åŠ¿å°±æ˜¯éœ€è¦æ›´å¤šçš„å¼€å‘å·¥ä½œé‡å’ŒæŠ€æœ¯æ·±åº¦ã€‚
- äº‘åŽ‚å•† MaaS å¹³å°æ–¹æ¡ˆ
  - å¦‚é˜¿é‡Œäº‘ç™¾ç‚¼, ç™¾åº¦æ™ºèƒ½äº‘åƒå¸†, AWS Bedrock, Google Vertex AI Search ç­‰çš„ç§æœ‰åŒ–éƒ¨ç½²æ–¹æ¡ˆï¼Œæ— è®ºæ˜¯å›½å†…è¿˜æ˜¯å›½å¤–çš„è¿™äº›äº‘æœåŠ¡å•†ï¼Œéƒ½æ˜¯æŠŠ RAG ç›¸å…³çš„èƒ½åŠ›å°è£…æˆæœåŠ¡æˆ–æä¾›ç§æœ‰åŒ–éƒ¨ç½²åŒ…ï¼Œé€šå¸¸ä¸Žè‡ªå®¶çš„å¤§æ¨¡åž‹ã€è®¡ç®—èµ„æºã€æ•°æ®å­˜å‚¨ç­‰æ·±åº¦é›†æˆã€‚ä¸€èˆ¬ä¹Ÿä¼šæä¾›æ¨¡åž‹é€‰æ‹©ã€å¾®è°ƒã€éƒ¨ç½²ã€ç›‘æŽ§ç­‰ä¸€ç«™å¼æœåŠ¡ã€‚
  - é‡‡ç”¨è¿™ç§æ–¹æ¡ˆé€šå¸¸èƒ½æä¾›ç¨³å®šçš„åŸºç¡€è®¾æ–½ã€ä¾¿æ·çš„æ¨¡åž‹ç®¡ç†å’Œéƒ¨ç½²ã€ä»¥åŠä¸Žå…¶ä»–äº‘æœåŠ¡çš„è‰¯å¥½å…¼å®¹æ€§ã€‚å¯¹äºŽå·²ç»æ·±åº¦ä½¿ç”¨ç‰¹å®šäº‘ç”Ÿæ€çš„ä¼ä¸šæ¥è¯´ï¼Œé›†æˆæˆæœ¬è¾ƒä½Žã€‚
  - ä½†åŠ£åŠ¿å°±å¦‚åŒä½¿ç”¨å¤§æ¨¡åž‹ä¸€ä½“æœºä¸€æ ·ï¼Œå¯èƒ½å­˜åœ¨åŽ‚å•†é”å®šçš„é£Žé™©ï¼Œè·¨äº‘è¿ç§»æˆ–é›†æˆéžè¯¥åŽ‚å•†çš„æœåŠ¡å¯èƒ½ä¼šæ¯”è¾ƒå¤æ‚ã€‚
  - å½“ç„¶ï¼Œè´¹ç”¨å’Œçµæ´»æ€§ä¹Ÿéœ€è¦è€ƒè™‘ã€‚
- æ€»ç»“æ¥è¯´ï¼Œå®žé™…ç”Ÿäº§åœºæ™¯ä¸­ä»¥ä¸Šä¸‰ç§é€‰æ‹©å¹¶éžå®Œå…¨äº’æ–¥ï¼Œä¾‹å¦‚ä»¥åº•å±‚æ¡†æž¶ä¸ºåŸºç¡€ï¼Œä½†å€ŸåŠ©äº‘åŽ‚å•†çš„éƒ¨åˆ†æ¨¡åž‹æœåŠ¡æˆ–åŸºç¡€è®¾æ–½ä¹Ÿæ˜¯ä¸­ç›®å‰å¸¸è§çš„ç»„åˆã€‚

- RAGä¸»è¦å¯ä»¥è§£å†³å¤§æ¨¡åž‹å¹»è§‰é—®é¢˜ï¼Œä»¥åŠé€šè¿‡å¤–æŒ‚çŸ¥è¯†ï¼Œé¿å…æ¨¡åž‹é¢‘ç¹è¿›è¡ŒçŸ¥è¯†æ›´æ–°
  - å³ä½¿æ¨¡åž‹å¯æŽ¥å—ä¸Šä¸‹æ–‡é•¿åº¦ä¸º128kï¼Œä½†æ˜¯æˆ‘çš„çŸ¥è¯†åº“é‡Œæœ‰10wç¯‡æ–‡æ¡£ï¼Œè¿œè¿œè¶…è¿‡æ¨¡åž‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæ‰€ä»¥ä½ è¿˜æ˜¯éœ€è¦æ£€ç´¢ã€‚
  - å½“ç„¶æ¨¡åž‹ä¸Šä¸‹æ–‡é•¿åº¦å˜é•¿ï¼Œå¯¹äºŽæ£€ç´¢æ›´åŠ æœ‰å¥½ï¼Œç²’åº¦å˜å¤§ï¼Œå¬å›žå˜é«˜ï¼Œç›¸è¾…ç›¸æˆã€‚
  - å³ä½¿ä½ å¯ä»¥æŽ¥å—128kçš„é•¿åº¦ï¼Œä½†æ˜¯ä½ çš„è®¾å¤‡æ¶ˆè€—ä¸èµ·ï¼Œæ›´å¤§çš„é•¿åº¦éœ€è¦æ›´å¤šçš„æ˜¾å­˜ï¼Œæ¨¡åž‹éƒ¨ç½²æˆæœ¬æ›´é«˜ï¼Œé€Ÿåº¦æ›´æ…¢ã€‚
  - å¦‚æžœä½ è°ƒç”¨apiï¼Œé‚£tokenæ¶ˆè€—ä¹Ÿå¤ªå¤§äº†ï¼Œæˆæœ¬è¿‡é«˜ã€‚

- Context Length å˜å¤§ â‰  ä¸éœ€è¦ RAG
  - æˆæœ¬é—®é¢˜: ä¸Šä¸‹æ–‡è¶Šé•¿ï¼ŒæŽ¨ç†æˆæœ¬ï¼ˆæ˜¾å­˜ã€æ—¶é—´ã€ç®—åŠ›ï¼‰ä¼šçº¿æ€§ç”šè‡³è¶…çº¿æ€§å¢žåŠ ã€‚ æ¯”å¦‚ï¼Œ100k tokens å¤„ç†ä¸€æ¬¡å°±å¾ˆçƒ§é’±ï¼Œè€Œ RAG å¯ä»¥ç”¨è¾ƒçŸ­ä¸Šä¸‹æ–‡åå¤è°ƒç”¨ã€‚
  - ä¿¡æ¯æ£€ç´¢æ•ˆçŽ‡: å¦‚æžœä½ æœ‰ 1TB çš„çŸ¥è¯†åº“ï¼ŒæŠŠå®ƒå…¨å¡žè¿›ä¸Šä¸‹æ–‡æ˜¯ä¸å¯èƒ½çš„ã€‚ RAG å…ˆæ£€ç´¢ï¼Œå†é€è¿›ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥è®©æ¨¡åž‹ä¸“æ³¨äºŽç›¸å…³ä¿¡æ¯ã€‚
  - å™ªéŸ³ä¸Žå¹²æ‰°: å¤§ä¸Šä¸‹æ–‡å¹¶ä¸ä»£è¡¨æ¨¡åž‹ä¼šå¿½ç•¥æ— å…³ä¿¡æ¯ï¼Œç›¸åå¤ªå¤šæ— å…³å†…å®¹ä¼šç¨€é‡Šå…³é”®ä¿¡æ¯ï¼Œå¯¼è‡´æŽ¨ç†æ•ˆæžœä¸‹é™ã€‚RAG çš„â€œæ£€ç´¢+è¿‡æ»¤â€èƒ½æé«˜ä¿¡å™ªæ¯”ã€‚
- Context å˜å¤§åŽï¼ŒRAG çš„å½¢æ€ä¼šæ¼”åŒ–
  - ä»Žâ€œæŸ¥æ‰¾èµ„æ–™â€åˆ°â€œåŠ¨æ€çŸ¥è¯†ç»„ç»‡â€: æœªæ¥çš„ RAG å¯èƒ½ä¸åªæ˜¯æ£€ç´¢ï¼Œè€Œæ˜¯è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªä¸´æ—¶çš„çŸ¥è¯†æ‘˜è¦/çŸ¥è¯†å›¾è°±å†äº¤ç»™å¤§æ¨¡åž‹
  - å¤šé˜¶æ®µæŽ¨ç†ï¼ˆMulti-turn Reasoningï¼‰: å¤§ä¸Šä¸‹æ–‡å…è®¸ä¸€æ¬¡æ€§æ”¾æ›´å¤šä¿¡æ¯ï¼Œä½† RAG å¯ä»¥åœ¨æ¯ä¸€æ­¥æŽ¨ç†æ—¶æ›´æ–°ä¸Šä¸‹æ–‡ï¼Œå½¢æˆ äº¤äº’å¼æŽ¨ç†
  - Hybrid RAGï¼ˆæ··åˆæ¨¡å¼ï¼‰: æŠŠé™æ€çŸ¥è¯†ï¼ˆå¸¸ç”¨ä¿¡æ¯ï¼‰ç›´æŽ¥æ”¾åœ¨é•¿ä¸Šä¸‹æ–‡é‡Œï¼ŒæŠŠåŠ¨æ€æ•°æ®ï¼ˆå®žæ—¶ä¿¡æ¯ã€æœç´¢ç»“æžœï¼‰ç”¨ RAG æ–¹å¼æ’å…¥ã€‚
- ä¸ºä»€ä¹ˆ RAG åœ¨é•¿ä¸Šä¸‹æ–‡æ—¶ä»£ä¾ç„¶é‡è¦
  - å¯æ›´æ–°æ€§: æ¨¡åž‹è®­ç»ƒæ•°æ®æ˜¯é™æ€çš„ï¼ŒRAG å¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å¼•å…¥æœ€æ–°çŸ¥è¯†ã€‚
  - éšç§ä¸Žå®šåˆ¶åŒ–: ç§æœ‰ä¼ä¸šæˆ–ä¸ªäººæ•°æ®ä¸ä¼šç›´æŽ¥æ”¾åˆ°å¤§æ¨¡åž‹é‡Œï¼Œè€Œæ˜¯é€šè¿‡ RAG åŠ¨æ€æ³¨å…¥ã€‚
  - å‡å°‘å¹»è§‰: æ¨¡åž‹ç›´æŽ¥å¼•ç”¨æ£€ç´¢åˆ°çš„åŽŸæ–‡ï¼Œå¯ä»¥æ˜¾è‘—é™ä½Žèƒ¡ç¼–ä¹±é€ çš„æ¦‚çŽ‡ã€‚
  - å¤šæ¨¡æ€æ£€ç´¢: RAG æœªæ¥ä¼šä¸ä»…æ˜¯æ–‡æœ¬ï¼Œè¿˜ä¼šæ£€ç´¢å›¾ç‰‡ã€è§†é¢‘ã€éŸ³é¢‘ã€ç»“æž„åŒ–æ•°æ®ï¼Œå†ç»Ÿä¸€ç»™å¤§æ¨¡åž‹å¤„ç†
- æœªæ¥ RAG çš„é‡ç‚¹ä¼šä»Žâ€œæ‰¾æ–‡æ¡£â€å˜æˆâ€œæž„å»ºæœ€ä¼˜æŽ¨ç†ä¸Šä¸‹æ–‡â€ï¼Œå®ƒä¼šå˜å¾—æ›´æ™ºèƒ½ã€æ›´åŠ¨æ€ã€‚
  - éšç€ Context > 1M tokensï¼Œéƒ¨åˆ†åœºæ™¯å¯ç›´æŽ¥ç”¨å¤§æ¨¡åž‹çš„é•¿è®°å¿†ï¼Œä½† RAG ä¼šæ¼”å˜ä¸ºâ€œæ™ºèƒ½çŸ¥è¯†ç¼–æŽ’å™¨â€ã€‚
  - å¦‚æžœå‡ºçŽ°æ— é™ä¸Šä¸‹æ–‡ä¸”æŽ¨ç†æˆæœ¬æžä½Žçš„æ¨¡åž‹ï¼ŒRAG ä¼šé€€åŒ–æˆâ€œæ•°æ®ç´¢å¼•å·¥å…·â€ï¼Œä¸»è¦ç”¨äºŽåŠ é€Ÿæ•°æ®ç»„ç»‡è€Œä¸æ˜¯å¿…é¡»æ­¥éª¤ã€‚

- ä¸ä¼šã€‚ä¸Šä¸‹æ–‡é•¿åº¦å˜é•¿åªä¼šè®©RAGæ›´å¥½ç”¨ã€‚ä»Žç›®å‰çš„æƒ…å†µæ¥çœ‹ï¼ŒRAGçš„æˆæœ¬ä¾æ—§ä½ŽäºŽå¾®è°ƒï¼Œæ•ˆæžœé«˜äºŽå¾®è°ƒã€‚ç„¶åŽç›®å‰llmåº”ç”¨çš„æœ¬è´¨è¿˜æ˜¯promptï¼Œæ²¡æœ‰ä»€ä¹ˆå¤§èŠ±æ ·ï¼Œä¹Ÿè„±ç¦»ä¸äº†çŽ°æœ‰çš„äººç±»å­¦ç§‘ã€‚
  - å½“ç„¶ï¼Œå¦‚æžœæœ‰äººå¼ºè¡Œè¯´RAGå°±æ˜¯ç”¨åµŒå…¥å‘é‡ä»€ä¹ˆçš„ï¼Œé‚£ä¹ˆæˆ‘åªèƒ½è¯´å¯èƒ½ä¼šæ¶ˆäº¡ã€‚retrieval è‚¯å®šæ˜¯åŒ…æ‹¬äº†å…¶ä»–æ–¹å¼çš„ï¼Œæˆ‘ä»¬ç”¨geminiå‘æ–‡ä»¶æˆ–è€…å›¾ç‰‡ç»™ä»–ä½•å°ä¸æ˜¯ä¸€ç§RAGå‘¢ï¼Œä¸è¿‡åªæ˜¯äººè„‘é©±åŠ¨ç½¢äº†

- ## RAG is anti pattern.
- https://x.com/pelaseyed/status/1882471632129994914
  - I donâ€™t do RAG anymore, get 10x the result just spinning up a pipeline and feed all content to Deepseek. And yes it scales to over 10K docs. 

- And you can cache it, so doesnâ€™t even cost that much

- Does deepseek have caching like Googleâ€™s media.upload? How do you otherwise avoid rebuilding the context all the time (Iâ€™ve not used the DS APIs).

- I wish I could fit the entire Internet in the prompt for @ask_pandi but it doesn't fit. RAG is still needed for this.
  - Run multiple LLMs in parallell, you will have infinite context window.

- You put everything into context?
  - Everything in context(s)

- RAG is A pattern, just not useful when your context is under 128k token and when the context is known. Pick the right tool for the job.
# discuss-solutions
- ## 

- ## 

- ## 

- ## 

- ## [I've been noticing more and more people are using GraphRAG instead of embedding and vector databases...is it really helpful or just the hype? : r/LangChain _202507](https://www.reddit.com/r/LangChain/comments/1e66e9r/graphrag/)
- Been working with graphrag on my current project. It is good, but it has its fallbacks. The good thing is that you can model you data to preserve relations. 
- The bad thing is:
  - you rely on your LLM to create queries based on your question to the answer. Lots of times the query created is pure shit, even with the most advanced models.
  - It gets really expensive. Like really expensive. Imagine that for every document you're storing in your graphDB you need to pass it to a LLM so it can extract the content and create relations so you can store it on your DB.

- If you ask me, people just follow the trends without understanding whether their application really needs something (eg GraphRAG here). 

- This is all basically more of a hype I would say. Why? Because you can use your own database that handles multiple data types and construct a knowledge graph. The knowledge graph basically shows the entity relationship between different nodes and this can be easily fed into your databases as tables and then, using retrieval methods, you can easily retrieve the most relevant chunk and the relationship score. 
  - I mean, the specialise databases like vector databases or even graph databases are really hyped, your own databases whether it is MongoDB, SingleStore, MariaDB, or even Couchbase can easily help you with creating graph knowledge and store them and then retrieve whenever required.

- how GraphRAG works with existing graphs, meaning if I already have entities and relationships, how would I load this graph in GraphRAG? 

- ## æœ‰æ²¡æœ‰æƒ³è¿‡å°†è‡ªå·±çš„ä»£ç åº“æ‰“åŒ…ç„¶åŽç›´æŽ¥å¡žç»™å¤§è¯­è¨€æ¨¡åž‹æ¥å¤„ç†ï¼Ÿä¹Ÿè®¸ä½ ä¼šæƒ³åˆ°ç”¨RAGæˆ–è€…ç”¨windsurfæˆ–cursorã€‚çŽ°åœ¨æœ‰äº†æ›´ç®€å•çš„åŠžæ³•â€”â€”Repomix
- https://x.com/karminski3/status/1881150047276138689
  - è¿™ä¸ªåº“å¯å°†æ•´ä¸ªå­˜å‚¨åº“æ‰“åŒ…åˆ°ä¸€ä¸ª AI å‹å¥½çš„æ–‡ä»¶ä¸­ï¼Œæ–¹ä¾¿ç»™å¤§è¯­è¨€æ¨¡åž‹ä½¿ç”¨ã€‚

- ## ðŸ“Œ A list of software that allows searching the web with the assistance of AI.
- https://x.com/tom_doerr/status/1856778512612667838

# discuss-rag-local
- ## 

- ## 

- ## 

- ## ðŸ’¡ [Embedding With LM Studio - what am i doing wrong : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1lharbh/embedding_with_lm_studio_what_am_i_doing_wrong/)
  - "text-embedding-nomic-embed-text-v1.5" loads fine and works with Anything.
  - but text-embedding-qwen3-embedding-0.6b & 8B and any other Embed model i use i get the below error: Failed to load embedding model. Internal error: Failed to initialize the context: failed to allocate compute pp buffers

- SOLVED
  - LM Studio was defaulting to 4096 Tokens where as the model can only handle 1024. Same goes for AnythingLLM , that defaults to 8192 tokens. set everything to 1024 and now its working

- Your "SOLVED" update is wrong. I ended up on this post after also having memory allocation issues with Qwen3 embedding models. 
  - The "1024" value you are quoting is the number of output dimensions of the model that clients would use to interpret results. You don't set that anywhere in LM Studio.
  - all you did by lowering the context length to 1024 tokens is reduce memory usage enough to fix your issue. You also made the output quality worse. Your clients will have to make more requests for more chunks for indexing since the model only accepts 1024 tokens at a time. This will in turn increase the number of vector entries that need to be stored.

- I had to "Override Domain Type" to Text Embedding to get it to work my computer (Models -> settings gear -> bottom setting). Not sure what else to try if you've already done that.
  - yep, did forget to mention that i tried both configurations

- try using any model you have as an embedder. It works just fine.

- ## [What Embedding Models Are You Using For RAG? : r/LocalLLaMA _202312](https://www.reddit.com/r/LocalLLaMA/comments/18j39qt/what_embedding_models_are_you_using_for_rag/)
- For RAG, I have been using multilingual minilm model from HF hub (I have multiligual requirement), and qdrant vector database. Happy with both. Specially, minilm embeddings are 368 dims, so some space saving there as well.

- Chinese embedding models and llms are now the top performers. Qwen-1.5B Yi-34B gte-large bge-large

- I chose the most lightweight stack possibleâ€” using all-MiniLM-L6-v2 for embeddings and HyperDB for local storage/retrieval (this is super fast for <100k documents)

- I am currently using all-MiniLM-L6-v2 but responses are generally not that good my data size is almost 600mb and it is giving me not that good results, what kind of embedding should i choose considering the size of data and database of pgvector postgres

- What Embedding model is best for Text-To-SQL for Financial data? Currently I'm using OpenAIEmbeddings
  - Text-to-SQL is a sequence to sequence task. My very first thought was "zero shot learning" because I have done this successfully in other domains. 

- I use whatever models are on top 1-5 on the MTEB leaderboard and run my custom evaluation 
# discuss-rag-tips/tricks
- ## 

- ## 

- ## 

- ## [RAG over Database : r/LangChain _202407](https://www.reddit.com/r/LangChain/comments/1efnx5u/rag_over_database/)
  - I have been trying to build a RAG over a database that has mulitple tables. Often times, for a user query, the data has to be searched by joining multiple tables. I followed this approach as mentioned in Langchain documents.
  - What I am observing is that many times the query generated by LLM is not correct and the data that user wants is incorrect. 

- Try to make all the joins yourself in the form of view and provide a simple view for the LLM

- dont use joins. use views or make stored procs to call for summaries etc you want.

- I would use the traditional RAG-based approach more in cases when the knowledge base is made of documents instead of structured data.
# discuss-rag-db
- ## 

- ## 

- ## 

- ## 

- ## 

- ## 

- ## [Best Vector Database for RAG : r/vectordatabase _202501](https://www.reddit.com/r/vectordatabase/comments/1hzovpy/best_vector_database_for_rag/)
- All vector databases work well for RAG. The selection of a vector database usually depends on your preference: cloud based vs self-hosted, open source vs private, programming languages / clients, API access, already part of SQL etc.
- Some common ones are (not in any order, I'm not affiliated with any):
  - Pinecone: private, cloud based, very popular
  - pgvector: Postres vector search extension, useful if you have already data in Postgres.
  - Faiss: A library for efficient similarity search and clustering of dense vectors.
  - Qdrant: Open source vector search engine written in Rust.
  - Chroma: Yet another vector database
  - Milvus: An open-source similarity search engine for embedding vectors.
  - Weaviate: Open source vector database written in Go.

- Pgvector because it is part of Postgres. In the end Vectors are just a data type in a databases. If you only need vectoring and not SQL then it is better to look search engine that does that I think.
  - PGVector, is like PostGIS for geospatial ops, an extension for Postgre to support vector search, it is not a true vector-native DB, so it adds operational overhead and lacks advanced features that a vector DB provides. 

- Quadrant, it's a vector DB designed specifically for RAG/AI workflows. It offers advanced filtering features (multi-tenancy) that are natively baked-in into its core architecture. These features would require more legwork using other data stores like Redis etc...
- FAISS: great for vector search is not really a DB, it's a lib and not a true DB
# discuss-code-rag
- ## 

- ## [Cline doesn't index your codebase. No RAG, no embeddings, no vector databases. : r/CLine _202505](https://www.reddit.com/r/CLine/comments/1kwhcz0/cline_doesnt_index_your_codebase_no_rag_no/)
- Does Roo Code also do that or is this just a Cline thing?
  - Roo recently added it for code search. It works with local models
  - They also added prompt caching for Gemini flash.

- Exceedingly large context windows don't just result in extreme costs but it also will slow down your every operation.
  - While prompt caching reduce costs it is not entirely free, nor does every model or provider support it. It's also like treating a symptom rather than the disease.

- RAG can be good. See how fast and precise is AUgment Code search
  - ive found Augment to miss obvious things at times

- cursor has code indexing

- RAG shines when you donâ€™t know the exact function / class names in a repo. ripgrep file search that Cline uses for context search is awesomeâ€”if you already have the right keywords. Thatâ€™s easy on your own project, but on a massive, unfamiliar codebase ripgrep can stall while RAG keeps rolling (although it can return the wrong chunks).

- Privacy issue first. Never is sending low quality windows off the code base to the LLM is bad for quality.

- ## ðŸ¤¼ Cline doesn't index your codebase. No RAG, no embeddings, no vector databases. _202505
- https://x.com/cline/status/1927226680206131530
  - [Why Cline Doesn't Index Your Codebase (And Why That's a Good Thing) - Cline Blog _202505](https://cline.bot/blog/why-cline-doesnt-index-your-codebase-and-why-thats-a-good-thing)
  - This isn't a limitation -- it's a deliberate design choice. 
  - As context windows increase, this approach enhances Cline's ability to understand your code.
  - The industry default: chunk your codebase, create embeddings, store in vector databases, retrieve "relevant" pieces.
  - But code doesn't work in chunks. A function call in chunk 47, its definition in chunk 892, the context that explains why? Scattered everywhere.
  - We believe in the agentic power of the modesl, and with Claude's 200K+ context window, we don't need clever retrieval. We need intelligent exploration.
  - So Cline reads code the way you do -- following imports, tracing dependencies, building connected understanding.
  - ðŸ’¡ Instead of indexing, Cline starts with structure. Using ASTs, it maps your codebase architecture -- classes, functions, relationships.
  - Then it explores. Need to add error handling? It traces from your function to your error utilities to similar patterns. Connected comprehension.
- The problems with RAG for code run deeper:
  - Chunking breaks the logical connections between functions and their dependencies
  - Indexes become stale the moment you push a commit
  - Your IP gets duplicated in vector embeddings (security nightmare)

- ## [Unpopular opinion: RAG is actively hurting your coding agents : r/ChatGPTCoding _202505](https://www.reddit.com/r/ChatGPTCoding/comments/1ktt4ab/unpopular_opinion_rag_is_actively_hurting_your/)
- I've been saying this since RAG first became the term used to describe the method. And you are exactly right, the whole reason it became a thing was because, back when context windows were 4k or 8k max, it was out of necessity. Now, in the age where context windows are 1M or 10M tokens, it only makes sense in specific enterprise cases where you have vast datasets to query for specific, isolated information.

- RAG isnâ€™t just calling vector stores, itâ€™s also prompt priming before generation using various sources. Dynamically priming the prompt with relevant information before the LLM generates a response. 
  - A lot of the large context models drop off in accuracy after 100k tokens, anyway.

- frontier models have large context windows, but they all equate to a token, which equals a cost
  - Knowledge Graphs + RAG. An syntax tree is constructed of the code , where each class, function , method etc become nodes in the graph. The LLM can then traverse the graph to get only what it needs.

- One thing Aider has that Roo and Cline don't, is what they call a "RepoMap" (and it ties aider to git). 
  - But the advantage of it is, given a class, it can easily determine all the related classes, so it doesn't have to go digging through folders, trying to figure out which file is actually relevant, it knows, because the RepoMap shows which classes use which classes.
  - I pulled the RepoMap class out of aider and eventually managed to remove all the aider dependencies and got running as a command-line app. I might try making an MCP with it and giving that to Cline and see if it can improve its ability to understand the app structure.
- I thought cline already had a tree sitter which did the same thing.
  - it does use tree-sitter, but not in the same way and not to the same effect. 
  - How Aider understands you program vs how Cline understands it is very apparent in working with them. Aider has a much better understanding of which bits of code are related to which other bits, whereas Cline is frequently guessing based on the filenames.
  - Using tree-sitter isn't enough. You need to actually map out the relationships and Cline doesn't seem to do this, at least not nearly as effectively as Aider does.
- using the LSP if available surley is the best approach?
  - No, beyond getting LSP diagnostic, which you can get somewhere else anyway. Using LSP server requires counting rows and columns, and LLMs are bad at it.
- but you don't have to use the interface literally
  - Then I'm not using LSP, I'm using something that uses LSP under the hood to point that is an implementation detail. Take a look at your favorite LSP server, tell me what is useful there to LLM agent besides diagnostics?
  - I can see how you give LLM diagnostics after it update the file is useful, but giving LLM access to LSP overall is IMO utterly useless.

- I was watching an interview video with Claude Code engineers and they mentioned along the lines of not using RAG or GraphRAG because they found that Claude Code performed better without it and just relying on tools, memory and agentic search.

- RAG still makes sense for latency sensitive use cases like real-time chat, although prompt caching helps with this. 

- ## [Lessons learned from implementing RAG for code generation : r/LLMDevs _202501](https://www.reddit.com/r/LLMDevs/comments/1hw1n5o/lessons_learned_from_implementing_rag_for_code/)
  - ðŸ“ [A Recipe for a Better AI-based Code Generator | Pulumi Blog _202501](https://www.pulumi.com/blog/codegen-learnings/)
  - We wrote a blog post documenting how we do retrieval augmented generation (RAG) for code generation in our AI assistant, Pulumi Copilot. 
  - Measure and tune recall (how many relevant documents are retrieved out of all relevant documents) and precision (how many of the retrieved documents are relevant)
  - Implement end-to-end testing and monitoring across development and production
  - Create self-debugging capabilities to handle common issues like type checking errors

- when I need understanding a medium sized code base I have dumped the whole thing into gemini in Google AI workbench. It can do 2 million tokens, though it is SLOOW.

- I wonder if you've considered feeding more tokens from your retrieval result into your code gen step? Why 20k? Is that always enough? How would you even know if it weren't?
  - You've got part of the answer right there: SLOOW :-) Also expensive!
  - Seriously though, you want to optimize for both recall and precision. Too much irrelevant data (poor precision) can confuse the LLM leading to hallucinations.
  - 20k is based on "it feels right" and empirical data but honestly we have not done enough analysis to conclude that it is the perfect number for all scenarios - we will continue to measure and adjust.

- Did you experiment with other retrieval methods besides or in addition to semantic similarity? I've done some work using different techniques, like parsing dependency trees out of the current file, with promising results for code RAG.
  - We did look into BM25 for FT search but did not see measurable benefits for our use cases. Our approach relies on getting a lot of documents first and then pruning - it would be better to get just what's needed in the first place, I still hope BM25 can help there. Worth another look!
# discuss
- ## 

- ## 

- ## 

- ## ðŸ†š [Does anyone know how much of a performance difference between knowledge graphs and vector based searches? : r/LangChain _202408](https://www.reddit.com/r/LangChain/comments/1eragqk/does_anyone_know_how_much_of_a_performance/)
  - I made a pretty simple vector based RAG search, and it performs "okay" and doesn't always generate the expected results. I have the pieces for a knowledge graph, but I was wondering if people knew the expected improvements that I should expect to see by moving to knowledge graphs?

- Vector search alone is usually faster. Without any bells and whistles like metadata filtering or stepback, it's just an optimized KNN. 
  - Graph based RAG involves traversing a graph in addition to searching for the appropriate starting node, which often times is a done via vector search. 
  - The graph structure usually leads to good results though.

- ## LobeChat v1.85.0 ä¼˜åŒ–äº†å¯¹è¯ä¸Šä¼ æ–‡ä»¶çš„é€»è¾‘ï¼ŒæŠŠå¤§å®¶å‘¼å£°æ¯”è¾ƒé«˜çš„å…¨æ–‡ä¸Šä¼ çš„èƒ½åŠ›åšå¥½äº†ï¼Œä¸å†èµ° åˆ†å— + embedding çš„æœ´ç´  RAG ã€‚å¸¦æ¥çš„å¥½å¤„æ˜¯é€Ÿåº¦å˜å¿«äº†ï¼Œæ•ˆæžœä¹Ÿæ›´åŠ ç†æƒ³äº†ã€‚
- https://x.com/arvin17x/status/1921101310004179082
  - æ„Ÿè§‰å¤§æ¨¡åž‹å†è¿­ä»£ä¸ªä¸€å¹´ï¼Œæœ´ç´  RAG å¯èƒ½å°±è¦é€€å‡ºåŽ†å²èˆžå°äº†
  - æ¯”å¦‚ä¸€ä¸ªæ–‡ä»¶ 10w å­—çš„å†…å®¹ï¼Œå…¨æ–‡ä¸Šä¼ å°±æ˜¯10wå­—å…¨æ”¾ä¸Šä¸‹æ–‡é‡Œã€‚ RAG æ˜¯æ‰¾å‡ºæœ€ç›¸å…³çš„ N ä¸ªå†…å®¹ç‰‡æ®µï¼ˆå‡è®¾ä¸€ä¸ªç‰‡æ®µ1kï¼ŒåŠ èµ·æ¥ N kä¸ªå­—ï¼‰æ”¾ä¸Šä¸‹æ–‡é‡Œã€‚ å…¨æ–‡ä¸Šä¼ çš„æ•ˆæžœåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¸€å®šæ˜¯æ¯” RAG è¦å¥½çš„ã€‚
  - çŽ°åœ¨é»˜è®¤ä¸Šä¼ å°±æ˜¯å…¨æ–‡ã€‚ä¸è¿‡çŸ¥è¯†åº“è¯»å–è¿˜æ˜¯åŽŸæ¥çš„
- ä¸ç”¨ RAGï¼Œç›´æŽ¥ä¼ ç»™ LLMï¼Œé’±åŒ…é¡¶ä¸ä½å•Š
- å…¨æ–‡ä¸Šä¼ ï¼Œåªæœ‰Googleå®¶æ‰æœ‰å¾ˆå¤§çš„ä¸Šä¸‹æ–‡å§ï¼Ÿå…¶ä»–å®¶æ•ˆæžœå°±ä¸å¥½äº†
  - ä¸å•Šï¼ŒçŽ°åœ¨gpt4.1miniéƒ½æœ‰1Mä¸Šä¸‹æ–‡äº†ï¼Œä¹Ÿä¾¿å®œã€‚æµ‹ä¸‹æ¥æ•ˆæžœéžå¸¸å¥½

- ## å¼€æºçš„ llm ç³»ç»Ÿé‡Œï¼Œå¸¦çŸ¥è¯†åº“åŠŸèƒ½çš„ï¼Œç›®å‰è¿˜æ²¡æ‰¾åˆ°ä¸€ä¸ªåšçš„å¥½çš„ã€‚
- https://x.com/wwwgoubuli/status/1830227047173751206
  - ä¸è¿‡è¿™ä¹ˆè¯´å€’ä¹Ÿä¸æ˜¯è¦è´¬ä½Žä»€ä¹ˆï¼Œæˆ‘è‡ªå·±å¸®å®¢æˆ·å®šåˆ¶è¿‡çš„çŸ¥è¯†åº“ä¸­ï¼Œåšçš„ç¨å¾®å¥½ä¸€ç‚¹çš„ï¼Œä¹ŸæŠ•å…¥äº†å¤§é‡çš„ç²¾åŠ›å’Œæˆæœ¬æ¥å®žçŽ°æ•°æ®é¢„å¤„ç†ï¼Œå•å•ä¸€ä¸ªæ–‡æœ¬åˆ†æ®µéƒ½æ‰¾ä¸åˆ°é€šç”¨èŒƒå¼ï¼Œå¾—å¤§é‡å®šåˆ¶ã€‚

- é—®é¢˜å°±æ˜¯ to b é‡Œã€‚ to c é‚£ä¸€ä¸ªç”¨æˆ·ä¸¢ä¸ª pdf ç®—ä¸ä¸Šä»€ä¹ˆçŸ¥è¯†åº“ã€‚ åˆ°äº† to b  é¢†åŸŸï¼Œä½ çœ‹ç€ä¸€å † excel ï¼Œpdfï¼Œ ä½ çš„æ¢¦é­‡æ‰åˆšå¼€å§‹

- æˆ‘ä»¬è‡ªå·±åšå”®åŽå®¢æœæœºå™¨äººï¼Œèµ·æ­¥æ˜¯æ‰¾å®žä¹ ç”Ÿæ’¸äº†1ä¸‡æ¡é—®ç­”å¯¹ï¼Œä¸Šçº¿åŽæ¯å‘¨æ ¹æ®bad caseè¿­ä»£ç»§ç»­åšæ•°æ®ã€‚è¿™çŽ©æ„æ•ˆæžœå°±æ˜¯çœ‹æ ‡æ³¨æ•°æ®ï¼Œæƒ³è¦å¥½çš„æ•ˆæžœï¼Œå°±å¥½å¥½åšæ•°æ®ã€‚

- çŸ¥è¯†åº“å¦‚æžœåƒç¼–ç¨‹è¯­æ³•æ‰‹å†Œä¸€æ ·ï¼Œé‚£ragæ•ˆæžœä¼šéžå¸¸å¥½ï¼Œå¦‚æžœåƒæ•£æ–‡è¯—ï¼Œragæ•ˆæžœå°±ä¼šå¾ˆå·®ã€‚æˆ‘ä»¬æ‹¿åˆ°æ‰‹çš„æ–‡æ¡£ä¸€èˆ¬ä»‹äºŽä¸¤è€…ä¹‹é—´ï¼Œæ‰€ä»¥æƒ³æé«˜ragæ•ˆæžœï¼Œåªå¥½æŠŠæ–‡æ¡£æœ¬èº«å¾€ç¼–ç¨‹è¯­æ³•æ‰‹å†Œæ–¹å‘æ”¹

- æˆ‘çŽ°åœ¨æœ€å¤´ç–¼çš„æ˜¯pdfæ–‡æ¡£ï¼Œé‡Œé¢å…¨æ˜¯å›¾ç‰‡çš„é‚£ç§ã€‚ï¼Œ
  - è®©ç”²æ–¹åŠ é’±ï¼Œå›¾ç‰‡åˆ°æ–‡å­—çš„è½¬æ¢ä¸ä»…éœ€è¦å·¥å…·ï¼Œè¿˜éœ€è¦äººå·¥å®¡é˜…ï¼Œä¸åŠ é’±æ²¡æ³•å¹²
