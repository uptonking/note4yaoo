---
title: lib-ai-app-community-rag
tags: [ai, community, rag]
created: 2024-09-08T20:08:04.573Z
modified: 2024-09-08T20:08:16.088Z
---

# lib-ai-app-community-rag

# guide

- pros-rag
  - solutions rich
  - save cost

- cons
  - extra infra like vectordb/retrive-api

- usecases-rag
  - long-docs/pdf
  - chat-history
  - ragçš„å®ç°è¿˜å¯ä»¥å‚è€ƒai-codingä¸­çš„å®è·µå’Œæ¨¡å¼
  - é’ˆå¯¹è¡¨æ ¼excel/è„‘å›¾çš„rag
  - â³ å¤šç‰ˆæœ¬æ–‡ä»¶çš„rag

- leaderboard-rag
  - [MTEB Leaderboard - a Hugging Face Space by mteb](https://huggingface.co/spaces/mteb/leaderboard)
    - https://github.com/embeddings-benchmark/mteb
    - top202509: gemini-embedding-001, embeddinggemma-300m, Qwen3-Embedding-8B/4B/0.6B, linq-mistral, jina, granite, nomic
    - ğŸ‘·å®æµ‹, embeddinggemmaåœ¨Ollamaç»“æœå·®, åœ¨LM Studioç»“æœè¿˜è¡Œ ~~åœ¨ similaritySearch æ—¶å¾ˆå·®~~
      - è·Ÿæ¨èqwen/granite
- resources
  - [RAG+AIå·¥ä½œæµ+Agentï¼šLLMæ¡†æ¶è¯¥å¦‚ä½•é€‰æ‹©ï¼Œå…¨é¢å¯¹æ¯”MaxKBã€Difyã€FastGPTã€RagFlowã€Anything-LLM, ä»¥åŠæ›´å¤šæ¨è - çŸ¥ä¹ _202407](https://zhuanlan.zhihu.com/p/711761781)
# discuss-stars
- ## 

- ## 

- ## 

- ## 

- ## [Using LMStudio for RAG/File Search with LibreChat? Â· danny-avila/LibreChat _202506](https://github.com/danny-avila/LibreChat/discussions/7713)
  - I run LibreChat locally inside Docker, I run local models in LMStudio, and want to use LMStudio's nomic (or otherwise) embedding capabilities instead of an external provider like OpenAI.
  - (A) i want to do it for free/private locally, and B ) especially because when I tested file upload directly within LMStudio [using its RAG] it correctly pulled all text from the PDF, but when I used OpenAI cloud embedding it missed a huge chunk of the textâ€”perhaps because the API version uses a text-only embedding model without vision
- UPDATE / SOLUTION:
  - Making a copy of the config.py file in my LibreChat root folder and pointing the compose-override to it did solve the problem and allowed me to use the openai embeddings with LMStudio. I seconded the feature request to make this an easy option for people to opt into

- been down that rabbit hole too, and yep: this kind of embedding mismatch + RAG retrieval chaos is super common when things try to speak in different tensor dialects.
  - the error from LMStudio (input field must be a string or array of strings) usually hits when the upstream retrieval returns a raw int list or malformed embedding body. 
  - i eventually got tired of patching these one by one and built a semantic rescue engine that stabilizes the entire pipeline.

- ## ğŸ¤” [RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ä¼šä¸ä¼šæ¶ˆäº¡å‘¢ï¼Ÿä¸€æ—¦å¤§æ¨¡å‹çš„Context Lengthå˜å¤§ï¼ŒRAGè¿˜æœ‰å­˜æ´»çš„å¿…è¦å—ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/637421964)
- RAG çš„åŸºæœ¬åŸç†å¹¶ä¸å¤æ‚ï¼š
  1.  å°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆå¤šä¸ª **chunk**ï¼›  
  2.  é€šè¿‡å‘é‡åŒ–æ¨¡å‹ï¼ˆå¦‚ [OpenAI Embeddings] ã€Sentence-Transformerï¼‰ä¸ºæ¯ä¸ª chunk ç”Ÿæˆå‘é‡ï¼Œå¹¶å­˜å…¥æ•°æ®åº“ï¼›  
  3.  åœ¨ç”¨æˆ·æé—®æ—¶ï¼Œæ£€ç´¢å‡º **Top-K ç›¸ä¼¼æ–‡æ¡£**ï¼›  
  4.  æŠŠè¿™äº›æ–‡æ¡£ä¸é—®é¢˜ä¸€èµ·è¾“å…¥å¤§æ¨¡å‹ï¼Œä½œä¸ºé¢å¤–ä¸Šä¸‹æ–‡ï¼›  
  5.  å¤§æ¨¡å‹åŸºäºé—®é¢˜ + æ£€ç´¢ç»“æœï¼Œç”Ÿæˆæ›´å‡†ç¡®çš„å›ç­”ã€‚
- ç®€å•æ¥è¯´ï¼ŒRAG å°±åƒæ˜¯ç»™å¤§æ¨¡å‹è£…ä¸Šäº†ä¸€ä¸ªâ€œå¤–æŒ‚æœç´¢å¼•æ“â€ã€‚
  - ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¼ ç»Ÿçš„ å¾®è°ƒï¼ˆFine-tuningï¼‰ æ–¹å¼è™½ç„¶èƒ½æŠŠæ–°çŸ¥è¯†å†™è¿›æ¨¡å‹ï¼Œä½†å¾€å¾€æˆæœ¬é«˜æ˜‚ã€ä¸å¯é€†ï¼Œè¿˜å¯èƒ½â€œç‰ºç‰²â€åŸæœ‰èƒ½åŠ›ã€‚è€Œ RAG çš„ä¼˜åŠ¿åœ¨äºï¼šè½»é‡ã€çµæ´»ã€éšå–éšç”¨
- ç›®å‰å¤§å¤šæ•° RAG ç³»ç»Ÿå…¶å®æ˜¯â€œFrozen RAGâ€â€”â€”Retriever ä¸ LLM æ˜¯åˆ†å¼€çš„ï¼ŒRetriever å›ºå®šä¸å˜ï¼ŒLLM ä¹Ÿä¸æ›´æ–°ã€‚å®ƒä»¬æ‹¼åœ¨ä¸€èµ·èƒ½è·‘ï¼Œä½†æ¨¡å—ä¹‹é—´å¹¶æ²¡æœ‰æ·±åº¦ååŒã€‚
- RAG 2.0 çš„æ ¸å¿ƒç›®æ ‡å°±æ˜¯ï¼š è®©æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨æˆä¸ºä¸€ä¸ªâ€œå¯å…±åŒè®­ç»ƒâ€çš„æ•´ä½“ï¼Œä»è€Œè¾¾åˆ°ç«¯åˆ°ç«¯çš„ä¼˜åŒ–ã€‚

- ä»çº¯ä¸šåŠ¡ç«¯çš„è§’åº¦æ¥è¯´ï¼Œä¸ä½†ä¸ä¼šæ¶ˆäº¡ï¼Œè¿˜æœ‰å¯èƒ½åœ¨æœªæ¥çš„3-5å¹´é‡Œå¤§è¡Œå…¶é“ã€‚
  - è¡¨é¢åŸå› éå¸¸ç®€å•ï¼šRAGå‡ ä¹æ˜¯ç›®å‰å”¯ä¸€ä¸€ç§å…¼å…·é—¨æ§›ä½+å¯è§‚å¯Ÿ+æ§åˆ¶å‡†+è§æ•ˆå¿«çš„è·¯çº¿ã€‚
  - æœ€åæä¸€å˜´æ·±å±‚åŸå› ï¼šRAGçš„æŠ€æœ¯è·¯çº¿æå…¶ç¬¦åˆå›½æƒ…ï¼Œå…¶æœ¬è´¨æ˜¯çœŸç†çŒé¡¶ä¸”å¯ä»¥éšæ—¶æ›¿æ¢ï¼Œæ¯”å¤§ç‚¼æ¨¡å‹å¾—åˆ°çš„é†‰æ‹³å¹»è§‰å¯æ§å¾—å¤šã€‚
  - To Bå’ŒTo Géƒ½æ˜¯æ•°æ®å¯†é›†å‹å¯¹è±¡ï¼Œè‡ªèº«æ•°æ®å°±æ˜¯æœ€å¤§çš„ç­¹ç ï¼Œä¸è¦æŒ‡æœ›äººå®¶ä¼šæŠŠèº«å®¶æ€§å‘½äº¤åˆ°AIå‚å•†æ‰‹é‡Œï¼Œå»æ‰“é€ ä¸€ä¸ªé¥é¥é¢†å…ˆçš„å¤§æ¨¡å‹ã€‚
- å¤§æ¨¡å‹ç›®å‰æ²¡çœ‹åˆ°ä¼ä¸šå†…å¥½çš„è½åœ°åœºæ™¯ã€‚ragå¤§éƒ¨åˆ†æ˜¯ç”¨äºçŸ¥è¯†åº“ï¼Œä½†æ˜¯ä¹Ÿå°±åŸºæœ¬èƒ½ç”¨ï¼Œå¦‚æœæé—®ä¸èƒ½ä»å…³é”®è¯å’Œå‘é‡ä¸ŠåŒ¹é…ï¼Œé‚£ä½ ç»™å¤§æ¨¡å‹çš„å†…å®¹å°±å’Œé—®é¢˜æ— å…³ï¼Œè‡ªç„¶ç­”æ¡ˆå°±ä¸å¯¹ã€‚å¹¶ä¸”æ–‡æ¡£æ‹†åˆ†åï¼Œé€»è¾‘ä¸Šå°±è¢«æ‹†åˆ†äº†ï¼Œå¾ˆå¤šé—®é¢˜æ˜¯è¦ä»å„ä¸ªæ–‡æ¡£é€»è¾‘ä¸Šåˆ†æï¼Œä¸æ˜¯ç®€å•çš„æ£€ç´¢ã€‚

- å›é¡¾ä¸‹å½“å‰å¸‚åœºä¸Šä¼ä¸š RAG çŸ¥è¯†åº“è½åœ°çš„ä¸‰ç§ä¸»æµè·¯å¾„ã€‚
- ç›´æ¥ä½¿ç”¨é«˜å±‚çº§å¼€æºæ¡†æ¶
  - è¿™ç±»æ¡†æ¶å¦‚ RAGFlow, Dify, FastGPT ç­‰ï¼Œä¸»è¦ç‰¹ç‚¹æ˜¯æä¾›ç›¸å¯¹å®Œæ•´ã€å¼€ç®±å³ç”¨çš„ RAG å·¥ä½œæµï¼Œ
  - ç›®æ ‡æ˜¯ç®€åŒ– RAG åº”ç”¨çš„æ­å»ºè¿‡ç¨‹ï¼Œé™ä½å¼€å‘é—¨æ§›ã€‚
  - åä¹‹ï¼ŒåŠ£åŠ¿ä¸»è¦æ˜¯å®šåˆ¶åŒ–å’Œçµæ´»æ€§å—é™ï¼Œæ·±åº¦ä¼˜åŒ–å’Œé›†æˆç‰¹å®šç»„ä»¶æ—¶å¤æ‚åº¦è¾ƒé«˜ã€‚
- åº•å±‚å¼€å‘æ¡†æ¶è‡ªä¸»å¼€å‘
  - è¿™ç±»æ¡†æ¶åŒ…æ‹¬ LangChain, LlamaIndex, Haystack ç­‰ï¼Œéƒ½æä¾›äº†ä¸€ç³»åˆ—æ¨¡å—åŒ–çš„æ„å»ºå—ã€å·¥å…·å’Œæ¥å£ã€‚å¼€å‘è€…å¯ä»¥æ ¹æ®éœ€æ±‚çµæ´»ç»„åˆå’Œç¼–æ’ RAG æµç¨‹çš„å„ä¸ªç¯èŠ‚ï¼ˆå¦‚æ•°æ®åŠ è½½ã€æ–‡æœ¬åˆ†å‰²ã€åµŒå…¥ã€å‘é‡å­˜å‚¨ã€æ£€ç´¢ç­–ç•¥ã€LLM è°ƒç”¨ã€è®°å¿†ç®¡ç†ã€Agent æ„å»ºç­‰ï¼‰ã€‚
  - ä¼˜åŠ¿å¾ˆæ˜æ˜¾å°±æ˜¯çµåº¦çµæ´»ï¼Œå®šåˆ¶åŒ–èƒ½åŠ›å¼ºï¼Œèƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šä¸šåŠ¡åœºæ™¯è¿›è¡Œæ·±åº¦ä¼˜åŒ–å’Œé›†æˆã€‚é€‚åˆå¯¹ RAG æµç¨‹æœ‰ç²¾ç»†åŒ–æ§åˆ¶éœ€æ±‚çš„ä¼ä¸šã€‚
  - åä¹‹åŠ£åŠ¿å°±æ˜¯éœ€è¦æ›´å¤šçš„å¼€å‘å·¥ä½œé‡å’ŒæŠ€æœ¯æ·±åº¦ã€‚
- äº‘å‚å•† MaaS å¹³å°æ–¹æ¡ˆ
  - å¦‚é˜¿é‡Œäº‘ç™¾ç‚¼, ç™¾åº¦æ™ºèƒ½äº‘åƒå¸†, AWS Bedrock, Google Vertex AI Search ç­‰çš„ç§æœ‰åŒ–éƒ¨ç½²æ–¹æ¡ˆï¼Œæ— è®ºæ˜¯å›½å†…è¿˜æ˜¯å›½å¤–çš„è¿™äº›äº‘æœåŠ¡å•†ï¼Œéƒ½æ˜¯æŠŠ RAG ç›¸å…³çš„èƒ½åŠ›å°è£…æˆæœåŠ¡æˆ–æä¾›ç§æœ‰åŒ–éƒ¨ç½²åŒ…ï¼Œé€šå¸¸ä¸è‡ªå®¶çš„å¤§æ¨¡å‹ã€è®¡ç®—èµ„æºã€æ•°æ®å­˜å‚¨ç­‰æ·±åº¦é›†æˆã€‚ä¸€èˆ¬ä¹Ÿä¼šæä¾›æ¨¡å‹é€‰æ‹©ã€å¾®è°ƒã€éƒ¨ç½²ã€ç›‘æ§ç­‰ä¸€ç«™å¼æœåŠ¡ã€‚
  - é‡‡ç”¨è¿™ç§æ–¹æ¡ˆé€šå¸¸èƒ½æä¾›ç¨³å®šçš„åŸºç¡€è®¾æ–½ã€ä¾¿æ·çš„æ¨¡å‹ç®¡ç†å’Œéƒ¨ç½²ã€ä»¥åŠä¸å…¶ä»–äº‘æœåŠ¡çš„è‰¯å¥½å…¼å®¹æ€§ã€‚å¯¹äºå·²ç»æ·±åº¦ä½¿ç”¨ç‰¹å®šäº‘ç”Ÿæ€çš„ä¼ä¸šæ¥è¯´ï¼Œé›†æˆæˆæœ¬è¾ƒä½ã€‚
  - ä½†åŠ£åŠ¿å°±å¦‚åŒä½¿ç”¨å¤§æ¨¡å‹ä¸€ä½“æœºä¸€æ ·ï¼Œå¯èƒ½å­˜åœ¨å‚å•†é”å®šçš„é£é™©ï¼Œè·¨äº‘è¿ç§»æˆ–é›†æˆéè¯¥å‚å•†çš„æœåŠ¡å¯èƒ½ä¼šæ¯”è¾ƒå¤æ‚ã€‚
  - å½“ç„¶ï¼Œè´¹ç”¨å’Œçµæ´»æ€§ä¹Ÿéœ€è¦è€ƒè™‘ã€‚
- æ€»ç»“æ¥è¯´ï¼Œå®é™…ç”Ÿäº§åœºæ™¯ä¸­ä»¥ä¸Šä¸‰ç§é€‰æ‹©å¹¶éå®Œå…¨äº’æ–¥ï¼Œä¾‹å¦‚ä»¥åº•å±‚æ¡†æ¶ä¸ºåŸºç¡€ï¼Œä½†å€ŸåŠ©äº‘å‚å•†çš„éƒ¨åˆ†æ¨¡å‹æœåŠ¡æˆ–åŸºç¡€è®¾æ–½ä¹Ÿæ˜¯ä¸­ç›®å‰å¸¸è§çš„ç»„åˆã€‚

- RAGä¸»è¦å¯ä»¥è§£å†³å¤§æ¨¡å‹å¹»è§‰é—®é¢˜ï¼Œä»¥åŠé€šè¿‡å¤–æŒ‚çŸ¥è¯†ï¼Œé¿å…æ¨¡å‹é¢‘ç¹è¿›è¡ŒçŸ¥è¯†æ›´æ–°
  - å³ä½¿æ¨¡å‹å¯æ¥å—ä¸Šä¸‹æ–‡é•¿åº¦ä¸º128kï¼Œä½†æ˜¯æˆ‘çš„çŸ¥è¯†åº“é‡Œæœ‰10wç¯‡æ–‡æ¡£ï¼Œè¿œè¿œè¶…è¿‡æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæ‰€ä»¥ä½ è¿˜æ˜¯éœ€è¦æ£€ç´¢ã€‚
  - å½“ç„¶æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦å˜é•¿ï¼Œå¯¹äºæ£€ç´¢æ›´åŠ æœ‰å¥½ï¼Œç²’åº¦å˜å¤§ï¼Œå¬å›å˜é«˜ï¼Œç›¸è¾…ç›¸æˆã€‚
  - å³ä½¿ä½ å¯ä»¥æ¥å—128kçš„é•¿åº¦ï¼Œä½†æ˜¯ä½ çš„è®¾å¤‡æ¶ˆè€—ä¸èµ·ï¼Œæ›´å¤§çš„é•¿åº¦éœ€è¦æ›´å¤šçš„æ˜¾å­˜ï¼Œæ¨¡å‹éƒ¨ç½²æˆæœ¬æ›´é«˜ï¼Œé€Ÿåº¦æ›´æ…¢ã€‚
  - å¦‚æœä½ è°ƒç”¨apiï¼Œé‚£tokenæ¶ˆè€—ä¹Ÿå¤ªå¤§äº†ï¼Œæˆæœ¬è¿‡é«˜ã€‚

- Context Length å˜å¤§ â‰  ä¸éœ€è¦ RAG
  - æˆæœ¬é—®é¢˜: ä¸Šä¸‹æ–‡è¶Šé•¿ï¼Œæ¨ç†æˆæœ¬ï¼ˆæ˜¾å­˜ã€æ—¶é—´ã€ç®—åŠ›ï¼‰ä¼šçº¿æ€§ç”šè‡³è¶…çº¿æ€§å¢åŠ ã€‚ æ¯”å¦‚ï¼Œ100k tokens å¤„ç†ä¸€æ¬¡å°±å¾ˆçƒ§é’±ï¼Œè€Œ RAG å¯ä»¥ç”¨è¾ƒçŸ­ä¸Šä¸‹æ–‡åå¤è°ƒç”¨ã€‚
  - ä¿¡æ¯æ£€ç´¢æ•ˆç‡: å¦‚æœä½ æœ‰ 1TB çš„çŸ¥è¯†åº“ï¼ŒæŠŠå®ƒå…¨å¡è¿›ä¸Šä¸‹æ–‡æ˜¯ä¸å¯èƒ½çš„ã€‚ RAG å…ˆæ£€ç´¢ï¼Œå†é€è¿›ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥è®©æ¨¡å‹ä¸“æ³¨äºç›¸å…³ä¿¡æ¯ã€‚
  - å™ªéŸ³ä¸å¹²æ‰°: å¤§ä¸Šä¸‹æ–‡å¹¶ä¸ä»£è¡¨æ¨¡å‹ä¼šå¿½ç•¥æ— å…³ä¿¡æ¯ï¼Œç›¸åå¤ªå¤šæ— å…³å†…å®¹ä¼šç¨€é‡Šå…³é”®ä¿¡æ¯ï¼Œå¯¼è‡´æ¨ç†æ•ˆæœä¸‹é™ã€‚RAG çš„â€œæ£€ç´¢+è¿‡æ»¤â€èƒ½æé«˜ä¿¡å™ªæ¯”ã€‚
- Context å˜å¤§åï¼ŒRAG çš„å½¢æ€ä¼šæ¼”åŒ–
  - ä»â€œæŸ¥æ‰¾èµ„æ–™â€åˆ°â€œåŠ¨æ€çŸ¥è¯†ç»„ç»‡â€: æœªæ¥çš„ RAG å¯èƒ½ä¸åªæ˜¯æ£€ç´¢ï¼Œè€Œæ˜¯è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªä¸´æ—¶çš„çŸ¥è¯†æ‘˜è¦/çŸ¥è¯†å›¾è°±å†äº¤ç»™å¤§æ¨¡å‹
  - å¤šé˜¶æ®µæ¨ç†ï¼ˆMulti-turn Reasoningï¼‰: å¤§ä¸Šä¸‹æ–‡å…è®¸ä¸€æ¬¡æ€§æ”¾æ›´å¤šä¿¡æ¯ï¼Œä½† RAG å¯ä»¥åœ¨æ¯ä¸€æ­¥æ¨ç†æ—¶æ›´æ–°ä¸Šä¸‹æ–‡ï¼Œå½¢æˆ äº¤äº’å¼æ¨ç†
  - Hybrid RAGï¼ˆæ··åˆæ¨¡å¼ï¼‰: æŠŠé™æ€çŸ¥è¯†ï¼ˆå¸¸ç”¨ä¿¡æ¯ï¼‰ç›´æ¥æ”¾åœ¨é•¿ä¸Šä¸‹æ–‡é‡Œï¼ŒæŠŠåŠ¨æ€æ•°æ®ï¼ˆå®æ—¶ä¿¡æ¯ã€æœç´¢ç»“æœï¼‰ç”¨ RAG æ–¹å¼æ’å…¥ã€‚
- ä¸ºä»€ä¹ˆ RAG åœ¨é•¿ä¸Šä¸‹æ–‡æ—¶ä»£ä¾ç„¶é‡è¦
  - å¯æ›´æ–°æ€§: æ¨¡å‹è®­ç»ƒæ•°æ®æ˜¯é™æ€çš„ï¼ŒRAG å¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å¼•å…¥æœ€æ–°çŸ¥è¯†ã€‚
  - éšç§ä¸å®šåˆ¶åŒ–: ç§æœ‰ä¼ä¸šæˆ–ä¸ªäººæ•°æ®ä¸ä¼šç›´æ¥æ”¾åˆ°å¤§æ¨¡å‹é‡Œï¼Œè€Œæ˜¯é€šè¿‡ RAG åŠ¨æ€æ³¨å…¥ã€‚
  - å‡å°‘å¹»è§‰: æ¨¡å‹ç›´æ¥å¼•ç”¨æ£€ç´¢åˆ°çš„åŸæ–‡ï¼Œå¯ä»¥æ˜¾è‘—é™ä½èƒ¡ç¼–ä¹±é€ çš„æ¦‚ç‡ã€‚
  - å¤šæ¨¡æ€æ£€ç´¢: RAG æœªæ¥ä¼šä¸ä»…æ˜¯æ–‡æœ¬ï¼Œè¿˜ä¼šæ£€ç´¢å›¾ç‰‡ã€è§†é¢‘ã€éŸ³é¢‘ã€ç»“æ„åŒ–æ•°æ®ï¼Œå†ç»Ÿä¸€ç»™å¤§æ¨¡å‹å¤„ç†
- æœªæ¥ RAG çš„é‡ç‚¹ä¼šä»â€œæ‰¾æ–‡æ¡£â€å˜æˆâ€œæ„å»ºæœ€ä¼˜æ¨ç†ä¸Šä¸‹æ–‡â€ï¼Œå®ƒä¼šå˜å¾—æ›´æ™ºèƒ½ã€æ›´åŠ¨æ€ã€‚
  - éšç€ Context > 1M tokensï¼Œéƒ¨åˆ†åœºæ™¯å¯ç›´æ¥ç”¨å¤§æ¨¡å‹çš„é•¿è®°å¿†ï¼Œä½† RAG ä¼šæ¼”å˜ä¸ºâ€œæ™ºèƒ½çŸ¥è¯†ç¼–æ’å™¨â€ã€‚
  - å¦‚æœå‡ºç°æ— é™ä¸Šä¸‹æ–‡ä¸”æ¨ç†æˆæœ¬æä½çš„æ¨¡å‹ï¼ŒRAG ä¼šé€€åŒ–æˆâ€œæ•°æ®ç´¢å¼•å·¥å…·â€ï¼Œä¸»è¦ç”¨äºåŠ é€Ÿæ•°æ®ç»„ç»‡è€Œä¸æ˜¯å¿…é¡»æ­¥éª¤ã€‚

- ä¸ä¼šã€‚ä¸Šä¸‹æ–‡é•¿åº¦å˜é•¿åªä¼šè®©RAGæ›´å¥½ç”¨ã€‚ä»ç›®å‰çš„æƒ…å†µæ¥çœ‹ï¼ŒRAGçš„æˆæœ¬ä¾æ—§ä½äºå¾®è°ƒï¼Œæ•ˆæœé«˜äºå¾®è°ƒã€‚ç„¶åç›®å‰llmåº”ç”¨çš„æœ¬è´¨è¿˜æ˜¯promptï¼Œæ²¡æœ‰ä»€ä¹ˆå¤§èŠ±æ ·ï¼Œä¹Ÿè„±ç¦»ä¸äº†ç°æœ‰çš„äººç±»å­¦ç§‘ã€‚
  - å½“ç„¶ï¼Œå¦‚æœæœ‰äººå¼ºè¡Œè¯´RAGå°±æ˜¯ç”¨åµŒå…¥å‘é‡ä»€ä¹ˆçš„ï¼Œé‚£ä¹ˆæˆ‘åªèƒ½è¯´å¯èƒ½ä¼šæ¶ˆäº¡ã€‚retrieval è‚¯å®šæ˜¯åŒ…æ‹¬äº†å…¶ä»–æ–¹å¼çš„ï¼Œæˆ‘ä»¬ç”¨geminiå‘æ–‡ä»¶æˆ–è€…å›¾ç‰‡ç»™ä»–ä½•å°ä¸æ˜¯ä¸€ç§RAGå‘¢ï¼Œä¸è¿‡åªæ˜¯äººè„‘é©±åŠ¨ç½¢äº†

- ## RAG is anti pattern.
- https://x.com/pelaseyed/status/1882471632129994914
  - I donâ€™t do RAG anymore, get 10x the result just spinning up a pipeline and feed all content to Deepseek. And yes it scales to over 10K docs. 

- And you can cache it, so doesnâ€™t even cost that much

- Does deepseek have caching like Googleâ€™s media.upload? How do you otherwise avoid rebuilding the context all the time (Iâ€™ve not used the DS APIs).

- I wish I could fit the entire Internet in the prompt for @ask_pandi but it doesn't fit. RAG is still needed for this.
  - Run multiple LLMs in parallell, you will have infinite context window.

- You put everything into context?
  - Everything in context(s)

- RAG is A pattern, just not useful when your context is under 128k token and when the context is known. Pick the right tool for the job.
# discuss-solutions
- ## 

- ## 

- ## 

- ## 

- ## [I've been noticing more and more people are using GraphRAG instead of embedding and vector databases...is it really helpful or just the hype? : r/LangChain _202507](https://www.reddit.com/r/LangChain/comments/1e66e9r/graphrag/)
- Been working with graphrag on my current project. It is good, but it has its fallbacks. The good thing is that you can model you data to preserve relations. 
- The bad thing is:
  - you rely on your LLM to create queries based on your question to the answer. Lots of times the query created is pure shit, even with the most advanced models.
  - It gets really expensive. Like really expensive. Imagine that for every document you're storing in your graphDB you need to pass it to a LLM so it can extract the content and create relations so you can store it on your DB.

- If you ask me, people just follow the trends without understanding whether their application really needs something (eg GraphRAG here). 

- This is all basically more of a hype I would say. Why? Because you can use your own database that handles multiple data types and construct a knowledge graph. The knowledge graph basically shows the entity relationship between different nodes and this can be easily fed into your databases as tables and then, using retrieval methods, you can easily retrieve the most relevant chunk and the relationship score. 
  - I mean, the specialise databases like vector databases or even graph databases are really hyped, your own databases whether it is MongoDB, SingleStore, MariaDB, or even Couchbase can easily help you with creating graph knowledge and store them and then retrieve whenever required.

- how GraphRAG works with existing graphs, meaning if I already have entities and relationships, how would I load this graph in GraphRAG? 

- ## æœ‰æ²¡æœ‰æƒ³è¿‡å°†è‡ªå·±çš„ä»£ç åº“æ‰“åŒ…ç„¶åç›´æ¥å¡ç»™å¤§è¯­è¨€æ¨¡å‹æ¥å¤„ç†ï¼Ÿä¹Ÿè®¸ä½ ä¼šæƒ³åˆ°ç”¨RAGæˆ–è€…ç”¨windsurfæˆ–cursorã€‚ç°åœ¨æœ‰äº†æ›´ç®€å•çš„åŠæ³•â€”â€”Repomix
- https://x.com/karminski3/status/1881150047276138689
  - è¿™ä¸ªåº“å¯å°†æ•´ä¸ªå­˜å‚¨åº“æ‰“åŒ…åˆ°ä¸€ä¸ª AI å‹å¥½çš„æ–‡ä»¶ä¸­ï¼Œæ–¹ä¾¿ç»™å¤§è¯­è¨€æ¨¡å‹ä½¿ç”¨ã€‚

- ## ğŸ“Œ A list of software that allows searching the web with the assistance of AI.
- https://x.com/tom_doerr/status/1856778512612667838

# discuss-ocr
- ## 

- ## 

- ## [Are vision models (like qwen3-vl) good for OCR? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1nu7vw8/are_vision_models_like_qwen3vl_good_for_ocr/)
- Your concern about hallucination is totally valid but honestly the accuracy issues with traditional OCR like tesseract are way worse than occasional VL model hallucinations, especially for real world documents. 
  - I've been dealing with this exact problem for years and tesseract just falls apart with anything that isnt a perfect scan - weird fonts, slight rotations, quality issues, you name it. 
  - Vision models like qwen2.5-vl actually have much better accuracy because they understand context and can handle imperfect inputs way better than traditional engines.
  - The key is setting up proper prompting to minimize hallucinations - be very specific about what fields you want extracted and ask the model to indicate confidence levels or mark uncertain text.
  - We built Docstrange specifically for this use case and found that combining good prompting with validation rules catches most issues. For invoices and licenses, the structured nature actually helps a lot since VL models can understand document layout and context much better than traditional OCR engines that just see individual characters.

- I use Qwen2.5-VL for OCR all the time. Works great. I've OCR'd entire PDF scans of documents, and traditional OCR usually OCRs decently but has layout problems, whereas AI OCR usually does a good job of understanding and flattening layout. I've used it for Chinese, Japanese, German, and (of course) English.

- My best results for processing tabular PDFs has been: https://huggingface.co/ibm-granite/granite-docling-258M
  - Qwen-2.5 oddly either gives me great results, or the first row and nothing else.

- Gemma3 27b is pretty great for OCR.

- 
- 
- 
- 
- 

# discuss-rag-local
- ## 

- ## 

- ## 

- ## ğŸ’¡ [Embedding With LM Studio - what am i doing wrong : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1lharbh/embedding_with_lm_studio_what_am_i_doing_wrong/)
  - "text-embedding-nomic-embed-text-v1.5" loads fine and works with Anything.
  - but text-embedding-qwen3-embedding-0.6b & 8B and any other Embed model i use i get the below error: Failed to load embedding model. Internal error: Failed to initialize the context: failed to allocate compute pp buffers

- SOLVED
  - LM Studio was defaulting to 4096 Tokens where as the model can only handle 1024. Same goes for AnythingLLM , that defaults to 8192 tokens. set everything to 1024 and now its working

- Your "SOLVED" update is wrong. I ended up on this post after also having memory allocation issues with Qwen3 embedding models. 
  - The "1024" value you are quoting is the number of output dimensions of the model that clients would use to interpret results. You don't set that anywhere in LM Studio.
  - all you did by lowering the context length to 1024 tokens is reduce memory usage enough to fix your issue. You also made the output quality worse. Your clients will have to make more requests for more chunks for indexing since the model only accepts 1024 tokens at a time. This will in turn increase the number of vector entries that need to be stored.

- I had to "Override Domain Type" to Text Embedding to get it to work my computer (Models -> settings gear -> bottom setting). Not sure what else to try if you've already done that.
  - yep, did forget to mention that i tried both configurations

- try using any model you have as an embedder. It works just fine.

- ## [What Embedding Models Are You Using For RAG? : r/LocalLLaMA _202312](https://www.reddit.com/r/LocalLLaMA/comments/18j39qt/what_embedding_models_are_you_using_for_rag/)
- For RAG, I have been using multilingual minilm model from HF hub (I have multiligual requirement), and qdrant vector database. Happy with both. Specially, minilm embeddings are 368 dims, so some space saving there as well.

- Chinese embedding models and llms are now the top performers. Qwen-1.5B Yi-34B gte-large bge-large

- I chose the most lightweight stack possibleâ€” using all-MiniLM-L6-v2 for embeddings and HyperDB for local storage/retrieval (this is super fast for <100k documents)

- I am currently using all-MiniLM-L6-v2 but responses are generally not that good my data size is almost 600mb and it is giving me not that good results, what kind of embedding should i choose considering the size of data and database of pgvector postgres

- What Embedding model is best for Text-To-SQL for Financial data? Currently I'm using OpenAIEmbeddings
  - Text-to-SQL is a sequence to sequence task. My very first thought was "zero shot learning" because I have done this successfully in other domains. 

- I use whatever models are on top 1-5 on the MTEB leaderboard and run my custom evaluation 
# discuss-rag-tips/tricks
- ## 

- ## 

- ## 

- ## [RAG over Database : r/LangChain _202407](https://www.reddit.com/r/LangChain/comments/1efnx5u/rag_over_database/)
  - I have been trying to build a RAG over a database that has mulitple tables. Often times, for a user query, the data has to be searched by joining multiple tables. I followed this approach as mentioned in Langchain documents.
  - What I am observing is that many times the query generated by LLM is not correct and the data that user wants is incorrect. 

- Try to make all the joins yourself in the form of view and provide a simple view for the LLM

- dont use joins. use views or make stored procs to call for summaries etc you want.

- I would use the traditional RAG-based approach more in cases when the knowledge base is made of documents instead of structured data.
# discuss-rag-db
- ## 

- ## 

- ## 

- ## 

- ## 

- ## 

- ## [Best Vector Database for RAG : r/vectordatabase _202501](https://www.reddit.com/r/vectordatabase/comments/1hzovpy/best_vector_database_for_rag/)
- All vector databases work well for RAG. The selection of a vector database usually depends on your preference: cloud based vs self-hosted, open source vs private, programming languages / clients, API access, already part of SQL etc.
- Some common ones are (not in any order, I'm not affiliated with any):
  - Pinecone: private, cloud based, very popular
  - pgvector: Postres vector search extension, useful if you have already data in Postgres.
  - Faiss: A library for efficient similarity search and clustering of dense vectors.
  - Qdrant: Open source vector search engine written in Rust.
  - Chroma: Yet another vector database
  - Milvus: An open-source similarity search engine for embedding vectors.
  - Weaviate: Open source vector database written in Go.

- Pgvector because it is part of Postgres. In the end Vectors are just a data type in a databases. If you only need vectoring and not SQL then it is better to look search engine that does that I think.
  - PGVector, is like PostGIS for geospatial ops, an extension for Postgre to support vector search, it is not a true vector-native DB, so it adds operational overhead and lacks advanced features that a vector DB provides. 

- Quadrant, it's a vector DB designed specifically for RAG/AI workflows. It offers advanced filtering features (multi-tenancy) that are natively baked-in into its core architecture. These features would require more legwork using other data stores like Redis etc...
- FAISS: great for vector search is not really a DB, it's a lib and not a true DB
# discuss-code-rag
- ## 

- ## [Cline doesn't index your codebase. No RAG, no embeddings, no vector databases. : r/CLine _202505](https://www.reddit.com/r/CLine/comments/1kwhcz0/cline_doesnt_index_your_codebase_no_rag_no/)
- Does Roo Code also do that or is this just a Cline thing?
  - Roo recently added it for code search. It works with local models
  - They also added prompt caching for Gemini flash.

- Exceedingly large context windows don't just result in extreme costs but it also will slow down your every operation.
  - While prompt caching reduce costs it is not entirely free, nor does every model or provider support it. It's also like treating a symptom rather than the disease.

- RAG can be good. See how fast and precise is AUgment Code search
  - ive found Augment to miss obvious things at times

- cursor has code indexing

- RAG shines when you donâ€™t know the exact function / class names in a repo. ripgrep file search that Cline uses for context search is awesomeâ€”if you already have the right keywords. Thatâ€™s easy on your own project, but on a massive, unfamiliar codebase ripgrep can stall while RAG keeps rolling (although it can return the wrong chunks).

- Privacy issue first. Never is sending low quality windows off the code base to the LLM is bad for quality.

- ## ğŸ¤¼ Cline doesn't index your codebase. No RAG, no embeddings, no vector databases. _202505
- https://x.com/cline/status/1927226680206131530
  - [Why Cline Doesn't Index Your Codebase (And Why That's a Good Thing) - Cline Blog _202505](https://cline.bot/blog/why-cline-doesnt-index-your-codebase-and-why-thats-a-good-thing)
  - This isn't a limitation -- it's a deliberate design choice. 
  - As context windows increase, this approach enhances Cline's ability to understand your code.
  - The industry default: chunk your codebase, create embeddings, store in vector databases, retrieve "relevant" pieces.
  - But code doesn't work in chunks. A function call in chunk 47, its definition in chunk 892, the context that explains why? Scattered everywhere.
  - We believe in the agentic power of the modesl, and with Claude's 200K+ context window, we don't need clever retrieval. We need intelligent exploration.
  - So Cline reads code the way you do -- following imports, tracing dependencies, building connected understanding.
  - ğŸ’¡ Instead of indexing, Cline starts with structure. Using ASTs, it maps your codebase architecture -- classes, functions, relationships.
  - Then it explores. Need to add error handling? It traces from your function to your error utilities to similar patterns. Connected comprehension.
- The problems with RAG for code run deeper:
  - Chunking breaks the logical connections between functions and their dependencies
  - Indexes become stale the moment you push a commit
  - Your IP gets duplicated in vector embeddings (security nightmare)

- ## [Unpopular opinion: RAG is actively hurting your coding agents : r/ChatGPTCoding _202505](https://www.reddit.com/r/ChatGPTCoding/comments/1ktt4ab/unpopular_opinion_rag_is_actively_hurting_your/)
- I've been saying this since RAG first became the term used to describe the method. And you are exactly right, the whole reason it became a thing was because, back when context windows were 4k or 8k max, it was out of necessity. Now, in the age where context windows are 1M or 10M tokens, it only makes sense in specific enterprise cases where you have vast datasets to query for specific, isolated information.

- RAG isnâ€™t just calling vector stores, itâ€™s also prompt priming before generation using various sources. Dynamically priming the prompt with relevant information before the LLM generates a response. 
  - A lot of the large context models drop off in accuracy after 100k tokens, anyway.

- frontier models have large context windows, but they all equate to a token, which equals a cost
  - Knowledge Graphs + RAG. An syntax tree is constructed of the code , where each class, function , method etc become nodes in the graph. The LLM can then traverse the graph to get only what it needs.

- One thing Aider has that Roo and Cline don't, is what they call a "RepoMap" (and it ties aider to git). 
  - But the advantage of it is, given a class, it can easily determine all the related classes, so it doesn't have to go digging through folders, trying to figure out which file is actually relevant, it knows, because the RepoMap shows which classes use which classes.
  - I pulled the RepoMap class out of aider and eventually managed to remove all the aider dependencies and got running as a command-line app. I might try making an MCP with it and giving that to Cline and see if it can improve its ability to understand the app structure.
- I thought cline already had a tree sitter which did the same thing.
  - it does use tree-sitter, but not in the same way and not to the same effect. 
  - How Aider understands you program vs how Cline understands it is very apparent in working with them. Aider has a much better understanding of which bits of code are related to which other bits, whereas Cline is frequently guessing based on the filenames.
  - Using tree-sitter isn't enough. You need to actually map out the relationships and Cline doesn't seem to do this, at least not nearly as effectively as Aider does.
- using the LSP if available surley is the best approach?
  - No, beyond getting LSP diagnostic, which you can get somewhere else anyway. Using LSP server requires counting rows and columns, and LLMs are bad at it.
- but you don't have to use the interface literally
  - Then I'm not using LSP, I'm using something that uses LSP under the hood to point that is an implementation detail. Take a look at your favorite LSP server, tell me what is useful there to LLM agent besides diagnostics?
  - I can see how you give LLM diagnostics after it update the file is useful, but giving LLM access to LSP overall is IMO utterly useless.

- I was watching an interview video with Claude Code engineers and they mentioned along the lines of not using RAG or GraphRAG because they found that Claude Code performed better without it and just relying on tools, memory and agentic search.

- RAG still makes sense for latency sensitive use cases like real-time chat, although prompt caching helps with this. 

- ## [Lessons learned from implementing RAG for code generation : r/LLMDevs _202501](https://www.reddit.com/r/LLMDevs/comments/1hw1n5o/lessons_learned_from_implementing_rag_for_code/)
  - ğŸ“ [A Recipe for a Better AI-based Code Generator | Pulumi Blog _202501](https://www.pulumi.com/blog/codegen-learnings/)
  - We wrote a blog post documenting how we do retrieval augmented generation (RAG) for code generation in our AI assistant, Pulumi Copilot. 
  - Measure and tune recall (how many relevant documents are retrieved out of all relevant documents) and precision (how many of the retrieved documents are relevant)
  - Implement end-to-end testing and monitoring across development and production
  - Create self-debugging capabilities to handle common issues like type checking errors

- when I need understanding a medium sized code base I have dumped the whole thing into gemini in Google AI workbench. It can do 2 million tokens, though it is SLOOW.

- I wonder if you've considered feeding more tokens from your retrieval result into your code gen step? Why 20k? Is that always enough? How would you even know if it weren't?
  - You've got part of the answer right there: SLOOW :-) Also expensive!
  - Seriously though, you want to optimize for both recall and precision. Too much irrelevant data (poor precision) can confuse the LLM leading to hallucinations.
  - 20k is based on "it feels right" and empirical data but honestly we have not done enough analysis to conclude that it is the perfect number for all scenarios - we will continue to measure and adjust.

- Did you experiment with other retrieval methods besides or in addition to semantic similarity? I've done some work using different techniques, like parsing dependency trees out of the current file, with promising results for code RAG.
  - We did look into BM25 for FT search but did not see measurable benefits for our use cases. Our approach relies on getting a lot of documents first and then pruning - it would be better to get just what's needed in the first place, I still hope BM25 can help there. Worth another look!
# discuss-embedding
- ## 

- ## 

- ## 

- ## [Open-source embedding models: which one's the best? : r/Rag _202509](https://www.reddit.com/r/Rag/comments/1nro65j/opensource_embedding_models_which_ones_the_best/)
- tried embeddinggemma:300m and qwen3-embedding-0.6b and 4b

- [Open-source embedding models: which one's the best? : r/ollama](https://www.reddit.com/r/ollama/comments/1nro30i/opensource_embedding_models_which_ones_the_best/)
- I find the latest Qwen embeddings and reranker models the best out there today even when comparing with offerings from OpenAI and VoyageAI
  - I tried BGE, Nomic, and Qwen using ChunkHound I don't care so much about the raw numbers but about the end to end result. You can easily see the difference in results by running the chunkhound search command

- [Open-source embedding models: which one to use? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1nrgklt/opensource_embedding_models_which_one_to_use/)
- I've had my best results with bge-m3 or qwen3-embedding

- ## [Is there a classification of worst vs best ai models for RAG : r/Rag _202509](https://www.reddit.com/r/Rag/comments/1nk2rza/is_there_a_classification_of_worst_vs_best_ai/)
- For small models, most of our users have converged around: Gemma3, Qwen3, and DeepSeek
  - I always use for demos Mistral Medium 3.1 (mistral-medium-2508), which is a very good middle ground.
  - For the "large" LLMs, Gemini Flash variants and Claude Sonnet or Haiku. OpenAI models have *never* been good at this use case. The GPT-OSS models have been abysmally bad in testing.

- For an embeddings model, we've been using all-MiniLM-L6-v2 since we released TrustGraph, and haven't really seen any need to change. The platform allows you to choose any embeddings model from HF, but all-MiniLM-L6-v2 seems to do just fine in most use cases. If you want be able to try out all these model combinations, you can give them a try with TrustGraph (open source).

- gemma3 worked best in my case(turkish docs)
  - but rag mostly depends on understanding and memorization abilities

- I have tried several OpenAI models for our RAG system, and got great results using gpt-4.1-mini.
  - Via the API, it is the OpenAI model that has the largest context window right now.

- ## [embeddinggemma has higher memory footprint than qwen3:0.6b? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nbbi1c/embeddinggemma_has_higher_memory_footprint_than/)
- For whatever reason, Gemma models have a larger vocab size of 256K whereas most models have a vocab size of around 120k. This adds to the size

- confirmed, i use ollama just for embedding, download it from official ollama library. The size is 621MB on command ollama list. But getting bigger like 2.8GB on ollama ps
  - and it very slow compared to qwen embedding 4b.

- ## [Real life experience with Qwen3 embeddings? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nezmfi/real_life_experience_with_qwen3_embeddings/)
  - I need to decide on an embedding model for our new vector store and Iâ€™m torn between Qwen3 0.6b and OpenAI v3 small.
  - The qwen3 embeddings top the MTEB leaderboards scoring even higher than the new Gemini embeddings. Qwen3 has been killing it, but embeddings can be a fragile thing.

- I don't think Qwen3-Embedding-0.6B performs better than previous encoder models of similar size (e.g., bge-m3); 
  - its main advantage is long-context support. 
  - Overall, it's only a little bit better than other prior state-of-the-art LLM-based embedding models (e.g., Kalm-v2), with advantages mainly comes from instruction tuning on the query side, which improves adaptability.
- Qwen3-Embedding-4B is good. It outperforms bge by 2â€“3 points (on my own dataset, using NDCG@10), and maintains strong retrieval performance at 2â€“4k tokens per chunk. 
  - However, the GGUF version of this model seems inconsistent with the original checkpointâ€”this discrepancy is unclear (I suspect it may be related to the pooling configuration).
- Qwen3-Embedding-8B might indeed be a SOTA model, but it costs too much.

- I never tested that quant. There are so many mistakes you can make with embeddings (omitting required eot tokens, missing instructions, wrong padding alignment etc.) 

- I always include something like an embedding version so it is always possible to change embedding algo without reencoding old data so long as you are willing to do a search per algo and re-rank them.

- I've used 8B and 4B as GGUF at Q4_K_M and never had issues some are pointing.
  - Found the 4B most efficient as the difference between the 8B is small for such resource difference.
  - Been using for code bases, currently over 380 files with code. No issues.

- ## [Are Qwen3 Embedding GGUF faulty? : r/LocalLLaMA _202507](https://www.reddit.com/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/)
  - Qwen3 Embedding has great retrieval results on MTEB.
  - However, I tried it in llama.cpp. The results were much worse than competitors.

- You have to add the EOS Token manually "<|endoftext|>" as of here: https://github.com/ggml-org/llama.cpp/issues/14234

- In multilingual, I was very disappointed with qwen3 embedding compared to jinaai/jina-embeddings-v3 which remains my favorite for the moment
- v4 is out btw: https://huggingface.co/jinaai/jina-embeddings-v4
  - It does work much better, getting 48.11% on the same benchmark.
  - The official JINA API is very slow though. Half a minute for a batch of 32.

- Would you believe I was just trying it out today and it was all messed up. Swapped from Q3 4B and 0.6B to granite 278m and all my problems went away.

- Yes, though if I tried generating the embeddings through the SentenceTransformers module instead, I got the state-of-the-art results I was hoping for on my benchmark. A code snippet for how to do so is listed on their HF page.
# discuss
- ## 

- ## 

- ## ğŸ¤” [Is RAG system actually slow because of tool calling protocol? : r/Rag _202509](https://www.reddit.com/r/Rag/comments/1ntc1ky/is_rag_system_actually_slow_because_of_tool/)
  - Just came across few wild comparison between MCP and UTCP protocols and honestly... my mind is blown.
  - For RAG systems where every millisecond counts when we are retrieving documents. UTCP is 30-40% faster performance than MCP. that's HUGE.

- Why would you use mcp in RAG to begin with?
  - The consumer of a RAG might use tools, but â€˜theâ€™ RAG components do not need tools.

- Find it funny everyone so focused on speed, but go ask gpt and wait 15min is fine

- ## ğŸ†š [Does anyone know how much of a performance difference between knowledge graphs and vector based searches? : r/LangChain _202408](https://www.reddit.com/r/LangChain/comments/1eragqk/does_anyone_know_how_much_of_a_performance/)
  - I made a pretty simple vector based RAG search, and it performs "okay" and doesn't always generate the expected results. I have the pieces for a knowledge graph, but I was wondering if people knew the expected improvements that I should expect to see by moving to knowledge graphs?

- Vector search alone is usually faster. Without any bells and whistles like metadata filtering or stepback, it's just an optimized KNN. 
  - Graph based RAG involves traversing a graph in addition to searching for the appropriate starting node, which often times is a done via vector search. 
  - The graph structure usually leads to good results though.

- ## LobeChat v1.85.0 ä¼˜åŒ–äº†å¯¹è¯ä¸Šä¼ æ–‡ä»¶çš„é€»è¾‘ï¼ŒæŠŠå¤§å®¶å‘¼å£°æ¯”è¾ƒé«˜çš„å…¨æ–‡ä¸Šä¼ çš„èƒ½åŠ›åšå¥½äº†ï¼Œä¸å†èµ° åˆ†å— + embedding çš„æœ´ç´  RAG ã€‚å¸¦æ¥çš„å¥½å¤„æ˜¯é€Ÿåº¦å˜å¿«äº†ï¼Œæ•ˆæœä¹Ÿæ›´åŠ ç†æƒ³äº†ã€‚
- https://x.com/arvin17x/status/1921101310004179082
  - æ„Ÿè§‰å¤§æ¨¡å‹å†è¿­ä»£ä¸ªä¸€å¹´ï¼Œæœ´ç´  RAG å¯èƒ½å°±è¦é€€å‡ºå†å²èˆå°äº†
  - æ¯”å¦‚ä¸€ä¸ªæ–‡ä»¶ 10w å­—çš„å†…å®¹ï¼Œå…¨æ–‡ä¸Šä¼ å°±æ˜¯10wå­—å…¨æ”¾ä¸Šä¸‹æ–‡é‡Œã€‚ RAG æ˜¯æ‰¾å‡ºæœ€ç›¸å…³çš„ N ä¸ªå†…å®¹ç‰‡æ®µï¼ˆå‡è®¾ä¸€ä¸ªç‰‡æ®µ1kï¼ŒåŠ èµ·æ¥ N kä¸ªå­—ï¼‰æ”¾ä¸Šä¸‹æ–‡é‡Œã€‚ å…¨æ–‡ä¸Šä¼ çš„æ•ˆæœåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¸€å®šæ˜¯æ¯” RAG è¦å¥½çš„ã€‚
  - ç°åœ¨é»˜è®¤ä¸Šä¼ å°±æ˜¯å…¨æ–‡ã€‚ä¸è¿‡çŸ¥è¯†åº“è¯»å–è¿˜æ˜¯åŸæ¥çš„
- ä¸ç”¨ RAGï¼Œç›´æ¥ä¼ ç»™ LLMï¼Œé’±åŒ…é¡¶ä¸ä½å•Š
- å…¨æ–‡ä¸Šä¼ ï¼Œåªæœ‰Googleå®¶æ‰æœ‰å¾ˆå¤§çš„ä¸Šä¸‹æ–‡å§ï¼Ÿå…¶ä»–å®¶æ•ˆæœå°±ä¸å¥½äº†
  - ä¸å•Šï¼Œç°åœ¨gpt4.1miniéƒ½æœ‰1Mä¸Šä¸‹æ–‡äº†ï¼Œä¹Ÿä¾¿å®œã€‚æµ‹ä¸‹æ¥æ•ˆæœéå¸¸å¥½

- ## å¼€æºçš„ llm ç³»ç»Ÿé‡Œï¼Œå¸¦çŸ¥è¯†åº“åŠŸèƒ½çš„ï¼Œç›®å‰è¿˜æ²¡æ‰¾åˆ°ä¸€ä¸ªåšçš„å¥½çš„ã€‚
- https://x.com/wwwgoubuli/status/1830227047173751206
  - ä¸è¿‡è¿™ä¹ˆè¯´å€’ä¹Ÿä¸æ˜¯è¦è´¬ä½ä»€ä¹ˆï¼Œæˆ‘è‡ªå·±å¸®å®¢æˆ·å®šåˆ¶è¿‡çš„çŸ¥è¯†åº“ä¸­ï¼Œåšçš„ç¨å¾®å¥½ä¸€ç‚¹çš„ï¼Œä¹ŸæŠ•å…¥äº†å¤§é‡çš„ç²¾åŠ›å’Œæˆæœ¬æ¥å®ç°æ•°æ®é¢„å¤„ç†ï¼Œå•å•ä¸€ä¸ªæ–‡æœ¬åˆ†æ®µéƒ½æ‰¾ä¸åˆ°é€šç”¨èŒƒå¼ï¼Œå¾—å¤§é‡å®šåˆ¶ã€‚

- é—®é¢˜å°±æ˜¯ to b é‡Œã€‚ to c é‚£ä¸€ä¸ªç”¨æˆ·ä¸¢ä¸ª pdf ç®—ä¸ä¸Šä»€ä¹ˆçŸ¥è¯†åº“ã€‚ åˆ°äº† to b  é¢†åŸŸï¼Œä½ çœ‹ç€ä¸€å † excel ï¼Œpdfï¼Œ ä½ çš„æ¢¦é­‡æ‰åˆšå¼€å§‹

- æˆ‘ä»¬è‡ªå·±åšå”®åå®¢æœæœºå™¨äººï¼Œèµ·æ­¥æ˜¯æ‰¾å®ä¹ ç”Ÿæ’¸äº†1ä¸‡æ¡é—®ç­”å¯¹ï¼Œä¸Šçº¿åæ¯å‘¨æ ¹æ®bad caseè¿­ä»£ç»§ç»­åšæ•°æ®ã€‚è¿™ç©æ„æ•ˆæœå°±æ˜¯çœ‹æ ‡æ³¨æ•°æ®ï¼Œæƒ³è¦å¥½çš„æ•ˆæœï¼Œå°±å¥½å¥½åšæ•°æ®ã€‚

- çŸ¥è¯†åº“å¦‚æœåƒç¼–ç¨‹è¯­æ³•æ‰‹å†Œä¸€æ ·ï¼Œé‚£ragæ•ˆæœä¼šéå¸¸å¥½ï¼Œå¦‚æœåƒæ•£æ–‡è¯—ï¼Œragæ•ˆæœå°±ä¼šå¾ˆå·®ã€‚æˆ‘ä»¬æ‹¿åˆ°æ‰‹çš„æ–‡æ¡£ä¸€èˆ¬ä»‹äºä¸¤è€…ä¹‹é—´ï¼Œæ‰€ä»¥æƒ³æé«˜ragæ•ˆæœï¼Œåªå¥½æŠŠæ–‡æ¡£æœ¬èº«å¾€ç¼–ç¨‹è¯­æ³•æ‰‹å†Œæ–¹å‘æ”¹

- æˆ‘ç°åœ¨æœ€å¤´ç–¼çš„æ˜¯pdfæ–‡æ¡£ï¼Œé‡Œé¢å…¨æ˜¯å›¾ç‰‡çš„é‚£ç§ã€‚ï¼Œ
  - è®©ç”²æ–¹åŠ é’±ï¼Œå›¾ç‰‡åˆ°æ–‡å­—çš„è½¬æ¢ä¸ä»…éœ€è¦å·¥å…·ï¼Œè¿˜éœ€è¦äººå·¥å®¡é˜…ï¼Œä¸åŠ é’±æ²¡æ³•å¹²
