---
title: lib-ai-app-community-rag
tags: [ai, community, rag]
created: 2024-09-08T20:08:04.573Z
modified: 2024-09-08T20:08:16.088Z
---

# lib-ai-app-community-rag

# guide

- pros-rag
  - solutions rich
  - save cost

- cons
  - extra infra like vectordb/retrive-api

- usecases-rag
  - long-docs/pdf
  - chat-history
  - ragçš„å®ç°è¿˜å¯ä»¥å‚è€ƒai-codingä¸­çš„å®è·µå’Œæ¨¡å¼
  - é’ˆå¯¹è¡¨æ ¼excel/è„‘å›¾çš„rag
  - â³ å¤šç‰ˆæœ¬æ–‡ä»¶çš„rag

- leaderboard-rag
  - [MTEB Leaderboard - a Hugging Face Space by mteb](https://huggingface.co/spaces/mteb/leaderboard)
    - https://github.com/embeddings-benchmark/mteb
    - top202509: gemini-embedding-001, embeddinggemma-300m, Qwen3-Embedding-8B/4B/0.6B, linq-mistral, jina, granite, nomic
    - ğŸ‘·å®æµ‹, embeddinggemmaåœ¨Ollamaç»“æœå·®, åœ¨LM Studioç»“æœè¿˜è¡Œ ~~åœ¨ similaritySearch æ—¶å¾ˆå·®~~
      - è·Ÿæ¨èqwen/granite
- resources
  - [RAG+AIå·¥ä½œæµ+Agentï¼šLLMæ¡†æ¶è¯¥å¦‚ä½•é€‰æ‹©ï¼Œå…¨é¢å¯¹æ¯”MaxKBã€Difyã€FastGPTã€RagFlowã€Anything-LLM, ä»¥åŠæ›´å¤šæ¨è - çŸ¥ä¹ _202407](https://zhuanlan.zhihu.com/p/711761781)
# discuss-stars
- ## 

- ## 

- ## 

- ## 

- ## ğŸ–¼ï¸ğŸ“„ğŸ“ˆ [Multi-modal RAG at scale: Processing 200K+ documents (pharma/finance/aerospace). What works with tables/Excel/charts, what breaks, and why it costs way more than you think : r/LLMDevs _202510](https://www.reddit.com/r/LLMDevs/comments/1o5oaas/multimodal_rag_at_scale_processing_200k_documents/)
  - TL; DR: Built RAG systems for 10+ enterprise clients where 40-60% of critical information was locked in tables, Excel files, and diagrams. Standard text-based RAG completely misses this. 
  - This covers what actually works, when to use vision models vs traditional parsing, and the production issues nobody warns you about.
- Here's what nobody tells you: most enterprise knowledge isn't in clean text. It's in Excel spreadsheets with 50 linked sheets, tables buried in 200-page PDFs, and charts where the visual layout matters more than any text.
- Why Text-Only RAG Fails
- Quick context: pharmaceutical client had 50K+ documents where critical dosage data lived in tables. Banks had financial models spanning 50+ Excel sheets. Aerospace client's rocket schematics contained engineering specs that text extraction would completely mangle.
  - When a researcher asks "what were cardiovascular safety signals in Phase III trials?" and the answer is in Table 4 of document 8, 432, text-based RAG returns nothing useful.
- The Three Categories (and different approaches for each)
- ğŸ“Œ Simple Tables
  - Standard tables with clear headers. Financial reports, clinical trial demographics, product specifications.
  - What works: Traditional parsing with `pymupdf or pdfplumber` , extract to CSV or JSON, then embed both the structured data AND a text description. Store the table data, but also generate something like "Table showing cardiovascular adverse events by age group, n=2, 847 patients." Queries can match either.
  - Production issue: PDFs don't mark where tables start or end. Used heuristics like consistent spacing and grid patterns, but false positives were constant. Built quality scoring - if table extraction looked weird, flag for manual review.
- ğŸ“Œ Complex Visual Content
  - Rocket schematics, combustion chamber diagrams, financial charts where information IS the visual layout.
  - Traditional OCR extracts gibberish. What works: Vision language models. Used Qwen2.5-VL-32b for aerospace, GPT-4o for financial charts, Claude 3.5 Sonnet for complex layouts.
  - The process: Extract images at high resolution, use vision model to generate descriptions, embed the description plus preserve image reference. During retrieval, return both description and original image so users can verify.
  - The catch: Vision models are SLOW and EXPENSIVE. Processing 125K documents with image extraction plus VLM descriptions took 200+ GPU hours.
- ğŸ“Œ Excel Files (the special circle of hell)
  - Not just tables - formulas, multiple sheets, cross-sheet references, embedded charts, conditional formatting that carries meaning.
  - Financial models with 50+ linked sheets where summary depends on 12 others. Excel files where cell color indicates status. Files with millions of rows.
  - ğŸ’¡ For simple Excel use pandas. 
  - For complex Excel use `openpyxl` to preserve formulas, build a dependency graph showing which sheets feed into others. 
  - For massive files, process in chunks with metadata, use filtering to find right section before pulling actual data.
  - ğŸ’¡ Excel files with external links to other workbooks. Parser would crash. Solution: detect external references during preprocessing, flag for manual handling.
  - Vision model trick: For sheets with complex visual layouts like dashboards, screenshot the sheet and use vision model to understand layout, then combine with structured data extraction. Sounds crazy but worked better than pure parsing.

- When to Use What
  - Use traditional parsing when: clear grid structure, cleanly embedded text, you need exact values, high volume where cost matters.
  - Use vision models when: scanned documents, information IS the visual layout, spatial relationships matter, traditional parsers fail, you need conceptual understanding not just data extraction.
  - Use hybrid when: tables span multiple pages, mixed content on same page, you need both precise data AND contextual understanding.
  - Real example: Page has both detailed schematic (vision model) and data table with test results (traditional parsing). Process twice, combine results. 
  - ğŸ’¡ Vision model explains schematic, parser extracts exact values.

- Production Issues Nobody Warns You About
  - Tables spanning multiple pages: My hacky solution detects when table ends at page boundary, checks if next page starts with similar structure, attempts to stitch. Works maybe 70% of the time.
  - Image quality degradation: Client uploads scanned PDF photocopied three times. Vision models hallucinate. 
    - ğŸ’¡ Solution: document quality scoring during ingestion, flag low-quality docs, warn users results may be unreliable.
  - Memory explosions: Processing 300-page PDF with 50 embedded charts at high resolution ate 10GB+ RAM and crashed the server. 
    - ğŸ’¡ Solution: lazy loading, process pages incrementally, aggressive caching.
  - Vision model hallucinations: This almost destroyed client trust. Bank client had a chart, GPT-4o returned revenue numbers that were close but WRONG. Dangerous for financial data. 
    - ğŸ’¡ Solution: Always show original images alongside AI descriptions. For critical data, require human verification. Make it clear what's AI-generated vs extracted.

- The Metadata Architecture
  - This is where most implementations fail. You can't just embed a table and hope semantic search finds it.
  - For tables I tag content_type, column_headers, section, what data it contains, parent document, page number. 
  - For charts I tag visual description, diagram type, system, components. 
  - For Excel I tag sheet name, parent workbook, what sheets it depends on, data types.
  - Why this matters: When someone asks "what were Q3 revenue projections, " metadata filtering finds the right Excel sheet BEFORE semantic search runs. Without this, you're searching through every table in 50K documents.

- Cost Reality Check
  - Multi-modal processing is EXPENSIVE. For 50K documents with average 5 images each, that's 250K images. At roughly one cent per image with GPT-4o, that's around $2, 500 just for initial processing. Doesn't include re-processing or experimentation.
  - Self-hosted vision models like from Qwen need around 80GB VRAM. Processing 250K images takes 139-347 hours of compute. Way slower but cheaper long-term for high volume.
  - My approach: Self-hosted models for bulk processing, API calls for real-time complex cases, aggressive caching, filter by relevance before processing everything.

- What I'd Do Differently
  - Start with document quality assessment - don't build one pipeline for everything. 
  - Build the metadata schema first - spent weeks debugging retrieval issues that were actually metadata problems. Always show the source visual alongside AI descriptions. 
  - Test on garbage data early - production documents are never clean. Set expectations around accuracy - vision models aren't perfect.

- Multi-modal RAG pays off when critical information lives in tables and charts, document volumes are high, users waste hours manually searching, and you can handle the complexity and cost.
  - Skip it when most information is clean text, small document sets work with manual search, budget is tight and traditional RAG solves 80% of problems. 
  - Real ROI: Pharma client's researchers spent 10-15 hours per week finding trial data in tables. System reduced that to 1-2 hours. Paid for itself in three months.
- Multi-modal RAG is messy, expensive, and frustrating. But when 40-60% of your client's critical information is locked in tables, charts, and Excel files, you don't have a choice. The tech is getting better, but production challenges remain.

- My questions would be as a provider/consultant, how do provide support and maintenance moving forward? Long term, how do you ensure the old way of them putting data in tables is easily digested into the new RAG system automatically or that someone owns the knowledge base? The â€œdata cleaningâ€ and organization seems like a full time contract just by itself. Then, how do you test for accuracy and showcase those results to stakeholders for renewal or as case studies for other businesses?
  - support and maintenance was something i completely underestimated early on. did the typical "build it and hand off" approach with my first few clients. total disaster. systems would work great for 2-3 months then drift, new document types would break the pipeline, accuracy would drop. now i do hybrid - initial build plus ongoing support contract. usually quarterly check-ins, pipeline updates when they add new document types, on-call for critical issues.
  - for data cleaning i train someone on their team to own document quality. they become the gatekeeper, i provide the tooling. the alternative where i do it forever doesn't scale and they know their domain way better anyway. built quality detection into the ingestion pipeline that flags problematic docs automatically but someone still needs to make decisions about what to do with garbage scans from the 90s.
  - testing is golden test sets with domain experts - 100-200 real questions with known answers. run these quarterly but focus stakeholder reports on business metrics they actually care about like "researchers spend 2 hours per week searching vs 15 hours before" or "regulatory response time dropped from 5 days to 6 hours."
  - initial builds were $50k+ (currently 100k+) but support contracts run $3k-5k monthly. way more profitable long-term plus you learn edge cases that feed back into the product and i'm full-time on this, but trying to build something in the rag space as well.

- iâ€™ve battled the same multiâ€‘modal rag pain at scale in pharma/finance/aerospace. if you want this to survive production, bake in lineage, governance, and observability from day one, not after it drifts.
  - table-first retrieval: tag content_type, column_headers, section, page, and parent doc; prefilter by metadata before semantic search so â€œq3 revenue projectionsâ€ lands on the right sheet/table.
  - excel lineage and verification: parse formulas with openpyxl, build a crossâ€‘sheet dependency graph, bind screenshots of dashboards to underlying cells when layout matters, and require human verification for any financial figures the vlm â€œreads.â€
  - vlm governance and cost controls: quality-score every image/page, batch and cache captions, separate â€œconceptualâ€ descriptions from â€œnumericâ€ extraction, and run a quarterly golden test set that mixes tables, scanned PDFs, and charts.
  - production observability: trace every ingestion step (parse, stitch, caption, embed), cap memory per page, add fallbacks for multiâ€‘page table stitching, and alert on eval drift rather than waiting for user complaints.
- maxim ai helps here with endâ€‘toâ€‘end evaluation, simulation, experimentation, and observability for agents and rag pipelines: scenario-based tests at scale, prompt/version experiments, live tracing with online evaluations, ci/cd hooks, and enterprise deploy options. (builder here!)

- I'm currently working on a solution to address product cataloging issues for an ecommerce platform. Right now, analysts manually review Excel files and images, then fill out Excel templates with product attributes (title, description, features, etc.). My proposed solution would automatically extract attributes from files, populate templates, validate images, and extract attributes from images (like dimensions in cm, brand, model, and others). Also check images if this image is "main", "lateral", "left view", "right view" for an SKU and if these images contains a QR code.
  - The main challenge I'm facing is that the image processing workflow is too slow and expensive (using GPT-4o model). 
  - The goal is to scale up to processing 1, 000, 000 product reviews daily. I'm currently using AWS with Fargate jobs that launch dockerized processes for image processing, orchestrated by Step Functions.
  - Currently handling <1, 000 SKUs daily, which is far from the 1, 000, 000 target.
  - Any recommendations or advice for optimizing this process?

- If table with headers , and each table column is well aligned , in most scenarios, you can check the column alignment condition and assuming that table schema did not change to determine whether table contents flow over pages

- ## [Using LMStudio for RAG/File Search with LibreChat? Â· danny-avila/LibreChat _202506](https://github.com/danny-avila/LibreChat/discussions/7713)
  - I run LibreChat locally inside Docker, I run local models in LMStudio, and want to use LMStudio's nomic (or otherwise) embedding capabilities instead of an external provider like OpenAI.
  - (A) i want to do it for free/private locally, and (B) especially because when I tested file upload directly within LMStudio [using its RAG] it correctly pulled all text from the PDF, but when I used OpenAI cloud embedding it missed a huge chunk of the textâ€”perhaps because the API version uses a text-only embedding model without vision
- UPDATE / SOLUTION:
  - Making a copy of the config.py file in my LibreChat root folder and pointing the compose-override to it did solve the problem and allowed me to use the openai embeddings with LMStudio. I seconded the feature request to make this an easy option for people to opt into

- been down that rabbit hole too, and yep: this kind of embedding mismatch + RAG retrieval chaos is super common when things try to speak in different tensor dialects.
  - the error from LMStudio (input field must be a string or array of strings) usually hits when the upstream retrieval returns a raw int list or malformed embedding body. 
  - i eventually got tired of patching these one by one and built a semantic rescue engine that stabilizes the entire pipeline.

- ## ğŸ¤” [RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ä¼šä¸ä¼šæ¶ˆäº¡å‘¢ï¼Ÿä¸€æ—¦å¤§æ¨¡å‹çš„Context Lengthå˜å¤§ï¼ŒRAGè¿˜æœ‰å­˜æ´»çš„å¿…è¦å—ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/637421964)
- RAG çš„åŸºæœ¬åŸç†å¹¶ä¸å¤æ‚ï¼š
  1.  å°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆå¤šä¸ª **chunk**ï¼›  
  2.  é€šè¿‡å‘é‡åŒ–æ¨¡å‹ï¼ˆå¦‚ [OpenAI Embeddings] ã€Sentence-Transformerï¼‰ä¸ºæ¯ä¸ª chunk ç”Ÿæˆå‘é‡ï¼Œå¹¶å­˜å…¥æ•°æ®åº“ï¼›  
  3.  åœ¨ç”¨æˆ·æé—®æ—¶ï¼Œæ£€ç´¢å‡º **Top-K ç›¸ä¼¼æ–‡æ¡£**ï¼›  
  4.  æŠŠè¿™äº›æ–‡æ¡£ä¸é—®é¢˜ä¸€èµ·è¾“å…¥å¤§æ¨¡å‹ï¼Œä½œä¸ºé¢å¤–ä¸Šä¸‹æ–‡ï¼›  
  5.  å¤§æ¨¡å‹åŸºäºé—®é¢˜ + æ£€ç´¢ç»“æœï¼Œç”Ÿæˆæ›´å‡†ç¡®çš„å›ç­”ã€‚
- ç®€å•æ¥è¯´ï¼ŒRAG å°±åƒæ˜¯ç»™å¤§æ¨¡å‹è£…ä¸Šäº†ä¸€ä¸ªâ€œå¤–æŒ‚æœç´¢å¼•æ“â€ã€‚
  - ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¼ ç»Ÿçš„ å¾®è°ƒï¼ˆFine-tuningï¼‰ æ–¹å¼è™½ç„¶èƒ½æŠŠæ–°çŸ¥è¯†å†™è¿›æ¨¡å‹ï¼Œä½†å¾€å¾€æˆæœ¬é«˜æ˜‚ã€ä¸å¯é€†ï¼Œè¿˜å¯èƒ½â€œç‰ºç‰²â€åŸæœ‰èƒ½åŠ›ã€‚è€Œ RAG çš„ä¼˜åŠ¿åœ¨äºï¼šè½»é‡ã€çµæ´»ã€éšå–éšç”¨
- ç›®å‰å¤§å¤šæ•° RAG ç³»ç»Ÿå…¶å®æ˜¯â€œFrozen RAGâ€â€”â€”Retriever ä¸ LLM æ˜¯åˆ†å¼€çš„ï¼ŒRetriever å›ºå®šä¸å˜ï¼ŒLLM ä¹Ÿä¸æ›´æ–°ã€‚å®ƒä»¬æ‹¼åœ¨ä¸€èµ·èƒ½è·‘ï¼Œä½†æ¨¡å—ä¹‹é—´å¹¶æ²¡æœ‰æ·±åº¦ååŒã€‚
- RAG 2.0 çš„æ ¸å¿ƒç›®æ ‡å°±æ˜¯ï¼š è®©æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨æˆä¸ºä¸€ä¸ªâ€œå¯å…±åŒè®­ç»ƒâ€çš„æ•´ä½“ï¼Œä»è€Œè¾¾åˆ°ç«¯åˆ°ç«¯çš„ä¼˜åŒ–ã€‚

- ä»çº¯ä¸šåŠ¡ç«¯çš„è§’åº¦æ¥è¯´ï¼Œä¸ä½†ä¸ä¼šæ¶ˆäº¡ï¼Œè¿˜æœ‰å¯èƒ½åœ¨æœªæ¥çš„3-5å¹´é‡Œå¤§è¡Œå…¶é“ã€‚
  - è¡¨é¢åŸå› éå¸¸ç®€å•ï¼šRAGå‡ ä¹æ˜¯ç›®å‰å”¯ä¸€ä¸€ç§å…¼å…·é—¨æ§›ä½+å¯è§‚å¯Ÿ+æ§åˆ¶å‡†+è§æ•ˆå¿«çš„è·¯çº¿ã€‚
  - æœ€åæä¸€å˜´æ·±å±‚åŸå› ï¼šRAGçš„æŠ€æœ¯è·¯çº¿æå…¶ç¬¦åˆå›½æƒ…ï¼Œå…¶æœ¬è´¨æ˜¯çœŸç†çŒé¡¶ä¸”å¯ä»¥éšæ—¶æ›¿æ¢ï¼Œæ¯”å¤§ç‚¼æ¨¡å‹å¾—åˆ°çš„é†‰æ‹³å¹»è§‰å¯æ§å¾—å¤šã€‚
  - To Bå’ŒTo Géƒ½æ˜¯æ•°æ®å¯†é›†å‹å¯¹è±¡ï¼Œè‡ªèº«æ•°æ®å°±æ˜¯æœ€å¤§çš„ç­¹ç ï¼Œä¸è¦æŒ‡æœ›äººå®¶ä¼šæŠŠèº«å®¶æ€§å‘½äº¤åˆ°AIå‚å•†æ‰‹é‡Œï¼Œå»æ‰“é€ ä¸€ä¸ªé¥é¥é¢†å…ˆçš„å¤§æ¨¡å‹ã€‚
- å¤§æ¨¡å‹ç›®å‰æ²¡çœ‹åˆ°ä¼ä¸šå†…å¥½çš„è½åœ°åœºæ™¯ã€‚ragå¤§éƒ¨åˆ†æ˜¯ç”¨äºçŸ¥è¯†åº“ï¼Œä½†æ˜¯ä¹Ÿå°±åŸºæœ¬èƒ½ç”¨ï¼Œå¦‚æœæé—®ä¸èƒ½ä»å…³é”®è¯å’Œå‘é‡ä¸ŠåŒ¹é…ï¼Œé‚£ä½ ç»™å¤§æ¨¡å‹çš„å†…å®¹å°±å’Œé—®é¢˜æ— å…³ï¼Œè‡ªç„¶ç­”æ¡ˆå°±ä¸å¯¹ã€‚å¹¶ä¸”æ–‡æ¡£æ‹†åˆ†åï¼Œé€»è¾‘ä¸Šå°±è¢«æ‹†åˆ†äº†ï¼Œå¾ˆå¤šé—®é¢˜æ˜¯è¦ä»å„ä¸ªæ–‡æ¡£é€»è¾‘ä¸Šåˆ†æï¼Œä¸æ˜¯ç®€å•çš„æ£€ç´¢ã€‚

- å›é¡¾ä¸‹å½“å‰å¸‚åœºä¸Šä¼ä¸š RAG çŸ¥è¯†åº“è½åœ°çš„ä¸‰ç§ä¸»æµè·¯å¾„ã€‚
- ç›´æ¥ä½¿ç”¨é«˜å±‚çº§å¼€æºæ¡†æ¶
  - è¿™ç±»æ¡†æ¶å¦‚ RAGFlow, Dify, FastGPT ç­‰ï¼Œä¸»è¦ç‰¹ç‚¹æ˜¯æä¾›ç›¸å¯¹å®Œæ•´ã€å¼€ç®±å³ç”¨çš„ RAG å·¥ä½œæµï¼Œ
  - ç›®æ ‡æ˜¯ç®€åŒ– RAG åº”ç”¨çš„æ­å»ºè¿‡ç¨‹ï¼Œé™ä½å¼€å‘é—¨æ§›ã€‚
  - åä¹‹ï¼ŒåŠ£åŠ¿ä¸»è¦æ˜¯å®šåˆ¶åŒ–å’Œçµæ´»æ€§å—é™ï¼Œæ·±åº¦ä¼˜åŒ–å’Œé›†æˆç‰¹å®šç»„ä»¶æ—¶å¤æ‚åº¦è¾ƒé«˜ã€‚
- åº•å±‚å¼€å‘æ¡†æ¶è‡ªä¸»å¼€å‘
  - è¿™ç±»æ¡†æ¶åŒ…æ‹¬ LangChain, LlamaIndex, Haystack ç­‰ï¼Œéƒ½æä¾›äº†ä¸€ç³»åˆ—æ¨¡å—åŒ–çš„æ„å»ºå—ã€å·¥å…·å’Œæ¥å£ã€‚å¼€å‘è€…å¯ä»¥æ ¹æ®éœ€æ±‚çµæ´»ç»„åˆå’Œç¼–æ’ RAG æµç¨‹çš„å„ä¸ªç¯èŠ‚ï¼ˆå¦‚æ•°æ®åŠ è½½ã€æ–‡æœ¬åˆ†å‰²ã€åµŒå…¥ã€å‘é‡å­˜å‚¨ã€æ£€ç´¢ç­–ç•¥ã€LLM è°ƒç”¨ã€è®°å¿†ç®¡ç†ã€Agent æ„å»ºç­‰ï¼‰ã€‚
  - ä¼˜åŠ¿å¾ˆæ˜æ˜¾å°±æ˜¯çµåº¦çµæ´»ï¼Œå®šåˆ¶åŒ–èƒ½åŠ›å¼ºï¼Œèƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šä¸šåŠ¡åœºæ™¯è¿›è¡Œæ·±åº¦ä¼˜åŒ–å’Œé›†æˆã€‚é€‚åˆå¯¹ RAG æµç¨‹æœ‰ç²¾ç»†åŒ–æ§åˆ¶éœ€æ±‚çš„ä¼ä¸šã€‚
  - åä¹‹åŠ£åŠ¿å°±æ˜¯éœ€è¦æ›´å¤šçš„å¼€å‘å·¥ä½œé‡å’ŒæŠ€æœ¯æ·±åº¦ã€‚
- äº‘å‚å•† MaaS å¹³å°æ–¹æ¡ˆ
  - å¦‚é˜¿é‡Œäº‘ç™¾ç‚¼, ç™¾åº¦æ™ºèƒ½äº‘åƒå¸†, AWS Bedrock, Google Vertex AI Search ç­‰çš„ç§æœ‰åŒ–éƒ¨ç½²æ–¹æ¡ˆï¼Œæ— è®ºæ˜¯å›½å†…è¿˜æ˜¯å›½å¤–çš„è¿™äº›äº‘æœåŠ¡å•†ï¼Œéƒ½æ˜¯æŠŠ RAG ç›¸å…³çš„èƒ½åŠ›å°è£…æˆæœåŠ¡æˆ–æä¾›ç§æœ‰åŒ–éƒ¨ç½²åŒ…ï¼Œé€šå¸¸ä¸è‡ªå®¶çš„å¤§æ¨¡å‹ã€è®¡ç®—èµ„æºã€æ•°æ®å­˜å‚¨ç­‰æ·±åº¦é›†æˆã€‚ä¸€èˆ¬ä¹Ÿä¼šæä¾›æ¨¡å‹é€‰æ‹©ã€å¾®è°ƒã€éƒ¨ç½²ã€ç›‘æ§ç­‰ä¸€ç«™å¼æœåŠ¡ã€‚
  - é‡‡ç”¨è¿™ç§æ–¹æ¡ˆé€šå¸¸èƒ½æä¾›ç¨³å®šçš„åŸºç¡€è®¾æ–½ã€ä¾¿æ·çš„æ¨¡å‹ç®¡ç†å’Œéƒ¨ç½²ã€ä»¥åŠä¸å…¶ä»–äº‘æœåŠ¡çš„è‰¯å¥½å…¼å®¹æ€§ã€‚å¯¹äºå·²ç»æ·±åº¦ä½¿ç”¨ç‰¹å®šäº‘ç”Ÿæ€çš„ä¼ä¸šæ¥è¯´ï¼Œé›†æˆæˆæœ¬è¾ƒä½ã€‚
  - ä½†åŠ£åŠ¿å°±å¦‚åŒä½¿ç”¨å¤§æ¨¡å‹ä¸€ä½“æœºä¸€æ ·ï¼Œå¯èƒ½å­˜åœ¨å‚å•†é”å®šçš„é£é™©ï¼Œè·¨äº‘è¿ç§»æˆ–é›†æˆéè¯¥å‚å•†çš„æœåŠ¡å¯èƒ½ä¼šæ¯”è¾ƒå¤æ‚ã€‚
  - å½“ç„¶ï¼Œè´¹ç”¨å’Œçµæ´»æ€§ä¹Ÿéœ€è¦è€ƒè™‘ã€‚
- æ€»ç»“æ¥è¯´ï¼Œå®é™…ç”Ÿäº§åœºæ™¯ä¸­ä»¥ä¸Šä¸‰ç§é€‰æ‹©å¹¶éå®Œå…¨äº’æ–¥ï¼Œä¾‹å¦‚ä»¥åº•å±‚æ¡†æ¶ä¸ºåŸºç¡€ï¼Œä½†å€ŸåŠ©äº‘å‚å•†çš„éƒ¨åˆ†æ¨¡å‹æœåŠ¡æˆ–åŸºç¡€è®¾æ–½ä¹Ÿæ˜¯ä¸­ç›®å‰å¸¸è§çš„ç»„åˆã€‚

- RAGä¸»è¦å¯ä»¥è§£å†³å¤§æ¨¡å‹å¹»è§‰é—®é¢˜ï¼Œä»¥åŠé€šè¿‡å¤–æŒ‚çŸ¥è¯†ï¼Œé¿å…æ¨¡å‹é¢‘ç¹è¿›è¡ŒçŸ¥è¯†æ›´æ–°
  - å³ä½¿æ¨¡å‹å¯æ¥å—ä¸Šä¸‹æ–‡é•¿åº¦ä¸º128kï¼Œä½†æ˜¯æˆ‘çš„çŸ¥è¯†åº“é‡Œæœ‰10wç¯‡æ–‡æ¡£ï¼Œè¿œè¿œè¶…è¿‡æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæ‰€ä»¥ä½ è¿˜æ˜¯éœ€è¦æ£€ç´¢ã€‚
  - å½“ç„¶æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦å˜é•¿ï¼Œå¯¹äºæ£€ç´¢æ›´åŠ æœ‰å¥½ï¼Œç²’åº¦å˜å¤§ï¼Œå¬å›å˜é«˜ï¼Œç›¸è¾…ç›¸æˆã€‚
  - å³ä½¿ä½ å¯ä»¥æ¥å—128kçš„é•¿åº¦ï¼Œä½†æ˜¯ä½ çš„è®¾å¤‡æ¶ˆè€—ä¸èµ·ï¼Œæ›´å¤§çš„é•¿åº¦éœ€è¦æ›´å¤šçš„æ˜¾å­˜ï¼Œæ¨¡å‹éƒ¨ç½²æˆæœ¬æ›´é«˜ï¼Œé€Ÿåº¦æ›´æ…¢ã€‚
  - å¦‚æœä½ è°ƒç”¨apiï¼Œé‚£tokenæ¶ˆè€—ä¹Ÿå¤ªå¤§äº†ï¼Œæˆæœ¬è¿‡é«˜ã€‚

- Context Length å˜å¤§ â‰  ä¸éœ€è¦ RAG
  - æˆæœ¬é—®é¢˜: ä¸Šä¸‹æ–‡è¶Šé•¿ï¼Œæ¨ç†æˆæœ¬ï¼ˆæ˜¾å­˜ã€æ—¶é—´ã€ç®—åŠ›ï¼‰ä¼šçº¿æ€§ç”šè‡³è¶…çº¿æ€§å¢åŠ ã€‚ æ¯”å¦‚ï¼Œ100k tokens å¤„ç†ä¸€æ¬¡å°±å¾ˆçƒ§é’±ï¼Œè€Œ RAG å¯ä»¥ç”¨è¾ƒçŸ­ä¸Šä¸‹æ–‡åå¤è°ƒç”¨ã€‚
  - ä¿¡æ¯æ£€ç´¢æ•ˆç‡: å¦‚æœä½ æœ‰ 1TB çš„çŸ¥è¯†åº“ï¼ŒæŠŠå®ƒå…¨å¡è¿›ä¸Šä¸‹æ–‡æ˜¯ä¸å¯èƒ½çš„ã€‚ RAG å…ˆæ£€ç´¢ï¼Œå†é€è¿›ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥è®©æ¨¡å‹ä¸“æ³¨äºç›¸å…³ä¿¡æ¯ã€‚
  - å™ªéŸ³ä¸å¹²æ‰°: å¤§ä¸Šä¸‹æ–‡å¹¶ä¸ä»£è¡¨æ¨¡å‹ä¼šå¿½ç•¥æ— å…³ä¿¡æ¯ï¼Œç›¸åå¤ªå¤šæ— å…³å†…å®¹ä¼šç¨€é‡Šå…³é”®ä¿¡æ¯ï¼Œå¯¼è‡´æ¨ç†æ•ˆæœä¸‹é™ã€‚RAG çš„â€œæ£€ç´¢+è¿‡æ»¤â€èƒ½æé«˜ä¿¡å™ªæ¯”ã€‚
- Context å˜å¤§åï¼ŒRAG çš„å½¢æ€ä¼šæ¼”åŒ–
  - ä»â€œæŸ¥æ‰¾èµ„æ–™â€åˆ°â€œåŠ¨æ€çŸ¥è¯†ç»„ç»‡â€: æœªæ¥çš„ RAG å¯èƒ½ä¸åªæ˜¯æ£€ç´¢ï¼Œè€Œæ˜¯è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªä¸´æ—¶çš„çŸ¥è¯†æ‘˜è¦/çŸ¥è¯†å›¾è°±å†äº¤ç»™å¤§æ¨¡å‹
  - å¤šé˜¶æ®µæ¨ç†ï¼ˆMulti-turn Reasoningï¼‰: å¤§ä¸Šä¸‹æ–‡å…è®¸ä¸€æ¬¡æ€§æ”¾æ›´å¤šä¿¡æ¯ï¼Œä½† RAG å¯ä»¥åœ¨æ¯ä¸€æ­¥æ¨ç†æ—¶æ›´æ–°ä¸Šä¸‹æ–‡ï¼Œå½¢æˆ äº¤äº’å¼æ¨ç†
  - Hybrid RAGï¼ˆæ··åˆæ¨¡å¼ï¼‰: æŠŠé™æ€çŸ¥è¯†ï¼ˆå¸¸ç”¨ä¿¡æ¯ï¼‰ç›´æ¥æ”¾åœ¨é•¿ä¸Šä¸‹æ–‡é‡Œï¼ŒæŠŠåŠ¨æ€æ•°æ®ï¼ˆå®æ—¶ä¿¡æ¯ã€æœç´¢ç»“æœï¼‰ç”¨ RAG æ–¹å¼æ’å…¥ã€‚
- ä¸ºä»€ä¹ˆ RAG åœ¨é•¿ä¸Šä¸‹æ–‡æ—¶ä»£ä¾ç„¶é‡è¦
  - å¯æ›´æ–°æ€§: æ¨¡å‹è®­ç»ƒæ•°æ®æ˜¯é™æ€çš„ï¼ŒRAG å¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å¼•å…¥æœ€æ–°çŸ¥è¯†ã€‚
  - éšç§ä¸å®šåˆ¶åŒ–: ç§æœ‰ä¼ä¸šæˆ–ä¸ªäººæ•°æ®ä¸ä¼šç›´æ¥æ”¾åˆ°å¤§æ¨¡å‹é‡Œï¼Œè€Œæ˜¯é€šè¿‡ RAG åŠ¨æ€æ³¨å…¥ã€‚
  - å‡å°‘å¹»è§‰: æ¨¡å‹ç›´æ¥å¼•ç”¨æ£€ç´¢åˆ°çš„åŸæ–‡ï¼Œå¯ä»¥æ˜¾è‘—é™ä½èƒ¡ç¼–ä¹±é€ çš„æ¦‚ç‡ã€‚
  - å¤šæ¨¡æ€æ£€ç´¢: RAG æœªæ¥ä¼šä¸ä»…æ˜¯æ–‡æœ¬ï¼Œè¿˜ä¼šæ£€ç´¢å›¾ç‰‡ã€è§†é¢‘ã€éŸ³é¢‘ã€ç»“æ„åŒ–æ•°æ®ï¼Œå†ç»Ÿä¸€ç»™å¤§æ¨¡å‹å¤„ç†
- æœªæ¥ RAG çš„é‡ç‚¹ä¼šä»â€œæ‰¾æ–‡æ¡£â€å˜æˆâ€œæ„å»ºæœ€ä¼˜æ¨ç†ä¸Šä¸‹æ–‡â€ï¼Œå®ƒä¼šå˜å¾—æ›´æ™ºèƒ½ã€æ›´åŠ¨æ€ã€‚
  - éšç€ Context > 1M tokensï¼Œéƒ¨åˆ†åœºæ™¯å¯ç›´æ¥ç”¨å¤§æ¨¡å‹çš„é•¿è®°å¿†ï¼Œä½† RAG ä¼šæ¼”å˜ä¸ºâ€œæ™ºèƒ½çŸ¥è¯†ç¼–æ’å™¨â€ã€‚
  - å¦‚æœå‡ºç°æ— é™ä¸Šä¸‹æ–‡ä¸”æ¨ç†æˆæœ¬æä½çš„æ¨¡å‹ï¼ŒRAG ä¼šé€€åŒ–æˆâ€œæ•°æ®ç´¢å¼•å·¥å…·â€ï¼Œä¸»è¦ç”¨äºåŠ é€Ÿæ•°æ®ç»„ç»‡è€Œä¸æ˜¯å¿…é¡»æ­¥éª¤ã€‚

- ä¸ä¼šã€‚ä¸Šä¸‹æ–‡é•¿åº¦å˜é•¿åªä¼šè®©RAGæ›´å¥½ç”¨ã€‚ä»ç›®å‰çš„æƒ…å†µæ¥çœ‹ï¼ŒRAGçš„æˆæœ¬ä¾æ—§ä½äºå¾®è°ƒï¼Œæ•ˆæœé«˜äºå¾®è°ƒã€‚ç„¶åç›®å‰llmåº”ç”¨çš„æœ¬è´¨è¿˜æ˜¯promptï¼Œæ²¡æœ‰ä»€ä¹ˆå¤§èŠ±æ ·ï¼Œä¹Ÿè„±ç¦»ä¸äº†ç°æœ‰çš„äººç±»å­¦ç§‘ã€‚
  - å½“ç„¶ï¼Œå¦‚æœæœ‰äººå¼ºè¡Œè¯´RAGå°±æ˜¯ç”¨åµŒå…¥å‘é‡ä»€ä¹ˆçš„ï¼Œé‚£ä¹ˆæˆ‘åªèƒ½è¯´å¯èƒ½ä¼šæ¶ˆäº¡ã€‚retrieval è‚¯å®šæ˜¯åŒ…æ‹¬äº†å…¶ä»–æ–¹å¼çš„ï¼Œæˆ‘ä»¬ç”¨geminiå‘æ–‡ä»¶æˆ–è€…å›¾ç‰‡ç»™ä»–ä½•å°ä¸æ˜¯ä¸€ç§RAGå‘¢ï¼Œä¸è¿‡åªæ˜¯äººè„‘é©±åŠ¨ç½¢äº†

- ## RAG is anti pattern.
- https://x.com/pelaseyed/status/1882471632129994914
  - I donâ€™t do RAG anymore, get 10x the result just spinning up a pipeline and feed all content to Deepseek. And yes it scales to over 10K docs. 

- And you can cache it, so doesnâ€™t even cost that much

- Does deepseek have caching like Googleâ€™s media.upload? How do you otherwise avoid rebuilding the context all the time (Iâ€™ve not used the DS APIs).

- I wish I could fit the entire Internet in the prompt for @ask_pandi but it doesn't fit. RAG is still needed for this.
  - Run multiple LLMs in parallell, you will have infinite context window.

- You put everything into context?
  - Everything in context(s)

- RAG is A pattern, just not useful when your context is under 128k token and when the context is known. Pick the right tool for the job.
# discuss-solutions
- ## 

- ## 

- ## 

- ## 

- ## [I've been noticing more and more people are using GraphRAG instead of embedding and vector databases...is it really helpful or just the hype? : r/LangChain _202507](https://www.reddit.com/r/LangChain/comments/1e66e9r/graphrag/)
- Been working with graphrag on my current project. It is good, but it has its fallbacks. The good thing is that you can model you data to preserve relations. 
- The bad thing is:
  - you rely on your LLM to create queries based on your question to the answer. Lots of times the query created is pure shit, even with the most advanced models.
  - It gets really expensive. Like really expensive. Imagine that for every document you're storing in your graphDB you need to pass it to a LLM so it can extract the content and create relations so you can store it on your DB.

- If you ask me, people just follow the trends without understanding whether their application really needs something (eg GraphRAG here). 

- This is all basically more of a hype I would say. Why? Because you can use your own database that handles multiple data types and construct a knowledge graph. The knowledge graph basically shows the entity relationship between different nodes and this can be easily fed into your databases as tables and then, using retrieval methods, you can easily retrieve the most relevant chunk and the relationship score. 
  - I mean, the specialise databases like vector databases or even graph databases are really hyped, your own databases whether it is MongoDB, SingleStore, MariaDB, or even Couchbase can easily help you with creating graph knowledge and store them and then retrieve whenever required.

- how GraphRAG works with existing graphs, meaning if I already have entities and relationships, how would I load this graph in GraphRAG? 

- ## æœ‰æ²¡æœ‰æƒ³è¿‡å°†è‡ªå·±çš„ä»£ç åº“æ‰“åŒ…ç„¶åç›´æ¥å¡ç»™å¤§è¯­è¨€æ¨¡å‹æ¥å¤„ç†ï¼Ÿä¹Ÿè®¸ä½ ä¼šæƒ³åˆ°ç”¨RAGæˆ–è€…ç”¨windsurfæˆ–cursorã€‚ç°åœ¨æœ‰äº†æ›´ç®€å•çš„åŠæ³•â€”â€”Repomix
- https://x.com/karminski3/status/1881150047276138689
  - è¿™ä¸ªåº“å¯å°†æ•´ä¸ªå­˜å‚¨åº“æ‰“åŒ…åˆ°ä¸€ä¸ª AI å‹å¥½çš„æ–‡ä»¶ä¸­ï¼Œæ–¹ä¾¿ç»™å¤§è¯­è¨€æ¨¡å‹ä½¿ç”¨ã€‚

- ## ğŸ“Œ A list of software that allows searching the web with the assistance of AI.
- https://x.com/tom_doerr/status/1856778512612667838

# discuss-ocr
- ## 

- ## 

- ## 

- ## 

- ## 

- ## [Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes & More : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/)
  - We're excited to share Nanonets-OCR2, a state-of-the-art suite of models designed for advanced image-to-markdown conversion and Visual Question Answering (VQA).
  - It is trained on tons of financial documents. Since the output is in markdown with the tables as html, they can be converted to CSVs also. We have some samples examples for bank statements in the docstrange demo. 
  - `Nanonets-OCR2-1.5B-exp` is experimental model. Full training is not complete yet. We will release the final model when the full training is done.
  - We will add support for Ollama in coming days. Meanwhile you can use the Docstrange (https://docstrange.nanonets.com/). We do have api support there, incase of large volume.

- Can you tell me what are the exact advances over nanonets-ocr-s ? Specifically the 3B model.
  - Thanks. We have scaled our datasets by a lot (close to 3 million documents). New model should work better on multilingual, handwritten data, flowcharts, financial complex tables. 
  - This time we have added Visual Question Answering support. 
  - Fixed some of the edge-cases where model used to give infinite generation for empty tables and stuff. 
  - Also you should be able to change the prompt based on your use case. Nanonets-ocr-s does not work if you change the prompt much.

- tables will already be in html format. You can use this prompt for both getting complex table and header and footer.
  - Also for tables you should use `repetition_penalty=1` for best result. You can try in docstrange 
  - `user_prompt = """Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using â˜ and â˜‘ for check boxes."""`

- Can this model provide bboxes with recognized box types (header, text, table) via special prompts or special formats like it did qwen2-vl / qwen3-vl ?
  - We don't support boxes yet. That's in plan for next release.

- I do not see a comparison with the Docling document understanding pipeline from IBM.
  - We will add more evals. But generally in all evals Gemini models are in top. Thats why we first evaluated against Gemini. But for complex document these models, specially the 3B one should be better than docling.
- It will be better than mistral ocr. Our last model was better than mistral. This one is improvement on top of the last model.

- Small models like this one or Docling deliver phenomenal results when the PDFs you are dealing with are not overly complex. While they handle TeX-equations well, the difference to large LLMs becomes very obvious when presenting them graphics. 
  - Default model is trained to give small description. You can change the prompt to have detailed description. Since the model also supports VQA you can do multi-turn multiple questions.

- The issue of any ocr model its wide multilingual support. What about your model?
  - We have trained on multilingual as well as handwritten data.

- Did you also incorporate historical texts? I tried with 18th century fraktur and it often mixed up long s and f. There are quite good sets of historical training data available: https://zenodo.org/records/15764161
  - No we have not trained on historical texts, all the handwritten and multi-lingual datasets are recent data. This is because old text fonts are quite different from recent documents and texts, and these models were mainly used on recents documents. But if there is enough annotated datasets we can definitely include those in next iteration. Thanks for sharing!

- Tested with my handwritten diary (that none other model could parse anything at all) - and all text was extracted! Thank you

- Very cool and excited to see these models keep getting smaller! FWIW I've been building a collection of uv scripts that aim to make it easier to run these new VLM based OCR models across a whole dataset using vLLM for inference. They can be run locally or using HF Jobs. Just added this model to that repo! https://huggingface.co/datasets/uv-scripts/ocr

- ## [Are vision models (like qwen3-vl) good for OCR? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1nu7vw8/are_vision_models_like_qwen3vl_good_for_ocr/)
- Your concern about hallucination is totally valid but honestly the accuracy issues with traditional OCR like tesseract are way worse than occasional VL model hallucinations, especially for real world documents. 
  - I've been dealing with this exact problem for years and tesseract just falls apart with anything that isnt a perfect scan - weird fonts, slight rotations, quality issues, you name it. 
  - Vision models like qwen2.5-vl actually have much better accuracy because they understand context and can handle imperfect inputs way better than traditional engines.
  - The key is setting up proper prompting to minimize hallucinations - be very specific about what fields you want extracted and ask the model to indicate confidence levels or mark uncertain text.
  - We built Docstrange specifically for this use case and found that combining good prompting with validation rules catches most issues. For invoices and licenses, the structured nature actually helps a lot since VL models can understand document layout and context much better than traditional OCR engines that just see individual characters.

- I use Qwen2.5-VL for OCR all the time. Works great. I've OCR'd entire PDF scans of documents, and traditional OCR usually OCRs decently but has layout problems, whereas AI OCR usually does a good job of understanding and flattening layout. I've used it for Chinese, Japanese, German, and (of course) English.

- My best results for processing tabular PDFs has been: https://huggingface.co/ibm-granite/granite-docling-258M
  - Qwen-2.5 oddly either gives me great results, or the first row and nothing else.

- Gemma3 27b is pretty great for OCR.

- 
- 
- 
- 
- 

# discuss-rag-local
- ## 

- ## 

- ## 

- ## ğŸ’¡ [Embedding With LM Studio - what am i doing wrong : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1lharbh/embedding_with_lm_studio_what_am_i_doing_wrong/)
  - "text-embedding-nomic-embed-text-v1.5" loads fine and works with Anything.
  - but text-embedding-qwen3-embedding-0.6b & 8B and any other Embed model i use i get the below error: Failed to load embedding model. Internal error: Failed to initialize the context: failed to allocate compute pp buffers

- SOLVED
  - LM Studio was defaulting to 4096 Tokens where as the model can only handle 1024. Same goes for AnythingLLM , that defaults to 8192 tokens. set everything to 1024 and now its working

- Your "SOLVED" update is wrong. I ended up on this post after also having memory allocation issues with Qwen3 embedding models. 
  - The "1024" value you are quoting is the number of output dimensions of the model that clients would use to interpret results. You don't set that anywhere in LM Studio.
  - all you did by lowering the context length to 1024 tokens is reduce memory usage enough to fix your issue. You also made the output quality worse. Your clients will have to make more requests for more chunks for indexing since the model only accepts 1024 tokens at a time. This will in turn increase the number of vector entries that need to be stored.

- I had to "Override Domain Type" to Text Embedding to get it to work my computer (Models -> settings gear -> bottom setting). Not sure what else to try if you've already done that.
  - yep, did forget to mention that i tried both configurations

- try using any model you have as an embedder. It works just fine.

- ## [What Embedding Models Are You Using For RAG? : r/LocalLLaMA _202312](https://www.reddit.com/r/LocalLLaMA/comments/18j39qt/what_embedding_models_are_you_using_for_rag/)
- For RAG, I have been using multilingual minilm model from HF hub (I have multiligual requirement), and qdrant vector database. Happy with both. Specially, minilm embeddings are 368 dims, so some space saving there as well.

- Chinese embedding models and llms are now the top performers. Qwen-1.5B Yi-34B gte-large bge-large

- I chose the most lightweight stack possibleâ€” using all-MiniLM-L6-v2 for embeddings and HyperDB for local storage/retrieval (this is super fast for <100k documents)

- I am currently using all-MiniLM-L6-v2 but responses are generally not that good my data size is almost 600mb and it is giving me not that good results, what kind of embedding should i choose considering the size of data and database of pgvector postgres

- What Embedding model is best for Text-To-SQL for Financial data? Currently I'm using OpenAIEmbeddings
  - Text-to-SQL is a sequence to sequence task. My very first thought was "zero shot learning" because I have done this successfully in other domains. 

- I use whatever models are on top 1-5 on the MTEB leaderboard and run my custom evaluation 
# discuss-rag-tips/tricks
- ## 

- ## 

- ## 

- ## [RAG over Database : r/LangChain _202407](https://www.reddit.com/r/LangChain/comments/1efnx5u/rag_over_database/)
  - I have been trying to build a RAG over a database that has mulitple tables. Often times, for a user query, the data has to be searched by joining multiple tables. I followed this approach as mentioned in Langchain documents.
  - What I am observing is that many times the query generated by LLM is not correct and the data that user wants is incorrect. 

- Try to make all the joins yourself in the form of view and provide a simple view for the LLM

- dont use joins. use views or make stored procs to call for summaries etc you want.

- I would use the traditional RAG-based approach more in cases when the knowledge base is made of documents instead of structured data.
# discuss-rag-db
- ## 

- ## 

- ## 

- ## 

- ## 

- ## 

- ## [Best Vector Database for RAG : r/vectordatabase _202501](https://www.reddit.com/r/vectordatabase/comments/1hzovpy/best_vector_database_for_rag/)
- All vector databases work well for RAG. The selection of a vector database usually depends on your preference: cloud based vs self-hosted, open source vs private, programming languages / clients, API access, already part of SQL etc.
- Some common ones are (not in any order, I'm not affiliated with any):
  - Pinecone: private, cloud based, very popular
  - pgvector: Postres vector search extension, useful if you have already data in Postgres.
  - Faiss: A library for efficient similarity search and clustering of dense vectors.
  - Qdrant: Open source vector search engine written in Rust.
  - Chroma: Yet another vector database
  - Milvus: An open-source similarity search engine for embedding vectors.
  - Weaviate: Open source vector database written in Go.

- Pgvector because it is part of Postgres. In the end Vectors are just a data type in a databases. If you only need vectoring and not SQL then it is better to look search engine that does that I think.
  - PGVector, is like PostGIS for geospatial ops, an extension for Postgre to support vector search, it is not a true vector-native DB, so it adds operational overhead and lacks advanced features that a vector DB provides. 

- Quadrant, it's a vector DB designed specifically for RAG/AI workflows. It offers advanced filtering features (multi-tenancy) that are natively baked-in into its core architecture. These features would require more legwork using other data stores like Redis etc...
- FAISS: great for vector search is not really a DB, it's a lib and not a true DB
# discuss-code-rag
- ## 

- ## [Cline doesn't index your codebase. No RAG, no embeddings, no vector databases. : r/CLine _202505](https://www.reddit.com/r/CLine/comments/1kwhcz0/cline_doesnt_index_your_codebase_no_rag_no/)
- Does Roo Code also do that or is this just a Cline thing?
  - Roo recently added it for code search. It works with local models
  - They also added prompt caching for Gemini flash.

- Exceedingly large context windows don't just result in extreme costs but it also will slow down your every operation.
  - While prompt caching reduce costs it is not entirely free, nor does every model or provider support it. It's also like treating a symptom rather than the disease.

- RAG can be good. See how fast and precise is AUgment Code search
  - ive found Augment to miss obvious things at times

- cursor has code indexing

- RAG shines when you donâ€™t know the exact function / class names in a repo. ripgrep file search that Cline uses for context search is awesomeâ€”if you already have the right keywords. Thatâ€™s easy on your own project, but on a massive, unfamiliar codebase ripgrep can stall while RAG keeps rolling (although it can return the wrong chunks).

- Privacy issue first. Never is sending low quality windows off the code base to the LLM is bad for quality.

- ## ğŸ¤¼ Cline doesn't index your codebase. No RAG, no embeddings, no vector databases. _202505
- https://x.com/cline/status/1927226680206131530
  - [Why Cline Doesn't Index Your Codebase (And Why That's a Good Thing) - Cline Blog _202505](https://cline.bot/blog/why-cline-doesnt-index-your-codebase-and-why-thats-a-good-thing)
  - This isn't a limitation -- it's a deliberate design choice. 
  - As context windows increase, this approach enhances Cline's ability to understand your code.
  - The industry default: chunk your codebase, create embeddings, store in vector databases, retrieve "relevant" pieces.
  - But code doesn't work in chunks. A function call in chunk 47, its definition in chunk 892, the context that explains why? Scattered everywhere.
  - We believe in the agentic power of the modesl, and with Claude's 200K+ context window, we don't need clever retrieval. We need intelligent exploration.
  - So Cline reads code the way you do -- following imports, tracing dependencies, building connected understanding.
  - ğŸ’¡ Instead of indexing, Cline starts with structure. Using ASTs, it maps your codebase architecture -- classes, functions, relationships.
  - Then it explores. Need to add error handling? It traces from your function to your error utilities to similar patterns. Connected comprehension.
- The problems with RAG for code run deeper:
  - Chunking breaks the logical connections between functions and their dependencies
  - Indexes become stale the moment you push a commit
  - Your IP gets duplicated in vector embeddings (security nightmare)

- ## [Unpopular opinion: RAG is actively hurting your coding agents : r/ChatGPTCoding _202505](https://www.reddit.com/r/ChatGPTCoding/comments/1ktt4ab/unpopular_opinion_rag_is_actively_hurting_your/)
- I've been saying this since RAG first became the term used to describe the method. And you are exactly right, the whole reason it became a thing was because, back when context windows were 4k or 8k max, it was out of necessity. Now, in the age where context windows are 1M or 10M tokens, it only makes sense in specific enterprise cases where you have vast datasets to query for specific, isolated information.

- RAG isnâ€™t just calling vector stores, itâ€™s also prompt priming before generation using various sources. Dynamically priming the prompt with relevant information before the LLM generates a response. 
  - A lot of the large context models drop off in accuracy after 100k tokens, anyway.

- frontier models have large context windows, but they all equate to a token, which equals a cost
  - Knowledge Graphs + RAG. An syntax tree is constructed of the code , where each class, function , method etc become nodes in the graph. The LLM can then traverse the graph to get only what it needs.

- One thing Aider has that Roo and Cline don't, is what they call a "RepoMap" (and it ties aider to git). 
  - But the advantage of it is, given a class, it can easily determine all the related classes, so it doesn't have to go digging through folders, trying to figure out which file is actually relevant, it knows, because the RepoMap shows which classes use which classes.
  - I pulled the RepoMap class out of aider and eventually managed to remove all the aider dependencies and got running as a command-line app. I might try making an MCP with it and giving that to Cline and see if it can improve its ability to understand the app structure.
- I thought cline already had a tree sitter which did the same thing.
  - it does use tree-sitter, but not in the same way and not to the same effect. 
  - How Aider understands you program vs how Cline understands it is very apparent in working with them. Aider has a much better understanding of which bits of code are related to which other bits, whereas Cline is frequently guessing based on the filenames.
  - Using tree-sitter isn't enough. You need to actually map out the relationships and Cline doesn't seem to do this, at least not nearly as effectively as Aider does.
- using the LSP if available surley is the best approach?
  - No, beyond getting LSP diagnostic, which you can get somewhere else anyway. Using LSP server requires counting rows and columns, and LLMs are bad at it.
- but you don't have to use the interface literally
  - Then I'm not using LSP, I'm using something that uses LSP under the hood to point that is an implementation detail. Take a look at your favorite LSP server, tell me what is useful there to LLM agent besides diagnostics?
  - I can see how you give LLM diagnostics after it update the file is useful, but giving LLM access to LSP overall is IMO utterly useless.

- I was watching an interview video with Claude Code engineers and they mentioned along the lines of not using RAG or GraphRAG because they found that Claude Code performed better without it and just relying on tools, memory and agentic search.

- RAG still makes sense for latency sensitive use cases like real-time chat, although prompt caching helps with this. 

- ## [Lessons learned from implementing RAG for code generation : r/LLMDevs _202501](https://www.reddit.com/r/LLMDevs/comments/1hw1n5o/lessons_learned_from_implementing_rag_for_code/)
  - ğŸ“ [A Recipe for a Better AI-based Code Generator | Pulumi Blog _202501](https://www.pulumi.com/blog/codegen-learnings/)
  - We wrote a blog post documenting how we do retrieval augmented generation (RAG) for code generation in our AI assistant, Pulumi Copilot. 
  - Measure and tune recall (how many relevant documents are retrieved out of all relevant documents) and precision (how many of the retrieved documents are relevant)
  - Implement end-to-end testing and monitoring across development and production
  - Create self-debugging capabilities to handle common issues like type checking errors

- when I need understanding a medium sized code base I have dumped the whole thing into gemini in Google AI workbench. It can do 2 million tokens, though it is SLOOW.

- I wonder if you've considered feeding more tokens from your retrieval result into your code gen step? Why 20k? Is that always enough? How would you even know if it weren't?
  - You've got part of the answer right there: SLOOW :-) Also expensive!
  - Seriously though, you want to optimize for both recall and precision. Too much irrelevant data (poor precision) can confuse the LLM leading to hallucinations.
  - 20k is based on "it feels right" and empirical data but honestly we have not done enough analysis to conclude that it is the perfect number for all scenarios - we will continue to measure and adjust.

- Did you experiment with other retrieval methods besides or in addition to semantic similarity? I've done some work using different techniques, like parsing dependency trees out of the current file, with promising results for code RAG.
  - We did look into BM25 for FT search but did not see measurable benefits for our use cases. Our approach relies on getting a lot of documents first and then pruning - it would be better to get just what's needed in the first place, I still hope BM25 can help there. Worth another look!
# discuss-embedding
- ## 

- ## 

- ## 

- ## [Open-source embedding models: which one's the best? : r/Rag _202509](https://www.reddit.com/r/Rag/comments/1nro65j/opensource_embedding_models_which_ones_the_best/)
- tried embeddinggemma:300m and qwen3-embedding-0.6b and 4b

- [Open-source embedding models: which one's the best? : r/ollama](https://www.reddit.com/r/ollama/comments/1nro30i/opensource_embedding_models_which_ones_the_best/)
- I find the latest Qwen embeddings and reranker models the best out there today even when comparing with offerings from OpenAI and VoyageAI
  - I tried BGE, Nomic, and Qwen using ChunkHound I don't care so much about the raw numbers but about the end to end result. You can easily see the difference in results by running the chunkhound search command

- [Open-source embedding models: which one to use? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1nrgklt/opensource_embedding_models_which_one_to_use/)
- I've had my best results with bge-m3 or qwen3-embedding

- ## [Is there a classification of worst vs best ai models for RAG : r/Rag _202509](https://www.reddit.com/r/Rag/comments/1nk2rza/is_there_a_classification_of_worst_vs_best_ai/)
- For small models, most of our users have converged around: Gemma3, Qwen3, and DeepSeek
  - I always use for demos Mistral Medium 3.1 (mistral-medium-2508), which is a very good middle ground.
  - For the "large" LLMs, Gemini Flash variants and Claude Sonnet or Haiku. OpenAI models have *never* been good at this use case. The GPT-OSS models have been abysmally bad in testing.

- For an embeddings model, we've been using all-MiniLM-L6-v2 since we released TrustGraph, and haven't really seen any need to change. The platform allows you to choose any embeddings model from HF, but all-MiniLM-L6-v2 seems to do just fine in most use cases. If you want be able to try out all these model combinations, you can give them a try with TrustGraph (open source).

- gemma3 worked best in my case(turkish docs)
  - but rag mostly depends on understanding and memorization abilities

- I have tried several OpenAI models for our RAG system, and got great results using gpt-4.1-mini.
  - Via the API, it is the OpenAI model that has the largest context window right now.

- ## [embeddinggemma has higher memory footprint than qwen3:0.6b? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nbbi1c/embeddinggemma_has_higher_memory_footprint_than/)
- For whatever reason, Gemma models have a larger vocab size of 256K whereas most models have a vocab size of around 120k. This adds to the size

- confirmed, i use ollama just for embedding, download it from official ollama library. The size is 621MB on command ollama list. But getting bigger like 2.8GB on ollama ps
  - and it very slow compared to qwen embedding 4b.

- ## [Real life experience with Qwen3 embeddings? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nezmfi/real_life_experience_with_qwen3_embeddings/)
  - I need to decide on an embedding model for our new vector store and Iâ€™m torn between Qwen3 0.6b and OpenAI v3 small.
  - The qwen3 embeddings top the MTEB leaderboards scoring even higher than the new Gemini embeddings. Qwen3 has been killing it, but embeddings can be a fragile thing.

- I don't think Qwen3-Embedding-0.6B performs better than previous encoder models of similar size (e.g., bge-m3); 
  - its main advantage is long-context support. 
  - Overall, it's only a little bit better than other prior state-of-the-art LLM-based embedding models (e.g., Kalm-v2), with advantages mainly comes from instruction tuning on the query side, which improves adaptability.
- Qwen3-Embedding-4B is good. It outperforms bge by 2â€“3 points (on my own dataset, using NDCG@10), and maintains strong retrieval performance at 2â€“4k tokens per chunk. 
  - However, the GGUF version of this model seems inconsistent with the original checkpointâ€”this discrepancy is unclear (I suspect it may be related to the pooling configuration).
- Qwen3-Embedding-8B might indeed be a SOTA model, but it costs too much.

- I never tested that quant. There are so many mistakes you can make with embeddings (omitting required eot tokens, missing instructions, wrong padding alignment etc.) 

- I always include something like an embedding version so it is always possible to change embedding algo without reencoding old data so long as you are willing to do a search per algo and re-rank them.

- I've used 8B and 4B as GGUF at Q4_K_M and never had issues some are pointing.
  - Found the 4B most efficient as the difference between the 8B is small for such resource difference.
  - Been using for code bases, currently over 380 files with code. No issues.

- ## [Are Qwen3 Embedding GGUF faulty? : r/LocalLLaMA _202507](https://www.reddit.com/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/)
  - Qwen3 Embedding has great retrieval results on MTEB.
  - However, I tried it in llama.cpp. The results were much worse than competitors.

- You have to add the EOS Token manually "<|endoftext|>" as of here: https://github.com/ggml-org/llama.cpp/issues/14234

- In multilingual, I was very disappointed with qwen3 embedding compared to jinaai/jina-embeddings-v3 which remains my favorite for the moment
- v4 is out btw: https://huggingface.co/jinaai/jina-embeddings-v4
  - It does work much better, getting 48.11% on the same benchmark.
  - The official JINA API is very slow though. Half a minute for a batch of 32.

- Would you believe I was just trying it out today and it was all messed up. Swapped from Q3 4B and 0.6B to granite 278m and all my problems went away.

- Yes, though if I tried generating the embeddings through the SentenceTransformers module instead, I got the state-of-the-art results I was hoping for on my benchmark. A code snippet for how to do so is listed on their HF page.
# discuss
- ## 

- ## 

- ## 

- ## ğŸ†š [I tested local models on 100+ real RAG tasks. Here are the best 1B model picks : r/Rag _202510](https://www.reddit.com/r/Rag/comments/1o60ib6/i_tested_local_models_on_100_real_rag_tasks_here/)
- Best model by real-life file QA tasks (Tested on 16GB Macbook Air M2)
  - Iâ€™m building this local file agent for RAG - Hyperlink. The idea of this test is to really understand how models perform in privacy-concerned real-life tasks
- tests result
  - A â€” Find facts + cite sources â†’ Qwen3â€“1.7B-MLX-8bit
  - B â€” Compare evidence across files â†’ LMF2â€“1.2B-MLX
  - C â€” Build timelines â†’ LMF2â€“1.2B-MLX
  - D â€” Summarize documents â†’ Qwen3â€“1.7B-MLX-8bit & LMF2â€“1.2B-MLX
  - E â€” Organize themed collections â†’ stronger models needed
- Tasks Types (High Frequency, Low NPS file RAG scenarios)
  - Find facts + cite sources â€” 10 PDFs consisting of project management documents
  - Compare evidence across documents â€” 12 PDFs of contract and pricing review documents
  - Build timelines â€” 13 deposition transcripts in PDF format
  - Summarize documents â€” 13 deposition transcripts in PDF format.
  - Organize themed collections â€” 1158 MD files of an Obsidian note-taking user.

- ## [The â€œnew RAG every weekâ€ problem â€” do we really need to switch frameworks this often? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o764lj/the_new_rag_every_week_problem_do_we_really_need/)
  - It feels like every couple of weeks, thereâ€™s a brand-new â€œnext-generationâ€ RAG framework popping up â€” R2R, LightRAG, RAGFlow, and now a few others that just launched on GitHub.
  - But when you look closer, most of them arenâ€™t new paradigms at all.
  - Theyâ€™re slight variations of existing pipelines â€” same retrieval backbone, same chunking strategies â€” maybe with a bit of optimization in one step (say, hybrid retriever logic, graph-based context linking, or compression on long contexts).

- AI is a rapidly growing tech space. There will be 10 technologies that mildly differ each other and then there will be that 1 that will make some kind of a breakthrough. That's how it is. It's far too early for any real standards yet, and honestly that's the exciting part of following the AI space.

- ## ğŸ†š [Tested 9 RAG query transformation techniques â€“ HydE is absurdly underrated : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o6s89n/tested_9_rag_query_transformation_techniques_hyde/)
  - Your RAG system isn't bad. Your queries are.
- I just tested 9 query transformation techniques. Here's what actually moved the needle
- Top 3:
  - HydE â€“ Generate a hypothetical answer, search for docs similar to that. Sounds dumb, works incredibly well. Solves the semantic gap problem.
  - RAG-Fusion â€“ Multi-query + reranking. Simple, effective, production-ready.
  - Step-Back â€“ Ask abstract questions first. "What is photosynthesis?" before "How do C4 plants fix carbon?"
- Meh tier:
  - Multi-Query: Good baseline, nothing special
  - Decomposition: Works but adds complexity
  - Recursive: Slow, minimal quality gain for simple queries
- Key insight: You're spending time optimizing embeddings when your query formulation is the actual bottleneck.

- Many larger RAG platforms I've worked on or seen in use are making embeddings from the text to be retrieved and also generating a couple of questions that can be answered by the same chunk, saving those embeddings as well (so you have several embeddings pointing at the same chunk).
  - This performs a lot like HydE, but shifting the extra compute (generation step) to the ingestion stages instead of query time for better latency/performance in exchange for a larger index to store and query, which is usually the desired tradeoff for interactive systems.

- ## ğŸ¤” [Is RAG system actually slow because of tool calling protocol? : r/Rag _202509](https://www.reddit.com/r/Rag/comments/1ntc1ky/is_rag_system_actually_slow_because_of_tool/)
  - Just came across few wild comparison between MCP and UTCP protocols and honestly... my mind is blown.
  - For RAG systems where every millisecond counts when we are retrieving documents. UTCP is 30-40% faster performance than MCP. that's HUGE.

- Why would you use mcp in RAG to begin with?
  - The consumer of a RAG might use tools, but â€˜theâ€™ RAG components do not need tools.

- Find it funny everyone so focused on speed, but go ask gpt and wait 15min is fine

- ## ğŸ†š [Does anyone know how much of a performance difference between knowledge graphs and vector based searches? : r/LangChain _202408](https://www.reddit.com/r/LangChain/comments/1eragqk/does_anyone_know_how_much_of_a_performance/)
  - I made a pretty simple vector based RAG search, and it performs "okay" and doesn't always generate the expected results. I have the pieces for a knowledge graph, but I was wondering if people knew the expected improvements that I should expect to see by moving to knowledge graphs?

- Vector search alone is usually faster. Without any bells and whistles like metadata filtering or stepback, it's just an optimized KNN. 
  - Graph based RAG involves traversing a graph in addition to searching for the appropriate starting node, which often times is a done via vector search. 
  - The graph structure usually leads to good results though.

- ## LobeChat v1.85.0 ä¼˜åŒ–äº†å¯¹è¯ä¸Šä¼ æ–‡ä»¶çš„é€»è¾‘ï¼ŒæŠŠå¤§å®¶å‘¼å£°æ¯”è¾ƒé«˜çš„å…¨æ–‡ä¸Šä¼ çš„èƒ½åŠ›åšå¥½äº†ï¼Œä¸å†èµ° åˆ†å— + embedding çš„æœ´ç´  RAG ã€‚å¸¦æ¥çš„å¥½å¤„æ˜¯é€Ÿåº¦å˜å¿«äº†ï¼Œæ•ˆæœä¹Ÿæ›´åŠ ç†æƒ³äº†ã€‚
- https://x.com/arvin17x/status/1921101310004179082
  - æ„Ÿè§‰å¤§æ¨¡å‹å†è¿­ä»£ä¸ªä¸€å¹´ï¼Œæœ´ç´  RAG å¯èƒ½å°±è¦é€€å‡ºå†å²èˆå°äº†
  - æ¯”å¦‚ä¸€ä¸ªæ–‡ä»¶ 10w å­—çš„å†…å®¹ï¼Œå…¨æ–‡ä¸Šä¼ å°±æ˜¯10wå­—å…¨æ”¾ä¸Šä¸‹æ–‡é‡Œã€‚ RAG æ˜¯æ‰¾å‡ºæœ€ç›¸å…³çš„ N ä¸ªå†…å®¹ç‰‡æ®µï¼ˆå‡è®¾ä¸€ä¸ªç‰‡æ®µ1kï¼ŒåŠ èµ·æ¥ N kä¸ªå­—ï¼‰æ”¾ä¸Šä¸‹æ–‡é‡Œã€‚ å…¨æ–‡ä¸Šä¼ çš„æ•ˆæœåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¸€å®šæ˜¯æ¯” RAG è¦å¥½çš„ã€‚
  - ç°åœ¨é»˜è®¤ä¸Šä¼ å°±æ˜¯å…¨æ–‡ã€‚ä¸è¿‡çŸ¥è¯†åº“è¯»å–è¿˜æ˜¯åŸæ¥çš„
- ä¸ç”¨ RAGï¼Œç›´æ¥ä¼ ç»™ LLMï¼Œé’±åŒ…é¡¶ä¸ä½å•Š
- å…¨æ–‡ä¸Šä¼ ï¼Œåªæœ‰Googleå®¶æ‰æœ‰å¾ˆå¤§çš„ä¸Šä¸‹æ–‡å§ï¼Ÿå…¶ä»–å®¶æ•ˆæœå°±ä¸å¥½äº†
  - ä¸å•Šï¼Œç°åœ¨gpt4.1miniéƒ½æœ‰1Mä¸Šä¸‹æ–‡äº†ï¼Œä¹Ÿä¾¿å®œã€‚æµ‹ä¸‹æ¥æ•ˆæœéå¸¸å¥½

- ## å¼€æºçš„ llm ç³»ç»Ÿé‡Œï¼Œå¸¦çŸ¥è¯†åº“åŠŸèƒ½çš„ï¼Œç›®å‰è¿˜æ²¡æ‰¾åˆ°ä¸€ä¸ªåšçš„å¥½çš„ã€‚
- https://x.com/wwwgoubuli/status/1830227047173751206
  - ä¸è¿‡è¿™ä¹ˆè¯´å€’ä¹Ÿä¸æ˜¯è¦è´¬ä½ä»€ä¹ˆï¼Œæˆ‘è‡ªå·±å¸®å®¢æˆ·å®šåˆ¶è¿‡çš„çŸ¥è¯†åº“ä¸­ï¼Œåšçš„ç¨å¾®å¥½ä¸€ç‚¹çš„ï¼Œä¹ŸæŠ•å…¥äº†å¤§é‡çš„ç²¾åŠ›å’Œæˆæœ¬æ¥å®ç°æ•°æ®é¢„å¤„ç†ï¼Œå•å•ä¸€ä¸ªæ–‡æœ¬åˆ†æ®µéƒ½æ‰¾ä¸åˆ°é€šç”¨èŒƒå¼ï¼Œå¾—å¤§é‡å®šåˆ¶ã€‚

- é—®é¢˜å°±æ˜¯ to b é‡Œã€‚ to c é‚£ä¸€ä¸ªç”¨æˆ·ä¸¢ä¸ª pdf ç®—ä¸ä¸Šä»€ä¹ˆçŸ¥è¯†åº“ã€‚ åˆ°äº† to b  é¢†åŸŸï¼Œä½ çœ‹ç€ä¸€å † excel ï¼Œpdfï¼Œ ä½ çš„æ¢¦é­‡æ‰åˆšå¼€å§‹

- æˆ‘ä»¬è‡ªå·±åšå”®åå®¢æœæœºå™¨äººï¼Œèµ·æ­¥æ˜¯æ‰¾å®ä¹ ç”Ÿæ’¸äº†1ä¸‡æ¡é—®ç­”å¯¹ï¼Œä¸Šçº¿åæ¯å‘¨æ ¹æ®bad caseè¿­ä»£ç»§ç»­åšæ•°æ®ã€‚è¿™ç©æ„æ•ˆæœå°±æ˜¯çœ‹æ ‡æ³¨æ•°æ®ï¼Œæƒ³è¦å¥½çš„æ•ˆæœï¼Œå°±å¥½å¥½åšæ•°æ®ã€‚

- çŸ¥è¯†åº“å¦‚æœåƒç¼–ç¨‹è¯­æ³•æ‰‹å†Œä¸€æ ·ï¼Œé‚£ragæ•ˆæœä¼šéå¸¸å¥½ï¼Œå¦‚æœåƒæ•£æ–‡è¯—ï¼Œragæ•ˆæœå°±ä¼šå¾ˆå·®ã€‚æˆ‘ä»¬æ‹¿åˆ°æ‰‹çš„æ–‡æ¡£ä¸€èˆ¬ä»‹äºä¸¤è€…ä¹‹é—´ï¼Œæ‰€ä»¥æƒ³æé«˜ragæ•ˆæœï¼Œåªå¥½æŠŠæ–‡æ¡£æœ¬èº«å¾€ç¼–ç¨‹è¯­æ³•æ‰‹å†Œæ–¹å‘æ”¹

- æˆ‘ç°åœ¨æœ€å¤´ç–¼çš„æ˜¯pdfæ–‡æ¡£ï¼Œé‡Œé¢å…¨æ˜¯å›¾ç‰‡çš„é‚£ç§ã€‚ï¼Œ
  - è®©ç”²æ–¹åŠ é’±ï¼Œå›¾ç‰‡åˆ°æ–‡å­—çš„è½¬æ¢ä¸ä»…éœ€è¦å·¥å…·ï¼Œè¿˜éœ€è¦äººå·¥å®¡é˜…ï¼Œä¸åŠ é’±æ²¡æ³•å¹²
