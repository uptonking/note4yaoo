---
title: lib-excel-app-list-grid-blog-vendors
tags: [blog, excel, grid, spreadsheet, vendors]
created: 2023-11-12T06:45:18.495Z
modified: 2023-11-12T06:45:39.195Z
---

# lib-excel-app-list-grid-blog-vendors

# guide

# blogs-onlyoffice

## [ç²¾è¯»onlyofficeåœ¨çº¿è¡¨æ ¼å­˜å‚¨è®¾è®¡ - æŽ˜é‡‘](https://juejin.cn/post/7202252704978386999)

- ç”±äºŽè¡¨æ ¼åˆ†å¸ƒæœ¬è´¨ä¸Šæ˜¯m*nçŸ©é˜µï¼Œæ‰€ä»¥é—®é¢˜è½¬æ¢ä¸ºçŸ©é˜µçš„å­˜å‚¨æ–¹æ¡ˆï¼Œç›®å‰ä¸»è¦æœ‰ä¸¤ç§ï¼šäºŒç»´çŸ©é˜µå’Œç¨€ç–çŸ©é˜µã€‚

- äºŒç»´çŸ©é˜µ
  - è¿™ç§æ–¹æ¡ˆå°±æ˜¯ç”¨äºŒç»´æ•°ç»„å­˜å‚¨æ•°æ®ï¼Œé€‚åˆåœ¨çº¯æ•°æ®å±•ç¤ºåœºæ™¯ä¸‹ä½¿ç”¨ï¼Œåœ¨å¤§æ•°æ®é‡ä¸‹è¿›è¡Œæ’å…¥/åˆ é™¤æ“ä½œæ—¶æ€§èƒ½å·®ã€‚æ‰€ä»¥ä¸€èˆ¬ä¸ä½œä¸ºåœ¨çº¿è¡¨æ ¼çš„å­˜å‚¨æ–¹æ¡ˆ

- ç¨€ç–çŸ©é˜µ
  - ç¨€ç–çŸ©é˜µå­˜å‚¨æ–¹æ¡ˆæœ‰å¤šç§
- åŸºäºŽå­—å…¸ï¼šè¿™ç§å®žçŽ°æœ‰å¤šç§è¡¨è¾¾æ–¹å¼
  - é’ˆå¯¹å‰ç«¯å¼€å‘è€…ï¼Œæ¯”è¾ƒå¸¸è§çš„å†™æ³•æ˜¯ï¼š`dict[row] = {}; dict[row][col]=valueï¼›`å…¶æ‰§è¡Œæ•ˆçŽ‡å’ŒJavascriptçš„æ‰§è¡Œå¼•æ“Žæœ‰å…³ï¼Œ
  - ä»¥Chromeæµè§ˆå™¨å†…éƒ¨çš„V8ä¸ºä¾‹ï¼Œæ•°ç»„/å¯¹è±¡é»˜è®¤å°±æ”¯æŒç¨€ç–å­˜å‚¨å½¢å¼ï¼ŒæŸ¥è¯¢ã€æ’å…¥æ•ˆçŽ‡é«˜ï¼Œè€Œä¸”å¯¹è±¡çš„keyæ˜¯æœ‰åºçš„ï¼Œä½†åˆ é™¤æ“ä½œä¸‹ç”±äºŽå¼•æ“Žå†…éƒ¨è®¾è®¡å†³å®šå…¶æ•ˆçŽ‡ä¸é«˜ï¼›æ‰€ä»¥åœ¨æ•°æ®é‡ä¸å¤§æ—¶ï¼Œå¯ä»¥ä½œä¸ºè¡¨æ ¼çš„å­˜å‚¨æ–¹æ¡ˆ
- åŸºäºŽåŽ‹ç¼©ç¨€ç–åˆ—ï¼šè¿™ä¸ªæœ¬æ„æ˜¯é€šè¿‡åŽ‹ç¼©æŠ€æœ¯å‡å°‘ç©ºå•å…ƒæ ¼çš„æ•°é‡ï¼Œå®žé™…å·¥ç¨‹é‡Œæœ‰å¤šç§å®žçŽ°æ–¹å¼
  - ç”¨ä¸‰ä¸ªæ•°ç»„ï¼ˆvalueã€rowã€columnåç§»ï¼‰æ¥å­˜å‚¨çŸ©é˜µ
  - **è‘¡è„åŸŽå¼€å‘çš„SpreadJsä¹Ÿæ˜¯ä½¿ç”¨ç±»ä¼¼æ–¹æ¡ˆ**
- ä¸Šè¿°ä»‹ç»çš„å­˜å‚¨æ–¹æ¡ˆåªè€ƒè™‘valueæ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ å€¼ä¸ºæ•°å€¼çš„æƒ…å†µï¼Œ
  - ä»Žè¡¨æ ¼åŠŸèƒ½çœ‹ï¼Œæ¯ä¸ªå•å…ƒæ ¼çš„å†…å®¹è¿˜åŒ…æ‹¬å…¶ä»–ç±»åž‹ä¿¡æ¯ï¼ŒåŒ…æ‹¬å­˜å‚¨å€¼valueã€UIå±žæ€§ä¿¡æ¯ã€çŠ¶æ€æ ‡è®°ï¼ˆæ¯”å¦‚æ˜¯å¦å…¬å¼ï¼‰ã€å…¬å¼å†…å®¹ã€è¡Œ/åˆ—ä¿¡æ¯ï¼Œvalueå¯èƒ½çš„æ•°æ®ç±»åž‹åŒ…æ‹¬æ•°å­—ã€å­—ç¬¦ä¸²ã€å¤šè¡Œæ–‡æœ¬ç­‰
- å¦‚æžœç”¨æ™®é€šå¯¹è±¡å½¢å¼ä½œä¸ºæ ¼å­çš„åº•å±‚å­˜å‚¨ç»“æž„ï¼Œå—åˆ¶äºŽV8å¼•æ“Žå†…éƒ¨å†…å­˜åˆ†é…ç­–ç•¥ï¼Œå…¶å¾ˆéš¾æ”¯æ’‘åƒä¸‡é‡çº§æ ¼å­ä¸‹çš„æ“ä½œï¼Œå› æ­¤è€ƒè™‘é€šè¿‡åŽ‹ç¼©æŠ€æœ¯æ¥èŠ‚çœå†…å­˜æ¶ˆè€—

- é€šè¿‡è§‚å¯ŸCellå†…éƒ¨ç»“æž„å­—æ®µå†…å®¹ï¼Œè¿™é‡Œåˆ—å‡ºä»¥ä¸‹ç›´è§‚æƒ³æ³•
  - ä»Žæ•°æ®ç±»åž‹çœ‹ï¼ŒCellå†…éƒ¨åŒ…å«booleanã€stringã€numberä¸‰ç§åŸºæœ¬ç±»åž‹ï¼Œbooleanå’Œnumberç±»åž‹çš„æ•°æ®è€ƒè™‘é€šè¿‡å­—æ®µåŽ‹ç¼©æŠ€æœ¯ï¼Œå°†è¿™å‡ ä¸ªå­—æ®µçš„å†…å®¹ç¼–ç åœ¨ä¸€ä¸ªByteå†…ï¼Œä¾‹å¦‚ï¼štypeæ˜¯ä¸€ä¸ªæžšä¸¾ç±»åž‹ï¼Œå‡è®¾æœ‰ä¸‰ç§ç±»åž‹ï¼Œç”¨2ä¸ªbitæ¥è¡¨ç¤ºï¼ŒåŠ ä¸ŠisDirtyå’ŒisCalcç±»åž‹ï¼ŒåŸºæœ¬ä¸Šä¸€ä¸ªByteå°±èƒ½è¡¨ç¤ºä¸‰ä¸ªå­—æ®µçš„å€¼
  - ä¸€èˆ¬æ¥è¯´ä¸šåŠ¡è¡¨æ ¼å†…ä¼šåŒ…å«é‡å¤çš„å­—ç¬¦ä¸²ï¼Œæˆ–è€…å•å…ƒæ ¼ä¹‹é—´å…±äº«é»˜è®¤çš„æ ·å¼ä¿¡æ¯ï¼Œæ‰€ä»¥å‚ç…§XLSXåè®®ï¼Œå¼•å…¥å…±äº«å­—ç¬¦ä¸²è¡¨æŠ€æœ¯ï¼Œä¿è¯workbookä¸­å”¯ä¸€çš„å­—ç¬¦ä¸²ç±»åž‹å‡ºçŽ°ä¸€æ¬¡ï¼Œç„¶åŽå•å…ƒæ ¼é€šè¿‡ç´¢å¼•ï¼ˆè€Œä¸æ˜¯å†…è”æ–¹å¼å­˜å‚¨åˆ°å•å…ƒæ ¼ä¸­ï¼‰æ¥å¼•ç”¨å­—ç¬¦ä¸²ã€‚

- ä¸Šè¿°ä¹Ÿæ˜¯Onlyofficeå†…éƒ¨ä½¿ç”¨æ€è·¯ï¼Œç»“åˆçŸ©é˜µçš„ç¨€ç–åˆ—å­˜å‚¨æ€æƒ³ï¼ŒSdkå†…éƒ¨æœ€ç»ˆçš„å­˜å‚¨æ•ˆæžœå¦‚ä¸‹ï¼Œ
  - ç®€å•æ¥è¯´ï¼Œæ˜¯é€šè¿‡ç±»åž‹æ•°ç»„ï¼Œå°†è¡¨æ ¼æ•°æ®ä»¥äºŒè¿›åˆ¶æ ¼å¼å­˜å‚¨ï¼Œé€šè¿‡å­—æ®µåŽ‹ç¼©+å…±äº«å­—ç¬¦ä¸²è¡¨æ¥ä¼˜åŒ–å†…å­˜ç©ºé—´ã€‚
- è¿™é‡Œè¯´æ˜Žä¸‹å’Œä¸Šè¿°ç¨€ç–åˆ—å­˜å‚¨çš„åŒºåˆ«
  - ç”±äºŽå•å…ƒæ ¼é€šè¿‡å­—æ®µåŽ‹ç¼©æŠ€æœ¯ï¼Œæ‰€ä»¥rowæ•°ç»„æŒ‰ç…§ç´¢å¼•é¡ºåºä¾æ¬¡æŽ’åˆ—ï¼Œè€Œä¸æ˜¯åªä¿å­˜éžç©ºçš„è¡Œã€‚
  - colç´¢å¼•æ•°ç»„æ˜¯ç¨€ç–çš„ï¼Œå…¶å†…å®¹æŒ‡å‘å…·ä½“rowæ•°ç»„ï¼Œè¿™å’Œä¸Šè¿°æåˆ°çš„columnæ•°ç»„æ˜¯ä¸ä¸€æ ·ã€‚å½“ç„¶å¥½å¤„æ˜¯ç›´æŽ¥é€šè¿‡ä¸‹æ ‡è®¿é—®åˆ°rowæ•°ç»„ï¼Œç¼–ç¨‹ä¸Šæ›´ç¬¦åˆå¼€å‘è€…çš„ç›´è§‰

- å•å…ƒæ ¼ä¿¡æ¯åŽ‹ç¼©
  - é€šè¿‡äºŒè¿›åˆ¶å½¢å¼ï¼Œå°†ä¸€ä¸ªCellå†…å®¹åŽ‹ç¼©æˆå¦‚ä¸‹ç»“æž„
  - ä¸€ä¸ªCellçš„åŸºæœ¬ä¿¡æ¯å ç”¨äº†17Bï¼Œå³ä½¿æ˜¯ç©ºçš„Cellã€‚
  - å¦‚æžœå•å…ƒæ ¼å®žé™…æ•°æ®æ˜¯Numberç±»åž‹ï¼Œé‚£ä¹ˆæ•°æ®ç›´æŽ¥å­˜å‚¨åœ¨Cellç»“æž„å†…ï¼›
  - å¦‚æžœæ˜¯å­—ç¬¦ä¸²ç±»åž‹ï¼Œé‚£ä¹ˆé€šè¿‡ç´¢å¼•+SSTè¡¨æ¥è¡¨ç¤ºä¸€ä¸ªå­—ç¬¦ä¸²
  - æ ·å¼ä¸‹æ ‡ã€å…¬å¼ç´¢å¼•ä¸‹æ ‡ä¹Ÿæ˜¯ç±»ä¼¼çš„ï¼Œå®ƒä»¬éƒ½å­˜åœ¨ç±»ä¼¼SSTçš„è¡¨ï¼ŒåŒæ ·å®žçŽ°å†…å­˜åŽ‹ç¼©ç›®çš„ã€‚
  - å…³äºŽCellå†…éƒ¨çŠ¶æ€ä¿¡æ¯ï¼Œé€šè¿‡å­—æ®µåŽ‹ç¼©æŠ€æœ¯ï¼Œäº‹å…ˆæžšä¸¾æ‰€æœ‰çŠ¶æ€ä¿¡æ¯çš„å¯èƒ½å€¼ï¼Œç¡®å®šæ¯ä¸ªçŠ¶æ€å ç”¨çš„bitå¤§å°ï¼Œç„¶åŽçº¦å®šå¥½åè®®åœ¨å•ä¸ªByteå†…ç¼–ç å„ä¸ªçŠ¶æ€ä½ç½®å’Œbitå¤§å°ï¼Œæœ€åŽé€šè¿‡ä½æ“ä½œä»ŽByteå†…è®¾ç½®/èŽ·å–å¯¹åº”çš„çŠ¶æ€ä¿¡æ¯ã€‚

- å•å…ƒæ ¼åˆ—è¡¨
  - ä¸ºäº†å­˜å‚¨å•å…ƒæ ¼åˆ—è¡¨ï¼Œonlyofficeå†…éƒ¨å¼€å‘SheetMemoryæ•°æ®ç»“æž„ï¼Œé€šè¿‡Jsæä¾›çš„ç±»åž‹æ•°ç»„ï¼ˆTypeArrayï¼‰ç”¨æ¥å­˜å‚¨æŸæ®µè¿žç»­çš„äºŒè¿›åˆ¶æ ¼å¼æ•°æ®ï¼ŒåŒ…æ‹¬æŸè¡Œ/æŸåˆ—ä¸‹å•å…ƒæ ¼åˆ—è¡¨ã€‚
- å…³äºŽSheetMemoryçš„è®¾è®¡è¯»è€…å¯ä»¥å…ˆå°è¯•æƒ³ä¸‹éœ€è¦è€ƒè™‘çš„å› ç´ ï¼Œè¿™é‡Œæ ¹æ®è¡¨æ ¼ä¸Šå±‚æ”¯æŒçš„åŠŸèƒ½ï¼Œæ€»ç»“å¦‚ä¸‹è€ƒè™‘è¦ç‚¹ï¼š
  - æ”¯æŒçµæ´»æ‰©å®¹ï¼šç”±äºŽè¡¨æ ¼ç¨€ç–ç‰¹æ€§ï¼Œå†³å®šäº†è°ƒç”¨è€…å¯ä»¥åœ¨ä»»ä½•ä¸€ä¸ªCellä½ç½®è®¾ç½®å†…å®¹ï¼›æ‰€ä»¥å¦‚æžœè®¾ç½®çš„Cellè¡Œ/åˆ—ç´¢å¼•è¶…è¿‡å½“å‰æ•°æ®ç»“æž„å¤§å°æ—¶ï¼Œå°±è¦æ‰©å®¹ï¼Œé‚£ä¹ˆåœ¨å…·ä½“ç¼–ç æ—¶å°±è¦è€ƒè™‘æ‰©å®¹çš„ç­–ç•¥
  - æ”¯æŒæ‰¹é‡æ’å…¥/åˆ é™¤å•å…ƒæ ¼ï¼šæƒ³è±¡ä¸‹ç”¨æˆ·æ‰¹é‡æ’å…¥/åˆ é™¤å¤šè¡Œæ•°æ®ï¼Œè¿™ä¸ªè¡Œä¸ºå¯¹åº•å±‚å­˜å‚¨çš„è¦æ±‚å°±æ˜¯åœ¨æŒ‡å®šä½ç½®æ’å…¥å¤šä¸ªç©ºçš„è®°å½•
  - æ”¯æŒæ‹·è´æŸä¸ªèŒƒå›´å†…çš„æ•°æ®ï¼Œæ¯”å¦‚ç”¨æˆ·é€šè¿‡æ‹·è´/ç§»åŠ¨æŸä¸ªåˆ—çš„æ•°æ®åˆ°å¦ä¸€ä¸ªåˆ—ï¼Œæˆ–è€…æŸä¸ªåˆ—è¿›è¡ŒæŽ’åºæ“ä½œç»“æŸåŽäº¤æ¢å•å…ƒæ ¼åŒºåŸŸ
  - åœ¨æŒ‡å®šæ•°æ®ç±»åž‹ä¸‹ï¼Œæ”¯æŒè®¾ç½®/èŽ·å–æŸä¸ªä½ç½®çš„æ•°æ®ï¼šæ•°æ®ç±»åž‹æœ‰UInt8ã€Uint32ã€Uint64ç­‰ï¼Œæ‰€ä»¥éœ€è¦æä¾›APIæ–¹ä¾¿ä¸Šå±‚è°ƒç”¨è®¾ç½®Cellæ•°æ®

- æ‰©å®¹ç­–ç•¥æ˜¯åœ¨ï¼ˆå½“å‰æ•°æ®é‡å¤§å°*1.5å€ï¼‰å’Œindexä¹‹é—´å–æœ€å¤§å€¼ï¼Œä¸ºä½•æ˜¯1.5ï¼Ÿ
  - è¿™å’Œåˆ†é…æ—¶é—´å¤æ‚åº¦æœ‰å…³ï¼Œå‡è®¾å½“å‰æ•°æ®å¤§å°è¦æ‰©å¤§åˆ°maxIndexä½ç½®ï¼Œå¦‚æžœæŒ‰ç…§structSizeå—å¤§å°è¿›è¡Œæ‰©å®¹ï¼Œé‚£ä¹ˆå†…éƒ¨ä¼šæŒ‰ç…§çº¿æ€§é€’å¢žè¿›è¡Œæ‰©å¤§ï¼Œæ—¶é—´å¤æ‚åº¦æ˜¯O(N)ï¼›
  - æŒ‰ç…§æŒ‡æ•°å€æ•°ï¼ˆæ¯”å¦‚2å€ï¼‰æ‰©å®¹ï¼Œé‚£ä¹ˆæ‰§è¡Œlog(N)æ¬¡æ•°å³å¯ï¼Œæ—¶é—´å¤æ‚åº¦æœ€ä¼˜ã€‚
  - ä½†æ˜¯è€ƒè™‘åˆ°2å€çš„æ‰©å®¹ç­–ç•¥å¯èƒ½ä¼šä½¿å†…å­˜æµªè´¹æ¯”è¾ƒå¤šï¼Œæ‰€ä»¥åœ¨æ—¶é—´å’Œç©ºé—´ä¹‹é—´åšäº†æƒè¡¡ï¼Œå–1.5ä½œä¸ºæ‰©å®¹æŒ‡æ•°ã€‚

- onlyofficeè®¾è®¡çš„è¿™å¥—å­˜å‚¨æ–¹æ¡ˆæ”¯æŒåœ¨åƒä¸‡ä¸ªæ ¼å­ä¸‹å†…å­˜æ¶ˆè€—è¶³å¤Ÿå°‘ï¼Œè¿™å°±ä¸ºè¡¨æ ¼ä¸Šå±‚è®¡ç®—ã€æ¸²æŸ“çŽ¯èŠ‚çš„ç¼“å­˜æœºåˆ¶æä¾›æ›´å¤šå†…å­˜ç©ºé—´ã€‚
  - è¯¥æ–¹æ¡ˆä¹Ÿè¯´æ˜Žäº†åœ¨è®¾è®¡æ•°æ®ç»“æž„æ—¶è¦ç»“åˆè¯­è¨€ç‰¹æ€§æ¥ä¼˜åŒ–ï¼Œç‰¹åˆ«æ˜¯å‰ç«¯åº”ç”¨ç¨‹åºæ¶‰åŠå¤§æ•°æ®é‡å­˜å‚¨ï¼Œè¦è€ƒè™‘è„šæœ¬æ‰§è¡Œå¼•æ“Žçš„é™åˆ¶ï¼Œäº†è§£å¼•æ“Žå†…éƒ¨çš„å†…å­˜ç®¡ç†æœºåˆ¶ï¼Œæ–¹ä¾¿åº”ç”¨å±‚ç¼–å†™æ›´é«˜æ€§èƒ½çš„ç¨‹åºã€‚

## [onlyofficeè¡¨æ ¼å­—ä½“æ¸²æŸ“å®žçŽ°æ€è·¯ - æŽ˜é‡‘](https://juejin.cn/post/7175924445490446393)

- å­—ä½“æ¸²æŸ“æ˜¯æµè§ˆå™¨è‡ªå¸¦çš„åŸºæœ¬èƒ½åŠ›ï¼Œä¸ç®¡æ˜¯åŸºäºŽHTMLè¿˜æ˜¯Canvasæ¸²æŸ“æŠ€æœ¯ï¼›
- é’ˆå¯¹å¤§åž‹Excelè¡¨æ ¼ï¼Œonlyofficeå†…éƒ¨ä½¿ç”¨CanvasæŠ€æœ¯æ¥æ¸²æŸ“è¡¨æ ¼å†…å®¹
- çŽ°ä»£æµè§ˆå™¨å®žçŽ°Canvaså†…å­—ä½“è®¾ç½®å’Œæ¸²æŸ“çš„æ–¹æ¡ˆæœ‰æˆç†Ÿçš„APIç›´æŽ¥ä½¿ç”¨ï¼Œä»£ç ç®€å†™å¦‚ä¸‹ï¼š

```JS
const canvas = document.createElement('canvas');
const ctx = canvas.getContext('2d');
ctx.font = 'å¾®è½¯é›…é»‘';
ctx.fillText('ä¸­', 0, 0)
```

- è¿™ç§æ–¹æ¡ˆèƒ½æ»¡è¶³å¤§éƒ¨åˆ†åœºæ™¯éœ€æ±‚ï¼Œä¸è¿‡ä¸åŒæµè§ˆå™¨åœ¨å­—ä½“è§£æžå’Œæ¸²æŸ“å®žçŽ°å­˜åœ¨å·®å¼‚ï¼Œè¿™ä¼šå‡ºçŽ°ä¸€è‡´æ€§é—®é¢˜
  - åŒæ ·çš„å­—ç¬¦ï¼Œåœ¨ä¸åŒæ“ä½œç³»ç»Ÿã€ä¸åŒæµè§ˆå™¨å±•ç¤ºæ•ˆæžœå¯èƒ½ä¸ä¸€è‡´
  - å¦‚æžœç¼–è¾‘å™¨äº§å“æ–¹æ¡ˆè¦è€ƒè™‘æµè§ˆå™¨å…¼å®¹æ€§å’Œæ¸²æŸ“ä¸€è‡´æ€§é—®é¢˜æ—¶ï¼Œé‚£ä¹ˆç¼–è¾‘å™¨å†…éƒ¨å°±è¦æŽ¥ç®¡å­—ä½“çš„è§£æžå’Œæ¸²æŸ“å·¥ä½œã€‚

- onlyofficeæ•´ä½“æ–¹æ¡ˆç±»ä¼¼æµè§ˆå™¨åº•å±‚æ–‡æœ¬æ¸²æŸ“å¼•æ“Žçš„å®žçŽ°æ€è·¯ï¼Œæµè§ˆå™¨å†…éƒ¨å±•ç¤ºæ–‡æœ¬çš„æ•´ä½“æµç¨‹åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼š
  - æŸ¥æ‰¾å­—ä½“ï¼šæ ¹æ®CSS font-faceè¯­æ³•æŒ‡å®šä¸€ç³»åˆ—å­—ä½“åˆ—è¡¨ï¼ŒæŸ¥æ‰¾æ—¶æŒ‰ç…§é¡ºåºéåŽ†åˆ—è¡¨ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªç¬¦åˆæ¡ä»¶ï¼ˆæ¯”å¦‚æœ¬åœ°æ˜¯å¦æœ‰è¯¥å­—ä½“çš„TTFæ–‡ä»¶ï¼‰çš„å­—ä½“
  - åŠ è½½å­—ä½“ï¼šåŠ è½½ç¬¦åˆæ¡ä»¶çš„æœ¬åœ°å­—ä½“æ–‡ä»¶
  - æ¸²æŸ“å­—ä½“ï¼šè¿™ä¸ªæ­¥éª¤æµè§ˆå™¨è°ƒç”¨OSæä¾›çš„æ–‡æœ¬æŽ’ç‰ˆå¼•æ“Žï¼Œç„¶åŽè°ƒç”¨æµè§ˆå™¨çš„æ¸²æŸ“å¼•æ“Žï¼ˆä¸€èˆ¬æ˜¯CGå›¾å½¢æŽ¥å£ï¼‰ç›´æŽ¥ç»˜åˆ¶ï¼Œä¸åŒOSå®žçŽ°äº†ä¸åŒæ–‡æœ¬æŽ’ç‰ˆå¼•æ“Žï¼Œæ¯”å¦‚IOSæä¾›CoreTextï¼ŒWindow7åŽæä¾›DirectWriteå¼•æ“Žï¼Œæ¯ç§æŽ’ç‰ˆå¼•æ“Žéƒ½æ˜¯å„è‡ªè‡ªç ”çš„ã€‚
- è¯»åˆ°è¿™é‡Œäº†è§£åˆ°å­—ç¬¦å±•ç¤ºçš„å…¨æµç¨‹ï¼Œä½†æ˜¯ç½‘é¡µåŒ…æ‹¬å¾ˆå¤šå­—ç¬¦ï¼Œè¿™äº›æ˜¯é€šè¿‡HTMLæ ‡ç­¾å’ŒCSSæ¥ç»„ç»‡ç»“æž„ï¼Œæœ€ç»ˆé€šè¿‡æµè§ˆå™¨å¸ƒå±€å¼•æ“Žå†³å®šæ–‡æœ¬ä½ç½®ï¼Œè€Œå¸ƒå±€å¼•æ“Žå†…éƒ¨æ¶‰åŠåˆ°æ–‡æœ¬æŽ’ç‰ˆç›¸å…³ä¹Ÿæ˜¯è°ƒç”¨äº†OSæä¾›çš„æ–‡æœ¬æŽ’ç‰ˆAPIå®žçŽ°ã€‚

- æœ¬æ–‡ä»‹ç»onlyofficeè¡¨æ ¼ç¼–è¾‘å™¨å†…éƒ¨æ–‡å­—æ¸²æŸ“çš„åŸºæœ¬æµç¨‹ï¼Œ
  - ä¸ºäº†æ•ˆæžœä¸€è‡´æ€§è€ƒè™‘ï¼Œå®ƒå¹¶æ²¡æœ‰ç›´æŽ¥è°ƒç”¨æµè§ˆå™¨å†…éƒ¨æä¾›çš„fillTextè¿™æ ·APIï¼Œè€Œæ˜¯åŸºäºŽFreeTypeå­—ä½“åº“é‡æ–°å®žçŽ°äº†ä¸€å¥—å…¨ç”Ÿå‘½å‘¨æœŸçš„æ¸²æŸ“é€»è¾‘ï¼ŒåŒ…æ‹¬å­—ä½“æŸ¥æ‰¾ã€åŠ è½½å’Œæ¸²æŸ“çš„æµç¨‹ï¼›
  - é€šè¿‡å‰åŽç«¯ååŒï¼ŒæœåŠ¡ç«¯äº‹å…ˆå‡†å¤‡å­—ä½“çš„å…ƒä¿¡æ¯ã€å­—ä½“é›†åˆ—è¡¨ã€å­—ä½“ç´¢å¼•åŒºé—´è¡¨ä¿¡æ¯å¹¶è¿”å›žç»™å‰ç«¯é¡µé¢ï¼Œç„¶åŽé¡µé¢å†…éƒ¨æ ¹æ®æŒ‡å®šçš„æ–‡å­—å’Œè®¾ç½®çš„å­—ä½“é›†ï¼ŒæŸ¥æ‰¾å¯¹åº”çš„å­—ä½“æ–‡ä»¶ï¼Œé€šè¿‡FreeTypeåº“æ¸²æŸ“å¾—åˆ°è¯¥æ–‡å­—å¯¹åº”çš„ä½å›¾ä¿¡æ¯ï¼Œæœ€ç»ˆç»˜åˆ¶è¯¥ä½å›¾åˆ°Canvasç”»å¸ƒä¸Šä»¥å±•ç¤ºæ–‡å­—

- å½“ç„¶è¿™å¥—é€»è¾‘æ¯”è¾ƒå®šåˆ¶åŒ–ï¼Œè¿™éƒ¨åˆ†æºç ä¸é‚£ä¹ˆæ¸…æ™°æ˜“æ‡‚ï¼Œè€Œä¸”FreeTypeæ¸²æŸ“å¼•æ“Žå’Œæ“ä½œç³»ç»Ÿè‡ªå¸¦çš„æ–‡å­—æ¸²æŸ“å¼•æ“Žå·®å¼‚å¯èƒ½ä¼šå‡ºçŽ°åŒæ ·çš„æ–‡å­—æ¸²æŸ“å‡ºç»†å¾®å·®å¼‚ï¼Œè¿™ç»™ç”¨æˆ·ä½“éªŒä¸Šä¹Ÿå¸¦æ¥å›°æ‰°ï¼Œæ‰€ä»¥ä¸Šè¿°æŠ€æœ¯å®žçŽ°æ€è·¯åœ¨å¤§éƒ¨åˆ†ä¸šåŠ¡åœºæ™¯æ˜¯ä½¿ç”¨ä¸åˆ°çš„ã€‚
  - ä½†ä¸å¦¨ä½œä¸ºä¸€æ¬¡å­¦ä¹ æœºä¼šäº†è§£æ–‡å­—æ¸²æŸ“åº•å±‚æ€è·¯ï¼Œä»¥åŠwasmæŠ€æœ¯çš„åº”ç”¨ï¼Œä¹Ÿèƒ½å¸®åŠ©ç†è§£åŸºäºŽCanvasæŠ€æœ¯å†…éƒ¨çš„æ–‡å­—æ¸²æŸ“åŽŸç†

## [onlyoffice webExcelæ•´ä½“æž¶æž„è§£è¯» - æŽ˜é‡‘](https://juejin.cn/post/7139915650964815909)

- æœ€è¿‘ç”±äºŽä¸šåŠ¡éœ€è¦ï¼Œéœ€è¦ä¸€ä¸ªweb-excelå¹³å°ç»™ç”¨æˆ·æä¾›æ•°æ®åˆ†æžèƒ½åŠ›ï¼Œç»è¿‡ä¸€ç•ªè¯„ä¼°ï¼Œç¡®å®šä½¿ç”¨onlyofficeæä¾›çš„web-excelèƒ½åŠ›
- æ–‡æ¡£ç®¡ç†æœåŠ¡
- ååŒæœåŠ¡
- è½¬æ¢æœåŠ¡

## [ç²¾è¯»ã€ŠWOPIåè®®ã€‹ - æŽ˜é‡‘](https://juejin.cn/post/7105322391597187103)

- WOPIæ˜¯å¾®è½¯åŸºäºŽREST APIçš„åè®®ï¼Œå®šä¹‰äº†ä¸€ç»„Httpæ“ä½œï¼Œä½¿å®¢æˆ·ç«¯èƒ½å¤Ÿè®¿é—®å’Œæ”¹å˜æœåŠ¡å™¨å­˜å‚¨çš„æ–‡ä»¶ã€‚
- å‡è®¾å¼€å‘è€…åœ¨Hostæœºå™¨ä¸Šéƒ¨ç½²æŸä¸ªä¸šåŠ¡WebæœåŠ¡ï¼ŒæŸå¤©äº§å“æåˆ°è¦åœ¨è¿™ä¸ªWebæœåŠ¡ä¸Šå±•ç¤ºå’Œç¼–è¾‘Excelæ–‡ä»¶ï¼Œè¿™ä¸ªéœ€æ±‚è§£å†³æ–¹æ¡ˆç›®å‰ä¸»è¦æœ‰ä¸¤ç§ï¼š
  - åˆ©ç”¨Javascript SDKï¼Œä»¥çº¯å‰ç«¯æ–¹æ¡ˆæ‰“å¼€è¯¥Excelæ–‡ä»¶ï¼Œè¿™ç±»åº“åŒ…æ‹¬LuckySheetã€SpreadJSç­‰
  - é›†æˆå·²æœ‰çš„åœ¨çº¿officeå¹³å°ï¼Œæ¯”å¦‚å¾®è½¯æä¾›äº†Office Online App Serverå¹³å°ï¼Œå…è®¸ç¬¬ä¸‰æ–¹é›†æˆä¸šåŠ¡ç›´æŽ¥åœ¨ç½‘é¡µä¸­ä»¥Iframeæ–¹å¼åµŒå…¥Officeé¡µé¢ï¼ŒOfficeé¡µé¢å†…éƒ¨æ‰“å¼€æŒ‡å®šçš„Excelæ–‡æ¡£
- ç¬¬äºŒç§å°±æ˜¯æœ¬æ–‡è¦è®¨è®ºå†…å®¹ä¾èµ–çš„å‰æï¼Œå®ƒçš„å¥½å¤„æ˜¯å¯¹å‰ç«¯å¼€å‘è€…è€Œè¨€é›†æˆæˆæœ¬ä½Žï¼Œåªéœ€è¦é€šè¿‡IframeåµŒå…¥åˆ°Hosté¡µé¢å³å¯ã€‚é‚£Officeé¡µé¢æ˜¯å¦‚ä½•çŸ¥é“åŽ»å“ªé‡Œæ‰“å¼€èŽ·å–åˆ°æ–‡æ¡£å†…å®¹ï¼Œæ–‡æ¡£ä¿¡æ¯æ˜¯æ€Žä¹ˆå‘ŠçŸ¥ç»™åœ¨çº¿Officeå¹³å°ï¼Ÿè¿™å°±æ˜¯WOPIåè®®è§£å†³çš„é—®é¢˜ã€‚
- WOPIåè®®çº¦å®šäº†Office OnlineæœåŠ¡å’Œé›†æˆä¸šåŠ¡ä¾§ä¹‹é—´çš„é€šä¿¡åè®®ï¼Œåè®®çº¦å®šäº†ä¸€ç»„æŽ¥å£æ“ä½œï¼ŒæŒ‡æ˜Žæ€Žä¹ˆä»Žé›†æˆä¸šåŠ¡æ–¹èŽ·å–å’Œæ”¹å˜æ–‡ä»¶ï¼Œè¯¥æ“ä½œåŸºäºŽRESTåè®®ï¼Œè¿™æ ·å¯¹é›†æˆæ–¹è€Œè¨€åªè¦æä¾›äº†è¿™äº›æŽ¥å£å®žçŽ°å³å¯ï¼Œå¼€å‘æˆæœ¬å¯æŽ§ã€‚

- åœ¨æ–‡æ¡£ç¼–è¾‘å™¨é¢†åŸŸå¾®è½¯æ˜¯ä¸šç•Œçš„æ ‡æ†ï¼Œå®ƒè®¾è®¡äº†å¥½å‡ ç§å¼€æ”¾åè®®ï¼ŒåŒ…æ‹¬æœ¬æ–‡è®¨è®ºçš„WOPIåè®®ï¼Œè¿˜æœ‰åŽæ¥è€…Vscodeä¸ºäº†æ”¯æŒå¤šè¯­è¨€çš„è¯­æ³•è¡¥å…¨åŠŸèƒ½æå‡ºäº†LSPåè®®ç­‰ï¼Œè¿™äº›å¹³å°è®¾è®¡æ€è·¯æ˜¯ç±»ä¼¼çš„ï¼šå¸Œæœ›å¼€å‘è€…èƒ½åŸºäºŽåè®®å…±å»ºå¹³å°çš„ç”Ÿæ€ï¼Œæ”¾å¤§å¹³å°çš„ä»·å€¼å’Œäº§å“ç”Ÿå‘½åŠ›
# blogs

## ðŸ“ðŸ“ˆ [Making GRID's spreadsheet engine 10% faster_202310](https://alexharri.com/blog/grid-engine-performance)

- GRID's product sports a feature-complete spreadsheet engine running in the browser, with advanced features such as spilling, iterative calculation, and the QUERY function. It's a beaut.

- Profiling the recalculation, about 12.5% of the time was spent in a method called _makeCalcCellEvaluationContext.
- The cost of recalculation can be split into two distinct parts:
  - Determining which cells to recalculate, and in which order.
  - Recalculating cells.
- The recalculation of cells can further be split up into 
  - the fixed cost associated with recalculating a cell, 
  - and the variable cost associated with recalculating a cell.
- The variable cost is more immediately obvious: A cell invoking an expensive function like QUERY on a large dataset will take longer to recalculate than a cell adding two numbers together.
  - For the most part, the variable cost is derived from how expensive the cell's user-written formula is.
- The fixed cost arises from setting up the context needed to evaluate the formula. 
  - For example, when evaluating a reference like A1, the engine needs a bit of context to know which workbook and sheet to resolve the reference to.
  - there's other contextual information that the engine may require during recalculation. For example: structured references, mode-specific functions

- When recalculating a cell, the evaluation context is created and passed to a function that evaluates the cell's formula (more specifically, evaluates the formula's AST).
  - Whether a piece of information is used during evaluation depends entirely on the cell's formula, and the functions it invokes. By computing all properties ahead of time, we expend a fixed amount of effort for a variable amount of benefit.
- The first way to do less work is to lazily compute information, which we implemented through the use of getters.
- By only creating a single shared evaluation context object, we avoid spreading the workbook into a new object repeatedly â€” which also creates less work for the garbage collector.

- Conclusion
- Aside from the positive effect this change had on GRID's performance, I think it serves as a useful example to think about performance:
  - Which information do we need to evaluate now, and which can we evaluate later?
  - What is the fixed cost associated with performing this operation?
  - Do we need to do this work in the first place?
  - Can we cache the result of this operation? How does that impact memory usage?
- Bear in mind that changes yielding a performance boost in some cases might cause degraded performance in others. Consider the worst case scenario and the circumstances under which it might occur.

## ðŸ‘¥ [Making GRID's spreadsheet engine 10% faster | Hacker News_202310](https://news.ycombinator.com/item?id=37842914)

- I wonder how exactly do you name this category of spreadsheet app for SEO purposes? When i search top spreadsheet apps or Excel alternatives, apps like yours donâ€™t often show up, instead I get AirTable, SmartSheets and other general spreadsheet apps that are not calculation intensive. 
- When we set out to build GRID, we quickly realized that what people use spreadsheets for can broadly be split in two categories: Models and tables.
  - Models are free-form sheets with formulas that typically generate insights in the form of output based on a set of inputs.
  - Tables are the structured sheets that typically have (somewhat) defined columns detailing the attributes of each row of data. Insights from tables generally come from filtering, grouping, aggregating and sorting the row-level data.
- As you point out, a lot more attention has been paid to the table use-case where people are essentially using their spreadsheets as small databases. 
  - Historically you could say this the usage pattern that gave birth to the modern BI tools (such as Qwik, Tableau and PowerBI). 
  - More recently, this is where companies like Airtable and Smartsheet play, but also to some extent what Coda and Notion are trying to achieve with their tables/databases.
- The models are where GRID shines. The unique value GRID offers is the ability to easily create and publish an interactive web document on top of spreadsheet calculations, whether the models are built in Excel, Google Sheets or in our own built-in GRID Sheets.
- I don't think this specific category has a name, but more broadly we've seen people talk about "next-gen spreadsheets" where - in addition to GRID - Equals, Rows and Spreadsheet.com also play, each with their own unique flavor.
- GRID is clearly the solution that leans most heavily into models and interactivity (hence the focus on our engine as evidenced e.g. by alexharri's article).
- While all the other solutions have capable calculation engines, I think it's fair to say that they are more focused on table-like use cases. 
  - Equals leans heavily into data connectivity, governance and reusability of spreadsheets, while Spreadsheet.com focuses a lot on project management use cases. Both offer an otherwise very traditional spreadsheet user experience. 
  - Rows has also focused a lot on connectivity, but lately they have leant heavily into AI to generate insights from data in their spreadsheets.

- Good write-up. What was understated but I appreciate was that this optimization was done late and after being picked up via profiling. For devs there is sometimes a drive to do this kind of work early. It really should always be done late just as in this post.

- This engine is closed source, it is used to power grid.is where you can make interactive blog posts, embeddable interactive graphs and so on.

## ðŸ“ðŸ“ˆ [Causal: Scaling our Spreadsheet Engine from Thousands to Billions of Cells - The Causal Blog_202207](https://www.causal.app/blog/scaling)

- [Scaling Causal's Spreadsheet Engine from Thousands to Billions of Cells: From Maps to Arrays](https://sirupsen.com/causal)

- Causal is a spreadsheet built for the 21st century to help people work better with numbers. 
- Behind Causalâ€™s innocent web UI is a complex calculation engine â€” an interpreter that executes formulas on an in-memory, multidimensional database. 
  - The engine sends the result from evaluating expressions like `Price * Units` to the browser. 
  - The engine calculates the result for each dimension such as time, product name, country
- In the early days of Causal, the calculation engine ran in Javascript in the browser, but that only scaled to 10, 000s of cells. 
  - So we moved the calculation engine out of the browser to a Node.js service, getting us to acceptable performance for low 100, 000s of cells. 
  - In its latest and current iteration, we moved the calculation engine to Go, getting us to 1, 000, 000s of cells.
- But every time we scale up by an order of magnitude, our customers find new use-cases that require yet another order of magnitude more cells!
- how can we scale the calculation engine 100x, from millions to billions of cells?
- In summary: by moving from maps to arrays. 

- To understand, we have to explain two concepts from Causal that help keep your spreadsheet organized: dimensions and variables.
  - We might have a variable "Sales'â€ that is broken down by the dimensions "Product" and "Country". 

- ðŸ‘£ Iteration 1: `map[int]*Cell`, 30m cells in ~6s âŒ 
  - In a first iteration we might represent Sales and its cells with a map. 
  - The integer index would be the dimension index to reference a specific cell.
  - An ostensible benefit of the map structure is that if a lot of cells are 0, then we donâ€™t have to store those cells at all. In other words, this data structure seems useful for sparse models.
  - a nice property of the map is that it allows us to build sparse models with lots of empty cells. 
  - how do hashmaps work? You hash a key to find the bucket that this key/value pair is stored in. In that bucket, you insert the key and value. When the average size of the buckets grows to around ~6.5 entries, the number of buckets will double and all the entries will get re-shuffled (fairly expensive, and a good size to pre-size your maps)
  - by far the most expensive are these random memory reads that the map entails(ä½¿æˆä¸ºå¿…ç„¶)
  - in this random-memory-read heavy world of using a hashmap that stores pointers, we canâ€™t trust that cells will be adjacent. This is enormously wasteful for our precious memory bandwidth.
- ðŸ‘£ Iteration 2: `[]Cell`, 30m cells in ~400ms
  - In the napkin math reference, random memory reads are ~50x slower than sequential access. A huge reason for this is that the CPUâ€™s memory prefetcher cannot predict memory access.
  - We could consider mapping the index for the cells into a large, pre-allocated array. Then cell access would be just a single random-read of 50ns! 
  - This means that the CPU can be smart and prefetch memory because it can reasonably predict what weâ€™ll access next.
- ðŸ‘£ Iteration 3: Threading, 250ms ðŸ¤”
  - Generally, we expect threading to speed things up substantially as weâ€™re able to utilize more cores. However, in this case, weâ€™re memory bound, not computationally bound.
  - if youâ€™re memory bound, you only need about ~3-4 cores to exhaust memory bandwidth. More wonâ€™t help much. But they do help, because a single thread cannot exhaust memory bandwidth on most CPUs.
  - When implemented however, we only get a 0.6x speedup (400ms â†’ 250ms), and not a 3x speed-up (130ms)? I am frankly not sure how to explain this ~120ms gap.
  - Either way, we definitely seem to be memory bound now. Then thereâ€™s only two ways forward: (1) Get more memory bandwidth on a different machine, or (2) Reduce the amount of memory weâ€™re using. Letâ€™s try to find some more brrr with (2).
- ðŸ‘£ Iteration 4: Smaller Cells, 88 bytes â†’ 32 bytes, 70ms ðŸ˜
  - The cell stores things like formulas, but for many cells, we donâ€™t actually need the formula stored with the cell. For most cells in Causal, the formula is the same as the previous cell. 
  - By more carefully writing the calculation engineâ€™s interpreter to keep track of the context, we should be able to remove various pointers to e.g. the parent variable.
  - As a general pattern, we can reduce the size of the cell by switching from an `array of structs` design to a `struct of arrays` design, in other words, if weâ€™re in a cell with index 328, and need the formula for the cell, we could look up index 328 in a formula array. These are called parallel arrays. Even if we access a different formula for every single cell the CPU is smart enough to detect that itâ€™s another sequential access. This is generally much faster than using pointers.
  - None of this is particularly hard to do, but it wasnâ€™t until now that we realized how paramount this was to the engineâ€™s performance! 
  - Unfortunately, the profiler isn't yet helpful enough to tell you that reducing the size of a struct below that 64-byte threshold can lead to non-linear performance increases. You need to know to use tools like pahole(1) for that.
- ðŸ‘£ Iteration 5: `[]float64` w/ Parallel Arrays, 20ms ðŸ¤¤
  - We get to ~2.4s for 1B cells by moving allocations into the threads that actually need them
  - However, localizing allocations start to get into a territory(èŒƒå›´ï¼Œé¢†åŸŸ) of what would be quite hard to implement generically in realityâ€”so weâ€™ll stop around here until we have the luxury of this problem being the bottleneck. 
- ðŸ‘£ Iteration N: SIMD, compression, GPU â€¦
  - there are lots of optimizations we can do. Goâ€™s compiler currently doesnâ€™t do SIMD, which allows us to get even more memory bandwidth. 
  - Another path for optimization thatâ€™s common for number-heavy programs is to encode the numbers, e.g. delta-encoding. Because weâ€™re constrained by memory bandwidth more than compute, counter-intuitively, compression can make the program faster. Since the CPU is stalled for tons of cycles while waiting for memory access, we can use these extra cycles to do simple arithmetic to decompress.
  - Another trend from the AI-community when it comes to number-crunching too is to leverage GPUs. These have enormous memory bandwidth. However, we can create serious bottlenecks when it comes to moving memory back and forth between the CPU and GPU

- Conclusion
- Rethinking the core data structure from first principles, and understanding exactly why each part of the current data structure and access patterns was slow got us out of disappointing
- This way of thinking about designing software is often referred to as data-oriented engineering, and this talk by Andrew Kelly, the author of the Zig compiler, is an excellent primer that was inspirational to the team.
- With these results, we were able to build a technical roadmap for incrementally moving the engine towards a more data-oriented design. The reality is far more complicated, as the calculation engine is north of 40K lines of code. 

- The biggest performance take-aways for us were:
  - When youâ€™re stuck with performance on profilers, start thinking about the problem from first principles
  - Use indices, not pointers when possible
  - Use array of structs when you access almost everything all the time, use struct of arrays when you donâ€™t
  - Use arrays instead of maps when possible; the data needs to be very sparse for the memory savings to be worth it
  - Memory bandwidth is precious, and you canâ€™t just parallelize your way out of it!

- Causal doesnâ€™t smoothly support 1 billion cells yet, but we feel confident in our ability to iterate our way there. 

- discussions

- 

## ðŸ‘¥ [Scaling our spreadsheet engine from thousands to billions of cells | Hacker News_202207](https://news.ycombinator.com/item?id=32000400)

- This brings me back to when I worked on Excel. The formatting structs (and most cell related ones) are so tightly packed and the byte code VM for the formulas is pretty neat.

- I work as a consultant and spend five out of eight hours a day in Excel, and I want the features in this app so bad. Sadly, the only way Iâ€™ll be able to use these features is if Microsoft bought them out and integrated them into Excel. 
  - Itâ€™s ubiquitous at almost every workplace you go to, but the only innovation of the past 20 years seems to be PQ (which is awesome), and xlookup.
- Excel added lambdas recently which is nice for defining custom functions without having to expect anything special on the client side (well, other than a newer copy of Excel at the moment).

- What is your strategy for storage? It seems that because you're offering this as SaaS, your customers both set a high bar for durability of their data and your COGS would be very high if you just kept a sheet this big in memory all the time.
  - Right now we keep our paid customer models' in memory. Models in free accounts are just evaluated every time (these models are usually much smaller). For now we want to avoid the complexity of distributed systems and just run this on very powerful machines with lots of RAM. 
  - For reference, one of our enterprise competitors has a calculation engine that can do 40B cells on a single host (using Java!).
- How are you ensuring the durability of the data?
  - We store the model representation (formulas, data) in Postgres. We just keep it in memory for computing updates faster.

- Indeed, moving memory back and forth from CPUs to GPUs has an overhead. There are ways to mitigate this though! I vaguely remember that one of the patterns in reducing this movement was to keep the data in the GPU as much as possible. I haven't kept up with the latest tech in GPUs off late. When I first played around with CUDA, ArrayFire (arrayfire.com) (no affiliation) was a promising library, and might be a good fit for your GPU prototypes?

- This needs to be able to be run locally/natively. Most companies would not want to run this with their proprietary data in a third party SaaS

- Iâ€™m curious how incremental updates fit into this? 
  - That's what I wonder about too. I imagine that a slow calculation would be fine if done once when the data is first loaded into the system. Once the results are in memory, any new data should only affect smallish subsets of cells.
  - ðŸ’¡ This is the approach our product Grist (https://www.getgrist.com) takes for its calculation engine. Keeping track of dependencies slows it down for the initial load, but beyond that, it only needs to calculate what's needed, so most updates are fast. Even though the engine literally evaluates user-defined Python code for every cell.

- ðŸŽ® Looks very similar to ECS-style data storage used in video games. Have you looked at using opaque index-based IDs instead of references?
  - I've implemented ECS many times but I still have a hard time to see what you're referring to. What are the similarities?

- I'm pretty sure this is better addressed with SQL.
  - We spent a fair amount of time trying to get this executing fast on SQL. However, with spreadsheets you recursively compute a lot of values. You can do this in SQL with recursive CTEs, but itâ€™s slow, theyâ€™re not optimized for this. They also of course wonâ€™t do smart caching and dependency management for the cache. Fundamentally, itâ€™s possible, but then weâ€™d need to start hacking on a DB engine to make it work as a sheet. We concluded itâ€™s better to continue on our existing engine.
# blogs-excel

## [Excel workbook layout and the performance of reading data with Power Query in Power BI_202311](https://blog.crossjoin.co.uk/2023/11/12/excel-workbook-layout-and-the-performance-of-reading-data-with-power-query-in-power-bi/)

- Excel workbooks are one of the slowest data sources you can use with Power Query in Excel or Power BI. 
  - So: reading a small amount of data from a table on a worksheet with a large amount of other data on it is very slow.
  - What can we learn from this? Well, if you can influence the structure and layout of the Excel workbooks you are using as a data source â€“ and thatâ€™s a big if, because in most cases you canâ€™t â€“ and you only need to read some of the data from them, you should put the tables of data you are using as a source on separate worksheets and not on the same worksheet as any other large ranges or tables of data.
# blogs-airtable-like

## ðŸ“ˆðŸ›¢ï¸ [mondayDB architecture _202307](https://medium.com/@liranbrimer/nice-to-meet-you-mondaydb-architecture-6d201b41e660)

- [Why Monday.com decided to build its new database instead of buying one | TechCrunch _202310](https://techcrunch.com/2023/10/22/monday-mondaydb-new-database/)

- mondayDB is the new in-house data engine we crafted at monday.com.
  - We refer to ourselves as a Work OS (Operating System) because we equip our users with a platform they can customize and extend, creating a tailored system to manage and automate any aspect of their work.
  - The key building block of our platform is the â€œboardâ€. This is basically a very rich table to manage any kind of data â€” from tasks and projects through deals, marketing campaigns
  - Each board has columns, which can contain a range of things from basics like text, numbers, or dates to more complex types like a person, team, tag, or even files and formulas. 
  - We offer over 40 types of columns, and our users enjoy the freedom to filter, sort, or aggregate just about any column combination

- when a user landed on their board, we threw all the board data right into the client (usually a web browser running on a desktop computer).
  - but this approach had obvious limitations. First and foremost, the client is limited in its resources.
- Something clearly had to change. So we decided to move everything to the server side, allowing the client to receive only a subset of items in pages as they scrolled. It was easier for the client but not so trivial for the backend.

- we explored many of them, including traditional RDBMS databases with partitions (think MySQL instance per account with a dedicated table per board), ElasticSearch, analytical databases like Apache Pinot, ClickHouse or Apache Druid, and a wide range of NoSQLs like CockroachDB, Couchbase, and more.
  - Eventually, we found that none of these options completely met our requirements. 
  - The reasons varied from our data being mutable, our requirement for numerous tables, not knowing in advance what users would want to filter, and many more.

- Concept #1: Columnar storage
  - In a traditional RDBMS such as MySQL, the row is king, and all the row data is stored together on a disk as an atomic unit.
  - While this setup works smoothly when accessing all the data from that row, itâ€™s less efficient when carrying out operations like filtering a specific column. 
  - This is because youâ€™d need to pull all the tableâ€™s data unless you had prepared a column index in advance (which is something we canâ€™t do without prior knowledge of the schema or queries).
  - A columnar database, on the other hand, slices the data vertically by column. What this essentially means is that the values of each columnâ€™s cells get stored together on the disk as an atomic unit.
  - An immediate downside that stands out is that we would have needed to store more data for the columnar store, as the item ID had to be repeated for each cell value. 
  - However, this issue is considerably less problematic when we realize that the data is highly compressible, especially when the column displays low cardinality, meaning we have limited repeating values.
  - the advantages are massive.weâ€™d only need to fetch a mere fraction of the actual data involved in the filter. Furthermore, as many of our columns are quite sparse, even if the Board is extensive, the data youâ€™d need to retrieve and process could be relatively small.
  - Lastly, the columnar structure opens up tons of optimization opportunities.

- Concept #2: Lambda architecture
  - we store all the columnâ€™s cells together as a single atomic data unit. It implies we canâ€™t fetch or update a single cell separately. 
  - letâ€™s face it, constantly fetching and re-writing an entire column for each single cell update is not only impractical
  - Lambda architecture to the rescue. Not to be confused with AWS lambda functions, it was invented by the big data industry to enable you to pre-calculate in advance query results on your data offline. This means queries are executed fast in runtime. 
  - The main advantage is that it still serves fresh data recently ingested after the last offline pre-calculation has occurred.
  - We divide our system into three components:
  - Speed layer â€” contains only recently changed data
  - Batch layer â€” contains all the past historical data
  - Serving layer â€” serves queries by merging the speed and batch layersâ€™ data in runtime
  - data flows from speed to batch layer. This is handled by a scheduled offline job flushing the data, and building the metadata and other heavy pre-calculations along the way.
  - we utilize Redis as our speed layer storage, and Cassandra as our batch layer storage. 
  - Both of those storages are treated as a straightforward key-value store, where the key corresponds to the column id and the value is the columnâ€™s cells data.
  - I mentioned that we store the entire column data on a disk as a single data unit. The truth is, we break down large columns into smaller partitions. This aligns with best practices for Cassandra, our batch layer storage, to avoid oversized partitions. 

- Concept #3: Separate storage from compute
  - The bottom line is we want our architecture to be elastic. 
  - When we need more computational power (CPU) for heavier queries or during peak hours, we want to scale up processing servers. 
  - And similarly, we want to be able to scale our storage layer to meet our data capacity demands, separated from any data processing considerations.
  - Our architecture offers precisely that. Our batch layer is our storage layer and can be scaled independent of our servers engaged in executing query logic, and those servers also scale independently.
  - The beauty of that approach is that we donâ€™t need to re-execute the query on every page the client requests. Plus, the client can skip by specific offset without having to go over all the items below that offset 
  - this approach isnâ€™t without its drawbacks. For instance, how would the user know how to render the page to simulate a lengthy scrollbar? Or figure out what offset to skip when they scroll quickly? And how could they update the view with live mutations by others since the last snapshot? Those are excellent questions (+1 to myself), but they really deserve their own blog post.

- Whatâ€™s next?
  - Itâ€™s a long journey as every component of the system needs to adapt to handle only subsets of the data using pagination, rather than having all data readily available to the client. Weâ€™ve implemented it for boards, but we have many other components such as our mobile apps, API, views, dashboards, and docs. 
  - Furthermore, we now need a more intelligent client that can function in hybrid mode. 
- thereâ€™s substantial room for improvement and lots of optimizations yet to be implemented. 
  - To name a few, we have many in-process executions we can parallelize, and we could considerably benefit from adding pre-calculations to generate metadata, indexes, and optimization structures like Bloom filter, among others.

- we have dozens of different column types, each one with its unique filter, sort, and aggregation logic. 
  - Itâ€™s all JavaScript, encapsulated in a shared package between our client and backend so it can run in hybrid mode. 
  - Accordingly, our engine is also JavaScript all the way down. 
  - We chose this direction to streamline our efforts, even though itâ€™s no secret that JavaScript may not be the optimal choice for a high-performance system.

### ðŸ‘¥ [MondayDB: A new in-house data engine from monday.com | Hacker News_202310](https://news.ycombinator.com/item?id=37818562)

- > Unlimited tables
  - If you ever find yourself in a position where you need a database table per instance of an abstraction, you've almost certainly failed to model the domain appropriately.

- AirTable developed their own DB, but their case may be justified. I listened to an interview with a founder once - very thoughtful and reasonable. They fully understood what they were getting into.
  - Not sure why they needed own DB. Fibery.io has similar domain and we built everything on Postgres. Works like a charm and you even don't have Airtable bases connectivity problem. We have schema-per-customer and table-per-entity-type model, performance is quite good.

- don't tell them about SQLite! It's the secret sauce behind Grist (I am a founder), and we are sort of competitors. Wouldn't want Monday.com knowing about our competitive advantage!

- Honestly, the end result was a lot worse than I expected. Using Redis & Cassandra as 1 db, when your requirements where unlimited tables, fast filter/sort, etc. Literally the worst of all worlds imaginable.
  - Imagine maintaining that (2 separate distributed async-by-default, no transactions/indexes, limited-types, etc etc clusters).
- A lot of the architectural decisions, from a high-level, make sense: compute and storage are separated, redis and cassandra are used instead of reinventing those. It's a bit OLAP and a bit OLTP in that users might make a few point updates to things here and there OLTP but then the filtering and aggregating and showing all sorts of views and such is clearly in the analytics domain hence OLAP and the use of a columnar setup.

- Postgres / MySQL with a table per board (or one giant partitioned table with a JSONb column) would have been totally fine, as would have one SQLite database per customer that you can load the initial page of and then ship the whole thing to the client for richer interactivity.

- Article shows that Cassandra is slow to retrieve data, so they added Redis as a cache to fetch recent events from there.
  - It is now planet scale! Proper database design and indexes be damned!

## ðŸ“ˆ [Building a modular software toolkit | The Airtable Engineering Blog | Medium_202102](https://medium.com/airtable-eng/building-a-modular-software-toolkit-ce4efd06e75c)

- At Airtable, weâ€™re building a toolkit that anybody can use to build their own software.
  - You can combine the many different building blocks provided in Airtable to create software that is truly tailored to your needs, rather than having to force your workflows into a one-size-fits-all solution that only vaguely applies to your problem.
  - Our toolkit-style approach brings significant benefits both for our customers and for our product development process

- Modularity as a lever
  - By building a flexible and modular set of building blocks, we can develop features that are highly leveraged. When we add a new building block to the toolkit, it can interact with every other existing building block
  - For example, take rich text formatting. Airtable currently provides over two dozen field types, from basic ones like text, number, and single select fields, to more advanced ones like formula, linked record
- Modularityâ€™s costs
  - This means that each feature in Airtable is akin to a public API that other features can depend on. 
  - As we design new features or improve existing ones, we must think very critically about the interface that each feature exposes
- Addressing these challenges at Airtable

## [The data model behind Notion's flexibility_202105](https://www.notion.so/blog/data-model-behind-notion)

- 
- 
- 
- 

# more
