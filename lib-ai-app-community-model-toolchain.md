---
title: lib-ai-app-community-model-toolchain
tags: [community, large-language-model, toolchain]
created: 2025-09-16T12:35:55.935Z
modified: 2025-09-16T12:36:12.968Z
---

# lib-ai-app-community-model-toolchain

# guide

- model with UD(unsloth dynamic quants)

- models-config
  - å¤§æ¨¡å‹çš„æµ‹è¯•ç»å¸¸éœ€è¦ä¿®æ”¹å‚æ•°ï¼Œæ”¯æŒä¸€é”®æ¢å¤é»˜è®¤é…ç½®æ›´å¥½

- tips-ai-tools
  - lm studioåº•å±‚ç”¨çš„ä¹Ÿæ˜¯llama.cpp, ä¸å¿…å¯»æ‰¾æ›¿ä»£ï¼Œæ·±å…¥åº•å±‚æ›´å®¹æ˜“æ›¿ä»£å’Œæ‰©å±•

- mlx
  - mlxçš„ä¼˜ç‚¹ä¹‹ä¸€æ˜¯æ–¹ä¾¿æ”¯æŒåœ¨iphoneä¸Šè¿è¡Œ
# lmstudio-xp
- not-yet
  - èŠå¤©å†…æœç´¢
  - æ ‡é¢˜åæœç´¢ï¼Œä¾¿äºæŸ¥çœ‹åŒ…å«æŸå…³é”®å­—çš„chats

- 
- 
- 
- 

# discuss-stars
- ## 

- ## 

- ## [What is your primary reason to run LLMâ€™s locally : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1nsjwv1/what_is_your_primary_reason_to_run_llms_locally/)
- For me, it is independence from Big Tech and venture capital. Open weight models can never be taken away from us.

- New open weight models come out like every week and I want to be able to try the ones that can can run on my PC without waiting for a subscription service to add them or paying per use.

- ## ğŸ§© [The difference between quantization methods for the same bits : r/LocalLLaMA _202307](https://www.reddit.com/r/LocalLLaMA/comments/159nrh5/the_difference_between_quantization_methods_for/)
  - Using GGML quantized models, let's say we are going to talk about 4bit
  - I see a lot of versions suffixed with either 0, 1, k_s or k_m
- [k-quants Â· Pull Request Â· ggml-org/llama.cpp _202306](https://github.com/ggml-org/llama.cpp/pull/1684)

- K-quantization options are labeled "S", "M", and "L" and stand for small, medium, and large model sizes, respectively. 
  - Option "0" represents baseline quantization without extra calibration. 
  - In terms of quality and speed: 0 (lowest quality, fastest speed) < S < M < L (highest quality, slowest speed).

- k models are k-quant models and generally have less perplexity loss relative to size. A q4_K_M model will have much less perplexity loss than a q4_0 or even a q4_1 model.
- Generally, the K_M models have the best balance between size and PPL, so q3_K_M, q4_K_M, q5_K_M, etc. I like q5, and q4 best usually. 

- Aside from the number of bits per weight in a scaling group being obvious, here are the main differences:
  - Type _0 compression gives each group of weights a shared scale, but they are "symmetric" weights about 0. Type _1 weights add in a "bias" - an offset for each group of weights which allows them to be better resolved if they are mainly shifted away from zero. Type K is an enhancement in the way hierarchal groups are encoded to squeeze a little more compression into the mix. Finally, after the K we now have nothing, M, S and L variants - these actually refer to which tensors have the base precision. In the K_S models, all weight tensors have the stated precision. The simple K, K_M and K_L models specify varying amounts of weight tensors that will actually use higher precision to improve accuracy, typically 4-6 bits. This will no doubt keep expanding over time. Note that the PR referred to by u/lemon07r also contains descriptions of all the formats.

- With the older quantisation method, 4_0 is 4.5 bits per weight and 4_1 is 5 bits per weight.
  - The K quantisation methods are newer. Hopefully, they will get slightly better accuracy for roughly the same file size compared to the old methods.
- Thatâ€™s not correct. You will get the best speed with q4_K_S oder q4_K_M. This is because 3-bit and 2-bit needs more calculations.
  - If memory bandwidth is your bottleneck and not processor speed, then smaller is faster.

- ## [AMA with the Unsloth team : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/)
- Whatâ€™s your go-to quant for most models? I usually pick Q4_K_XL dynamic, but if I have enough VRAM, is there another Q4 youâ€™d recommend for better accuracy?
  - Yes correct, usually always got for the K_XL quants as they have the best ratios in terms of accuracy/speed/size etc 
  - My goto is probably Q3_K_XL as my laptop is incapable of handling anything larger

- ğŸ Do you plan to support Apple/MLX?
  - Yes definitely, it has been a super high request and we know there are soooo many Mac users out there so we'd be silly to not to. As for when, mmm to be honest maybe late this year? Unfortunately we are team constrained at the moment

- Any plannings on the TPU full integration?
  - It is possible yes but probably after MLX/AMD/Intel etc first
- I think they have it in the roadmap but I do not think anytime soon. I think it would be better for Unsloth if they are support Apple/MLX first and then TPU
  - Yes, AMD/Intel/ MLX will come first then TPU

- Iâ€™d like to use your model in a distributed llama cluster using all my old computer at home. Any planning?
  - We support multiGPU which might help with your setup but won't be officially announcing multigpu until maybe later this year as it's not up to the standard we would like!

- 
- 

- ## [Poor performance qwen3 235B 2507 mlx vs. unsloth variant : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mj7m20/poor_performance_qwen3_235b_2507_mlx_vs_unsloth/)
  - I just downloaded the Qwen3 235B 2507 instruct model for LmStudio on a M2 studio ultra. 
  - I got the MLX 4bit and Unsloth q4_0 versions. 
  - I am getting very low generation speeds on the MLX version ~0.3 tokens/s while on the other hand I am getting ~27 tokens/s on the Unsloth variant.

- Unsloth is highly optimized with advanced quantization for massive models, while MLX is known to have performance issues with models larger than ~22B parameters, which explains the speed difference you're seeing.
  - Where can I read more about MLX having problems with >22B parameters? First time hearing about it

- For unsloth would recommend q3_k_xl or q4_k_xl, should be more accurate than simple q4_o quantâ€¦ MLX should be faster, maybe the full model is not loaded to GPU?
  - Thanks. I double checked if all layers were loaded onto the GPU and that's the case, so I don't know what is the problem

- [Performance Qwen3 30BQ4 and 235B Unsloth DQ2 on MBP M4 Max 128GB : r/LocalLLaMA _202504](https://www.reddit.com/r/LocalLLaMA/comments/1kbacf2/performance_qwen3_30bq4_and_235b_unsloth_dq2_on/)
  - I was wondering what performance I could get out of the Mac MBP M4 Max 128GB
  - LMStudio Qwen3 30BQ4 MLX: 100tokens/s
  - LMStudio Qwen3 30BQ4 GUFF: 65tokens/s
  - LMStudio Qwen3 235B USDQ2: 2 tokens per second?
  - So I tried llama-server with the models, 30B same speed as LMStudio but the 235B went to 20 t/s!!! So starting to become usable

- ## ğŸ†š [MLX vs. UD GGUF : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1kpqrzz/mlx_vs_ud_gguf/)
- I benchmarked Unsloth's Qwen3-30B-A3B Dynamic 2.0 GGUF against the MLX version. Both models are the 8-bit quantization. Both are running on LM Studio with the recommended Qwen 3 settings for samplers and temperature.
  - MLX: 3, 516 tokens generated, 1.0s to first token, 70.6 tokens/second
  - UD GGUF: 3, 321 tokens generated, 0.12s to first token, 23.41 tokens/second
  - This is on an MacBook M4 Max with 128 GB of RAM, all layers offloaded to the GPU.

- UD q8 xl is not efficient for Mac. Use normal q8_0

- Isn't the entire point of using the UD XL GGUF for higher quality responses? If you were comparing for speed alone why not use the normal Q8 GGUF?
  - UD also uses less VRAM. I was hoping if it was comparable in speed, UD would be more resource efficient.
- They use less vram than 16 bit, not 8 bit. The goal is better accuracy through selective quantization so it should use more memory than standard q8. This is also why thereâ€™s XL in the name.

- Turn on flash attention if you havenâ€™t. I wish I could use MLX, it it faster but the output by comparison was worse by a margin Iâ€™ve never seen before between the formats. I have an M1 ultra 64gb. Itâ€™s even worse with Qwen3 32B

- In my experience MLX is only marginally faster, less than 10% usually; it has an edge when it comes to speculative decoding, though.

- ## ğŸ¤” [Genuine question: Why are the Unsloth GGUFs more preferred than the official ones? : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1ksw070/genuine_question_why_are_the_unsloth_ggufs_more/)
- They often write "getting started" blog posts along with their quants of popular models where they share insights. That's valuable to newcomers. 

- They waste their time on stuff so you don't have to. When some meta data is wrong or a model outputs gibberish for some reason, they check it out and update it with a fix. The other top uploaders aren't bad either and do the same I imagine if an issue get raised. But random uploaders, who knows. And the official model creators do weird shit on their uploads like require login tokens on HF because they don't want you to download from them.

- There was a nicely done test recently that showed that they (quants by unsloth, bartowski, mrademacher) are all good. There is no clear winner. 
  - However, the "official" quants were often released without imatrix or broken / different in some other way. That's why those unofficial quants are usually preferred.
  - Also, unsloth made large MoE models usable on non-server machines with their dynamic Q2_XXS quants.
- The biggest difference I would say isn't the quants, but rather our bug fixes for every model
# discuss-llm-tools-tips/tricks
- ## 

- ## 

- ## 

- ## [weighted/imatrix VS static quants? : r/LocalLLaMA _202405](https://www.reddit.com/r/LocalLLaMA/comments/1ck76rk/weightedimatrix_vs_static_quants/)
- Imatrix quants were introduced a couple of months ago and are recommended over static quants because they have better output quality. 
  - For example, a Q4_K_M quant made with imatrix should be closer to a Q5_K_M non-imatrix quant in quality.

- Importance matrix is used to choose vocabulary to preserve precision for.
  - Both normal Q_k quants and IQ quants can use imatrix.
  - IQ quants, I don't know the expansion of the I, but they attempt to approximate the original value at runtime. They do extra math, and slow down CPU inference but GPUs generally are still memory speed limited.

- Are there any disadvantages? I usually go for Q4k_m and tried iq4_nl or something, the IQ is slightly smaller in file size but inference speed seems to be basically the same. If imatrix is better why do people still release/use static? 

- [Static vs imatrix? : r/SillyTavernAI](https://www.reddit.com/r/SillyTavernAI/comments/1ggfpo8/static_vs_imatrix/)
  - imatrix is distinct from i-quants.
  - i-quants are better if your hardware(CPU in this case, not GPU/RAM/VRAM) is beefy enough to run them at a speed you find usable.
  - K-quants if your CPU is struggling with i-quants.

- ## [Getting counter-intuitive results with local KV Cache Quantization Benchmark - am I doing something wrong? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nn2nqz/getting_counterintuitive_results_with_local_kv/)
  - I've been running some benchmarks on KV cache quantization for long-context tasks, and I'm getting results that don't make much sense to me. 
  - The Weird Results: I was expecting to see a clear trend where higher quantization (like q4_0) would lead to a drop in accuracy compared to the f16 baseline. Instead, I'm seeing the opposite. My best performing combination is k-f16_v-q5_0 with 16.79% accuracy, while the f16-f16 baseline only gets 13.74%.

- You see scattered reports of quantized KV increasing accuracy because it â€œfuzzesâ€ attention in a way that actually benefits (specifically) low bit weight quants. Basically it acts as an implicit smoothing function. Iâ€™ve not had amazing luck with llama.cppâ€™s implementation, but EXllamaâ€™s KV cache quants seem to perform exceptionally well at even 4-bits. 

- ## [å›½å†…å¤–å¤§æ¨¡å‹APIå¹³å°ä½“éªŒå¯¹æ¯” - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1914700194517345472)

- å¦‚æœæƒ³ä½¿ç”¨å›½å†…çš„æ¨¡å‹APIï¼Œæ¨èä½¿ç”¨å›½å†…äº‘å‚å•†æä¾›çš„APIæœåŠ¡æˆ–ç¬¬ä¸‰æ–¹ä»£ç†å¹³å°ã€‚
  - å¦‚æœæƒ³åœ¨å›½å†…ä½¿ç”¨å›½å¤–çš„æ¨¡å‹APIï¼Œæˆ–è€…æƒ³åŒæ—¶ä½¿ç”¨å¾ˆå¤šä¸åŒç³»åˆ—çš„æ¨¡å‹ï¼Œæ¨èä½¿ç”¨ç¬¬ä¸‰æ–¹ä»£ç†å¹³å°ã€‚
  - å¦‚æœæœåŠ¡å™¨åœ¨å›½å¤–ï¼Œæƒ³è°ƒç”¨å›½å¤–çš„æŸä¸ªAPIæ¨¡å‹ï¼Œåˆ™æ¨èä½¿ç”¨å›½å¤–äº‘å‚å•†ï¼Œæˆ–ç¬¬ä¸‰æ–¹ä»£ç†å¹³å°ã€‚

- 2025/9/1 æ›´æ–°ï¼š
  - 1. å¢åŠ DMXAPIï¼Œä¸€ä¸ªä¸é”™çš„APIä¸­è½¬å•†ï¼Œæ¨¡å‹æ¯”è¾ƒå…¨ã€‚
  - 2. åˆ é™¤åšé˜…ï¼Œå› ä¸ºå…¶ä¸å¼€æ”¾æ–°ç”¨æˆ·å……å€¼ã€‚
  - 3. é™ä½openrouterçš„æ¨èæŒ‡æ•°ï¼Œå› ä¸ºä½¿ç”¨è¿‡ç¨‹é‡åˆ°ç¨³å®šæ€§é—®é¢˜æ¯”è¾ƒå¤š 
  - 4. é™ä½ç¡…åŸºæµåŠ¨çš„æ¨èæŒ‡æ•°ï¼Œå› ä¸ºé™æµTPMå¤ªå°ï¼Œå½±å“ä½¿ç”¨ã€‚
  - 5. å¢åŠ GLMã€Kimiã€Minimaxæ¨¡å‹çš„æ¨èå‚å•†

- openrouter èƒŒåçš„ä¾›åº”å•†æ¯”è¾ƒä¹±, æœ‰äº›æ¨¡å‹ç‰ˆæœ¬ä¸æ˜¯æœ€æ–°çš„. è¿˜éœ€è¦è‡ªå·±æ’é™¤ä¾›åº”å•†

- ## ğŸ”§ [Open WebUI vs. LM Studio vs. MSTY vs. _insert-app-here_... What's your local LLM UI of choice? : r/LocalLLM _202502](https://www.reddit.com/r/LocalLLM/comments/1ij3j8m/open_webui_vs_lm_studio_vs_msty_vs_insertapphere/)
- Ollama vanilla CLI in tmux with vim copy/paste between terminals. I like pain

- All of them; donâ€™t lock into one solution.

- Open WebUI + LibreChat. LibreChat mainly for creating agents for RAG. Most painless interface for RAG.

- Open Web UI. MSTY is no alternative because it is an all-in-one solution.
  - Closed source right?
- Yep, they are selling it for businesses.

- KoboldAI Lite running on KoboldCpp. Most others aren't as flexible and just focused on instruct. This one can do instruct, but it can also do regular text generation for example. 
  - KoboldCpp meanwhile is a single executable with text gen, image gen, image recognition, speech to text and text to speech support. And it emulates the most popular API's if you prefer another UI (KoboldAI LIte doesn't need the backend to have any UI code so if its not open in the browser it does not effect you).

- ## [intelçš„cpuè¿å¤§æ¨¡å‹éƒ½æ²¡æ³•è·‘, æ€ä¹ˆè¿˜å¤©å¤©åœ¨æ¨aipc? - çŸ¥ä¹](https://www.zhihu.com/question/668042879/answers/updated)
- å¯¹äºç«¯ä¾§AIï¼Œæˆ‘ä¸ªäººçš„æƒ³æ³•ï¼Œæœ€å¤§çš„ä»·å€¼åº”è¯¥æ˜¯æ‹‰é«˜ä¸Šä¸‹æ–‡çª—å£ï¼Œåœ¨æœ¬åœ°åšä¸ªäººçŸ¥è¯†åº“ï¼Œä»¥åŠæœ¬åœ°æ‰¹é‡æ¨ç†ï¼Œæ¯”å¦‚åšç§‘ç ”çš„ï¼Œæ‡’å¾—è¯»è®ºæ–‡ï¼Œè®©AIæ‰¹é‡æ€»ç»“å†™ä¸ªç»¼è¿°ã€‚è¿™ä¸¤ç§åšæ³•å¦‚æœè°ƒç”¨çº¿ä¸Šçš„APIï¼Œå…¶å®æŒºè´µçš„ã€‚é˜…è¯»ä¸€ç¯‡è®ºæ–‡å°‘åˆ™å‡ åƒtokensï¼Œå¤šåˆ™ä¸¤ä¸‰ä¸‡tokensã€‚æœ¬åœ°ä½¿ç”¨32768çš„ä¸Šä¸‹æ–‡é•¿åº¦çš„Qwen3 8Bï¼Œä¹Ÿèƒ½å®Œæˆå¾—ä¸é”™
  - åšé•¿çª—å£æœ¬èº«å°±éœ€è¦èµ„æºï¼Œtransformerçš„kqvè®¡ç®—æ˜¯ä¸ªo(n^2)çš„å¤æ‚åº¦

- ç«¯ä¾§ AI ç°åœ¨çš„åº”ç”¨åœºæ™¯å®åœ¨æ˜¯å¤ªå°ã€‚å°¤å…¶æ˜¯è®¾å¤‡æ— æ—¶æ— åˆ»åœ¨çº¿çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä¸ºä»€ä¹ˆæ”¾ç€å…è´¹çš„å¼ºå¤§çš„åœ¨çº¿ AI ä¸ç”¨ï¼Œè½¬è€Œå»ç”¨ä¸èƒ½è”ç½‘æœç´¢ã€æ›´å¼±ï¼Œæ›´å¡ã€æ›´è€—ç”µçš„ç«¯ä¾§çš„ AI å‘¢ï¼Ÿ
  - æˆ‘æœ¬åœ°éƒ¨ç½²äº†chatgml çš„ 6B å’Œç§‹å¶çš„ [SD æ•´åˆåŒ…] ï¼Œä½†æ˜¯ç”¨åˆ°çš„çœŸçš„ä¸å¤šï¼Œå¾—åˆ°çš„ç»“æœä¹Ÿæ¯”ä¸äº†åœ¨çº¿çš„

- ## ğŸ†š [ä¸ºä»€ä¹ˆéƒ½åœ¨ç”¨ollamaè€Œlm studioå´æ›´å°‘äººä½¿ç”¨? - çŸ¥ä¹](https://www.zhihu.com/question/654357364)
- éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸åŒäºå…¨éƒ¨ä»£ç åœ¨githubå¼€æºã€ç”šè‡³å¯ä»¥è‡ªå·±åŠ¨æ‰‹ç¼–è¯‘çš„ollamaï¼Œlm studioè‡³ä»Šä»æ˜¯é—­æºå•†ä¸šè½¯ä»¶ï¼ˆä»…ä¸€éƒ¨åˆ†éæ ¸å¿ƒä»£ç é™¤å¤–ï¼‰

- Ollama äº 5 æœˆä»½æ¨å‡ºçš„å…¨æ–°å¤šæ¨¡æ€å¼•æ“ã€‚åŸºäºå¤šæ¨¡æ€å¼•æ“ï¼ŒOllama å¯ä»¥æ”¯æŒè¿è¡Œèƒ½åŒæ—¶å¤„ç†å›¾åƒã€æ–‡æœ¬çš„æ¨¡å‹ã€‚æ–°æ¶æ„ä¸ä»…è§£å†³å½“å‰å¤šæ¨¡æ€æŒ‘æˆ˜ï¼Œä¹Ÿä¸ºé›†æˆæ›´å¤æ‚çš„èƒ½åŠ›ï¼ˆè¯­éŸ³ã€ç”Ÿæˆç­‰ï¼‰å’Œä¼˜åŒ–æ€§èƒ½ï¼ˆæ›´é•¿ä¸Šä¸‹æ–‡ã€æ›´é«˜å¹¶å‘ï¼‰æ‰“ä¸‹åŸºç¡€ã€‚

- æˆ‘çš„ä½“ä¼šï¼š
  - ollamaç”¨èµ·æ¥å’Œdockerä¸€æ ·çš„æ„Ÿè§‰ï¼Œpullæ¨¡å‹ï¼Œrunæ¨¡å‹ï¼Œlsçœ‹æ¨¡å‹ï¼Œpsçœ‹è¿è¡Œã€‚éå¸¸é¡ºæ‰‹ä¸æ»‘ï¼Œå…¥æ‰‹æ— é—¨æ§›ã€‚
  - llamaçš„ä¸­æ–‡ï¼Œå¾®è°ƒå„ç§chatï¼Œcodeï¼Œå¤Ÿç”¨ã€‚è€Œä¸”éƒ½æ˜¯é‡åŒ–å¥½çš„ï¼Œéšæ‹‰éšç”¨ï¼Œ4090å°±è·‘çš„èµ·æ¥ã€‚å°¤å…¶æ˜¯åœ¨å›½å†…æ‹‰æ¨¡å‹é€Ÿåº¦æå¿«ï¼Œæˆ‘çš„ç¯å¢ƒæœ€é«˜å¯è¾¾15m/s
  - ollamaæ˜¯llama.cppå®ç°æ¨¡å‹æ¨ç†ï¼Œæ¨¡å‹å°ï¼Œé€Ÿåº¦å¿«ã€‚
  - ollamaæä¾›11434ç«¯å£çš„webæœåŠ¡ï¼Œé‡è¦çš„æ˜¯è¿˜å…¼å®¹openaiçš„ç«¯ç‚¹æ¥å£ï¼Œå¯ä»¥å’Œå„ç§å‰ç«¯é…åˆï¼Œæ¯”å¦‚ollamaè‡ªå·±open webuiï¼Œå›½äº§çš„chatboxï¼Œè¿åç«¯å¸¦ç•Œé¢ï¼Œä¸€å¥—æå®š
  - ollamaæ˜¯ç³»ç»ŸæœåŠ¡å½¢å¼ï¼ˆä¹Ÿèƒ½å®¹å™¨è¿è¡Œï¼‰ï¼Œå‰åç«¯åˆ†ç¦»ï¼ˆ ä¸¥æ ¼æ¥è¯´æ²¡æœ‰å‰ç«¯ï¼Œåªæœ‰å‘½ä»¤è¡Œå…¥å£ï¼‰ï¼Œè€¦åˆå°ï¼Œæ­é…çµæ´»ã€‚
- ollamaçš„è¿­ä»£å¾ˆå¿«ï¼Œç°åœ¨å¤šæ¨¡å‹å¹¶å‘çš„é—®é¢˜å·²ç»è§£å†³äº†

- ollamaç°åœ¨æ”¯æŒåˆ†å¸ƒå¼æ¨ç†å—
  - ç°åœ¨ä¸æ”¯æŒï¼Œä½†æ”¯æŒå•æœºå¤šgpuè¿è¡Œ

- æ‹‰æ¨¡å‹éº»çƒ¦å°±å›°æ‰°å¾ˆå¤šäººäº†ï¼Œollamaç…§ç€æ•™ç¨‹åšï¼ŒåŸºæœ¬æ²¡æœ‰é—®é¢˜

- ollamaé…åˆopen webuiä¸é”™çš„

- ollamaçœ‹ç€é€Ÿåº¦å¿«ï¼Œå…¶å®å®ƒæœ€å¤§é—®é¢˜æ˜¯ä¿®æ”¹ä¸ªä¸Šä¸‹æ–‡éƒ½å¾ˆéº»çƒ¦
  - å¯ä»¥ç”¨chatboxä¹‹ç±»çš„èŠå¤©æ¡†

- lm studio ä¹Ÿæä¾›openai-likeçš„apiåç«¯ï¼Œæ”¯æŒå‰åç«¯åˆ†ç¦»ï¼Œä¹Ÿæ”¯æŒllama.cppï¼ˆåœ¨macè¿˜æ”¯æŒmlxæ¡†æ¶ï¼‰. æ¨¡å‹åŸºæœ¬ä¸Šä¹Ÿéƒ½æ”¯æŒï¼Œè€Œä¸”éƒ½æ˜¯ä»hfä¸Šä¸€é”®å¼ä¸‹è½½éƒ¨ç½²ï¼ˆä¸è¿‡ç½‘ç»œç¡®å®æ˜¯é—®é¢˜ï¼‰
  - æ€»çš„è¯´ï¼Œlinuxä¸Šéƒ¨ç½²è¿˜æ˜¯åˆé€‚ï¼Œä¸ç”¨ollamaï¼Œè¿˜æœ‰vllmï¼Œlmdeloyï¼Œä»¥åŠsglangï¼Œéƒ½æ˜¯å¼€æºçš„ï¼Œå¾ˆé€‚åˆå¼€å‘äººå‘˜æˆ–è€…æ˜¯ç»„ç»‡å†…éƒ¨éƒ¨ç½²ã€‚windowsä¸Šéƒ½å·®ç‚¹ï¼Œlmstudioç•Œé¢è€¦åˆï¼Œé—­æºï¼Œæ‹‰æ¨¡å‹èµ°é•œåƒç‚¹ï¼Œä¸èƒ½åˆ†å¸ƒå¼æ¨ç†ï¼Œä¸ªäººè‡ªå·±å®éªŒæ€§è·‘è·‘å¥½ä¸€äº›ã€‚

- LMå›¾å½¢åŒ–ç•Œé¢ä¹Ÿå¯ä»¥å¾®è°ƒå¤§é‡çš„æ¨¡å‹è¿è¡Œå‚æ•°ï¼Œollamaè¿™æ–¹é¢å°±å¯¹å°ç™½ä¸å¤Ÿå‹å¥½äº†ï¼Œåªèƒ½ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
  - LMåšserveræœåŠ¡å™¨æ—¶ï¼Œç•Œé¢æœ‰åŠ¨æ€åé¦ˆçš„æ‰§è¡Œä¿¡æ¯å¯ä»¥ä¾›ç”¨æˆ·è§‚å¯Ÿåˆ°ï¼Œè°ƒè¯•é—®é¢˜æ–¹ä¾¿ã€‚
  - LMæœ€æ–°çš„ç‰ˆæœ¬åœ¨chatç•Œé¢ä¸‹ï¼Œè¿˜æ”¯æŒäº†ollamaä¸æ”¯æŒçš„LaTexæ•°å­¦å…¬å¼è¡¨è¾¾ï¼Œå’Œæœ¬åœ°RAGæ–‡æ¡£ä¿¡æ¯æ£€ç´¢ï¼Œè¿™äº›éƒ½æ˜¯ollamaè¿˜ä¸æä¾›çš„ç‰¹æ€§ã€‚

- ollama å’Œ LM studio å‡å¯ä»¥æ”¯æŒä¸åŒå¤§æ¨¡å‹çš„æœ¬åœ°éƒ¨ç½²ï¼Œå¹¶ä¸”ä¼šä¼˜å…ˆä½¿ç”¨ GPU è¿›è¡Œæ¨ç†ã€‚å¦‚æœæ²¡æœ‰å‘ç° GPUï¼Œå°±ä¼šä½¿ç”¨ CPU æ¨ç†ï¼Œå› æ­¤ä¹Ÿä¼šå ç”¨ä¸€éƒ¨åˆ†å†…å­˜ã€‚ä»å®é™…ä½¿ç”¨æ¥çœ‹ï¼Œç¬”è®°æœ¬å†…å­˜åº”è¯¥è‡³å°‘ä¸º 8GB æ‰èƒ½æ­£å¸¸è¿è¡Œã€‚

- å†™ç¨‹åºå’Œollamaäº¤äº’çœŸçš„æ˜¯é¡ºæ»‘ï¼Œå‡ å¥ä»£ç æ¨¡å‹å¿«é€Ÿåˆ‡æ¢ã€‚

- ollamaè·Ÿvllmã€sglangã€trtllmç›¸æ¯”æ˜¯ä¸ªç©å…·
  - lm studioè¿ç©å…·éƒ½ä¸å¦‚

- æ„Ÿå—ä¸ŠåŸå› æ˜¯dsæœ€ç«çš„æ—¶å€™ï¼Œé“ºå¤©ç›–åœ°çš„è‡ªåª’ä½“æ•™ç¨‹éƒ½æ˜¯ollamaï¼Œæåˆ°lmçš„å°‘çš„å¤šã€‚è¿™ä¸ªä¸œè¥¿ä¸€æ—¦å½¢æˆå®£ä¼ æ•ˆåº”äº†ï¼Œé™¤äº†ä¸“é—¨æè¿™ä¸ªçš„ï¼Œå°±ä¸ä¼šæœ‰äººæ·±å…¥ç ”ç©¶äº†ã€‚

- lm studioèƒ½åƒollamaé‚£æ ·åŒæ—¶ä½¿ç”¨chatå’Œembeddingå—ï¼Ÿlm studioæ¯æ¬¡éƒ½è¦é¢„å…ˆåŠ è½½ã€‚

- ## ğŸ [å¦‚ä½•çœ‹å¾…è‹¹æœå‘å¸ƒçš„ MLX æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Ÿ - çŸ¥ä¹ _202312](https://www.zhihu.com/question/633585779)
- MLXæ˜¯ä¸€ä¸ªç±»ä¼¼NumPyæ•°ç»„çš„æ¡†æ¶ï¼Œç›®çš„æ˜¯å¯ä»¥åœ¨è‹¹æœçš„èŠ¯ç‰‡ä¸Šæ›´åŠ é«˜æ•ˆåœ°è¿è¡Œå„ç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå½“ç„¶æœ€ä¸»è¦çš„ç›®çš„æ˜¯å¤§æ¨¡å‹ã€‚
  - MLXçš„è®¾è®¡å—åˆ°PyTorchã€Jaxå’ŒArrayFileçš„å¯å‘ï¼Œç›®çš„æ˜¯è®¾è®¡ä¸€ä¸ªå¯¹ç”¨æˆ·æå…¶å‹å¥½ï¼Œä½†åŒæ—¶åœ¨è®­ç»ƒå’Œéƒ¨ç½²ä¸Šä¹Ÿéå¸¸é«˜æ•ˆçš„æ¡†æ¶ã€‚
  - æ‰€ä»¥ï¼Œå®ƒçš„æ¥å£ä½ ä¼šéå¸¸ç†Ÿæ‚‰ï¼Œå› ä¸ºå®ƒçš„Pythonæ¥å£ä¸NumPyå¾ˆç›¸ä¼¼ï¼Œè€Œå®ƒçš„ç¥ç»ç½‘ç»œæ¨¡å‹çš„æ¥å£å’ŒPyTorchéå¸¸ç±»ä¼¼ã€‚

- 2023å¹´äº†ï¼ŒçœŸçš„è¿˜éœ€è¦ä¸ºäº†è‡ªå®¶èŠ¯ç‰‡ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªæ·±åº¦å­¦ä¹ è®­ç»ƒæ¡†æ¶å—
  - æœ‰çš„ï¼Œæ„Ÿè§‰æ˜¯ä¸“é—¨é’ˆå¯¹umaä¼˜åŒ–çš„
- æ¯•è®¾æ˜¯æ·±åº¦å­¦ä¹ åé—¨æ”»å‡»ç›¸å…³ è®¾å¤‡æ˜¯macmini m4 ä»pytorchæ¡†æ¶åˆ‡æ¢åˆ°mlxæ¡†æ¶ åŒæ ·çš„å®éªŒåŒæ ·çš„è®­ç»ƒé‡ mlxé€Ÿåº¦å¿«äº†å¥½å‡ å€ å®æ‰“å®çš„æå‡
  - ä½†æ˜¯éœ€è¦é‡æ„è®­ç»ƒä»£ç ï¼Œæˆ‘ä¸€å¥—pytorchä»£ç ï¼Œåªéœ€åšå°‘é‡é€‚é…ï¼Œå¯ä»¥è‡ªåŠ¨åœ¨æ˜‡è…¾ã€nVidiaã€mpsä¹‹é—´åˆ‡æ¢ã€‚è€Œä¸”ï¼Œmlxç¡®å®æ¯”mpsæå‡å¥½å‡ å€ï¼Œä½†æ˜¯æˆ‘å¦‚æœç”¨nvidiaæˆ–è€…æ˜‡è…¾910Bï¼Œmlxæ€§èƒ½è¿˜æœ‰å·®è·ã€‚
- åœ¨ MacBook Air m3 ä¸Šï¼Œmlx çš„çŸ©é˜µä¹˜æ³•ã€SVDåˆ†è§£è¿è¡Œé€Ÿåº¦æ˜æ˜¾å¿«äº PyTorch
  - å¦‚æœä¸ä½¿ç”¨ mlx æ¥è¿›è¡Œæ·±åº¦å­¦ä¹ ï¼Œä»…ç”¨äºå¸¸è§„ç§‘ç ”è®¡ç®—æˆ–ä¼˜åŒ–ä»»åŠ¡ï¼Œå®é™…ä¸Šè¿˜æ˜¯è›®é€‚åˆçš„ã€‚
  - ç„¶è€Œï¼Œmlx ç›®å‰ç”šè‡³ä¸æ”¯æŒ float64ï¼Œå¼€å‘å›¢é˜Ÿå½“å‰è€ƒè™‘çš„æ˜¯å®ƒä»¬ GPU åæ­£ç”¨ä¸åˆ°ï¼Œè¿™å¯¹ä¸€äº›é«˜ç²¾åº¦è®¡ç®—éœ€æ±‚åˆæ˜¯ä¸€ç§é™åˆ¶ï¼Œæ›´ä¸ç”¨è¯´ä¸€äº›æœªå®ç°çš„ç®—æ³•æˆ–è€…æ•°æ®ç±»å‹è¿˜éœ€è¦ä»é›¶å†™äº†ã€‚

- è¦æ˜¯kerasåç»­èƒ½å¤Ÿæ”¯æŒmlxåç«¯å°±å¥½äº†ï¼Œè¿™æ ·è‹¹æœè®¾å¤‡ä¸Šå†™çš„ä»£ç ä¸è‡³äºåˆ°åˆ«çš„å¹³å°ä¸Šè·‘ä¸äº†è¿˜è¦é‡æ„äº†ã€‚

- [è‹¹æœå‘è‹±ä¼Ÿè¾¾ç”Ÿæ€å¦¥åäº†ï¼MLXæ¡†æ¶ä¸»åŠ¨é€‚é…CUDA - çŸ¥ä¹ _202507](https://zhuanlan.zhihu.com/p/1929178251202376826)
  - è‹¹æœä¸€ç›´ä»¥æ¥éƒ½ä»¥â€œå°é—­â€è‘—ç§°ï¼Œä½†éšç€è‹±ä¼Ÿè¾¾CUDAç”Ÿæ€åœ¨AIå¼€å‘é¢†åŸŸå æ®ç»å¯¹ä¸»å¯¼åœ°ä½ï¼Œè‹¹æœè¿™ä¸‹ä¹Ÿä¸å¾—ä¸è½¬å˜å§¿æ€äº†ã€‚
  - è¿‡è®©MLXæ¡†æ¶ä¸»åŠ¨é€‚é…CUDAï¼Œä»Šåè‹¹æœå¼€å‘è€…ä¹Ÿèƒ½åˆ©ç”¨è‹±ä¼Ÿè¾¾GPUè®­ç»ƒæ¨¡å‹ã€‚å…¶æœ¬è´¨æ˜¯å¢åŠ äº†å¯¹CUDAçš„åç«¯æ”¯æŒï¼Œæ–¹ä¾¿å¼€å‘è€…åœ¨Windows/Linuxçš„è‹±ä¼Ÿè¾¾æ˜¾å¡ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œç„¶åå†éƒ¨ç½²å›Macã€iPhoneç­‰è®¾å¤‡ã€‚
  - æ˜¯ç»™Win-vidia/Lin-vidiaåšMLXæ”¯æŒï¼Œä¸æ˜¯ç»™Macç‰ˆMLXåšCUDAé©±åŠ¨

- è¿™ä¸¤å¤©å°è¯•å»å­¦ä¹ äº†ä¸€ä¸‹ mlx å’Œ mlx çš„æºç ï¼Œç²—ç•¥çš„çœ‹æ³•æœ‰ 2 ç‚¹ï¼š
  - å®è§‚ä¸Šï¼Œæ¡†æ¶çš„è®¾è®¡å¾ˆæ£’ï¼Œæ•´ä½“è®¾è®¡ä¸Šå¾ˆå¥½åœ°å¸å–äº†å‰äººçš„ç»éªŒï¼Œåœ¨ä¿è¯æ˜“ç”¨æ€§çš„å‰æä¸‹ï¼Œä¿ç•™äº†è¶³å¤Ÿé«˜çš„ä¼˜åŒ–ä¸Šé™ï¼›
  - å¾®è§‚ä¸Šï¼Œé¡¹ç›®çš„ C++ æ°´å¹³æ¯”è¾ƒä¸€èˆ¬ï¼Œæ¯”è¾ƒåƒæ˜¯å‡ ä¸ªå·¥ç¨‹å¸ˆçš„ side projectï¼Œå¦‚æœè¦å˜æˆé•¿é’æ ‘ï¼Œå†…éƒ¨å¤§é‡çš„å®ç°éƒ½éœ€è¦é‡æ„
- å…³äºæ¶æ„ã€‚æˆ‘è§‰å¾—ä¹‹å‰å‡ å¹´ï¼Œè®­ç»ƒæ¡†æ¶ä¸Šçš„å®è·µæ€»ç»“äº†è¿™æ ·çš„ä¸€äº›ç»éªŒï¼š
  - pytorch å’Œ tensorflow çš„äº‰æ–—ç»“æœå‘Šè¯‰æˆ‘ä»¬ï¼Œè®©å¤§å®¶å†™ pythonï¼Œè€Œä¸æ˜¯åœ¨ python é‡Œå†™ DSLï¼Œæå‡æ˜“ç”¨æ€§æ˜¯å¾ˆé‡è¦çš„ï¼›
  - torchscriptã€libtorch çš„å„ç§å‘å‘Šè¯‰æˆ‘ä»¬ï¼Œå®Œå…¨ä¾èµ–åœ¨ python ä¸Šï¼Œå›¾ä¼˜åŒ–å¾ˆæ£˜æ‰‹ï¼Œæ‰”æ‰‹æœºè¿™æ ·æ²¡æœ‰ python runtime çš„åœ°æ–¹ä¹Ÿæ˜¯çœŸçš„å¤´ç–¼ã€‚æœ€å¥½ä¸è¦çº¯ eager æ¨¡å¼ï¼Œè€Œæ˜¯èƒ½è®©æ¡†æ¶æå–å‡ºæ¥ä¸€äº›å­å›¾ã€‚æˆ–è€…æ˜¯æœ‰ä¸ªå›ºå®šç‚¹çš„ c apiï¼Œç»™ä¸è®­æ¨¡å‹çš„å·¥ç¨‹å¸ˆä»¬ç•™æ¡æ´»è·¯ï¼›
  - transformer å‘Šè¯‰æˆ‘ä»¬ï¼Œä¹Ÿè®¸ä»å­¦æœ¯è§’åº¦æ¥è¯´ï¼Œæ§åˆ¶æµä¼˜åŒ–èµ·æ¥å¾ˆæ€§æ„Ÿï¼Œä½†æ˜¯å®é™…åº”ç”¨ä¸­ï¼Œå…¨æ˜¯ä¸€æ¡è·¯èµ°åˆ°é»‘ï¼›
- ä¹‹å‰æˆ‘è§‰å¾— jax å°±æ˜¯è®­ç»ƒæ¡†æ¶ä¸Šçš„é›†å¤§æˆè€…ï¼Œå„ç§è®¾è®¡çœŸçš„å¾ˆæ¼‚äº®ã€‚å¯æ˜¯å®ƒæ‹–äº†ä¸ªé‡é‡çš„ XLA çš„å°¾å·´ï¼Œå¯èƒ½æ˜¯ä¸ºäº†è·‘åœ¨ TPU ä¸Šï¼Œä¹Ÿå¯èƒ½æ˜¯ jax çš„æˆå‘˜ä¸­æœ‰å¾ˆå¤š tf é‡Œåšç¼–è¯‘ä¼˜åŒ–çš„äººå§ã€‚è¿™ä½¿å¾—æ—©æœŸçš„ jax å¯¹ç”¨ nv æ˜¾å¡çš„ç”¨æˆ·å¾ˆä¸å‹å¥½ï¼Œç­‰ pytorch/huggingface æˆäº†æ°”å€™ï¼Œä¹Ÿå°±æ— åŠ›å›å¤©äº†ã€‚
- ç°åœ¨ mlx åŸºæœ¬å¸æ”¶äº† jax çš„ç»éªŒï¼Œ
  - é‡‡ç”¨äº† numpy api + lazy evaluationï¼›
  - æœ‰ä¸€ä¸ªæ¯”è¾ƒå¹²å‡€çš„ c apiï¼Œç”šè‡³å·²ç»æœ‰äº† swift bindingï¼›
  - å†…éƒ¨ç•™å‡ºäº†ç®€å•çš„å›¾ä¼˜åŒ–æ¥å£ mlx.core.compile ï¼Œä¿è¯äº†ä¼˜åŒ–çš„ä¸Šé™ã€‚

- çœ‹äº†ä¸€çœ¼æ¡†æ¶æºç ï¼Œå®Œå…¨æ˜¯é‡æ–°æäº†ä¸€å¥—æ¡†æ¶ï¼ŒAPIå°½é‡å¯¹æ ‡PyTorchï¼Œç›®å‰ä¹Ÿåªæ”¯æŒå°‘é‡APIã€‚è¿™æ ·æ¡†æ¶é€‚é…æˆæœ¬æ„Ÿè§‰æœ‰ç‚¹å¤§ï¼Œè¿å‰ç«¯Python APIéƒ½è¦è‡ªå·±æ„å»ºï¼Œå¤åˆ»å¤§é‡ä»£ç é€»è¾‘ä¸”ä¸è¯´ï¼ŒPyTorchç¤¾åŒºçš„å…¨é‡featureçš„è·Ÿè¸ªæ¼”è¿›å°±ä¼šæ˜¯ä¸€ä¸ªéš¾é¢˜ã€‚
  - æ„Ÿè§‰Appleå’Œæ˜‡è…¾èµ°å‘äº†ä¸¤ä¸ªæ–¹å‘çš„æç«¯ï¼Œæ˜‡è…¾æ˜¯å®Œå…¨ä¾èµ–åŸç”Ÿçš„æ’ä»¶åŒ–ï¼Œä¼šæƒ³å°½ä¸€åˆ‡åŠæ³•å¤ç”¨åŸç”Ÿé€»è¾‘ï¼Œå®åœ¨ä¸è¡Œå°±å¤åˆ»ï¼›Appleåˆ™æ˜¯å®Œå…¨æŠ›å¼ƒåŸç”Ÿé€»è¾‘ï¼Œç«¯åˆ°ç«¯é‡æ–°æä¸€å¥—ã€‚

- çœ‹äº†ä¸‹ä»£ç ï¼Œè‹¹æœæ˜¯é‡æ–°é€ äº†ä¸ªè½®å­ï¼Œå‰ç«¯å¹²çš„äº‹å’ŒPyTorchå’ŒJaxæ²¡å•¥åŒºåˆ«ï¼Œæ„Ÿè§‰é‡ç‚¹è¿˜æ˜¯åœ¨Apple Siliconçš„åç«¯ä¸Šã€‚ç›®å‰çš„å®ç°è¿˜æ˜¯æŒºå°å·§çš„ï¼Œç”¨æ¥å­¦ä¹ çš„è¯è¿˜ä¸é”™ï¼Œä½†è¦åœ¨featureä¸Šå¯¹é½ç°æœ‰çš„æ¡†æ¶è¿˜æœ‰ä¸å°‘çš„æ´»è¦å¹²ã€‚

- ä¸ªäººæ„Ÿè§‰è¿™ç©æ„æœ‰ç‚¹é¸¡è‚‹ã€‚
  - macå†™ä»£ç ã€è°ƒè¯•ï¼Œç„¶åæœåŠ¡å™¨å®Œæˆæ•´ä¸ªè®­ç»ƒã€‚è¿™å°±æœ‰é—®é¢˜äº†ï¼ŒæœåŠ¡å™¨ä¸æ”¯æŒMLXæ¡†æ¶ã€‚
  - macå†™ä»£ç ã€è°ƒè¯•ï¼Œç„¶åmacå®Œæˆæ•´ä¸ªè®­ç»ƒã€‚ æ­£å¸¸å…¬å¸è·‘æ¨¡å‹ï¼Œå¦‚æœå°‘é‡æ•°æ®ä½¿ç”¨å‡ å¼ æ·˜æ±°ä¸‹æ¥çš„1080å°±å¯ä»¥ï¼Œä»·æ ¼è¿˜ä¾¿å®œã€‚æ•°æ®é‡å¤§ï¼Œè¦ä½¿ç”¨åˆ†å¸ƒå¼å¹¶è¡Œè®­ç»ƒï¼Œé‚£å°±æ²¡macä»€ä¹ˆäº‹æƒ…äº†ã€‚
  - æœ€è¿‘ç«çƒ­çš„å¤§æ¨¡å‹è®­ç»ƒï¼Œå“ªå®¶å…¬å¸ä¼šç”¨macæ­å»ºé›†ç¾¤è®­ç»ƒå¤§æ¨¡å‹ï¼Œå¹¶ä¸”è®­ç»ƒå®Œæˆåä½¿ç”¨macåšæ¨ç†ã€‚ç”¨ä¸åˆ°macå°±æ²¡MLXä»€ä¹ˆäº‹æƒ…äº†ã€‚

- æˆ‘è§‰å¾—å¯ä»¥å‚è€ƒæ—©æœŸswiftï¼Œä¸€å¹´ä¸€ä¸ªç‰ˆæœ¬ï¼Œè®©ä½ æ„Ÿåˆ°å´©æºƒï¼Œè™½ç„¶è¿™æ¬¡å¼€æºï¼Œä½†æ˜¯ä¸»åŠ›åº”è¯¥è¿˜æ˜¯è‹¹æœè‡ªå·±

- [é˜¿é‡Œå·´å·´å‘å¸ƒäº†ä¸è‹¹æœ MLX æ¶æ„å…¼å®¹çš„ Qwen3 å‡çº§ç‰ˆï¼Œæ–°ç‰ˆæœ¬éƒ½æœ‰å“ªäº›æ–°ç‰¹æ€§ï¼Ÿ - çŸ¥ä¹ _202506](https://www.zhihu.com/question/1918255662552582106)
  - MLXæ˜¯è‹¹æœä¸“ä¸ºApple SiliconèŠ¯ç‰‡ï¼ˆMç³»åˆ—ï¼‰è®¾è®¡çš„å¼€æºæœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œå®ƒçš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºæ·±åº¦æ•´åˆè‹¹æœç¡¬ä»¶ç‰¹æ€§ï¼Œå°¤å…¶ä»¥â€œç»Ÿä¸€å†…å­˜æ¨¡å‹â€ä¸ºæ ¸å¿ƒåˆ›æ–°
  - ç»Ÿä¸€å†…å­˜æ¶æ„ï¼šæ•°æ®å­˜å‚¨åœ¨CPUä¸GPUå…±äº«çš„å†…å­˜ç©ºé—´ä¸­ï¼Œè·¨è®¾å¤‡è®¡ç®—æ—¶æ— éœ€æ˜¾å¼æ‹·è´æ•°æ®ï¼Œç›¸æ¯”ä¼ ç»Ÿæ¡†æ¶ï¼ˆå¦‚PyTorchï¼‰å‡å°‘90%ä»¥ä¸Šçš„å†…å­˜ä¼ è¾“å¼€é”€ã€‚

- ## [å¦‚ä½•çœ‹å¾… Google æœ€æ–°å¼€æºçš„ Gemma-3 ç³»åˆ—å¤§æ¨¡å‹ï¼Ÿ - çŸ¥ä¹ _202503](https://www.zhihu.com/question/14777841836)

- Gemma-3ä¸»æ‰“çš„å–ç‚¹æ˜¯å•å¡å¯è·‘+å¤šæ¨¡æ€ã€‚
- Gemma-3çš„æ ¸å¿ƒç«äº‰åŠ›åœ¨äºæ•ˆç‡é©å‘½ã€‚å…¶27Bç‰ˆæœ¬ä»…éœ€å•å¼ H100æ˜¾å¡å³å¯è¿è¡Œï¼Œè€Œç«å“è¾¾åˆ°åŒç­‰æ€§èƒ½å¾€å¾€éœ€è¦10å€ç®—åŠ›ã€‚è¿™ç§æ•ˆç‡æå‡å¹¶éç®€å•çš„å‚æ•°å‹ç¼©ï¼Œè€Œæ˜¯æ¶æ„å±‚é¢çš„åˆ›æ–°
  - çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼šåŸºäºGemini 2.0çš„"æ•™å¸ˆæ¨¡å‹"è¿›è¡Œè’¸é¦ï¼Œå°†å¤§æ¨¡å‹èƒ½åŠ›è¿ç§»åˆ°å°æ¨¡å‹ï¼Œå®ç°"ç”¨1/10å‚æ•°è¾¾åˆ°80%æ€§èƒ½"çš„æ•ˆæœã€‚
- å°½ç®¡Gemma-3è¡¨ç°äº®çœ¼ï¼Œä»éœ€æ³¨æ„ä»¥ä¸‹é—®é¢˜ï¼š 
  - 1. é•¿å°¾ä»»åŠ¡ç¼ºé™·ï¼šåœ¨æ•°å­¦æ¨ç†å’Œä¼¦ç†åˆ¤æ–­ç­‰ç‰¹å®šåœºæ™¯ï¼Œå®æµ‹ç»“æœæ˜¾ç¤ºå…¶è¡¨ç°ä¸å¦‚Qwen2.5-Maxã€‚ 
  - 2. å¤šæ¨¡æ€ç²¾åº¦ç“¶é¢ˆï¼šå¤„ç†å¤æ‚å›¾åƒæ—¶ï¼ŒSigLIPç¼–ç å™¨çš„ç»†èŠ‚è¯†åˆ«èƒ½åŠ›ä»è½åäºä¸“ä¸šCVæ¨¡å‹ã€‚ 
  - 3. è®­ç»ƒæ•°æ®ä¾èµ–ï¼šè™½ç„¶æ”¯æŒ140ç§è¯­è¨€ï¼Œä½†å®é™…è¡¨ç°å­˜åœ¨"è‹±è¯­æœ€ä¼˜ï¼Œå°è¯­ç§æ¬¡ä¹‹"çš„é—®é¢˜ã€‚ 
- æœªæ¥å¯èƒ½çš„çªç ´æ–¹å‘åŒ…æ‹¬ï¼š 
  - åŠ¨æ€æ¶æ„ï¼šæ ¹æ®è¾“å…¥å†…å®¹è‡ªåŠ¨åˆ‡æ¢è®¡ç®—èµ„æºåˆ†é…ï¼Œä¾‹å¦‚è§†é¢‘å¤„ç†æ—¶ä¸´æ—¶è°ƒç”¨æ›´å¤šGPUæ ¸å¿ƒã€‚ 
  - è·¨æ¨¡æ€ç”Ÿæˆï¼šä»"ç†è§£å¤šæ¨¡æ€"å‡çº§åˆ°"ç”Ÿæˆå¤šæ¨¡æ€"ï¼Œä¾‹å¦‚æ ¹æ®æ–‡æœ¬æè¿°ç›´æ¥ç”Ÿæˆ3Dæ¨¡å‹ã€‚ 
  - è¾¹ç¼˜ç«¯ä¼˜åŒ–ï¼šè¿›ä¸€æ­¥å‹ç¼©æ¨¡å‹ä½“ç§¯ï¼Œåœ¨æ‰‹æœºç«¯å®ç°å®æ—¶å¤šæ¨¡æ€äº¤äº’ã€‚

- å¼€æºGemmaç³»æ¨¡å‹æ­£å¼è¿­ä»£åˆ°ç¬¬ä¸‰ä»£ï¼ŒåŸç”Ÿæ”¯æŒå¤šæ¨¡æ€ï¼Œ128kä¸Šä¸‹æ–‡ã€‚
  - Gemma 3ä¸€å…±å¼€æºäº†å››ç§å‚æ•°ï¼Œ1Bã€4Bã€12Bå’Œ27Bã€‚æœ€æœ€æœ€å…³é”®çš„æ˜¯ï¼Œä¸€å—GPU/TPUå°±èƒ½è·‘æ¨¡å‹ã€‚
  - åœ¨LMArenaç«æŠ€åœºä¸­ï¼ŒGemma 3æ‹¿ä¸‹äº†1339 ELOé«˜åˆ†ï¼Œä»…ä»¥27Bå‚æ•°å‡»è´¥äº†o1-previewã€o3-mini highã€DeepSeek V3ï¼Œå ªç§°ä»…æ¬¡äºDeepSeek R1æœ€ä¼˜å¼€æºæ¨¡å‹ã€‚
  - Gemma3ç³»1Bã€4Bã€12Bã€27Båˆ†åˆ«åŸºäº2Tã€4Tã€12Tã€14T tokenæ•°æ®å®Œæˆè®­ç»ƒã€‚
  - ä¸é—­æºGemini 1.5å’Œ2.0ç›¸æ¯”ï¼ŒGemma 3-27BåŸºæœ¬ä¸Šç•¥é€Šè‰²äºFlashç‰ˆæœ¬ã€‚
- é‡‡ç”¨ä¸Gemini 2.0æ¨¡å‹ç›¸åŒçš„ç ”ç©¶å’ŒæŠ€æœ¯æ‰“é€ ã€‚

- æœ€å¤§çš„17GBæ˜¯ç¨³ç¨³å¯ä»¥è·‘åœ¨æ‹¥æœ‰24GBæ˜¾å­˜çš„Nvidia90ç³»æ˜¾å¡çš„ã€‚

- Gemma 3n æ˜¯ä¸€æ¬¾ä¸“ä¸ºç§»åŠ¨è®¾å¤‡é‡èº«æ‰“é€ çš„å¤šæ¨¡æ€ AI æ¶æ„ï¼Œå®ƒä¸ä»…æ”¯æŒå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘å’Œæ–‡æœ¬çš„è¾“å…¥è¾“å‡ºï¼Œè¿˜é€šè¿‡åˆ›æ–°çš„ MatFormer æ¶æ„å’Œ Per-Layer Embeddings æŠ€æœ¯ï¼Œå®ç°äº†é«˜æ€§èƒ½ä¸ä½å†…å­˜å ç”¨çš„å®Œç¾å¹³è¡¡ã€‚
  - E2B å’Œ E4B ä¸¤ç§æ¨¡å‹ç‰ˆæœ¬ï¼Œåˆ†åˆ«ä»…éœ€ 2GB å’Œ 3GB å†…å­˜å³å¯è¿è¡Œï¼Œè€Œä¸”è¿˜èƒ½é€šè¿‡ Mix-n-Match æ–¹æ³•çµæ´»è°ƒæ•´æ¨¡å‹å¤§å°ï¼Œæ»¡è¶³ä¸åŒåœºæ™¯çš„éœ€æ±‚ã€‚

- Gemma 3n æä¾›äº†å¤šç§éƒ¨ç½²é€‰é¡¹ï¼ŒåŒ…æ‹¬ Google GenAI APIã€Vertex AIã€SGLangã€vLLM å’Œ NVIDIA API Catalog ç­‰ã€‚
  - æ­¤å¤–ï¼ŒGemma 3n è¿˜ä¸ Hugging Faceã€Google AI Edgeã€Ollamaã€MLX ç­‰å¤šç§å·¥å…·å’Œå¹³å°æ— ç¼é›†æˆï¼Œè®©å¼€å‘è€…å¯ä»¥å¿«é€Ÿä¸Šæ‰‹ï¼Œè½»æ¾é›†æˆåˆ°ç°æœ‰é¡¹ç›®ä¸­ã€‚

- ### [Difference between Gemma and Gemini - Marvik _202407](https://blog.marvik.ai/2024/07/03/difference-between-gemma-and-gemini/)
- While Gemma is open source and lightweight, Gemini is proprietary and heavyweight. 

- ## [å¦‚æœæœ¬åœ°è¦è£…å¤§æ¨¡å‹ï¼Œå»ºè®®å“ªä¸ªå¼€æºå¤§æ¨¡å‹ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/653255605)
- ollamaåŸå§‹çš„ç‰ˆæœ¬æ˜¯å¹¶ä¸å…¼å®¹ OpenAI æ ¼å¼çš„è¯·æ±‚ï¼Œå› æ­¤åœ¨ä½¿ç”¨æ¯”å¦‚ langchainã€llamaindex ç­‰æ¡†æ¶çš„æ—¶å€™ï¼Œä¼šé‡åˆ°ä¸€äº›éº»çƒ¦çš„åœ°æ–¹ï¼Œéœ€è¦å†è½¬åŒ–ä¸‹ã€‚
  - ç°åœ¨ï¼Œæ›´æ–°åˆ°æ–°ç‰ˆæœ¬çš„æ—¶å€™ï¼Œå°±æä¾›äº† OpenAI æ ¼å¼çš„æ¥å£è°ƒç”¨
  - ç›®å‰è¿™ä¸ª OpenAI-compatible API è¿˜å¤„äºæµ‹è¯•é˜¶æ®µï¼Œå¹¶ä¸æ”¯æŒ function calling çš„åŠŸèƒ½ã€‚

- æˆ‘æœ¬åœ°ollamaè·‘ä¸€å¼ P40/24Gæ˜¾å­˜ï¼Œç©äº†ä¸€å¹´å¤šäº†ã€‚æˆ‘ä¸€èˆ¬è·‘é‡åŒ–è¿‡çš„32Bçš„æ¨¡å‹ï¼Œå†å¾€ä¸Šé€Ÿåº¦å¤ªæ…¢æˆ–è€…é‡åŒ–å¤ªç‹ ï¼Œéƒ½æ²¡å¤ªå¤§å®ç”¨ä»·å€¼ã€‚

- çœ‹ä½ æ˜¾å­˜å¤§å°é€‚åˆå¤šå°‘Bçš„æ¨¡å‹ã€‚è¯„ä¼°æ—¶å€™è¦è€ƒè™‘é‡åŒ–int4ã€int8è¿˜æ˜¯float16ã€‚å¯ä»¥åˆ©ç”¨huggingfaceä¸Šçš„è®¡ç®—å™¨è¯„ä¼° https://hf-accelerate-model-memory-usage.hf.space
  - é™¤äº†ollamaå®˜æ–¹åˆ—è¡¨ ä¹Ÿå¯ä»¥å»æ‰¾hugging faceä¸Šé‡åŒ–å¥½çš„æ¨¡å‹ã€‚
- ollamaæ˜¯åŸºäºllama.cpp, æ‰€ä»¥ä¹Ÿå¯ä»¥åŸºäºllama.cpp è‡ªå·±æŠ˜è…¾ä¸€éï¼Œä»è½¬æ¢æ¨¡å‹ä¸ºggufåˆ°é‡åŒ–ä¸ºint4 è¿˜æ˜¯int8ï¼Œå¯ä»¥æŒ‰ç…§æˆ‘ä¹‹å‰å†™çš„æ•™ç¨‹è‡ªå·±èµ°ä¸€éï¼Œå¯ç©æ€§æ›´å¼ºä¸€ç‚¹ã€‚æˆ‘ç”µè„‘mac m1 è·‘é€šä¹‰åƒé—®çš„7bçš„int4é‡åŒ–æ¨¡å‹ï¼Œå¤§çº¦æ¯ç§’12ä¸ªtokenã€‚

- éä¸“ä¸šäººå£«çš„è¯ï¼Œollamaå’Œlocalaiï¼Œè¿™ä¸¤æ— è„‘éƒ¨ç½²è¿è¡Œã€‚åŒæ—¶è¿™ä¸¤ä¸å±€é™ä½ æœ‰æ²¡æœ‰GPUï¼Œcpuä¹Ÿå¯ä»¥æã€‚

- å¤ªå°çš„æ¨¡å‹ï¼Œæƒ³1.5bå¤§å°çš„ï¼Œæ€§èƒ½è‚¯å®šå·®ä¸€äº›ï¼Œä¸è¿‡ç»“åˆragè¿™äº›ï¼Œå®ç°æœ¬åœ°çŸ¥è¯†åº“ä¹Ÿå·®ä¸å¤šäº†ã€‚åšå®éªŒï¼Œæ­£å¸¸è¿˜æ˜¯ç”¨7bå·¦å³çš„ã€‚

- è‡ªå·±æœ¬åœ°æ­å»ºçš„æ•°æ®é‡å¤ªå°‘äº†ï¼Œè”æƒ³å…³é”®è¯å›°éš¾

- ## [ç›®å‰æœ‰ä»€ä¹ˆå¯ä»¥æœ¬åœ°éƒ¨ç½²çš„å¤§æ¨¡å‹æ¨è? - çŸ¥ä¹](https://www.zhihu.com/question/648879790)
- å¼€æºå¤§æ¨¡å‹æ›´æ–°è¿­ä»£å¤ªå¿«ï¼Œä»Šå¹´åˆšæ¨å‡ºçš„æ¨¡å‹å¯èƒ½è¿‡å‡ ä¸ªæœˆå°±è¿‡æ—¶äº†ã€‚
- ä¸€æ˜¯å¦‚ä½•æ‰¾åˆ°æœ€æ–°çš„å¤§æ¨¡å‹ï¼Œ
  - huggingfaceå¯ä»¥ç†è§£ä¸ºå¯¹äºAIå¼€å‘è€…çš„GitHubï¼Œæä¾›äº†æ¨¡å‹ã€æ•°æ®é›†ï¼ˆæ–‡æœ¬|å›¾åƒ|éŸ³é¢‘|è§†é¢‘ï¼‰ã€ç±»åº“ï¼ˆæ¯”å¦‚transformers|peft|accelerateï¼‰ã€æ•™ç¨‹ç­‰ã€‚
  - huggingfaceæœ‰æ—¶å­˜åœ¨ç½‘ç»œä¸ç¨³å®šçš„é—®é¢˜, è¿™é‡Œæ¨èå›½å†…æ¯”è¾ƒå¥½çš„å¹³å°modelscope
- äºŒæ˜¯å¦‚ä½•åˆ¤æ–­æœ¬åœ°ç¡¬ä»¶èµ„æºæ˜¯å¦æ»¡è¶³å¤§æ¨¡å‹çš„éœ€æ±‚ï¼Œ
  - æœ¬åœ°å¯ä»¥éƒ¨ç½²ä»€ä¹ˆå¤§æ¨¡å‹ï¼Œå–å†³äºä½ çš„ç¡¬ä»¶é…ç½®ï¼ˆå°¤å…¶å…³æ³¨ä½ GPUçš„æ˜¾å­˜ï¼‰ã€‚
  - åœ¨æ²¡æœ‰è€ƒè™‘ä»»ä½•æ¨¡å‹é‡åŒ–æŠ€æœ¯çš„å‰æä¸‹ï¼š å…¬å¼ï¼šæ¨¡å‹æ˜¾å­˜å ç”¨ï¼ˆGBï¼‰ = å¤§æ¨¡å‹å‚æ•°ï¼ˆBï¼‰*2
  - åœ¨4bité‡åŒ–çš„æƒ…å†µï¼Œæ»¡è¶³æœ¬åœ°æœºå™¨GPUæ˜¾å­˜ï¼ˆGBï¼‰ >= å¤§æ¨¡å‹å‚æ•°ï¼ˆBï¼‰/2ï¼Œå¯ä»¥å°è¯•æœ¬åœ°éƒ¨ç½²ã€‚
- ä¸‰æ˜¯å¦‚ä½•å¿«é€Ÿéƒ¨ç½²å¤§æ¨¡å‹ã€‚
  - ollama pull llama3

- æˆ‘è¯´çš„æœ¬åœ°ï¼ŒæŒ‡çš„ä¸æ˜¯ä¸€å°ä¸ªäººç”µè„‘ä¸Šï¼Œè·‘ä¸€ä¸ª7Bã€13Bå‚æ•°çš„å¤§æ¨¡å‹ã€‚è€Œæ˜¯åœ¨ä¼ä¸šæœ¬åœ°ç®—åŠ›æœåŠ¡å™¨ä¸Šï¼Œç§æœ‰åŒ–éƒ¨ç½²çš„700äº¿å‚æ•°ä»¥ä¸Šè§„æ¨¡çš„å¤§æ¨¡å‹ï¼Œè¿™ç§å‚æ•°è§„æ¨¡çš„å¤§æ¨¡å‹ï¼Œæ‰æœ‰æ›´å¥½çš„æŒ‡ä»¤ä¾ä»æ€§ï¼Œç»“åˆRAGã€Agentç­‰æŠ€æœ¯ï¼Œèƒ½æœ‰æ•ˆçš„å®Œæˆä½ åˆ†é…ç»™ä»–çš„ä»»åŠ¡ã€‚ 
# discuss-nvidia/amd/linux
- ## 

- ## 

- ## 

- ## 

- ## ğŸ†š [AMD AI Max+ 395 CPU æœ¬åœ°å¤§æ¨¡å‹æ¨ç†æ€§èƒ½è¯„æµ‹æŠ¥å‘Š - çŸ¥ä¹ _202509](https://zhuanlan.zhihu.com/p/1952045270763283746)
- é’ˆå¯¹æ­è½½AMD AI Max+ 395 CPUçš„é›¶åˆ»GTR9è¿·ä½ ä¸»æœºè¿›è¡Œäº†ä¸€ç³»åˆ—ä¸¥æ ¼çš„å¤§æ¨¡å‹æ¨ç†é€Ÿåº¦æµ‹è¯•ã€‚
  - ç¡¬ä»¶å¹³å°: é›¶åˆ» (MINISFORUM) GTR9 è¿·ä½ ä¸»æœº
  - æ ¸å¿ƒç»„ä»¶: AMD AI Max+ 395 CPU
  - ä»»åŠ¡ç±»å‹: æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹æ¨ç†
  - æ€§èƒ½æŒ‡æ ‡: Tokens/s (æ¯ç§’ç”ŸæˆTokenæ•°) â€” è¯¥æ•°å€¼è¶Šé«˜ï¼Œä»£è¡¨æ¨ç†é€Ÿåº¦è¶Šå¿«
- è®¾è®¡äº†æ¶µç›–å¤šç§ä»»åŠ¡ç±»å‹çš„æ ‡å‡†åŒ–é—®é¢˜ï¼š
  - ç»¼åˆèƒ½åŠ›: "ä½ æ˜¯è°ï¼Ÿè¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹ä½ èƒ½å¹²ä»€ä¹ˆã€‚"
  - çŸ¥è¯†é—®ç­”: "ä½œä¸ºä¸“ä¸šäººå·¥æ™ºèƒ½ä¸“å®¶ï¼Œè¯·å‘Šè¯‰æˆ‘å¦‚ä½•å­¦ä¹ æ·±åº¦å­¦ä¹ ï¼Ÿ"
  - æ•°å­¦è®¡ç®—: "å¦‚æœA+B=12, A-B=10ï¼Œåˆ™Açš„å€¼æ˜¯ï¼Ÿ"
  - è‡ªç„¶è¯­è¨€ç†è§£: "è¯†åˆ«å¥å­â€˜æˆ‘å°†ä¼šåœ¨æ˜å¤©æ—©ä¸Šçš„8ç‚¹åˆ°æ¹–åŒ—é»„é™‚çš„æ£®æ—å…¬å›­â€™ä¸­çš„æ‰€æœ‰åœ°åã€‚"
  - ä»£ç ç”Ÿæˆ: "è¯·ä½¿ç”¨Pythonç¼–å†™ä¸€ä¸ªè´ªåƒè›‡æ¸¸æˆã€‚"

- å‚è¯„å¤§æ¨¡å‹:
  - deepseek-r1:70b, 30
  - qwen3 ç³»åˆ—ï¼ˆ32b / 30b / 14b / 8bï¼‰
  - gpt-ossï¼ˆ120b / 20bï¼‰

```markdown

- model,          ollama, lmstudio
- deepseek-r1:70b, 4.43,   4.97
- qwen3:32b,       8.97,   10.12
- qwen3:14b,       19.47,  21.70
- qwen3:8b,        29.93,  35.96
- gpt-oss:120b,    30.84,  42.07
- gpt-oss:20b,     42.57,  60.54
- qwen3:30b,       48.93,  68.70

```

- å¯¹æ¯”ä¸¤ç»„æ•°æ®å¯è§ï¼ŒåŒä¸€æ¨¡å‹åœ¨LM-Studioä¸­çš„æ¨ç†é€Ÿåº¦æ™®éä¼˜äºOllama
- AMD AI Max+ 395 CPUé‡‡ç”¨CPU/GPUå…±äº«å†…å­˜çš„ç»Ÿä¸€å†…å­˜æ¶æ„ï¼ˆUMAï¼‰ï¼Œè¿™ç§è®¾è®¡å¤©ç„¶é€‚åˆè¿è¡Œæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼ˆå¦‚gpt-ossç³»åˆ—ã€qwen3:30bï¼‰ã€‚
  - MoEæ¨¡å‹è™½ç„¶æ€»å‚æ•°é‡åºå¤§ï¼Œä½†æ¯æ¬¡æ¨ç†ä»…æ¿€æ´»éƒ¨åˆ†"ä¸“å®¶"å‚æ•°ï¼Œéå¸¸å¥‘åˆè¿™ç§å¤§å®¹é‡å†…å­˜ä½†ç»å¯¹ç®—åŠ›ç›¸å¯¹æœ‰é™çš„ç¡¬ä»¶ã€‚
  - ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¯¹äºå‚æ•°å¯†é›†çš„ä¼ ç»Ÿç¨ å¯†æ¨¡å‹ï¼ˆå¦‚deepseek-r1:70bã€qwen3:32bï¼‰ï¼Œç”±äºéœ€è¦æ›´é«˜çš„ç»å¯¹ç®—åŠ›ï¼Œè¯¥å¤„ç†å™¨çš„é›†æˆæ˜¾å¡åˆ™ç¨æ˜¾åƒåŠ›ã€‚

- DFRobotä½œä¸ºåœ¨å•æ¿è®¡ç®—æœºï¼ˆSBCï¼‰ã€AIè¾¹ç¼˜è®¡ç®—å’Œå¼€æºç¡¬ä»¶é¢†åŸŸçš„åˆ›æ–°è€…ï¼Œæ­¤æ¬¡æµ‹è¯•ç»“æœæ„ä¹‰éå‡¡ã€‚è‹¥æœªæ¥DFRobotæ¨å‡ºåŸºäºAMD AI Max+ 395 CPUçš„å•æ¿è®¡ç®—æœºï¼Œå°†å…¶å¼ºå¤§çš„æœ¬åœ°AIæ¨ç†èƒ½åŠ›ä¸DFRobotæˆç†Ÿçš„æ¨¡å—åŒ–ä¼ æ„Ÿå™¨ç”Ÿæ€ï¼ˆå¦‚Gravityç³»åˆ—ï¼‰ç›¸ç»“åˆï¼Œå°†å‚¬ç”Ÿå‡ºæ›´å¤šå®æ—¶ã€æ™ºèƒ½çš„ç‰©è”ç½‘ä¸æœºå™¨äººåº”ç”¨
# discuss-mac-mlx ğŸ
- ## 

- ## 

- ## 

- ## 

- ## [Best Local LLM for MacBook Air M3 with 24GB RAM : r/LocalLLaMA _202404](https://www.reddit.com/r/LocalLLaMA/comments/1bu65s6/best_local_llm_for_macbook_air_m3_with_24gb_ram/)
- Rough rule of thumb is 2xParam Count in B = GB needed for model in FP16.
  - From there reducing precision scales more or less linearly. 
  - So 1B param model = 1GB RAM needed INT8, or 0.5GB RAM needed INT4. 

- ## [Any experiences running LLMs on a MacBook? : r/LocalLLaMA _202507](https://www.reddit.com/r/LocalLLaMA/comments/1m1t19r/any_experiences_running_llms_on_a_macbook/)
- Pretty good experience on M4 Max with 128Gb, Qwen3-30B-A3B (8bit quants). Speed on small inputs is around 40-50 toks/s, which is very very usable.
  - I have the same setup I would say not very good. lol. But thatâ€™s because I try to use models for things like cline and opencode. Itâ€™s just soooo slow on initial prompt and even later on as well. For chats with 24bâ€™s itâ€™s great though
- I feel that almost any thinking models feel pretty slow in coding assistants. I would prefer them to answer faster with the same quality

- Qwen3-30B-A3B is the goat on my M3 Ultra (96 gb). I asked a few coding/ML questions, and I'm getting between 55 - 70 tok/s

- I get vastly better results with qwen3 32B in 4bit than qwen3 30B in 8bit. 

- Prompt: Write a binary search function in javascript
  - All models loaded w/ max context window, lm studio, mlx backend.
- M4 Max MacBook Pro 128GB (mostly 4bit):
  kimi-dev-72b-dwq: 11 tps
  qwen3-53b-a3b: 45tps
  qwen3-30b-a3b-dwq: 96tps
  devstral-small-2507-dwq: 33tps
  gemma-3n-e4b-it: 75tps
  jan-nank-128k (8bit): 90tps
  qwen3-4b-dwq-053125: 142tps
  qwen3-1.7b-dwq-053125: 252tps
- M2 MacBook Air 24GB:
  qwen3-30b-a3b (3bit): 32tps
  qwen3-4b-dwq-053125: 30tps
  qwen3-1.7b-dwq-053125: 66tps
  devstral-small-2507-dwq: 6tps
  gemma-3n-e4b-it: 25tps
  jan-nano-128k (8bit): 16tps
  jan-nano-128k (4bit): 31tps

- I have M4 pro with 48GB RAM. I can run local models maximum 32B (4/6Q). Gemma 3 27b/Qwen 3 32B is good enough to use for general QnA purpose. For dev assistance, it lacks accuracy and generation speed on Mac M4. I would choose R1 or else in Openrouter. However definitely battery runs out faster with local models.
  - iâ€™d recommend giving devstral small a shot itâ€™s surprisingly really good and punches above its weight.

- Get the pro and with as much unified memory as you can afford. Iâ€™m running a M4 Max with 64GB and I regret not getting the 128
  - you might want to increase the portion of unified memory allocated to the GPU
  - LM Studio has awful default settings when you download a model, always check online what are the modelâ€™s recommended settings (Unsloth blog or model pages on huggingface are great sources for this).
  - donâ€™t just max out context length, set one that makes sense for your hardware. You can find calculators online, or resort to the good old trial and error process. Be aware that, even if models support 128k or more tokens in the context, most of them degrade after 40/50k.

- I wouldn't recommend fine tuning on Macs - took 9hrs to train phi 3 mini on the guanaco dataset with autotrain.

- ## [MacBook Air M4/32gb Benchmarks : r/LocalLLaMA _202503](https://www.reddit.com/r/LocalLLaMA/comments/1jklk5y/macbook_air_m432gb_benchmarks/)
  - Phi4-mini (3.8b)- 34 t/s, 
  - Gemma3 (4b)- 35 t/s, 
  - Granite 3.2 (8b)- 18 t/s, 
  - Llama 3.1 (8b)- 20 t/s, 
  - Gemma3 (12b)- 13 t/s, 
  - Phi4 (14b)- 11 t/s, 
  - Gemma (27b)- 6 t/s, 
  - QWQ (32b)- 4 t/s

- I suspect it's heating up quite a bit like my 24 gb one does
  - Sure does, being completely silent is a nice tradeoff though. My old MacBook Pro would sound like a jet engine preparing to take off.

- Those figures are close to what I'm getting using accelerated ARM CPU inference on a Snapdragon X1 Elite with 12 cores. That's on a ThinkPad with fans and big cooling vents. It's incredible that the M4 Air has that much performance in a fanless design.
  - It definitely gets warm when inferencing with the larger models and longer contexts but being completely silent is pretty amazing. Models tested were Q4.

- People are angry with the MacBook Air M4. Without fans, the benchmarks drop by half compared to the Mac Mini with the same M4 chip.

- [Tested local LLMs on a maxed out M4 Macbook Pro so you don't have to : r/ollama _202503](https://www.reddit.com/r/ollama/comments/1j0by7r/tested_local_llms_on_a_maxed_out_m4_macbook_pro/)
  - æµ‹è¯•äº†qwen2.5çš„å„ç§é‡åŒ–ç‰ˆ
  - lmstudioçš„mlxæ¯”ollamaå¿«å¾ˆå¤š

- ## âš¡ï¸ [Is M4 MacBook Air 32 + 512 good for AI/LLM? : r/macbookair _202503](https://www.reddit.com/r/macbookair/comments/1jd2cbc/is_m4_macbook_air_32_512_good_for_aillm/)
  - Some preliminary benchmark tests show that the token per second is as follows in model M4 MBA 32 GB: 
  - 7b 20.8 token/s 
  - 14b 11.0 token/s 
  - 32b 4.9 token/s 
  - Is this acceptable or good enough for playing with local AI/LLM?

- deepseek-r114b on M4 is near 14 tokens but on the 4070 it is 50!!!
  - Then deepseek 32b does like 4.7 tokens on M4 32/512 and 5.7 on the 4070 ti.
  - For LLM running i'd go for the M4 PRO with more cores and 48gb ram but that configuration is like x2 expensive over the 32/512 M4

- The problem is LLM running locally are very CPU intensive. Which equals heat, which the Air has no internal fans to handle it, which equals the CPU throttling (under clocking) itself to prevent overheating. The AIR will get Hot AF!
  - The memory and storage is in the range of what is needed. Personally I would go for a 1TB of storage but you need something with a fan which would be a MacBook Pro or mini.

- The base M4 MBP would probably be best because of the active cooling, itâ€™ll allow for better sustained performance and also comes with a slightly bigger battery.
- Pro model recommended so the active cooling system, the MacBook Air doesnâ€™t handled loads well.

- ## [How are people running an MLX-compatible OpenAI API server locally? : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mg26g0/how_are_people_running_an_mlxcompatible_openai/)
- If you are on a Mac then LM Studio is about your only choice for a mature, stable, fast, reliable, supported, maintained MLX server.

- ## [Adjust VRAM/RAM split on Apple Silicon Â· ggml-org/llama.cpp _202307](https://github.com/ggml-org/llama.cpp/discussions/2182)
- just do: `sudo sysctl iogpu.wired_limit_mb=<mb>` from Terminal. Youâ€™d have to do it every boot as itâ€™s not sticky

- ## [MLX now has MXFP4 quantization support for GPT-OSS-20B, a 6.4% faster toks/sec vs GGUF on M3 Max. : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1n4mxrj/mlx_now_has_mxfp4_quantization_support_for/)
- The speed increase is pretty noticeable, tested with short prompts, on M4 MAX 128GB
  - mlx-community's gpt-oss-120b-MXFP4-Q8: 80tps
  - ggml 120b: 75tps, with fa enabled
  - unsloth 120b f16: 61tps, with fa enabled
- Agree with your observation, the new gpt-oss-120b-MXFP4-Q8 that was released just several days ago is indeed faster by ~25% based on my testing than the unsloth full precision (fp16) - even full FA enabled. Quality appears to be the same, because it doesnâ€™t really matter Q8 or fp16 for that model, as 95% of the params were trained with native MXFP4 â€œfull precisionâ€, that is why also all unsloth quants and MLX Q4 and Q8 weights almost the same!

- itâ€™s still has to be software optimisation right since M series chip doesnâ€™t natively support this quantization

- ## [Upcoming MLX support news? : r/unsloth _202508](https://www.reddit.com/r/unsloth/comments/1mlsoar/upcoming_mlx_support_news/)
- will there be UD MLX quants?
  - Oh maybe if demand is more!

- That would be so amazing! Love unsloth quants and I always feel like I need to make a tough choice between them and MLX ones. 

- ## ğŸ [Apple users: Unsloth's quants could be coming to MLX - if we show interest : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1moqhvf/apple_users_unsloths_quants_could_be_coming_to/)
- Mac user.  I would be very interested to see how they could benefit the MLX community.  
  - I highly respect Llama.cpp and GGUF but MLX DWQ quants are very high quality and faster than their GGUF cousins on this hardware. I find myself unwilling to go back unless I have to.  Thankfully MLX support, unless first party, often comes faster than Llama.cpp so we have been well served.
- Last I heard the performance delta was <10%. Id much prefer them just supporting the llama.cpp ecosystem rather than diluting their time
  - Then you've heard a while ago because the performance delta is huge now. I get 16t/s on MLX and 9t/s on llama.

- Would very much like to run unsloth dynamic mlx quants on my mac studio

- That would be amazing! MLX is great but still lack the support of other inference engines imo.

- ## ğŸ¤” [MLX you're using in release LM Studio seems to be outdated and broken for Q8 and larger models Â· Issue Â· lmstudio-ai/mlx-engine _202411](https://github.com/lmstudio-ai/mlx-engine/issues/34)
- The next version of our app should help remedy this, since we'll be shipping the latest version of MLX.

- The guidance is incomplete and solves nothing. The correct kernel tweaks to make these work are actually:
  - Without the `disable_wired_collector` setting, the problem remains.

```sh
sudo sysctl iogpu.disable_wired_collector=1
sudo sysctl iogpu.wired_limit_mb=n
```

- To make these persistent across reboots:
  - [Persistent sysctl Settings - Apple Community _202204](https://discussions.apple.com/thread/253840320?sortBy=rank)

- ## ğŸ’¡ [Run LLMs on macOS using llm-mlx and Appleâ€™s MLX framework | Lobsters _202502](https://lobste.rs/s/1reyhf/run_llms_on_macos_using_llm_mlx_apple_s_mlx)
  - By default, MacOS allows 2/3rds of this RAM to be used by the GPU on machines with up to 36GB / RAM and up to 3/4s to be used on machines with >36GB RAM.

- I havenâ€™t tried mlx the library yet, but when I was using LM Studio with their MLX backend, I had to run `sudo sysctl iogpu.wired_limit_mb=<something>` to increase the VRAM limit.
  - However, be aware that the system would panic when you try to load a big model or use a large context size when `<something>` is too big.

- By the way, I found that if you have models in` ~/.cache/huggingface/hub` already, due to having used mlx-lm directly, the llm mlx download-model mlx-community/â€¦ command will skip the download and just add them to llmâ€™s index.

- When comparing popular quant q4_K_M on Llama.cpp to MLX-4bit, in average, MLX processes tokens 1.14x faster and generates tokens 1.12x faster. This is what most people would be using.

- ## [Can't run Mixtral on GPU with M2 Pro 32 gb VRAM : r/LocalLLaMA _202312](https://www.reddit.com/r/LocalLLaMA/comments/18l9kal/cant_run_mixtral_on_gpu_with_m2_pro_32_gb_vram/)
  - I was able to run Mixtral Q5 on my macbook on CPU, but when I choose Apple Metal (GPU) in LM Studio I get this error:
  - GGML Metal Error: command buffer 4 failed with status 'Error'. Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory)
  - Worked for Q4_K_M when increasing the VRAM limit

- Gpu only has access to about 22gigs on a 32gig macbook, check the Llama.cpp or this community for a thread where setting a few command line arguments disables this limit.

- By default you can have up to 22GB of RAM allocated to the GPU but you can increase that amount up to 29GB with the system running fine. Try 27-28GB if you want to run a web browser on the side. In the terminal type the following (mb=0 to reset to default settings):
  - sudo sysctl iogpu.wired_limit_mb=29256
  - sudo sysctl iogpu.wired_limit_mb=0
  - Youâ€™d have to do it every boot as itâ€™s not sticky .
- 32GB might be too tight for 32k ctx and would usually crash the machine. 12k ctx seems to be the limit for 32GB.

- Why no one talks about the new ml framework mlx from apple? Is it not applicable here?
  - It would hit the same limit that a q5 quant of dolphin-mixtral needs more GPU memory than can be allocated on a 32GB Mac, even after tweaking the kernel limit.

- ## ğŸ§© [First time using MLX and MacOS for inference: getting poor results : r/LocalLLaMA _202409](https://www.reddit.com/r/LocalLLaMA/comments/1fpy26p/first_time_using_mlx_and_macos_for_inference/)
- Set sysctls `sudo sysctl iogpu.wired_lwm_mb=400000 && sudo sysctl iogpu.wired_limit_mb=150000` , reboot
- iogpu.wired_limit_mb=150000 sets the maximum amount of wired memory the GPU is allowed to allocate to 150, 000 MB (~150 GB)
  - sets the maximum GPU memory allocation to 150GB
- iogpu.wired_lwm_mb=400000 sets the low-water mark (LWM) for the GPUâ€™s wired memory to 400, 000 MB (~400 GB).
  - In memory management, a low water mark typically represents a threshold. When the amount of free memory drops below this mark, the system might take certain actions to free up more memory.

- iogpu.disable_wired_collector=1
  - tells macOS to disable the â€œwired memory collectorâ€ for the GPU.
  - Normally, the collector periodically trims/reclaims GPU wired memory (VRAM allocations backed by unified RAM) when the system is under pressure.
  - Disabling it (=1) means the GPU can hold onto large wired allocations without the OS trying to recycle them. This is sometimes done when running ML/LLM workloads on Apple Silicon, because the collector can otherwise kill big GPU allocations and crash jobs.
  - Setting this to 1 disables this collector, potentially leaving GPU memory allocations "locked" and unadjusted even when the system needs memory.

- MLX is lazy so it won't load models until it needs to. So the time-to-first-token should be a lot lower on the second request.

- Run the command to expand the max VRAM allocation (needs to be done after each reboot):
  - sudo sysctl iogpu.wired_limit_mb=120000

- ## [Can the iogpu.wired_limit_mb setting on MacOS be persisted? ğŸ“£ YES! : r/LocalLLaMA _202405](https://www.reddit.com/r/LocalLLaMA/comments/1cncgqe/can_the_iogpuwired_limit_mb_setting_on_macos_be/)
- Can CPU intensive, RAM demanding tasks take RAM seemlessly when not running GPU inference?
  - Yes, this sets the higher limit. Clearly the problem is that under heavy load, GPU will take it all leaving just 8GB to the other apps and OS.

- è®¾ç½®sysctlå¹¶æ‰§è¡Œ

```sh
sudo mv io.github.sysctl.plist /Library/LaunchDaemons/
sudo chown root:wheel /Library/LaunchDaemons/io.github.sysctl.plist
sudo launchctl load /Library/LaunchDaemons/io.yaoo.sysctl.plist
```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
 <key>Label</key>
 <string>io.github.sysctl</string>
 <key>ProgramArguments</key>
 <array>
 <string>/usr/sbin/sysctl</string>
 <string>-w</string>
 <string>iogpu.wired_limit_mb=27648</string>
 </array>
 <key>RunAtLoad</key>
 <true/>
</dict>
</plist>

```

- Rather than dealing with plist syntax, add `iogpu.wired_limit_mb=122800` on a line by itself to `/etc/sysctl.conf`. Create the file if it doesn't already exist. It should probably be owned by `root:wheel` and mode should be 644.

- ## ğŸ’¡ [M1/M2/M3: increase VRAM allocation with `sudo sysctl iogpu.wired\_limit\_mb=12345` (i.e. amount in mb to allocate) : r/LocalLLaMA _202311](https://www.reddit.com/r/LocalLLaMA/comments/186phti/m1m2m3_increase_vram_allocation_with_sudo_sysctl/)
  - One note on this ... All macos systems would be happiest to have at least 8gb available for OS stuff.
  - this is not a permanent change and it automatically resets on reboot.
  - PS: to get the current value use: sudo sysctl iogpu

- I looked at what wired memory (memory that can't be swapped) was without having an LLM loaded/running and then added a margin to that. I ended up allocating 26.5GB, up from 22.8GB default.
  - It worked, but it didn't work great because I still had a bunch of other stuff running on my Mac, so (not surprisingly) swapping slowed it down. For anything more than a proof of concept test I'd be shutting all the unnecessary stuff down.

- On my 32GB Mac, I allocate 30GB. [Macs with 32GB of memory can run 70B models with the GPU. : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/18674zd/macs_with_32gb_of_memory_can_run_70b_models_with/)
  - That's what I do and I have no swapping at all. I listed the two big things to turn off to save RAM. 
  - I don't use the GUI. Just simply logging in and doing nothing uses a fair amount of RAM. I run my Mac headless. I ssh in.
  - I stopped the `mds_stores` process from running

- consider llama.cpp instead of LM Studio to reduce RAM requirements a bit (still need to run terminal.app but this is pretty light)

- As per the latest developments in that discussion, "iogpu.wired_limit_mb" only works on Sonoma. So if you are on an older version of Mac OS, try "debug.iogpu.wired_limit" instead.

- How can I check if the changed worked? I did the intial code and said it was initially set to 0
  - One way I've verified is through the llama.cpp diagnostic output. It reports the available vram as well as the size of the model and how much vram it requires.
  - I've got 32gb total and I think the default availability is approx 22gb. So I can easily increase to 26gb and I see the difference immediately when I launch llama.cpp - the available vram will be reported as 26gb.

- ## ğŸ’¡ [Macs with 32GB of memory can run 70B models with the GPU. : r/LocalLLaMA _202311](https://www.reddit.com/r/LocalLLaMA/comments/18674zd/macs_with_32gb_of_memory_can_run_70b_models_with/)
  - I recently got a 32GB M1 Mac Studio. I was excited to see how big of a model it could run. It turns out that's 70B. It is a Q3_K_S model so the 2nd smallest for 70B in GGUF format
  - Mac shouldn't be able to dedicate that much RAM to the GPU. Apple limits it to 67%, which is about 21GB. This model is 28GB. So it shouldn't fit. But there's a solution to that thanks to these smart people here.
  - This new method is just setting the value of a variable. llama.cpp even tells you the value, how much RAM, it needs.
  - > sudo sysctl iogpu.wired_limit_mb=57344 ( I did that for my 64GB, you'd want to change the 57344 )

- â‰¥64GB allows 75% to be used by GPU. â‰¤32 its ~66%. Not sure about the 36GB machines.

- I also do these couple of things to save RAM.
  - I don't use the GUI. Just simply logging in and doing nothing uses a fair amount of RAM. I run my Mac headless. I ssh in.
  - I stopped the mds_stores process from running. I saw that it was using up between 500MB and 1GB of RAM. Its the processes that indexes the drives for faster search. 
  - With all that set, the highest I've seen in use memory is 31.02GB while running a 70B Q3_K_S model. So there's headroom. 

- You may try and run one of Q4 models without problems: because llama.cpp uses mmap to map files into memory, you can go above available RAM and because many models are sparse it will not use all mapped pages and even if it needs it, it will swap it out with other pages on demand... I was able to run falcon-180b-chat. Q6_K which uses about 141GB on a 128GB Windows PC with less than 1% SSD reads during inference... I could even run falcon-180b-chat. Q8 which uses about 182GB but in this case SSD was working heavily during inference and it was unbearably slow (0.01 tokens/second)...
  - Yes. I've done that before on my other machines. Llama.cpp in fact defaults to that. The hope for me was that since the models are sparse that the OS would cache the relevant parts of the models in RAM. So the first run through would be slow but subsequent runs would be fast since those pages are cached in RAM. 

- ## ğŸ§  [Instantly allocate more graphics memory on your Mac VRAM Pro : r/LocalLLaMA _202504](https://www.reddit.com/r/LocalLLaMA/comments/1k1tdpa/instantly_allocate_more_graphics_memory_on_your/)
  - I built a tiny macOS utility that does one very specific thing: It unlocks additional GPU memory on Apple Silicon Macs.
  - Why? Because macOS doesnâ€™t give you any control over VRAM â€” and hard caps it, leading to swap issues in certain use cases.
  - Do you need this app? No! You can do this with various commands in terminal. But wanted a nice and easy GUI way to do this.

- https://github.com/Sub-Soft/Siliv /NALic/202504/python
  - A simple macOS menuâ€‘bar utility to adjust Apple Silicon GPU VRAM allocation
  - Siliv provides a convenient way to view and set GPU VRAM allocation on Apple Silicon Macs via `sysctl` .
  - Persistence: `sysctl` changes may reset after reboot; 

- ğŸ¤” did you know that MLX ignores the VRAM limit global? do you know a way to make MLX use the extra VRAM allocated?
  - If you're on MacOS 15 or higher: Just run `sudo sysctl iogpu.wired_limit_mb=14336` on the terminal and replace 14336 (14GB) with your desired VRAM allocation in MB
- That's the global I use, and MLX ignores what's set for it. Using LM Studio, I can load and use q8 quants of 70B ggufs, but MLX models the same size, run out of memory.

- don't bother, this app just does this: `sudo sysctl iogpu.wired_limit_mb=24576`.
  - Yeah I already have a version of this aliased in my .zshrc file whenever I feel I need it (or to reset). 
  - Dont work , its a very old command for intel igpus ... 

- ## ğŸš€ [LM Studio ships an MLX backend! Run any LLM from the Hugging Face hub on Mac blazingly fast! âš¡ : r/LocalLLaMA _202410](https://www.reddit.com/r/LocalLLaMA/comments/1fz6z79/lm_studio_ships_an_mlx_backend_run_any_llm_from/)
- M1 Max 32GB, Qwen2.5-14B-Instruct
  - Q4_K_M: 15.15 t/s
  - 4 bit MLX: 23.10 t/s

- M2 Max 64GB, Qwen2.5-32B-Instruct
  - Q4_K_M: 14.78 tok/sec
  - 4 bit MLX: 17.62 tok/sec

- One bonus from MLX I hadn't anticipated but is nice is not the speed difference, but the VRAM savings
  - Running on an M2 Ultra Mac Studio 192GB Ram.
  - mlx-Llama-3.1-70B-Instruct-8Bit - 82GB RAM/VRAM used - 53 Watts Peak Power - 8.62 tk/s
  - Llama-3.1-70B-Instruct-GGUF Q8_0 - 123.15GB RAM/VRAM used (Flash Attention enabled) - 53.2 Watts Peak Power - 8.2 tk/s
- Yes, this! Plus (potentially in a future LMStudio update) with the circular buffer + infini cache, it means we can fit much stronger models in and not worry about memory footprint increases with conversation length! I can finally get 8bit 70b models comfortably in my 64gb and use them all the way up to the 100k token limit 

- just FYI the reduction in VRAM is because MLX doesn't pre-allocate memory, whereas llama.cpp does.
  - Simple calculation: Model size at Q8 is ~75 GB, KV cache for 128k context is 40GB
  - Required memory to load the model and fill the cache is at least 75+40 = 115 GB.
# discuss-llama.cpp
- ## 

- ## 

- ## 

- ## ğŸ†šâš¡ï¸ [Performance of llama.cpp on Apple Silicon M-series Â· ggml-org/llama.cpp _202311](https://github.com/ggml-org/llama.cpp/discussions/4167)
  - æä¾›äº†å„ç§mac, airçš„æ¨¡å‹æµ‹è¯•æ•°æ®

# discuss-lmstudio-roadmap
- ## 

- ## 

- ## 

- ## [Incompatible type of `tool_choice` Â· Issue Â· lmstudio-ai/lmstudio-bug-tracker _202505](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/670)
- 
- 

- [[Bug]: Invalid value for 'tool_choice' Â· Issue #7039 Â· All-Hands-AI/OpenHands](https://github.com/All-Hands-AI/OpenHands/issues/7039)
  - When I switch to use OpenAI (gpt-4o), it working fine.

- ## [Qwen3 Models not Recognized as Embedding Types for MLX Format Â· Issue Â· lmstudio-ai/lmstudio-bug-tracker _202507](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/808)
  - Qwen3 models not recognized as embedding models in MLX format (but same models are ok in GGUF)

- From what i can understand, while the chat models are using mlx as an inference engine, LMStudio would need to use a package like `mlx-embeddings` to properly handle MLX Embeddings, and integrate it in there LM Studio MLX Runtime.
  - This would explain why the `Override Domain Type` option is not available for MLX: Only the 'LLMs' models are supported for mlx.
- there is no support for mlx embeddings in LMStudio
- The only workaround for now using LMStudio is to use the GGUF versions and take the performance hit

- [[Models] Qwen3 Embedding shown as LLM instead of an embedding model Â· Issue Â· lmstudio-ai/lmstudio-bug-tracker](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/696)
# discuss-lmstudio
- ## 

- ## 

- ## 

- ## 

- ## 

- ## 

- ## [Why do people say LM Studio isn't open-sourced? : r/LocalLLaMA _202405](https://www.reddit.com/r/LocalLLaMA/comments/1cvawmz/why_do_people_say_lm_studio_isnt_opensourced/)
- LMstudio is free in cost but it's proprietary software.

- Surely not all freeware are open source.
- The power of LM Studio is 4 things:
  - model discovery is incredibly easy, directly to huggingface gguf repositories
  - it's a direct inferencing app, can load models itself
  - able to work as a standalone endpoint server
  - it can loads multiple model on available GPUs

- i'm currently building https://kolosal.ai it's a free and opensource platform to run LLM on device, 
  - and the best part? it's only 20mb and can run on both CPU and GPUs, 
  - and it's also using `llama.cpp` as backend so the performance wouldn't be any different with lmstudio
- Is this windows only? 
  - Unfortunately currently yes. But, we're using framework that mostly is crossplatform

- because it's not open source, you cannot view their code or fork it to change it. the open source direct alternative to lmstudio is jan ai

- ## [Is there an alternative to LM Studio with first class support for MLX models? : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1l0ct34/is_there_an_alternative_to_lm_studio_with_first/)
  - I've been using LM Studio for the last few months on my Macs due to it's first class support for MLX models (they implemented a very nice MLX engine which supports adjusting context length etc.
- While it works great, there are a few issues with it:
  - it doesn't work behind a company proxy, which means it's a pain in the ass to update the MLX engine etc when there is a new release, on my work computers
  - it's closed source, which I'm not a huge fan of
  - I can run the MLX models using `mlx_lm.server` and using open-webui or Jan as the front end; but running the models this way doesn't allow for adjustment of context window size (as far as I know)

- Now I use MLX more because of it's GPU usage is not blocking macOS visual fluidity. My Mac screen rendering (especially when doing multitasking with Stage Manager) a lot stutter when inferencing with llama.cpp, but still fluid with MLX. Yes, there are not as mature as llama.cpp, but this factor made me swith to MLX only. I run it using LM Studio as an endpoint.

- ## ğŸ¤” [LM Studio incredibly slow (1.2 tokens/sec) on a 3090, despite model (Qwen 2.5 32B 4xs) fitting entirely into VRAM and not yet hitting the context length. : r/LocalLLaMA _202411](https://www.reddit.com/r/LocalLLaMA/comments/1gqa5xx/lm_studio_incredibly_slow_12_tokenssec_on_a_3090/)
- increasing context size also increases Vram overhead. From what I remember it's roughly something to the tune of each 4k = 1gb vram.
  - Yeah. It's just that LM Studio shows VRAM usage and it's not yet hitting the limit. Unless it doesn't show if it goes over for some reason?
  - Halving the context to 16k allows me to fit everything.

- If you never figured it out its probably the Guardrails just go to hardware and turn them off, also bigger the context the more vram it needs, turn mmap and dont keep in ram off and turn flash attention adn K cache on.

- ## ğŸ§© [Does the number of bits in KV cache quantization affect quality/accuracy? : r/LocalLLaMA _202502](https://www.reddit.com/r/LocalLLaMA/comments/1iuw1kx/does_the_number_of_bits_in_kv_cache_quantization/)
- Setting the KV cache to Q8 has only a minimal influence on the results. 
  - Setting the KV cache to Q4 has quite an impact though. 
  - Setting K to F16 or Q8 and V to Q4 still achieves decent results though.
  - Just the extensive test that the author of the KV quantization in llama.cpp did that I linked above. The results make sense, as the keys are used to find the right value, and mismatching keys due to higher quantization will lead to incorrect values, whereas correctly looked up values that have been quantized will still be somewhat related to the original information.

- ## [LMStudio, KV cache and context length : r/LocalLLaMA _202502](https://www.reddit.com/r/LocalLLaMA/comments/1iyv8t6/lmstudio_kv_cache_and_context_length/)
  - I've turned on KV cache quantization because it "may lead to faster generation"
  - LMStudio's UI also mentions that when this feature is on, the context length value doesn't matter. So, I'm wondering how big the actual context is with this setting enabled?

- Multiplicative. Normal is fp16. Q4 quadruples your context length with minimal loss (except qwen)

- under the KV Cache Quantization option that "Context Length setting is ignored when using KV Cache Quantization" and unsurprisingly completely ignores whatever you have in the field.
  - Based on pasting a 10k-ish token article into a conversation, and LM Studio filling the context to 257.8%, it seems like it's set to 4096 by default... which really seems like a huge waste of being able to quantize the cache, but again there is no additional field or slider to adjust context size when quantization is enabled.
# discuss-ollama-roadmap
- ## 

- ## 

- ## 

- ## 

- ## [MLX backend Â· Issue Â· ollama/ollama _202312](https://github.com/ollama/ollama/issues/1730)
- Add mlx-vlm backend also.

- Lack of MLX support is the only reason I don't use Ollama. In some cases MLX Q8 is 20% faster than GGUF, and memory usage is better handled.

- ğŸ“¡ [Draft MLX go backend for new engine by dhiltgen Â· Pull Request Â· ollama/ollama _202502](https://github.com/ollama/ollama/pull/9118)
# discuss-ollama

```sh
# ollamaåŸå§‹çš„ç‰ˆæœ¬æ˜¯å¹¶ä¸å…¼å®¹ OpenAI æ ¼å¼çš„è¯·æ±‚
curl http://localhost:11434/api/chat -d '{
  "model": "gemma3:4b",
  "messages": [
    { "role": "user", "content": "why is the sky blue?" }
  ]
}'
# æ¨¡å‹å“åº”è¾“å‡ºå¦‚ä¸‹ 
{"model":"gemma3:4b","created_at":"2025-07-31T16:54:00.804772Z","message":{"role":"assistant","content":"That"},"done":false}

```

- ## 

- ## 

- ## ğŸï¸ [Can add Stable Diffusion 3.5 modelï¼Ÿ Â· Issue Â· ollama/ollama _202412](https://github.com/ollama/ollama/issues/8047)
  - Does Olama not support the Diffusion 3.5 model? Can we add an Diffusion 3.5 model?

- Short answer is no, you shouldn't be able to as Ollama is dedicated to Large LANGUAGE models.
  - Longer answer is that in general, Stable diffusion and Large Language Models require different architectures, and while some llava models (and more recently Llama models) do have the ability to reason about images, being able to identify an image / the contents of an image is inherently different from being able to generate one, and ollama isn't designed or equiped for that.
- For stable diffusion you should be looking into something like Automatic1111, forge, comfy, or swarm (to name a few of the more popular projects).

- ### [Image generation models Â· Issue Â· ollama/ollama](https://github.com/ollama/ollama/issues/786)
- Ollama currently uses llama.cpp to do a lot of the work of actually supporting a range of large language models. This choice allowed the team to focus on delivering value in other ways. Llama.cpp's reasons for not supporting text-to-image models are probably for similar reasons.

- There are plenty of Stable Diffusion UIs and all of them are drowning in issues and features because of this:
  - Automatic1111: most popular, most fully-integrated environment
  - ComfyUI: very modular, usually has the newest features first, allows flexible workflows
  - Fooocus: restricted and opinionated but focused on providing a simple UI which produces good output out of the box (original author is the inventor of ControlNets and has a deep understanding of the inner workings of Stable Diffusion). probably the closest to Dall-E 3 (Fooocus provides a GPT2 prompt rewrite engine in the background)
  - SD. Next: fork and overhaul of Automatic1111 because everything took sooo long to implement
  - (I think all of them provide API endpoints)

- I'd recommend Stable Diffusion 1.5 because it has the lowest hardware requirements and integration is simpler and will be faster. Later models have more specifics which require more attention to detail.

- ## [image generation : r/ollama _202410](https://www.reddit.com/r/ollama/comments/1g5lp9s/image_generation/)
  - is there any model that allows image generation?

- nope, but there are some vision models that capable of taking images as input

- You can add ComfyUI, Automatic1111 or LocalAI via API to generate images in OpenWebUI, though itâ€™s somewhat finicky. It creates images from the LLMâ€˜s answer, not the Edit: prompt

- I am running Flux locally. It does a decent job in the dev model, there are 2 higher levels that arenâ€™t for commercial use.

- [Image generator : r/ollama](https://www.reddit.com/r/ollama/comments/1iuzv2v/image_generator/)
  - No llm are able to generate image... However, you can let your llm reply with tool_call to fill a image promot to be sent to another ai model that generate images (chatgpt4o-dalle3 for exemple)

- [How can I generate images with ollama? : r/ollama](https://www.reddit.com/r/ollama/comments/18laoe6/how_can_i_generate_images_with_ollama/)
- Ollama doesn't yet support stable diffusion or creating images. It does support image to text though.

- If you are using Mac you can either use diffuser or diffustionbee.

- ## ğŸ¯ Ollama 0.10! Ollama's new app is here for macOS and Windows _20250731
- https://x.com/ollama/status/1950670503376761133
- [Ollama's new app](https://ollama.com/blog/new-app)
- Ollamaâ€™s macOS and Windows now include a way to download and chat with models.
  - Ollamaâ€™s new app supports file drag and drop, making it easier to reason with text or PDFs.
  - For processing large documents, Ollamaâ€™s context length can be increased in the settings. Note: this will require more memory.
- Building on Ollamaâ€™s new multimodal engine, images can be sent to models that support them, such as Google DeepMindâ€™s Gemma 3 model.
- Code files can be processed by models for understanding.

- Is this ollama CLI with its own GUI?
  - Ollamaâ€™s new app includes the CLI. You can also use Ollamaâ€™s API or try Ollamaâ€™s Python/JavaScript libraries.

- It would be nice it it recommended models for my hardware? or is it already doing that?
  - Not yet, but we have hand selected the top models that will run on most users' hardware.

- What would be the use case over LMStudio on Mac?
  - quality, ease of use, model support directly with the major labs, hardware support
- ğŸ› I understand and appreciate ease of use and minimalism. It's really great when you'd want to chat with a model. However, as far as I understand there is still no MLX support. I'm strictly talking about MacOS version here.

- All that's missing now is MCP support.

- Can i choose where to save my models?
  - Yes, check out Ollama settings

- Feature request: Allow the user to set the Ollama endpoint. I run it on a beefy desktop computer but would like to use the app as a client.

- can you use it if the models are a other pc as a server
  - not yet. In the future, we'll add the support.

- Ollama natively support new AMD gpus?
  - We are working with @AMD to support the new GPUs

- We need our own completions API â€” I have the feeling that tool calling doesn't work properly with the OpenAI SDK. The tool tags need improvement; for example, DeepSeek R1 shows tool support, but the 8B version doesnâ€™t, even though itâ€™s based on Qwen3, so it should.

- å…¶å®æ—©å°±æœ‰ç¬¬ä¸‰æ–¹çš„äº†...... ä¸è¿‡æ„Ÿè§‰å®˜æ–¹åˆå¹¶åŠŸèƒ½äº†åæ›´æ–¹ä¾¿äº†å±äºæ˜¯

- ## Ollama 0.2 is here! Concurrency is now enabled by default.  _20240709
- https://x.com/ollama/status/1810480544976626159
  - Parallel requests: Ollama can now serve multiple requests at the same time, using only a little bit of additional memory for each request.
  - Run multiple models: Ollama now supports loading different models at the same time

- ## So this is an LLM running locally? `ollama run llama3` .
- https://x.com/eatonphil/status/1797039865470570942
  - Asking it to help me with some Postgres C code. It's complete nonsense, but it's impressive complete nonsense
  - `ollama run codestral` looks about right though.
- How much RAM am I supposed to have for the 70B model?
  - B Ã— Q / 8 â†’ RAM requirement for llm inference in GB
  - B: number of parameters
  - Q: quantization (16 = no quantization)
  - useful rule of thumb for RAM requirement for llm inference via hn user CobaltFire
- 3bit quantized should work with 32 GB RAM with other apps closed

- based on the download size that looks like the 7b model (smallest), which are pretty hopeless at anything beyond basic coding in my experience (codestral is a 22b iirc)
  - also if you like using models from the command line i highly rec @simonw 's `llm` cli! i believe it supports ollama

- ## ğŸ› [Error: pull model manifest Â· ollama/ollama](https://github.com/ollama/ollama/issues/3434)
  - Error: pull model manifest: Get "https://registry.ollama.ai/v2/library/codellama/manifests/70b": read tcp 192.168.3.79:64976->172.67.182.229:443: read: operation timed out
- Error: max retries exceeded: Get "https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/14/1436d66b69757a245f02d000874c670507949d11ad5c188a623652052c6aa508/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%! F(MISSING)20240529%! F(MISSING)auto%! F(MISSING)s3%! F(MISSING)aws4_request&X-Amz-Date=20240529T155900Z&X-Amz-Expires=1200&X-Amz-SignedHeaders=host&X-Amz-Signature=cd4472bad19931e399f39a352a4a1b0902857996b7b784b8138f168d70532277": dial tcp 104.18.8.90:443: i/o timeout

- ## [ä¸ºä»€ä¹ˆLlama2å¤§æ¨¡å‹å¯ä»¥åœ¨ä¸ªäººç”µè„‘ä¸Šéƒ¨ç½² ï¼Ÿ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/646939066)
- æˆ‘åœ¨Meatçš„å®˜ç½‘ä¸Šçœ‹åˆ° llama2 æ˜¯æ„å»ºåœ¨PyTorchä¹‹ä¸Šçš„ï¼Œè€ŒChatGPTæ˜¯åŸºäºTensorFlow Probabilityæ¡†æ¶çš„ï¼Œæœ¬æ–‡é‡Œé¢å°±ç®€ç§°TFPã€‚

- ## [Meta AI ä¸ºä»€ä¹ˆä¼šå¼€æº Llama2 å‘¢? - çŸ¥ä¹](https://www.zhihu.com/question/613072688/answers/updated)
- å› ä¸ºæ‰€è°“çš„LLMå¼€æºåªæ˜¯å…¬å¸ƒè®­ç»ƒå¥½çš„ç»“æ„å’Œå‚æ•°è€Œå·²ï¼ŒçœŸæ­£é‡è¦çš„æ•°æ®å’Œè®­ç»ƒä»£ç å¹¶æ²¡æœ‰å¼€æºï¼Œæ›´åˆ«è¯´å¤§éƒ¨åˆ†äººè¿˜æ²¡æœ‰è¶³å¤Ÿçš„GPUã€‚
  - å³ä½¿å¦‚æ­¤ï¼Œç›®å‰mistralè¿™æ ·çš„ä¹Ÿåªå¼€æº7bä¸å¼€æºlargeï¼Œllamaåç»­è¿˜å¾—ç»§ç»­è§‚å¯Ÿ

- Llama2 å¼€æºä½†ä¸æ˜¯å¯ä»¥éšä¾¿ç”¨çš„å•†ç”¨è®¸å¯ã€‚ ç”¨æˆ·æ•°åˆ°äº†ä¸€å®šç¨‹åº¦å°±ä¸æ˜¯å…è´¹çš„ã€‚
  - 7äº¿æœˆæ´»

- ## [Allow listening on all local interfaces _202310](https://github.com/ollama/ollama/issues/703)
- If youâ€™re running Ollama directly from the command line, use the
`OLLAMA_HOST=0.0.0.0 ollama serve` command

- Edit the service file: Open `/etc/systemd/system/ollama.service` and add the following line inside the [Service] section:
 `Environment="OLLAMA_HOST=0.0.0.0"`

- sudo systemctl daemon-reload
- sudo systemctl restart ollama
# discuss-vllm
- ## 

- ## 

- ## 

- ## 

- ## [å¤§æ¨¡å‹æ¨ç†æ¡†æ¶ï¼ŒSGLangå’ŒvLLMæœ‰å“ªäº›åŒºåˆ«ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/666943660)
- PagedAttention vs RadixAttentionï¼šå†…å­˜ç®¡ç†çš„ä¸¤ç§å“²å­¦
- å…ˆè¯´vLLMçš„PagedAttentionã€‚
  - è¿˜è®°å¾—æ“ä½œç³»ç»Ÿè¯¾ä¸Šçš„è™šæ‹Ÿå†…å­˜å—ï¼ŸvLLMå°±æ˜¯æŠŠè¿™å¥—ç©æ³•æ¬åˆ°äº†GPUä¸Šã€‚æˆ‘ç¬¬ä¸€æ¬¡çœ‹åˆ°è¿™ä¸ªè®¾è®¡æ—¶ï¼Œç›´æ¥æƒŠäº†â€”â€”å±…ç„¶èƒ½æŠŠå†…å­˜åˆ©ç”¨ç‡ä»20%å¹²åˆ°96%
- SGLangçš„RadixAttentionã€‚
  - ç”¨çš„æ˜¯Radixæ ‘ï¼ˆå°±æ˜¯é‚£ä¸ªå‹ç¼©å‰ç¼€æ ‘ï¼‰ã€‚
  - åƒæ˜¯ç»™KV cacheå»ºäº†ä¸ª"æ—è°±"
  - åœ¨å¤šè½®å¯¹è¯åœºæ™¯ï¼ŒSGLangç¡®å®çˆ½ã€‚
  - éƒ¨ç½²èµ·æ¥æ˜¯çœŸçš„éº»çƒ¦

- ç¼–ç¨‹ä½“éªŒï¼š
  - vLLMçš„APIè®¾è®¡ï¼Œæ€ä¹ˆè¯´å‘¢ï¼Œå°±æ˜¯é‚£ç§"å‚»ç“œå¼"çš„ï¼ˆè¤’ä¹‰ï¼‰ã€‚ä¸‰è¡Œä»£ç è·‘èµ·æ¥
  - SGLangçš„ç¼–ç¨‹æ¨¡å‹, è¿™ä¸ªDSLï¼ˆé¢†åŸŸç‰¹å®šè¯­è¨€ï¼‰å­¦ä¹ æ›²çº¿æœ‰ç‚¹é™¡

- å¯æ˜¯ï¼Œé€šå¸¸éƒ¨ç½²éƒ½æ˜¯åªä½œä¸ºåç«¯æä¾› APIï¼Œè€Œä¸æ˜¯ç­”ä¸»è¿™æ ·åœ¨ Python é‡Œç›´æ¥åŠ è½½æ¨¡å‹ã€‚æ‰€ä»¥ç­”ä¸»çš„åšæ³•æ¯”è¾ƒå°‘è§å•¦ã€‚

- sglangä¸ç¨³å®šå…·ä½“æŒ‡çš„æ˜¯ï¼Ÿ
  - å„ç§å†…å­˜æ³„æ¼ï¼Œoomï¼Œcrashï¼Œä»£ç è§„æ¨¡ä¸å¤§ï¼Œå°±å¼€å§‹å±å±±å †å±ï¼Œç”Ÿäº§åˆ«ç”¨ï¼Œä¹Ÿåˆ«è¿½æ›´ï¼Œmerge masteråŸºæœ¬çš„å›å½’æµ‹è¯•éƒ½ä¼šç›´æ¥è·³è¿‡çš„ã€‚åšå¼€æºçš„ï¼Œå°±è¿™

- vllm ä»…æ”¯æŒ NVIDIA GPUã€éƒ¨ç½²å¤æ‚ã€æ˜¾å­˜éœ€æ±‚å¤§
# discuss-ai-api/tools
- ## 

- ## 

- ## 

- ## 

- ## [Jan: an open-source alternative to LM Studio providing both a frontend and a backend for running local large language models : r/LocalLLaMA _202401](https://www.reddit.com/r/LocalLLaMA/comments/193m27u/jan_an_opensource_alternative_to_lm_studio/)
- A Big problem all these LLM tools have is that they all have their own way of reading Models folders. I have a huge collection of GGUF's from llama.cpp usage that I want to use in different models. Symlinking isn't user friendly, why can't apps just make their Models folder a plain folder and allow people to point their already existing LLM folders to it
  - This is salient criticism, thank you. At the core, we're just an application framework. We should not be so opinionated about HOW users go about their filesystem.

- Ollama being the biggest offender, with that fake docker syntax for modelfiles, model import and renaming using sha hashes.

- The Stable Diffusion UI variants also had this problem - until Stability Matrix came along and resolved a number of inconveniences with model management. Wonder if something similar could be viable here too.
  - invokeai çš„é€‰æ‹©æ–‡ä»¶ä¹Ÿæ˜¯ä¸€ç§æ–¹æ¡ˆ

- Its why Koboldcpp just has a file selector popup, it doesn't make sense to tie people to a location.

- Is it better to use llama.cpp instead of LM Studio? 
  - Absolutely! KoboldCpp and Oobabooga are also worth a look. 
  - I'm trying out Jan right now, but my main setup is KoboldCpp's backend combined with SillyTavern on the frontend. 
  - They all have their pros and cons of course, but one thing they have in common is that they all do an excellent job of staying on the cutting edge of the local LLM scene (unlike LM Studio).

- 
- 
- 
- 
- 
- 
- 
- 
- 

- ## openrouter æ˜¯çœŸæ–¹ä¾¿ï¼Œä¸€ä¸ª Key æ‰€æœ‰æ¨¡å‹éƒ½èƒ½ç”¨ã€‚
- https://x.com/pengchujin/status/1894375539726803201
- æ¨¡å‹ä»·æ ¼ä¸€æ ·ï¼Œå……å€¼ 5% çš„æ‰‹ç»­è´¹ã€‚è¿˜æœ‰ä¸ªå¥½å¤„æ˜¯æœ‰ä¸€éƒ¨åˆ†å…è´¹æ¨¡å‹ï¼Œæ¯”å¦‚Azçš„R1å•¥çš„
  - ä»·æ ¼ä¸€æ ·çš„ï¼Œå®˜æ–¹é™ä»·ä»–ä¹Ÿè·Ÿç€é™
- Qwençš„keyï¼Œç«å±±çš„keyä¹Ÿæ˜¯è¿™æ ·çš„

- ## deepseek æœ€è¿‘å¼€æºçš„å‰é¢å‡ ä¸ªé¡¹ç›®å¯èƒ½æˆ‘ä¸äº†è§£, ä»Šå¤©è¿™ä¸ª3fs, smallpond æ­£å¥½æ˜¯æˆ‘ä»¬è¿™ä¸ªè¡Œä¸šçš„, æˆ‘ç†è§£å¹¶æ²¡æœ‰åƒæŠ€æœ¯ç‚¸å¼¹å§, éƒ½æ˜¯æ¯”è¾ƒæˆç†Ÿç¨³å®šçš„æŠ€æœ¯äº†
- https://x.com/baotiao/status/1895395027129655691
- y1s1 è¿™é‡Œçš„éš¾ç‚¹æ˜¯ç éœ€æ±‚ï¼Œä¸åšä»€ä¹ˆ

- ## DeepSeek å¼€æºå‘¨çš„ 5 å·ç‚¸å¼¹æ¥å•¦ï¼åˆæ˜¯é›†æŸç‚¸å¼¹ï¼3FS å’Œ smallpondï¼
- https://x.com/karminski3/status/1895280320989274560
  - æˆ‘ä¸æ•¢ç›¸ä¿¡DeepSeekç”šè‡³é¢ è¦†äº†å­˜å‚¨æ¶æ„...... æˆ‘ä¸Šæ¬¡ä¸ºç½‘ç»œæ–‡ä»¶ç³»ç»Ÿéœ‡æƒŠè¿˜æ˜¯HDFSå’ŒCEPH. ä½†è¿™äº›éƒ½æ˜¯é¢å‘ç£ç›˜çš„åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ. ç°åœ¨ä¸€ä¸ªçœŸæ­£æ„ä¹‰ä¸Šé¢å‘ç°ä»£SSDå’ŒRDMAç½‘ç»œçš„æ–‡ä»¶ç³»ç»Ÿè¯ç”Ÿäº†ï¼
  - é£ç«æµæ˜Ÿæ–‡ä»¶ç³»ç»Ÿï¼ˆ3FSï¼‰- ä¸€ç§åˆ©ç”¨ç°ä»£ SSD å’Œ RDMA ç½‘ç»œå…¨å¸¦å®½çš„å¹¶è¡Œæ–‡ä»¶ç³»ç»Ÿ
  - è¿™ä¸ªæ–‡ä»¶ç³»ç»Ÿå¯ä»¥åœ¨ 180 èŠ‚ç‚¹é›†ç¾¤ä¸­è¾¾åˆ°6.6 TiB/s æ€»è¯»å–ååé‡ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯èŠ‚ç‚¹ KVCache æŸ¥æ‰¾å³°å€¼ååé‡ 40+ GiBã€‚
  - å¦ä¸€ä¸ª smallpondï¼ˆå°æ± å¡˜ï¼‰æ˜¯åŸºäº 3FS çš„æ•°æ®å¤„ç†æ¡†æ¶ï¼
  - è¿™ä¸ªæ¡†æ¶ç”± DuckDB æä¾›çš„é«˜æ€§èƒ½æ•°æ®å¤„ç†ï¼Œå¯æ‰©å±•ä»¥å¤„ç† PB çº§æ•°æ®é›†ï¼
  - æˆ‘çœ‹äº†ä¸‹åº”è¯¥è¿˜æ˜¯KVå­˜å‚¨çš„ï¼ˆæ¯•ç«Ÿé¢å‘æœºå™¨å­¦ä¹ ï¼‰ï¼Œå¹¶ä¸æ˜¯å—å­˜å‚¨ã€‚å› æ­¤NASä½¬è¿˜æ˜¯ä¸å¤ªèƒ½ç”¨å¾—ä¸Šçš„ã€‚  ä¸€è‡´æ€§åè®®åŸºäºCRAQï¼Œæ¯•ç«ŸKVå­˜å‚¨ï¼ŒåŸºäºé“¾å¼å¤åˆ¶çš„ï¼Œå†™æ“ä½œä»ç„¶éœ€è¦é€šè¿‡æ•´ä¸ªé“¾ï¼Œæ‰€ä»¥å†™å»¶è¿Ÿå¤§ã€‚ä½†ä¼°è®¡å…¶å®ç»™è®­ç»ƒå½’æ¡£ç”¨ï¼Œå†™å»¶è¿Ÿå¤§æ— æ‰€è°“ã€‚å¼‚æ­¥å½’æ¡£è€Œå·²ã€‚
- æ˜¯æ–‡ä»¶ç³»ç»Ÿï¼Œå…ƒæ•°æ®æ”¾foundationdbé‡Œï¼Œæ•°æ®æ”¾xfsé‡Œ

- å¦å¤–Intelçš„DAOSå…¶å®æˆ‘æ„Ÿè§‰åšçš„æ›´æè‡´ï¼Œç›´æ¥ç”¨SPDKæ“ä½œè£¸ç›˜ï¼Œè¿˜åˆ©ç”¨NVMe-oFæŠ€æœ¯å……åˆ†å‘æŒ¥RDMAé›¶æ‹·è´çš„ç‰¹æ€§ï¼Œåªå¯æƒœå…ƒæ•°æ®ç³»ç»Ÿä¾èµ–äºNVMç¡¬ä»¶ï¼Œè€ŒNVMç¡¬ä»¶å·²ç»å‡‰å‡‰ï¼ŒDAOSä¹Ÿå–ç»™Delläº†

- cephæœ‰object storeå‘€â€¦ å¤´ä¸€æ¬¡è§åˆ°çœŸçš„æœ‰äººç”¨chain replicationçš„â€¦latencyä¸è¦å•¦ï¼Ÿrdmaç¡¬ä»¶è¿™ä¹ˆè´µç›´æ¥é ç¡¬ä»¶ç ¸å—ï¼Ÿ
- æ•°æ®ä¸­å¿ƒï¼ŒåœŸè±ªç©æ³•ï¼Œé¢ è¦†ä¸è‡³äºå§

- ## æˆ‘æ‰çŸ¥é“ å·¥å…·è°ƒç”¨æ˜¯ä¸ªå•ç‹¬çš„ call ä¸€æ—¦è°ƒç”¨å…¶ä»–ä»€ä¹ˆäº‹æƒ…éƒ½å¹²ä¸äº† æ€ä¹ˆè¿™ä¸ª llm ç°åœ¨è¿™ä¹ˆè‰å°ç­å­ã€‚ã€‚ã€‚
- https://x.com/Lakr233/status/1894950577416901018
  - è¿˜æœ‰ Agent è¿™ç§æ¦‚å¿µï¼Œå«å…¨è‡ªåŠ¨ç¼–æ’æ£€ç´¢å·¥å…·ä½¿ç”¨ä¸å¥½ä¹ˆæçš„è¿™ä¹ˆé«˜å¤§ä¸Šåˆ°å¤´æ¥è¿˜æ˜¯ GPT 3.5 å°±èƒ½å¹²çš„æ´»ã€‚ã€‚ã€‚

- tools å°±æ˜¯ä¸€è½®æ–°çš„å¯¹è¯ï¼Œå‘Šè¯‰ AI å¯ä»¥è°ƒè¿™ä¸ªï¼Œç„¶åæŠŠè°ƒç”¨ç»“æœæ‹¼æ¥åˆ°ä¸Šä¸€è½®å¯¹è¯åé¢ã€‚
  - agent å°±æ˜¯ pipeline/orchestration/schedulerã€‚
  - embedding/RAG å°±æ˜¯ searchã€‚
  - LLM framework å°±æ˜¯ HTTP API callsã€‚
- ä½ è¿™æ ·ï¼ŒPPTè¿˜æ€ä¹ˆç¼–ï¼Œå›¢é˜Ÿé¢„ç®—æ€ä¹ˆæ‰¹

- ä¹Ÿæœ‰ä¸€äº›å¼‚æ­¥è°ƒç”¨ +event çš„ç©æ³•, ä½†æ˜¯æ•´ä¸ª Agent ç¡®å®å°±æ˜¯å»ºç«‹åœ¨æ— é™çš„æ–‡æœ¬ç”Ÿæˆä¸Šçš„
  - Agent = FunctionCall + LLM + LOOP

- å·¥å…·è°ƒç”¨å…¶ä»–ä»€ä¹ˆäº‹æƒ…éƒ½å¹²ä¸äº†æŒ‡çš„æ˜¯ï¼Ÿå¯ä»¥å¹¶å‘å¤„ç†å…¶ä»–é—®é¢˜å‘€ï¼Ÿ

- ## æµ·å¤–å¤§æ¨¡å‹æ¥å…¥ä½¿ç”¨openrouter å·²ç»æ˜¯æœ€ä½³å®è·µäº†å˜›ï¼Ÿ
- https://x.com/leeoxiang/status/1887714022327525797
  1ã€æ”¯æŒäº†å¸‚é¢ä¸Šå¤§éƒ¨åˆ†æ¨¡å‹ï¼›
  2ã€å¯ä»¥è®¾ç½®æ¨¡å‹çš„æ¶ˆè€—é¢åº¦ï¼›
  3ã€èƒ½æ”¯æŒæ¨¡å‹çš„è´Ÿè½½å‡è¡¡ï¼›
  4ã€æ”¯æŒä¸€å¥—apié€‚é…å¤§å¤šæ•°æ¨¡å‹ï¼›
  5ã€æ”¯æŒå¤šä¸ªæ¨¡å‹çš„å¤±è´¥é‡è¯•ï¼›

- å¤ªè´µäº†ï¼è€Œä¸”ç‰¹æ®Šæ¨¡å‹è¿˜æ˜¯è¦æ·»åŠ è‡ªå·±çš„API Key

- ## We ( @jamesmurdza ) have been building Open Computer Use - 100% open source computer use agent.
- https://x.com/mlejva/status/1877054558481813799
  - The agent is using @e2b_dev 's Desktop Sandbox as virtual computer.
  - The agent is using 3 different LLMs: ğŸ”¸Llama 3.2 ( @AIatMeta ) ğŸ”¸Llama 3.3 ğŸ”¸OS-Atlas ( @Alibaba_Qwen )
  - It's slow and makes mistakes but this is a big milestone for OS AI community!

- ## æƒ³è¦éƒ¨ç½²æœ¬åœ°æ¨¡å‹ä½†æ˜¯ä¸ä¼šè®¡ç®— vRAM å ç”¨ 
- https://x.com/tuturetom/status/1842492423848804686
  - https://huggingface.co/spaces/hf-accelerate/model-memory-usage

- ## çœ‹æ¥å¤§å®¶ç»ˆäºè¾¾æˆå…±è¯†äº†ï¼šlangchain æ˜¯ç©å…·ï¼Œå¦‚æœéè¦åœ¨ç”Ÿäº§ç¯å¢ƒç”¨å®ƒï¼Œé‚£å®ƒå°±ä¼šå˜æˆå·¥ä¸šåƒåœ¾ã€‚
- https://x.com/beihuo/status/1840058205768167699
  - çœ‹è¿™ä»£ç æ¯”è¾ƒï¼Œlangchain å°±åƒä¸€ä¸ªæ²¡æœ‰å¤ªå¤šå·¥ç¨‹ç»éªŒï¼Œä½†æ˜¯åˆçœ‹äº†å¤ªå¤šè®¾è®¡æ¨¡å¼æ•™ç¨‹çš„äººå†™å‡ºæ¥çš„ä¸œè¥¿ã€‚ä½¿ç”¨å®ƒæ¥å®ç°ä¸€ä¸ªç”Ÿäº§ç³»ç»Ÿï¼Œå°±æ˜¯ä¸€ä¸ªç¾éš¾ã€‚
- åŒæ„Ÿ longchainæ„å»ºæ€ç»´é“¾ä¸å¦‚ç›´æ¥æŒ‰ç…§å·¥ä½œé€»è¾‘äººå·¥æ„å»ºæ€ç»´é“¾ã€‚çœŸæ­£çš„è¿æ€ç»´é“¾éƒ½ä¸æ¸…æ¥šçš„åˆ›é€ å‘æ˜ï¼Œç°åœ¨ç”¨AIæ¥åšä¸ºæ—¶å°šæ—©ã€‚

- ## ğŸ’„ ç”Ÿæˆå¼çŸ¥è¯† UI æœ€æ ¸å¿ƒçš„åŸºç¡€è®¾æ–½ï¼Œç›®å‰å›´ç»•æ­¤ç±»å½¢æ€è®¾è®¡çš„ http://Me.bot ä¹Ÿæ¯”è¾ƒå—æ¬¢è¿
- https://x.com/tuturetom/status/1835349759848333340

- ## Hugging Face å®£å¸ƒæŠ•å…¥ 1000 ä¸‡ç¾å…ƒç”¨äºå…è´¹å…±äº« GPUï¼Œæ—¨åœ¨å¸®åŠ©å°å‹å¼€å‘è€…ã€å­¦æœ¯ç•Œå’Œåˆåˆ›å…¬å¸å¼€å‘æ–°çš„ AI æŠ€æœ¯ï¼ŒæŠ—è¡¡ AI è¿›æ­¥çš„é›†ä¸­åŒ–ã€‚
- https://x.com/glow1n/status/1791488036259434749
  - CEO Clem Delangue è¡¨ç¤ºï¼Œè¿™ä¸€ä¸¾æªå°†é€šè¿‡ ZeroGPU è®¡åˆ’å®ç°ï¼Œä¿ƒè¿› AI æŠ€æœ¯çš„å»ä¸­å¿ƒåŒ–å‘å±•
  - ZeroGPU ä½¿ç”¨ Nvidia A100 GPU è®¾å¤‡ï¼Œæä¾›é«˜æ•ˆçš„è®¡ç®—èµ„æºã€‚
  - Hugging Face çš„ Spaces å¹³å°å·²æœ‰è¶…è¿‡ 30 ä¸‡ä¸ª AI æ¼”ç¤ºã€‚

- ## Cloudflare çš„ Workers AI æ¯å¤©å¯ä»¥å…è´¹ä½¿ç”¨ 10, 000 Neuronsï¼ˆç›¸å½“äºç”Ÿæˆ100-200ä¸ªLLMå“åº”ï¼Œ500æ¬¡ç¿»è¯‘ï¼Œ500ç§’çš„è¯­éŸ³è½¬æ–‡å­—éŸ³é¢‘ï¼‰ ï¼Œè°ƒç”¨æ–¹å¼å…¼å®¹ OpenAI 
- https://x.com/scomper/status/1791804644332908646
- å¥½åƒéƒ½æ˜¯å°æ¨¡å‹ä¸ºä¸»å§

# discuss-model-tuning
- ## 

- ## 

- ## 

- ## [Whatâ€™s the training cost for models like Qwen3 coder 30b and is the code for training it is open source or close source? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1njws3n/whats_the_training_cost_for_models_like_qwen3/)
- You can do fine tunes on the 4B real easy with Unsloth. LoRAs and QLoRAs, too. It wonâ€™t take much VRAM. You can fine tune a LoRA for Qwen3 30B A3B Instruct 2507 in BF16 on a single 96GB 6000 Pro in just a few hours.
  - That depends entirely on what you are training and the size of your dataset. True fine tuning, on a really small dataset will take a few hours, but if you are trying to train in new capabilities, that is going to take a much larger dataset, and could be months or over a year on a single RTX pro 6000.

- Their exact code isn't open source, but many companies train using Pytorch and Megatron-Core open source code.
  - I estimate GPU-time needed for training Qwen 30B A3B to be around 800 000 hours for H100 SXM5, so around 1.6 million dollars. That's for pretraining on 20T tokens, assuming 40% MFU.

- Stick to finetunes, you don't have the hardware or budget to pretrain models of practical size (believe me I have tried).
# discuss
- ## 

- ## 

- ## 

- ##
