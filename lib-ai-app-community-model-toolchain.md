---
title: lib-ai-app-community-model-toolchain
tags: [community, large-language-model, toolchain]
created: 2025-09-16T12:35:55.935Z
modified: 2025-09-16T12:36:12.968Z
---

# lib-ai-app-community-model-toolchain

# guide

- model with UD(unsloth dynamic quants)

- models-config
  - Â§ßÊ®°ÂûãÁöÑÊµãËØïÁªèÂ∏∏ÈúÄË¶Å‰øÆÊîπÂèÇÊï∞ÔºåÊîØÊåÅ‰∏ÄÈîÆÊÅ¢Â§çÈªòËÆ§ÈÖçÁΩÆÊõ¥Â•Ω
# discuss-stars
- ## 

- ## 

- ## 

- ## [Someone needs to create a "Can You Run It?" tool for open-source LLMs : r/LocalLLaMA _202501](https://www.reddit.com/r/LocalLLaMA/comments/1iaubfm/someone_needs_to_create_a_can_you_run_it_tool_for/)
- LM studio, Msty both has info on this before you download.

- You'll never guess who just spent all day making this. https://github.com/Raskoll2/LLMcalc

- Closest thing is this https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator Quantization is just compression, look at the file size + context and see if it's less than your VRAM. If it's less it will run at max speed. If it's more you will need to partially offload, and the speed hit from that depends on how many parameters the model has and how much you're offloading. If you don't care about speed, you can run literally anything of it if it is less than your amount of RAM

- ## [AirLLM make 8GB MacBook run 70B : r/LocalLLaMA _202312](https://www.reddit.com/r/LocalLLaMA/comments/18sj1ew/airllm_make_8gb_macbook_run_70b/)
- They page the model weights from disk.

- The real question is is it noticeably faster than for example llamacpp with a partial offload? Because anyone can make a 70B in 4GB of vram claim if your not placing most of it on the GPU. I know KoboldAI United already had something similar since 2021 but it was just to slow to be any use, and then Huggingface got Accelerate (Which we moved to) which is also basically the same concept of using system memory to swap things in and out of the GPU.

- It's always been possible by reading the model from disk. Just extremely slow. Let's see how fast it is before we get impressed.

- [AirLLM enables 8GB MacBook run 70B LLM | Hacker News](https://news.ycombinator.com/item?id=38790385)
- It's very slow, according to the comments in this thread by people who tried it

- ## [Is it possible to pack LLama into an offline software that you would distribute? : r/LocalLLaMA _202404](https://www.reddit.com/r/LocalLLaMA/comments/1c9kspi/is_it_possible_to_pack_llama_into_an_offline/)
- https://github.com/Mozilla-Ocho/llamafile

- ## [Is Q8 KV cache alright for vision models and high context : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1py4xp6/is_q8_kv_cache_alright_for_vision_models_and_high/)
- I don't have a mathematical evidence, but with KV at Q8 in all my cases qwen3 Vision couldn't do any tasks correctly in browser use cases with very low consistency, the same for picture tagging and descriptions.
  - The difference when observing logs is accuracy, where the model interprets small differences in inputs so differently they give the wrong outcome in terms of tool calls.

- Ruins the output even at a really short context (with gemma-3, pixtral-large, qwen2-72b-vl)

- I cant say anything about vision models as i haven't tested but when it comes to kv cache and llm's i never go below fp 16 because lowering the precision wrecks it. seems that trend would transfer over as well. but who knows maybe someone here sees things differently after experiments..

- ## üè† [Scaling with Open WebUI + Ollama and multiple GPUs? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o9ssqd/scaling_with_open_webui_ollama_and_multiple_gpus/)
  - At our organization, I am in charge of our local RAG System using Open WebUI and Ollama. So far, we only use a single GPU, and provide access to only our own department with 10 users. Because it works so well
  - The final goal will be to provide all our around 1000 users access to Open WebUI (and LLMs like Mistral 24b, Gemma3 27b, or Qwen3 30b, 100% on premises).
  - To provide sufficient VRAM and compute for this, we are going to buy a dedicated GPU server, for which currently the Dell Poweredge XE7745 in a configuration with 8x RTX 6000 Pro GPUs (96GB VRAM each) looks most appealing atm.
  - However, I am not sure how well Ollama is going to scale over several GPUs. Is Ollama going to load additional instances of the same model into additional GPUs automatically to parallelize execution when e.g. 50 users perform inference at the same time? Or how should we handle the scaling?
  - Would it be beneficial to buy a server with H200 GPUs and NVLink instead?

- Instead of complicating and getting overkill hardware, just check and learn out vLLM. Its much suitable for these usecases.
  - yep vLLM just `--data-parallel-size 8` out the entire system, and works well with Ray cluster so it has internal model router and LB for multiple model. and it has KV store backend, so multiple GPU can access the same already compute KV Cache
  - although you can use NGINX as model router (as in api router, not internal of MoE model)

- We just went through this exact scaling problem at Anthromind - started with one GPU for our internal team, then had to figure out multi-GPU serving when other departments wanted in. Ollama doesn't automatically parallelize across GPUs the way you're thinking.. it'll load one model instance per GPU but won't split a single request across multiple cards.
  - For 1000 users you're gonna need to run multiple ollama instances behind a load balancer, each managing different GPUs. 
  - The H200s with NVLink would be overkill for just inference honestly - those RTX 6000s should handle it fine if you set up proper request routing. 
  - We ended up using a simple nginx setup to distribute requests across ollama instances, each pinned to specific GPUs.

- I use LiteLLM to distribute my LLM requests to different ollama instances, running on different servers. This will solve your scaling challenge of the GPU. Also make sure, your embedding is also using a LLM instance, afaik default is cpu embedding in openweb zu instance

- Maybe vllm or sglang with some load balancer, although a single RTX 6000 Blackwell may have enough FLOPS for 50 concurrent requests.

- ## [What is your primary reason to run LLM‚Äôs locally : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1nsjwv1/what_is_your_primary_reason_to_run_llms_locally/)
- For me, it is independence from Big Tech and venture capital. Open weight models can never be taken away from us.

- New open weight models come out like every week and I want to be able to try the ones that can can run on my PC without waiting for a subscription service to add them or paying per use.

- ## üÜö [what is the bottom line difference between GGUF and FP8? : r/comfyui _202512](https://www.reddit.com/r/comfyui/comments/1pu6t8e/what_is_the_bottom_line_difference_between_gguf/)
  - You should ask about regular FP8, scaled FP8, and mixed FP8, instead of comparing it to GGUF because those FP8 differences can affects the quality/accuracy vs the base BF16 model.
  - Even FP8_e4 vs FP8_e5 can generates a different things on the same prompt & seed.
  - Meanwhile, GGUF are designed for low VRAM with the ability to offload some part of it on RAM/CPU.

- [Comparison of Qwen-Image-Edit GGUF models : r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/1my3lq0/comparison_of_qwenimageedit_gguf_models/)
  - Based on those result FP8 is close to Q4_K_M

- so what is the difference between fp8 and scaled fp8 (have not come across weights labeled mixed fp8 yet)?

- FP8 (8-bit Floating Point): Uses 8 bits divided into a sign, exponent, and mantissa. It is a "native" format for modern GPUs (RTX 40-series and newer), meaning the hardware can do the math directly in this format without converting it first.
- GGUF (e.g., Q5/Q6): These use integer-based quantization with "K-quants" (block-wise scaling). The model is broken into small blocks, and each block has its own scaling factor. This allows Q5 (5-bit) or Q6 (6-bit) to often retain more intelligence than a simple 8-bit float because it "allocates" precision more intelligently where it's needed most.
- does it mean we should go for G8 over FP8 if we can, considering they're around the same size? I mean, if G5/G6 is better than FP8, then G8 should be even better, right?
  - yes

- from my personal experience, i'd go for a higher precision gguf over an fp8‚Ä¶ i can't give u a technical explanation, but i think in a like for like quants, the gguf is a better option.
  - It may be better Quality but it will likely be a lot slower, as thats how .gguf works.

- there's no real difference between an fp8 safetensor and a Q8 gguf except that your gpu might natively accelerate the fp8. A gguf will unpack to the native data type your gpu supports (bf16/fp16/fp32). So that unpacking is negligible but not insignificant.

-Depends on your GPU if it's 40xx or 50xx series with fp8 native acceleration fp8 is better (at lest for Wan 2.2 and ZiT), despite the model being over your VRAM it would be faster. If you're with older Nvidia card Q8 and Q6 are faster. I've done tests on RTX 3060 12GB and 5070ti... and 5070ti is faster with the fp8 models. 

- ## üß© [The difference between quantization methods for the same bits : r/LocalLLaMA _202307](https://www.reddit.com/r/LocalLLaMA/comments/159nrh5/the_difference_between_quantization_methods_for/)
  - Using GGML quantized models, let's say we are going to talk about 4bit
  - I see a lot of versions suffixed with either 0, 1, k_s or k_m
- [k-quants ¬∑ Pull Request ¬∑ ggml-org/llama.cpp _202306](https://github.com/ggml-org/llama.cpp/pull/1684)

- K-quantization options are labeled "S", "M", and "L" and stand for small, medium, and large model sizes, respectively. 
  - Option "0" represents baseline quantization without extra calibration. 
  - In terms of quality and speed: 0 (lowest quality, fastest speed) < S < M < L (highest quality, slowest speed).

- k models are k-quant models and generally have less perplexity loss relative to size. A q4_K_M model will have much less perplexity loss than a q4_0 or even a q4_1 model.
- Generally, the K_M models have the best balance between size and PPL, so q3_K_M, q4_K_M, q5_K_M, etc. I like q5, and q4 best usually. 

- Aside from the number of bits per weight in a scaling group being obvious, here are the main differences:
  - Type _0 compression gives each group of weights a shared scale, but they are "symmetric" weights about 0. Type _1 weights add in a "bias" - an offset for each group of weights which allows them to be better resolved if they are mainly shifted away from zero. Type K is an enhancement in the way hierarchal groups are encoded to squeeze a little more compression into the mix. Finally, after the K we now have nothing, M, S and L variants - these actually refer to which tensors have the base precision. In the K_S models, all weight tensors have the stated precision. The simple K, K_M and K_L models specify varying amounts of weight tensors that will actually use higher precision to improve accuracy, typically 4-6 bits. This will no doubt keep expanding over time. Note that the PR referred to by u/lemon07r also contains descriptions of all the formats.

- With the older quantisation method, 4_0 is 4.5 bits per weight and 4_1 is 5 bits per weight.
  - The K quantisation methods are newer. Hopefully, they will get slightly better accuracy for roughly the same file size compared to the old methods.
- That‚Äôs not correct. You will get the best speed with q4_K_S oder q4_K_M. This is because 3-bit and 2-bit needs more calculations.
  - If memory bandwidth is your bottleneck and not processor speed, then smaller is faster.

- [Is IQ4_XS closer to Q4 or Q3 in terms of quality? : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pk1zpq/is_iq4_xs_closer_to_q4_or_q3_in_terms_of_quality/)
- it was accepted that IQ4_XS beat all Q3 quants, as well as the legacy Q4_0 quants. Bartowski has a table on every model that he uploads where he mentions that IQ4_XS is comparable in quality to Q4_K_S but smaller.

- All IQ4_XS I've tried were worse than Q4_K_M at creative writing.

- i1 means imatrix, IQ is the quantization type you are asking about, don't mix these concepts up. You can see that anything about about 5 bits with imatrix is getting quite close to max possible accuracy, and imatrix IQ4_something is very good as well.

- ## [AMA with the Unsloth team : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/)
- What‚Äôs your go-to quant for most models? I usually pick Q4_K_XL dynamic, but if I have enough VRAM, is there another Q4 you‚Äôd recommend for better accuracy?
  - Yes correct, usually always got for the K_XL quants as they have the best ratios in terms of accuracy/speed/size etc 
  - My goto is probably Q3_K_XL as my laptop is incapable of handling anything larger

- üçé Do you plan to support Apple/MLX?
  - Yes definitely, it has been a super high request and we know there are soooo many Mac users out there so we'd be silly to not to. As for when, mmm to be honest maybe late this year? Unfortunately we are team constrained at the moment

- Any plannings on the TPU full integration?
  - It is possible yes but probably after MLX/AMD/Intel etc first
- I think they have it in the roadmap but I do not think anytime soon. I think it would be better for Unsloth if they are support Apple/MLX first and then TPU
  - Yes, AMD/Intel/ MLX will come first then TPU

- I‚Äôd like to use your model in a distributed llama cluster using all my old computer at home. Any planning?
  - We support multiGPU which might help with your setup but won't be officially announcing multigpu until maybe later this year as it's not up to the standard we would like!

- 
- 

- ## [Poor performance qwen3 235B 2507 mlx vs. unsloth variant : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mj7m20/poor_performance_qwen3_235b_2507_mlx_vs_unsloth/)
  - I just downloaded the Qwen3 235B 2507 instruct model for LmStudio on a M2 studio ultra. 
  - I got the MLX 4bit and Unsloth q4_0 versions. 
  - I am getting very low generation speeds on the MLX version ~0.3 tokens/s while on the other hand I am getting ~27 tokens/s on the Unsloth variant.

- Unsloth is highly optimized with advanced quantization for massive models, while MLX is known to have performance issues with models larger than ~22B parameters, which explains the speed difference you're seeing.
  - Where can I read more about MLX having problems with >22B parameters? First time hearing about it

- For unsloth would recommend q3_k_xl or q4_k_xl, should be more accurate than simple q4_o quant‚Ä¶ MLX should be faster, maybe the full model is not loaded to GPU?
  - Thanks. I double checked if all layers were loaded onto the GPU and that's the case, so I don't know what is the problem

- [Performance Qwen3 30BQ4 and 235B Unsloth DQ2 on MBP M4 Max 128GB : r/LocalLLaMA _202504](https://www.reddit.com/r/LocalLLaMA/comments/1kbacf2/performance_qwen3_30bq4_and_235b_unsloth_dq2_on/)
  - I was wondering what performance I could get out of the Mac MBP M4 Max 128GB
  - LMStudio Qwen3 30BQ4 MLX: 100tokens/s
  - LMStudio Qwen3 30BQ4 GUFF: 65tokens/s
  - LMStudio Qwen3 235B USDQ2: 2 tokens per second?
  - So I tried llama-server with the models, 30B same speed as LMStudio but the 235B went to 20 t/s!!! So starting to become usable

- ## üÜö [MLX vs. UD GGUF : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1kpqrzz/mlx_vs_ud_gguf/)
- I benchmarked Unsloth's Qwen3-30B-A3B Dynamic 2.0 GGUF against the MLX version. Both models are the 8-bit quantization. Both are running on LM Studio with the recommended Qwen 3 settings for samplers and temperature.
  - MLX: 3, 516 tokens generated, 1.0s to first token, 70.6 tokens/second
  - UD GGUF: 3, 321 tokens generated, 0.12s to first token, 23.41 tokens/second
  - This is on an MacBook M4 Max with 128 GB of RAM, all layers offloaded to the GPU.

- UD q8 xl is not efficient for Mac. Use normal q8_0

- Isn't the entire point of using the UD XL GGUF for higher quality responses? If you were comparing for speed alone why not use the normal Q8 GGUF?
  - UD also uses less VRAM. I was hoping if it was comparable in speed, UD would be more resource efficient.
- They use less vram than 16 bit, not 8 bit. The goal is better accuracy through selective quantization so it should use more memory than standard q8. This is also why there‚Äôs XL in the name.

- Turn on flash attention if you haven‚Äôt. I wish I could use MLX, it it faster but the output by comparison was worse by a margin I‚Äôve never seen before between the formats. I have an M1 ultra 64gb. It‚Äôs even worse with Qwen3 32B

- In my experience MLX is only marginally faster, less than 10% usually; it has an edge when it comes to speculative decoding, though.

- ## ü§î [Genuine question: Why are the Unsloth GGUFs more preferred than the official ones? : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1ksw070/genuine_question_why_are_the_unsloth_ggufs_more/)
- They often write "getting started" blog posts along with their quants of popular models where they share insights. That's valuable to newcomers. 

- They waste their time on stuff so you don't have to. When some meta data is wrong or a model outputs gibberish for some reason, they check it out and update it with a fix. The other top uploaders aren't bad either and do the same I imagine if an issue get raised. But random uploaders, who knows. And the official model creators do weird shit on their uploads like require login tokens on HF because they don't want you to download from them.

- There was a nicely done test recently that showed that they (quants by unsloth, bartowski, mrademacher) are all good. There is no clear winner. 
  - However, the "official" quants were often released without imatrix or broken / different in some other way. That's why those unofficial quants are usually preferred.
  - Also, unsloth made large MoE models usable on non-server machines with their dynamic Q2_XXS quants.
- The biggest difference I would say isn't the quants, but rather our bug fixes for every model
# discuss-model-speed ‚ö°Ô∏è
- resources
  - [LocalScore - Local AI Benchmark](https://www.localscore.ai/)
    - LocalScore is an open benchmark which helps you understand how well your computer can handle local AI tasks.
  - [AMD Strix Halo ‚Äî Backend Benchmarks (Grid View)](https://kyuz0.github.io/amd-strix-halo-toolboxes/)

- ## 

- ## üÜö [When should you choose F16 over Q8_0 quantization? : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1q0ci23/when_should_you_choose_f16_over_q8_0_quantization/)
  - We've all read about how Q8_0 is "virtually indistinguishable" from F16 when doing inference.

- Every time I download native safetensors I'm amazed at just how good my Q4KL was the entire time. I don't ever recall being impressed by Q8 or BF16 over Q4KL.

- If it's a VLM I will choose F16, if it's text only model - Q8.
  - VLMs in higher precision have significantly better image understanding and bounding box accuracy.
- Do you mean the `mmproj`-part should be f16? Or also the LLM part?
  - I personally found a lot of success with even harsh quantization of the base model, but unquantized vision encoder, without much degradation in image understanding personally.
- I can't upvoter this enough. In llama.cpp I found huge accuracy improvements for some of my use cases by swapping the mmproj file to the F16 or F32 version, and increasing the number of tokens generated from each image. With those tweaks I can get away with running Qwen3-VL 8b instead of a much larger VL model. Especially when I'm using it as part of an agent system to just handle visual descriptions and letting another larger text only LLM handle final analysis and writing. This lets me balance my local agent deployments better and get much better token rates with almost no loss of analytical quality.

- PDE stuff is often done in FP64
  - Highly recurrent/compounding math, such as interest rate calculations or number theory, is often done in higher than 64 but such as 128 bit, 512 bit or 1024 bit etc

- In Agentic Coding you will ALWAYS see differences between FP16 and Q8.

- Got-oss should be left at f16 since it‚Äôs already quantized
  - Vision encoders should also be left at f16
  - Otherwise q8 is great and even q4kl is very good.

- ## [Most used models and performance on M3u 512 gb : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1ouogiq/most_used_models_and_performance_on_m3u_512_gb/)
  - Overall GLM 4.6 is queen right now.
- Model: GLM 4.6
  - Token Gen speed: generally 10-20
Use Case: vibe coding (slow but actually can create working software semi-autonomously with Cline); creative writing; expository/professional writing; general quality-sensitive use
PP Speed: 4 bit MLX 50-70 t/s at large context sizes (greater than 40k)
- Model: Kimi K2 thinking
  - Token gen speed: 3ish to 2
Use case: idk it's just cool having a huge model running local. I guess I will use it for brainstorming stuff, medical stuff, other questionable activities like academic writing. PP speed/context size is too limited for a lot of agentic workflows but it's a modest step above other open source models for pure smarts
PP speed: Q3 GGUF 19 t/s (26k context) faster with lower context; 
- Model: Minimax-m2
  - Token gen speed: 40-50 at modest sizes
Use case: Document review, finance, math. Like a smarter OSS 120.
PP Speed: MLX 4 bit 3-400 at modest sizes (10k ish)
- Model: GPT-OSS-120
  - Token gen speed: about 80 at medium context sizes
Use case: Agentic searching, large document ingesting; general medium-quality, fast use
PP speed: 4 bit MLX near 1000 at modest context sizes. But context caching doesn't work, so has to reprocess every turn.
- Model: Deepseek 3.1:
  - Token gen speed: 3-20 depending on context size
Use case: Used to be for roleplay, long context high quality slow work. Might be obsoleted by glm 4.6... not sure it can do anything better
PP Speed: Q3 GGUF: 50 t/s

- I‚Äôm ngl 4.6 running around 10-20tps is kinda disappointing for a $10k+ computer when you can run the same model on cpu for 2-3tps on a $1500~ ddr5 rig (pre price jump).
  - Don't get me wrong I‚Äôd still love those speeds, but idk if that‚Äôs worth spending an extra $500 per token in extra speed (at least for my use case); definitely reshapes my perspective of everything.
  - if the main reason of buying an expensive computer is to run big models faster, it‚Äôs a valid metric; how much value are you getting out of a $10k computer when a computer at 10% of the cost can do the same thing, just barely slower. I‚Äôd want to see at least 30-40tps out of a $10k computer.

- For comparison 12 3090 gets me 12k prompt processing with VLLM and 20 tokens per second for GLM and Minimax.

- ## [M2 Ultra Mac Studio with 64GB RAM and Ilama3.1 70b : r/ollama _202410](https://www.reddit.com/r/ollama/comments/1g0gycc/m2_ultra_mac_studio_with_64gb_ram_and_ilama31_70b/)
- I have M3 Max with 64 GB RAM and it can handle 70B (4 bit quantized) models. However, in my case, inference with 70B models isn‚Äôt at a comfortable speed.
  - Here‚Äôs the benchmark for Qwen2.5:72B with Ollama and MLX-ML:
  - ‚Ä¢ MLX (mlx-community/Qwen2.5-72B-Instruct-4bit): 8.14 t/s
  - ‚Ä¢ Ollama (Qwen2.5:72B): 6.95 t/s
  - ‚Ä¢ Ollama (Gemma2:27B): 19.39 t/s
- How much RAM does it use to run that 70B model?
  - About 55 out of 64Gb. I have the browser open at the same time, but it doesn‚Äôt slow down web browsing.

- ## [Any m3 ultra test requests for MLX models in LM Studio? : r/LocalLLaMA _202503](https://www.reddit.com/r/LocalLLaMA/comments/1jdvk7c/any_m3_ultra_test_requests_for_mlx_models_in_lm/)
  - Got my 512 gb. Happy with it so far. Prompt processing is not too bad for 70b models -- with about 7800 tokens of context, 8 bit MLX Llama 3.3 70b processes at about 145 t/s per second. It then generates at about 8.5 t/s.
  - 70b 8 bit llama 3 at 7800 context: 8.5 t/s generation; 150 t/s prompt eval; 
  - 70b 4 bit llama 3 at 7800 context: 15.5 t/s generation; 150 t/s prompt eval; 
  - 70b 4 bit llama 3 fine-tune at low context with speculative decoding (and coding related prompt): 23.89 t/s generation; 
  - Gemma 3 27b 4 bit at 2317 context: 331 t/s processing; 29.20 t/s generation
  - Gemma 3 27b 8 bit at 3310 context: 255 t/s processing; 18.71 t/s generation 

- ## [M3 Ultra Mac Studio Benchmarks (96gb VRAM, 60 GPU cores) : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1kvd0jr/m3_ultra_mac_studio_benchmarks_96gb_vram_60_gpu/)
  - I loaded each model freshly in LMStudio, and input 30-40k tokens of Lorem Ipsum text (the text itself shouldn't matter, all that matters is token counts)
  - Input Context Size (tokens): 32k ~ 40k
  - Model Name & Size	Time to First Token (s)	Tokens / Second
  - Mistral Large 123B (4-bit/dense)	900.61	7.75
  - Gemma 3 27B (4-bit)	108.15	29.55
  - Qwen3 30b-a3b (8-bit)	67.74	34.62

- I have the same machine as the OP (96GB, 60 cores) and am running Qwen3-30B-A3B 8bit and Qwen3-32b 6bit concurrently - great combo to use in Aider architect mode. Which two models have you chosen to work with in Roo Code? What has been your experience?
  - I typically use Qwen3 32b as Orchestrator and Architect, Qwen 2.5 32b 128 K as coder and debugger. I use Unsloth versions of all. They can handle certain projects just fine. Especially languages like python. If I run into issues, I mix in deepseek r1 or v3 from openrouter.

- Are you generally satisfied? Or would you rather have the 256GB version? Or the one with 80 GPU?
  - I have the 256 GB version with 60 cores. I would go for the 80 core if given the choice again. Every little bit helps with inference speed. However I can load several 32b 8 bit models concurrently which is great for things like orchestrator mode in Roo Code. Everything works, just could be faster.
- I have the 256 / 60 too. I‚Äôm not sure that the 80 would make such a big diff√©rence. With lama4 scout it would probably amount to 25 secs of pp time gain in the exemple given. But you still have to wait 80 seconds, which makes it too slow for conversational inf√©rence.

- I also have the 96GB/60 core. I am just a casual user and I couldn't justify another 2000‚Ç¨ for 256GB Ram or 80 core. And I think 256GB is not worth it for my purpose. I can use dense models up to 70B (at Q5) for chatting. Mistral Large and Command A (at Q4) are okayish but everything larger will be way too slow. So the only benefit of 256GB is for MoE models.
  - Shortly after I bought mine, Qwen3 235B A22B came out. Right now, this is the only reason (for me) wanting 256GB. But is it worth 2000‚Ç¨? No, not right now. If that model becomes everybodies darling for finetuning, then maybe.

- Can you explain how to load several models?
  - I use LM Studio, and you just keep loading models until you‚Äôve either loaded all of the ones that you need, or when you reach 85% of your memory capacity. It‚Äôs a good practice not to fill more than that.

- That's quite slow, on my 2x3090 I have
  - google_gemma-3-12b-it-Q8_0 - 30.68 t/s
  - Qwen_Qwen3-30B-A3B-Q8_0 - 90.43 t/s

- ## üÜö [[Benchmark] Quick‚Äëand‚Äëdirty test of 5 models on a Mac‚ÄØStudio‚ÄØM3‚ÄØUltra‚ÄØ512‚ÄØGB (LM‚ÄØStudio) ‚Äì Qwen3 runs away with it : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1kfi8xh/benchmark_quickanddirty_test_of_5_models_on_a_mac/)
  - M3‚ÄØUltra, 128‚ÄØCPU‚ÄØ/‚ÄØ80‚ÄØGPU cores, 512‚ÄØGB unified RAM
- Model	Quant. / RAM footprint 	Speed (tok/s)	Tokens out	1st‚Äëtoken latency
  - MLX Deepseek‚ÄëR1‚Äë4bit	402.17‚ÄØGB	16.55	 2‚ÄØ062	 15.01‚ÄØs
  - MLX deepseek‚ÄëV3‚Äë0324‚Äë4bit	355.95‚ÄØGB	19.34	 755	17.29‚ÄØs
  - MLX Qwen3‚Äë235‚ÄëA22B‚Äë8bit	233.79‚ÄØGB	18.86	 3‚ÄØ096	 9.02‚ÄØs
  - GGFU Qwen3‚Äë235‚ÄëA22B‚Äë8bit	‚ÄØ233.72‚ÄØGB	14.35	 2‚ÄØ883	 4.47‚ÄØs
  - MLX Gemma‚Äë3‚Äë27b‚Äëit‚Äëbf16	 52.57‚ÄØGB	11.19	 1‚ÄØ317	 1.72‚ÄØs
- TL; DR
  - Qwen3‚Äë8bit dominates ‚Äì PhD‚Äëlevel answers, fast enough, reasoning quick.
  - Thinking time isn‚Äôt the bottleneck; quantization + memory bandwidth are (if any expert wants to correct or improve this please do so).
  - Mac‚ÄØStudio M3‚ÄØUltra is a silence‚Äëloving, power‚Äësipping, tiny beast‚Äîjust not the rig for GPU fiends or upgrade addicts.

- Useful test as far as the raw numbers, but not an eval. Not even for quick and dirty. You can give a model the same prompt and get a "bad answer" once, then an amazing answer the next time. This is why some of the tests will run the same prompt 3 or 5 times.

- One thing I'd like to know is how you compare the MLX vs GGFU performance (for same models)?
  - I just run a simple test, based only on that I would only use ggfu if a MLX doesn't exist. It gave me almost the same output, definitely same quality, but almost 30% more fast.

- ## [M2 Ultra 192 gb + GLM Air 4.5/4.6 for local coding agents? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o7k78o/m2_ultra_192_gb_glm_air_4546_for_local_coding/)
- M2 Ultra 192 GB Running GLM 4.5 Air Q8_0 with flashattention in llama.cpp. 10k token prompt, responding with ~400 tokens.
  - prompt eval time =   29693.60 ms / 10565 tokens (2.81 ms per token, 355.80 tokens per second)
  - eval time =   14221.61 ms /   374 tokens ( 38.03 ms per token, 26.30 tokens per second)

- 4.6 at 4bit will need more than 192gb vram, you could use a 3 bit quant though.

- ## [GLM 4.6 already runs on MLX : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1nujx4x/glm_46_already_runs_on_mlx/)
- My Epyc workstation has 12 RAM channels and I have 8 sticks of 16GB each so I'll max at 192 GB sadly. To run this you'll want 12 sticks of 32 GB to get to 384GB. The RAM will cost roughly $2400.
  - I have DDR5-4800 which is the slowest DDR-5 (base JDEC standard) does 38.4GB/s
  - DDR4-3200, the highest supported speed on EPYC 7003 Milan, does 25.6 GB/s.
  - If you use DDR5-6400 on a 9005 series CPU it is roughly twice as fast. But the new EPYC processors support 12 channels vs 8 with DDR4, so you get an additional 50% bump.

- what's the prompt-processing speed? It sucks to wait 10 minutes every request.
  - As lim context->infinity, pp rate is proportional to attention speed, which is O(n2) and dominates the equation
  - Attention is usually tensor fp16 non-sparse, so 142 TFLOPs on a RTX 3090, or 57.3 TFLOPs on the M3 Ultra.
  - So about 40% the perf of a 3090. In practice, since FFN performance does matter, you'd get ~50% performance.
- Here is my M2 Ultra‚Äôs performance: context/prompt: 69780 tokens Result: 31.43tokens/second, 6574 tokens, 151.24s to first token. 
  - Model: Qwen-Next 80B at FP16
  - That is 500/s, but using full precision sparse MoE.
  - About 300/s for a dense 70b model, which you are not using to code. It will be faster for a 30b dense model which many use to code. Same for a 235billion sparse MoE, or in the case of GLM4.6 taking up 165gb, it is about 400/s. 

- CLine/Roo regularly uses up to 100k tokens on the context, it's slow even with GPUs.

- ## [I know nobody has asked before, but what is GLM 4.5 like on an M3 Ultra? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1n80nxn/i_know_nobody_has_asked_before_but_what_is_glm_45/)
  - Google claims m3 ultra ram bandwidth is barely slower than 3090 bandwidth. Does this mean I would get barely lower tps compared to a machine with 20 x 3090?
- I just tested 4bit MLX quant with simple task of creating calculator in single html page. This is a standard test I run to see how the model works with one shot tasks that are very well specified.
  - Compared to Air (I tested 4, 6 and 8 bit MLX quants there), it provided better working result on the first prompt.
  - Mac Studio M3 ultra with 256GB ram
  - 19.08 tok/sec ‚Ä¢ 10013 tokens ‚Ä¢ 14.29s to first token

- People suggesting Macs for LLMs are often not mentioning that they can have prompt processing times of ten minutes for every single large prompt. Personally I think that it is rather negligent to not mention this to people.
  - I have an M3 Ultra and based on how slow things are with any non-trivially sized context length, I am not even bothering to try a model as big as GLM 4.5
- Not to mention switching models to test different parameters. It is only strictly better than pure RAM setup

- 10 minutes would imply a prompt of around 200, 000 tokens. Is that actually a common use case for you?
- You need to factor in the prompt processing speed of the model/gpu and how big the context is, because pp speed slows down as context gets bigger.
  - Reaching 10 minutes of pp time is pretty easy on my 512 m3u, for example 60k of context with gguf deepseek.
  - But for smaller models it can be quite decent. Mlx gpt oss will be maybe only 1-2 minutes for that same prompt.

- M3 Ultra RAM bandwidth is 819.2 Gb/s while 3090 RAM bandwidth is 936 Gb/s 
  - So it‚Äôs probably true that for token generation, which is largely RAM bandwidth limited, that the M3 Ultra is ‚Äúcomparable‚Äù to a 3090
  - What‚Äôs actually different here is prompt processing speed which is compute bound.  In this realm a 3090 would be many multiples faster.  

- M3 Ultra, 512 GB RAM, 32/80-core variant.
  - I have a script processing files roughly 30-50k tokens in length, which I cache, and then ask subsequent questions of. I just fired it up on GLM4.5 4 bit MLX quant. For a 35000 token document, prompt processing took ~247 seconds. In subsequent turns of conversation generation speed was 10 tokens per second roughly speaking.
  - GLM4.5 Air, same document was ~104 seconds for prompt processing, and then generation is at 30 tokens per second or so.
# discuss-quantized
- ## 

- ## 

- ## 

- ## [Noob question: imatrix, yes or not? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1qcfto0/noob_question_imatrix_yes_or_not/)
  - Does it make sense to use imatrix for specialized models (i.e. RP, coding, medical models) or would regular/static ggufs be a better choice for these?
  - In the past I've been told imatrix (including unsloth?) affected things like thinking, so I was wondering if it may actually hurt specialized models.
  - EDIT: To clarify, I know imatrix is better in general. What I'm asking is, if imatrix datasets are generic, the quantization process might actually be overfitting the model on that specific dataset, not sure if that may affect how a medical or coding model works.

- Always use imatrix for all quants (except for Q8). It's "free quality". Here's a perplexity comparison that I made when the feature was introduced. You can see the jumps in similarity to the unquantized model there.
  - I also ran some extensive imatrix dataset tests which show that testing for the best imatrix/quant can be rather difficult. Even an unsuitable imatrix dataset seems better than no imatrix at all.

- Below 3.5 BPW, imatrix Is the only way. When you are about 4 BPW, you can choose.
  - If you are offloading tensors / layers to CPU, It might make sense to pick a static (non imatrix) quant, like IQ4_NL
  - If you have a rtx 3000+ Nvidia GPU and you fit everything on the GPU, it's very likely that IQ4_XS imatrix quant is on par with the static one, so you can pick the static, because all layers tensors use the IQ4_XS quant, which Is more efficient then the Q4_K_S or similar quants.
  - Above 5.5 BPW imatrix Is meaningless, Q6_K, imatrix or static, are the same.
  - Usually, my suggestion Is: Always go for imatrix. Unsloth (files with UD- prefix) or Barto are my fav. The only exception Is when you are using CPU, in those cases, pick static Q4_0 or IQ4_NL, instead of imatrix ones.

- No difference between a q4km, q4ks and ud Q4 xl for my use cases.

- most recommend imatrix quants, but i haven't noticed any differences between them and the normal ones.

- ## üß©üÜö [Choosing a GGUF Model: K-Quants, I-Quants, and Legacy Formats : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1q911fj/choosing_a_gguf_model_kquants_iquants_and_legacy/)
  - [Choosing a GGUF Model: K-Quants, I-Quants, and Legacy Formats _202510](https://kaitchup.substack.com/p/choosing-a-gguf-model-k-quants-i)

- Most GGUF weight formats are blockwise.
  - A matrix is split into fixed-size blocks, each block is represented with compact integer parameters, and a small set of per-block parameters reconstructs approximate floating weights at inference.

- Legacy Formats: Q_0 and Q_1
  - The legacy family of GGUF formats, Q4_0, Q4_1, Q5_0, Q5_1, Q8_0, implements classic per-block linear quantization. A block stores n-bit weight codes and either one scale (the ‚Äú_0‚Äù variants, symmetric) or one scale plus one offset/zero-point (the ‚Äú_1‚Äù variants, asymmetric). Dequantization is a single affine transform per block.
  - These formats are simple to decode and therefore fast. Their weakness is representational: one affine map per block cannot model skewed or heavy-tailed weight distributions as well as newer schemes.
  - At 8-bit, the difference is negligible, and Q8_0 is effectively near-lossless for most LLMs. That‚Äôs why we can still see a lot of Q8_0 models being published on the HF Hub. At 5- and especially 4-bit, legacy formats leave measurable accuracy on the table compared with modern alternatives.

- K-quants: Modern Default for 3‚Äì6 Bits
  - K-quants (Q2_K, Q3_K, Q4_K, Q5_K, Q6_K, and their mixed variants like _S, _M, _L) introduce structure beyond a single affine per block. 
  - The most common pattern is a two-level scheme: small blocks with their own scale and zero-point grouped into a super-block with an additional scale/offset.
  - The result is lower error at the same storage. For example, a typical Q4_K lands around the mid-4s bits/weight‚Äîslightly above Q4_0/1 once you count its extra parameters, but it achieves distinctly better fidelity. Q5_K and Q6_K cluster close to the original model in perplexity while remaining far smaller than FP16.
  - On modern CPUs and GPUs, K-quants generally match or beat legacy formats in throughput because you move fewer bytes for the same quality.
  - Keep in mind that for most models, you won‚Äôt see much difference in quality between S, M, and L variants, unless you are dealing with small models (let‚Äôs say <8B models).

- For very large LLMs, like DeepSeek models, you may also find a TQ1_0 version.
  - TQ1_0 encodes weights that are ternary (values in {‚àí1, 0, +1}) using a compact packing scheme. It lands around ~1.6‚Äì1.7 bits/weight depending on packing details.

- I-quants (IQ2_XXS, IQ2_XS, IQ2_S, IQ2_M; IQ3_XXS/XS/S/M; IQ4_XS; IQ4_NL) are purpose-built to hold up at 2‚Äì4 bits.
  - They go beyond piecewise-affine by introducing non-linear and table-assisted reconstruction.
  - The pay-off is quality per bit. IQ4_XS typically bests 4-bit K-quants at similar effective size. IQ3_XS and IQ3_M tend to outperform their 3-bit K counterparts.
  - IQ2_* is the frontier that makes very large models fit in places they simply could not before.
  - IQ4_NL is a special 4-bit non-linear variant that also uses smaller blocks. It targets CPU speed while retaining the non-linear benefits.

- IQ4_XS vs Q4_K_M
  - IQ4_XS and Q4_K_M are both ‚Äú4-bit class‚Äù GGUF quantizations, but they trade off size, speed, and robustness differently: Q4_K_M is the reliable default (slightly larger, generally predictable quality/perf), while IQ4_XS is compresses more aggressively (lower effective bits/weight), which can help you fit larger models/contexts and sometimes improve token generation speed, at the cost of being more sensitive to how the quant was produced (imatrix quality; see below) and to your hardware/kernel mix
  - In llama.cpp‚Äôs published Llama-3.1-8B numbers, IQ4_XS is ~4.46 bpw / 4.17 GiB vs Q4_K_M at ~4.89 bpw / 4.58 GiB, with IQ4_XS a bit faster for generation but a bit slower on prompt processing.

- IQ4_NL vs IQ4_XS
  - IQ4_NL and IQ4_XS are both llama.cpp ‚ÄúI-quant‚Äù GGUF formats aimed at strong quality at ~4-bit, but they optimize different things: IQ4_XS is the more aggressive/compressed option (~4.25 bpw) at the cost of being a bit more sensitive to how the quant was produced. 
  - IQ4_NL is a less compressed non-linear variant (~4.5 bpw) with a different dequantization rule and smaller-block design that‚Äôs often described as targeting CPU friendliness/speed while keeping the non-linear benefits.
  - In practice, many community benchmarks report IQ4_NL is very close to IQ4_XS (sometimes within noise), which is why some quant publishers drop IQ4_NL as ‚Äúredundant‚Äù 

- ## [Why not Qwen3-30B Quantized over qwen3-14B or gemma-12B? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1q62pyh/why_not_qwen330b_quantized_over_qwen314b_or/)
  - I have a 3080ti with 12GB of VRAM and 32GB of RAM and a 5900x. With this I can run qwen3-30b-a3b-thinking-2507 that does 3.3B activated parameters in LM studio 20 tok/sec which I believe is quantized right? It runs pretty well and has good answers. Why would I use the more recommended ones of qwen3-14b or gemma 12b over this that I see more often recommended for a computer of my specs?

- MoE models will have a higher "static" VRAM cost, so you have less KV-cache -> lower ceiling on parallel requests -> lower total TG. But active parameters are fewer -> so faster compute -> higher total TG.
  - In any case you have to evaluate your usecase; the quality of output and your throughput.
  - For example, if you want more speed and you don't need a lot of KV-cache/context you could try Qwen3-VL-8b at FP8 or lower quant. This will fit into your VRAM.

- The big advantage of MoE, especially for running on consumer hardware, is that the model doesn't have to fully fit into VRAM to give reasonable speed. I find models larger than 8B (active) parameters get really slow on CPU. Qwen 30BA3B or GPT-OSS-20B run quickly even on only CPU since they run as small models, but they're still big enough to be reasonably smart and useful. (And they run really fast with a hybrid GPU/CPU setup, even when they don't fully fit into VRAM).

- ## üÜö [MiniMax M2.1 quantization experience (Q6 vs. Q8) : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1q3579f/minimax_m21_quantization_experience_q6_vs_q8/)
  - I was using Bartowski's Q6_K quant of MiniMax M2.1 on llama.cpp's server with Opencode and it was giving me some very strange results.
  - The usual way I test coding models is by having them write some of the many, many missing unit tests.
  - I stepped up to Q8 just to see and it nailed everything on the first try with a tiny fraction of the tokens.
  - That's a small sample size and there's always the possibility of a random outcome. But, wow, yikes, I won't be trying Q6 again in a hurry.

- Minimax does not quantize like other models, and is native FP8. You can try my NVFP4 if you have Blackwell GPU‚Äôs

- I had that happen with Q8 KV cache. Not worth it.

- Did you try tweaking the settings, such as setting a repetition penalty, or other settings that are designed to reduce the amount of rambling a model does. A q6 should be virtually identical to a q8, there's a good chance it was just random. I always use rep pen of 1.05 for the qwen model I use, and have never encountered rambling with it set.
  - Also you're using a quant with an imatrix, and imatrices can mess up reasoning chains. I don't ever use imatrix quants for reasoning models. And at q6 or q8 you don't need an imatrix quant, it won't make the model any better than without.
- Im pretty sure unsloth uses imatrices. I don't have any references, it's from my own experience and I also remember bartowski talking about it in a thread once, which suggests it's a known issue. I normally use mradermachers non imatrix quants.

- ## üÜö [Benchmarks for Quantized Models? (for users locally running Q8/Q6/Q2 precision) : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pyrjke/benchmarks_for_quantized_models_for_users_locally/)
- Nope! Definitely a need for such a thing. Quantization has been around for a while but is still the wild-west of LLM's in terms of documenting the results/impact.

- Most of the benchmarks are too annoying or slow to run, unfortunately. MMLU-Pro compsci only isn't too terrible and Aider 'Polyglot' limited to python only (or whatever your preferred language is) is ok, too.
- [Unsloth Dynamic GGUFs on Aider Polyglot | Unsloth Documentation](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot)

- Devstral-Small-2-24B-Instruct-2512-Q8_0.gguf Lm-studio community VS Devstral-Small-2-24B-Instruct-2512-UD-Q4_K_XL.gguf Unsloth
  - It's a dense model, but it all fits comfortably in a 96GB RTX 6000, even with very long runs. The card is set to 450W instead of 600W.
  - The problem is simple: The Q4_K_XL version of unsloth returned the modified HTML file but without some of the required fixes, while the Q8 version solved every problem.
  - Obviously, it's only one test, but I definitely prefer the slower execution speed and greater precision. Now I'll try the Q6.

- I found MXFP4 definitely superior to Q4 across a variety of models considering their applied weights (MXFP4+F32), even better than Q8 in the few cases I tried - at the exception where GPT-OSS-20B MXFP4 was not as good as Unsloth slightly heavier F16.

- Primary CUDA maintainer for llama.cpp/ggml here, given enough time I'll eventually do it for quality control and publish the results here https://github.com/JohannesGaessler/elo_hellm
# discuss-llm-tools-tips/tricks
- ## 

- ## 

- ## 

- ## [New functiongemma model: not worth downloading : r/ollama _202512](https://www.reddit.com/r/ollama/comments/1pq69o5/new_functiongemma_model_not_worth_downloading/)
  - I have a valid MCP toolset that works great with other very small models such as qwen3:1.7b. I obtain quite reliable function calls. So, an even smaller model that could do this with the same quality sounds great. 
  - I downloaded the functiongemma:270m-it-fp16 version of 552MB and deleted after the second test
- I believe the purpose of this model is to be fine-tuned further on your specific tool use case.

- In practice I‚Äôve had the best luck with models that were explicitly trained for tool use / JSON (and I still add strict schemas + validation + retry).
  - If you want to stay local, I‚Äôd try Qwen2.5-Instruct (7B) or Llama 3.1 Instruct (8B) as a baseline and then see how far down you can compress without breaking tool calls.

- It‚Äôs a 270m parameter model. MCP dumps massive JSONs. Comparing it to Qwen1.7b is literally 6x its size. I think this model is good for small, focused use cases - not something like MCP.

- ## üí° [Suggestions to improve RAG with 200+ pdfs : r/Rag _202412](https://www.reddit.com/r/Rag/comments/1h8205b/suggestions_to_improve_rag_with_200_pdfs/)
- I‚Äôm working on a company where we are running a RAG for a company with more than 4.000.000 documents (more than 200.000.000 embeddings) so this might help you.
  - We start with an ingestion process where all PDFs or documents (regardless of source) are converted into markdown using LLMSherpa, an open-source library we‚Äôve fine-tuned for our needs. The markdown is then subdivided into sections, which are further broken down into smaller chunks. These chunks undergo a context retrieval process to add additional relevant information before being stored.
  - Next, we generate embeddings for these chunks using BAAI/bge-m3 and store them in a PostgreSQL database managed via pgvector.
  - When a user interacts with our playground, they send a message, which includes the chat history. To handle this, we first make an initial LLM call to rephrase the prompt and incorporate additional context. This step is crucial for questions referencing prior chat history, ensuring clarity and proper fine-tuning before the main LLM call.
  - The rephrased prompt is then matched against all the stored chunks in our database using inner product similarity, with a predefined threshold to filter the most relevant chunks. After retrieving these, a reranker further narrows the results to the top 50 most relevant chunks.
  - Finally, the rephrased prompt and the refined chunks are sent to the LLM, which generates the user‚Äôs final answer. To ensure fast and efficient database queries, we use HNSW (Hierarchical Navigable Small World) indexing, enabling low-latency and scalable retrieval.

- Converting the PDFs to images and using an LLM like Claude 3.5 Sonnet to convert it with custom instructions works quite well, though it's slower and pricier. I convert all my PDFs like that.

- ## [Best AI to search in large folder of PDFs : r/AI_Agents _202503](https://www.reddit.com/r/AI_Agents/comments/1j1pum1/best_ai_to_search_in_large_folder_of_pdfs/)
- You don‚Äôt want an AI, you want a robust indexing and search engine that feeds relevant docs/chunks to an LLM for a simple summarization. You can do that with RAG, vector DB, or just old school ELK stack. Plenty of options.

- Notebooklm can take up to 50 files. And it has the great podcast summary.

- I have efficient semantic chunking + STORM + meilisearch integration specifically for this problem.

- ## [How does having a very long context window impact performance? : r/LocalLLaMA _202507](https://www.reddit.com/r/LocalLLaMA/comments/1lxuu5m/how_does_having_a_very_long_context_window_impact/)
- Basically, the longer the context window the more memory is needed to store the model and the KV cache. The more memory the model takes up, the longer it will take for the GPU to read the model from memory.
  - keep in mind many models claim 128K context length, but their ability to recall information deep within a block of text will degrade as it fills up. QWQ appears to be one of best models for long context recall and creative writing.

- You should not max out the context window. You should be extracting what's valuable and reinjection it when necessary.
  - This is a lot more complex than it seems. But context summarization into RAG and MCP servers probably help get you there.

- My experience with Gemma3 27B is that speed fell off the cliff once the context is more than 4 windows (ie over 4K). I‚Äôm using a 5070ti and I get 30t/s using Q3KM model under 4K. Beyond 4K context it requires a lot more compute to look back and remember things is my understanding. So you get loooong prompt processing time. In my case it‚Äôs 5t/s‚Ä¶ Probably better off using Mistral Small which is what I‚Äôm doing, something a bit longer context and less cliff.
  - You can also write in auto-summarizing function just before it hits context limit. Or use something that has it. And when the sessions get very long use RAG.

- Gemma 3 had a very memory heavy context. Q4 quant with 32k context was barely fitting into 32gb vram. But people praised it for good attention to context. Then Google "fixed" it, introducing SWA, which made it lightweight, but after this I've seen many complaints about the model forgetting things very fast.
- Gemma3 does not understand long context that well. Even at 8k it is becoming quite inconsistent (contradicting what was in context before).

- ## [LLM with large context : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1kdnbhj/llm_with_large_context/)
- The current mainstream open-source LLMs have a context length of around 128K, but there are already some options that support longer contexts (Llama4, Minimax-Text, Qwen2.5-1M). 
  - However, the GPU memory overhead for long contexts is substantial. For example, Qwen2.5-1M-7B mentions that it requires approximately 120GB of GPU memory to deploy a model supporting a 1M context. 
  - It's difficult to fully run a model with a 1M context locally. However, such models might perform better than regular models in tasks requiring longer inputs(64K-128K) refer to Qwen2.5-1M.
- A significant issue with using long-context LLMs is that most LLMs' long contexts are extrapolated (for instance, Qwen-2.5 has a pre-training length of 4K ‚Üí long context training of 32K ‚Üí Yarn extrapolation to 128K, and Llama3.1 has pre-training up to 8K ‚Üí Rope scaling extrapolation to 128K), and only a small amount of long-context data is used during training. 
  - As a result, performance may degrade in actual long conversations (I believe most models start to degrade above 8K length, and performance notably worsens beyond 32K). 
  - Of course, if you only aim to extract some simple information from a long text, this performance degradation might be acceptable.

- Attention mechanism right now just don't handle large context very well. If there's too much hard distractors within context, the model just won't do too well.

- But fan of Qwen 3 8B or 32B. You can fit 128K with model in 24GB of VRAM, but you will have to trade Q8 for Q4 for KVcache on the 32B model. 

- 32k is where all models degrade, even if stated otherwise.
  - Qwen 3 are better ones though.
  - There is also Llama 3.1 8b Nemotron 1M, 2M and 4M; I had mixed success with them - they are strange, weird models, but handle context well.

- 1m context's kv cache takes too much vram for local use case. https://www.reddit.com/r/LocalLLaMA/comments/1jta5vj/vram_requirement_for_10m_context/

- [How large is your local LLM context? : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1j6xpvt/how_large_is_your_local_llm_context/)
  - Most local LLMs are massively degraded by 32K context. Both token quality and generation speed. I would say there's no point going over that, and you should try not to even get close. You have to do more work to fit in only the relevant context but that's the tradeoff with going local
- No I don't unless it's required. Having large input tokens impacts the throughput, so it's better to optimize the input tokens length.

- ## üÜö [Comparing Unsloth's GLM-4.6 IQ2_M -vs- GLM-4.6-REAP-268B Q2_K_XL : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1ozq14d/comparing_unsloths_glm46_iq2_m_vs_glm46reap268b/)
  - During my limited coding testing I'm seeing:
  - REAP_Q2_K_XL sometimes perform better but fail more often, including sometimes looping and some broken code outputs.
  - Full_IQ2_M retains more general and contextual knowledge and seems more consistent, but perhaps less chance of a great output.

- Tested REAP vs IQ a lot. IQ always better. Minimax M2 ---> same
- I had the same experience with GLM 4.6 and GLM 4.5 air. Eventually I think reap will work but at the current stage I view it as a tech demo.

- The trade‚Äëoffs you describe mirror the differences between quantization and Mixture‚Äëof‚ÄëExperts pruning at a systems level.
  - A Runpod overview notes that moving from FP32 to INT8 or 4‚Äëbit quantization can cut memory use by 60‚Äì80‚ÄØ% while preserving over 95‚ÄØ% of model accuracy. 
  - By contrast, REAP (Router‚Äëweighted Expert Activation Pruning) evaluates the router‚Äôs gate values and expert activation norms to remove low‚Äëimpact experts, achieving near‚Äëlossless compression even after pruning 50‚ÄØ% of experts on code‚Äëgeneration tasks.

- All the reaps I tried were slower and dumber. Then again i didn't use them for code as intended.

- Far as roleplay goes, I personally find that REAP loses a ton of flavor and personality. It just doesn't feel good.

- ## [What tools or workflows save you hours every week? : r/PKMS](https://www.reddit.com/r/PKMS/comments/1p00u1x/what_tools_or_workflows_save_you_hours_every_week/)
- A digital file cabinet (PKMS) to store/organize my notes/documents/files
  - For enhanced features, I use pkms app Devonthink; accessed with a Mac and iPad
  - Integrated scripting for workflow automations
  - I use AppleScript on my Mac

- Knowledge management: Mostly Note app + NotebookLM for long and hard PDF (I put that into the app and it turns that to podcasts!)
  - Daily planning: Brain dump + Saner: I basically just offload my thoughts and the app turn it to tasks with reminders automatically

- Zettelkasten for idea incubation/research/serendipity, PARA as archiving method, GTD/ZTD for running tasks. A personal combination of them

- ## [Best small LLM (‚â§4B) for function/tool calling with llama.cpp? : r/LocalLLM _202505](https://www.reddit.com/r/LocalLLM/comments/1kdva3y/best_small_llm_4b_for_functiontool_calling_with/)
- So far qwen3 is really the only game in town for consistent tool calling for me at small sizes. 
  - when I test I do not tell the model exactly what tools to use and try to keep my prompts sort of vague because I want to be able to ask for something without for example knowing a table name in a database.
  - I‚Äôve only really tried 4b and up. I had downloaded 1.7b and it worked like once out of the 3 runs I tried with it. I‚Äôd imagine a smaller model would do worse. If you‚Äôre very instructionally verbose it may work better though.
  - 4b, 8b, 14b, 32b all call functions really well and consistently.
  - 8b, 14b, 32b can digest the returned agent information and transform it.
  - 14b, 32b can transform it well and provide better context.
  - 32b is not noticeably better for agentic at least for my use cases than 14b
  - Sweet spot for me is 8b/14b. I‚Äôve used 8b extensively. It fails like 10% depending on instruction vagueness and how strict I am with temperature.

- Everyone‚Äôs talking about Qwen which makes sense due to its recent release but for an alternative, I‚Äôve had good success with the Gemma 3 4B and 12B models. Once you get around the Google ReAct logic it‚Äôs pretty manageable and it seems to be smart enough for my use cases

- ## [Cerebras REAP'd GLM4.6: 25%, 30%, 40% pruned FP8 checkpoints on HF! : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/)
  - The models deployed in Cerebras prod inference API are not pruned, and we don't have such plans for GLM4.6. The REAP pruning work is for research purposes and to give more efficient models to the community

- The MoE is sparse, yes. But the REAP technique appears to leverage the fact that ‚Äúexperts‚Äù are more or less just certain parameters highly correlated with other parameters. It‚Äôs not like there‚Äôs a programming expert, a language expert, etc. it‚Äôs more like you have some number of tokens that seem to be the strongest correlated to other sets of tokens, so an invocation of that token or closely related tokens results in that particular part of the neural net lighting up.

- I actually have the opposite question. Can we reduce the number of experts from 8 to 6 or 4 and still maintain good performance. Experts are largely CPU bound and would go a long way in speeding up the models.
  - my experience with qwen3 30b a3b
  - with 7 things are ok , but it fails sometimes (i would say 10% of the times you noticed is worse than the default)
  - with 6 it fails more often
  - with 4 it lose coherence in the majority of times
  - but increasing also doesnt help a lot
  - with 9-10 i barely see an improvement over 8
  - with 12-16 the output gets worse

- ## [I did not realize how easy and accessible local LLMs are with models like Qwen3 4b on pure CPU. : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o1z3hj/i_did_not_realize_how_easy_and_accessible_local/)
  - I hadn't tried running LLMs on my laptop until today. I thought CPUs were too slow and getting the old igpu working (AMD 4650U, so Vega something) would be driver hell. So I never bothered.
  - I downloaded LM Studio, downloaded Qwen3 4b q4, and I was getting 5 tok/sec generation with no hassle at all with the automatic Vulkan setup. Not bad
  - Then, just to be sure, I disabled the GPU and was surprised to get 10 tok/sec generation with CPU only! Wow! Very usable.
  - This means I can just buy any used laptop off ebay, install linux, and go wild??? It has a built in monitor, low power draw, everything for $200-300? My laptop only has DDR4-3200, so anything at that speed or above should be golden. Since async processing is fine I could do even more if I dared. Maybe throw in whisper.

- Qwen3-4B is really a good choice for 16GB laptops (common choice for general consumers). I use it for local PDF rag and it can provide me with accurate in-line citations + clear structured report.

- Or you can just run it much faster with a $60 GPU and have your low power kitchen computer connect to that via wifi.

- ## üÜö [8B models at full-size, or 32B models at Q4? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1o13oyn/30b_models_at_fullsize_or_120b_models_at_q4/)
- Q4 isn't much of a degradation and the base is so much stronger.
- The degradation depends on the models original size and architecture as MOE can be more sensitive but overall I would always pick the larger models when possible so long as you still have enough space for context.

- ## [Mixed precision KV cache quantization, Q8 for K / Q4 for V : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1kddcdp/mixed_precision_kv_cache_quantization_q8_for_k_q4/)
- Plenty of KV cache quantization literature suggest K is more sensitive to quantization than V (SKVQ, QAQ, etc.
  - No one can really answer whether K8V4 is good enough as this is highly model-task-setting-whatever else dependent, 
  - but conventional wisdom suggests: Granting Key cache more budget is generally the right move.
  - You probably need some kind of "fancy" KV cache quantization techniques ‚Äî like our KIVI, 

- Yes.. I try it all the time. 4/4 is really bad, 8/4 is as low as I'm willing to go. Don't forget to compile llama.cpp with all kernels.

- ## [Does Q4-8 'KV cache' quantization have any impact on quality with GGUF? : r/LocalLLaMA _202409](https://www.reddit.com/r/LocalLLaMA/comments/1flw4of/does_q48_kv_cache_quantization_have_any_impact_on/)
- Yes in my test. I was testing the summarizing ability of llama3.1 8B q5_ks on oobabooga a while back on youtube transcripts.
  - With q4 kv cache the accuracy was about 82%. (something like that) I think even q8 had a performance drop that was unacceptable. (if I remember correctly)
  - Without cache quantisation bullet point accuracy went up to 97.6% accuracy. This was over at least 5 youtube videos of 2-12 minutes each. (If I remember)

- Q8 is pretty painless, but Q4 can be pretty rough, although is usually usable. Smaller models will feel it worse. Just like with model quantization.

- Yes, I noticed some larger models suffer even with q8 so I don't even bother using it

- ## [Recommendation for getting the most out of Qwen3 Coder? : r/LocalLLM _202508](https://www.reddit.com/r/LocalLLM/comments/1mrzi5x/recommendation_for_getting_the_most_out_of_qwen3/)
- Setting K quantization to q4_0 has a pretty big impact on model‚Äôs quality of output. 
  - K is much more sensitive to quantization than V. 
  - You can get away with q4_0 for V but for K the lowest i‚Äôd go is q5_1 and ideally q8_0

- [Why is my llama so dumb? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ljn4h8/why_is_my_llama_so_dumb/)
  - I know a lot of models don't like going that small. Try upping that to Q8_0 or even fp16/bf16.
  - Q4_0 is a fairly very aggressive quantization. Quantization noise leads to loops.
  - Ive personally never used a model that didnt have a noticable decrease in quality at q4, often even at q8. Just leave it at fp16.

- [What's the best models available today to run on systems with 8 GB / 16 GB / 24 GB / 48 GB / 72 GB / 96 GB of VRAM today? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1k4avlq/whats_the_best_models_available_today_to_run_on/)
  - I wouldn't go as low as q4_0 for K cache, quality impact is quite noticeable. q4_0 on V cache seems fine though.
  - I use in larger models (nemotron 253B, deepseekv3 03-25) ctk q8_0 and ctv q4_0, haven't noticed much differences vs fp16 (on deepseek I had to test at like 4k ctx lol)

- ## [LM Studio released new version with Flash Attention - 0.2.22 : r/LocalLLaMA _202405](https://www.reddit.com/r/LocalLLaMA/comments/1cir98j/lm_studio_released_new_version_with_flash/)
- Must be because llama.cpp merged it recently.
  - Yes, from the changelog
- ROCm version already released you need to get the link from LMstudio discord sever

- The flash attention benefit becomes noticeable at LONG LONG contexts - at 26k, its the diff between 30 tok/sec and 10 tok/sec w/ LLama 3 8B on an M3 Max.

- the difference shows up at long contexts.

- Quick check for me didn't see any difference between flash attention on and off for llama3 8b. On a Macbook pro
  - Made huge difference for me on m2 max Mac studio in terms of ftimr to first token and general response rate for decently sized contexts. Details are also remembered much better
- Try with more context, or models like Mixtral. In llama.cpp (which LMStudio uses) I get about 4+ tokens a second higher and no real downside. I can store a lot more context at the same time.

- Flash Attention works on Mac? I always thought CUDA only.
  - Not only does it work it seems to be giving a bigger performance boost, at least on some quick tests

- ## üÜö [weighted/imatrix VS static quants? : r/LocalLLaMA _202405](https://www.reddit.com/r/LocalLLaMA/comments/1ck76rk/weightedimatrix_vs_static_quants/)
- Imatrix quants were introduced a couple of months ago and are recommended over static quants because they have better output quality. 
  - For example, a Q4_K_M quant made with imatrix should be closer to a Q5_K_M non-imatrix quant in quality.

- Importance matrix is used to choose vocabulary to preserve precision for.
  - Both normal Q_k quants and IQ quants can use imatrix.
  - IQ quants, I don't know the expansion of the I, but they attempt to approximate the original value at runtime. They do extra math, and slow down CPU inference but GPUs generally are still memory speed limited.

- Are there any disadvantages? I usually go for Q4k_m and tried iq4_nl or something, the IQ is slightly smaller in file size but inference speed seems to be basically the same. If imatrix is better why do people still release/use static? 

- [Static vs imatrix? : r/SillyTavernAI](https://www.reddit.com/r/SillyTavernAI/comments/1ggfpo8/static_vs_imatrix/)
  - imatrix is distinct from i-quants.
  - i-quants are better if your hardware(CPU in this case, not GPU/RAM/VRAM) is beefy enough to run them at a speed you find usable.
  - K-quants if your CPU is struggling with i-quants.

- [How does any of this work? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1p8up6n/how_does_any_of_this_work/)
  - i-matrix quants use special calibration matrix, they essentially maintain quality on selected type of tasks sacrificing anything else. 
  - My experience suggest that imatrix quants are ever so slightly messed up, especially for creative writing.

- ## [Getting counter-intuitive results with local KV Cache Quantization Benchmark - am I doing something wrong? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nn2nqz/getting_counterintuitive_results_with_local_kv/)
  - I've been running some benchmarks on KV cache quantization for long-context tasks, and I'm getting results that don't make much sense to me. 
  - The Weird Results: I was expecting to see a clear trend where higher quantization (like q4_0) would lead to a drop in accuracy compared to the f16 baseline. Instead, I'm seeing the opposite. My best performing combination is k-f16_v-q5_0 with 16.79% accuracy, while the f16-f16 baseline only gets 13.74%.

- You see scattered reports of quantized KV increasing accuracy because it ‚Äúfuzzes‚Äù attention in a way that actually benefits (specifically) low bit weight quants. Basically it acts as an implicit smoothing function. I‚Äôve not had amazing luck with llama.cpp‚Äôs implementation, but EXllama‚Äôs KV cache quants seem to perform exceptionally well at even 4-bits. 

- ## [ÂõΩÂÜÖÂ§ñÂ§ßÊ®°ÂûãAPIÂπ≥Âè∞‰ΩìÈ™åÂØπÊØî - Áü•‰πé](https://zhuanlan.zhihu.com/p/1914700194517345472)

- Â¶ÇÊûúÊÉ≥‰ΩøÁî®ÂõΩÂÜÖÁöÑÊ®°ÂûãAPIÔºåÊé®Ëçê‰ΩøÁî®ÂõΩÂÜÖ‰∫ëÂéÇÂïÜÊèê‰æõÁöÑAPIÊúçÂä°ÊàñÁ¨¨‰∏âÊñπ‰ª£ÁêÜÂπ≥Âè∞„ÄÇ
  - Â¶ÇÊûúÊÉ≥Âú®ÂõΩÂÜÖ‰ΩøÁî®ÂõΩÂ§ñÁöÑÊ®°ÂûãAPIÔºåÊàñËÄÖÊÉ≥ÂêåÊó∂‰ΩøÁî®ÂæàÂ§ö‰∏çÂêåÁ≥ªÂàóÁöÑÊ®°ÂûãÔºåÊé®Ëçê‰ΩøÁî®Á¨¨‰∏âÊñπ‰ª£ÁêÜÂπ≥Âè∞„ÄÇ
  - Â¶ÇÊûúÊúçÂä°Âô®Âú®ÂõΩÂ§ñÔºåÊÉ≥Ë∞ÉÁî®ÂõΩÂ§ñÁöÑÊüê‰∏™APIÊ®°ÂûãÔºåÂàôÊé®Ëçê‰ΩøÁî®ÂõΩÂ§ñ‰∫ëÂéÇÂïÜÔºåÊàñÁ¨¨‰∏âÊñπ‰ª£ÁêÜÂπ≥Âè∞„ÄÇ

- 2025/9/1 Êõ¥Êñ∞Ôºö
  - 1. Â¢ûÂä†DMXAPIÔºå‰∏Ä‰∏™‰∏çÈîôÁöÑAPI‰∏≠ËΩ¨ÂïÜÔºåÊ®°ÂûãÊØîËæÉÂÖ®„ÄÇ
  - 2. Âà†Èô§ÂçöÈòÖÔºåÂõ†‰∏∫ÂÖ∂‰∏çÂºÄÊîæÊñ∞Áî®Êà∑ÂÖÖÂÄº„ÄÇ
  - 3. Èôç‰ΩéopenrouterÁöÑÊé®ËçêÊåáÊï∞ÔºåÂõ†‰∏∫‰ΩøÁî®ËøáÁ®ãÈÅáÂà∞Á®≥ÂÆöÊÄßÈóÆÈ¢òÊØîËæÉÂ§ö 
  - 4. Èôç‰ΩéÁ°ÖÂü∫ÊµÅÂä®ÁöÑÊé®ËçêÊåáÊï∞ÔºåÂõ†‰∏∫ÈôêÊµÅTPMÂ§™Â∞èÔºåÂΩ±Âìç‰ΩøÁî®„ÄÇ
  - 5. Â¢ûÂä†GLM„ÄÅKimi„ÄÅMinimaxÊ®°ÂûãÁöÑÊé®ËçêÂéÇÂïÜ

- openrouter ËÉåÂêéÁöÑ‰æõÂ∫îÂïÜÊØîËæÉ‰π±, Êúâ‰∫õÊ®°ÂûãÁâàÊú¨‰∏çÊòØÊúÄÊñ∞ÁöÑ. ËøòÈúÄË¶ÅËá™Â∑±ÊéíÈô§‰æõÂ∫îÂïÜ

- ## üîß [Open WebUI vs. LM Studio vs. MSTY vs. _insert-app-here_... What's your local LLM UI of choice? : r/LocalLLM _202502](https://www.reddit.com/r/LocalLLM/comments/1ij3j8m/open_webui_vs_lm_studio_vs_msty_vs_insertapphere/)
- Ollama vanilla CLI in tmux with vim copy/paste between terminals. I like pain

- All of them; don‚Äôt lock into one solution.

- Open WebUI + LibreChat. LibreChat mainly for creating agents for RAG. Most painless interface for RAG.

- Open Web UI. MSTY is no alternative because it is an all-in-one solution.
  - Closed source right?
- Yep, they are selling it for businesses.

- KoboldAI Lite running on KoboldCpp. Most others aren't as flexible and just focused on instruct. This one can do instruct, but it can also do regular text generation for example. 
  - KoboldCpp meanwhile is a single executable with text gen, image gen, image recognition, speech to text and text to speech support. And it emulates the most popular API's if you prefer another UI (KoboldAI LIte doesn't need the backend to have any UI code so if its not open in the browser it does not effect you).

- ## [intelÁöÑcpuËøûÂ§ßÊ®°ÂûãÈÉΩÊ≤°Ê≥ïË∑ë, ÊÄé‰πàËøòÂ§©Â§©Âú®Êé®aipc? - Áü•‰πé](https://www.zhihu.com/question/668042879/answers/updated)
- ÂØπ‰∫éÁ´Ø‰æßAIÔºåÊàë‰∏™‰∫∫ÁöÑÊÉ≥Ê≥ïÔºåÊúÄÂ§ßÁöÑ‰ª∑ÂÄºÂ∫îËØ•ÊòØÊãâÈ´ò‰∏ä‰∏ãÊñáÁ™óÂè£ÔºåÂú®Êú¨Âú∞ÂÅö‰∏™‰∫∫Áü•ËØÜÂ∫ìÔºå‰ª•ÂèäÊú¨Âú∞ÊâπÈáèÊé®ÁêÜÔºåÊØîÂ¶ÇÂÅöÁßëÁ†îÁöÑÔºåÊáíÂæóËØªËÆ∫ÊñáÔºåËÆ©AIÊâπÈáèÊÄªÁªìÂÜô‰∏™ÁªºËø∞„ÄÇËøô‰∏§ÁßçÂÅöÊ≥ïÂ¶ÇÊûúË∞ÉÁî®Á∫ø‰∏äÁöÑAPIÔºåÂÖ∂ÂÆûÊå∫Ë¥µÁöÑ„ÄÇÈòÖËØª‰∏ÄÁØáËÆ∫ÊñáÂ∞ëÂàôÂá†ÂçÉtokensÔºåÂ§öÂàô‰∏§‰∏â‰∏átokens„ÄÇÊú¨Âú∞‰ΩøÁî®32768ÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÁöÑQwen3 8BÔºå‰πüËÉΩÂÆåÊàêÂæó‰∏çÈîô
  - ÂÅöÈïøÁ™óÂè£Êú¨Ë∫´Â∞±ÈúÄË¶ÅËµÑÊ∫êÔºåtransformerÁöÑkqvËÆ°ÁÆóÊòØ‰∏™o(n^2)ÁöÑÂ§çÊùÇÂ∫¶

- Á´Ø‰æß AI Áé∞Âú®ÁöÑÂ∫îÁî®Âú∫ÊôØÂÆûÂú®ÊòØÂ§™Â∞è„ÄÇÂ∞§ÂÖ∂ÊòØËÆæÂ§áÊó†Êó∂Êó†ÂàªÂú®Á∫øÁöÑÊÉÖÂÜµ‰∏ãÔºåÊàë‰∏∫‰ªÄ‰πàÊîæÁùÄÂÖçË¥πÁöÑÂº∫Â§ßÁöÑÂú®Á∫ø AI ‰∏çÁî®ÔºåËΩ¨ËÄåÂéªÁî®‰∏çËÉΩËÅîÁΩëÊêúÁ¥¢„ÄÅÊõ¥Âº±ÔºåÊõ¥Âç°„ÄÅÊõ¥ËÄóÁîµÁöÑÁ´Ø‰æßÁöÑ AI Âë¢Ôºü
  - ÊàëÊú¨Âú∞ÈÉ®ÁΩ≤‰∫Üchatgml ÁöÑ 6B ÂíåÁßãÂè∂ÁöÑ [SD Êï¥ÂêàÂåÖ] Ôºå‰ΩÜÊòØÁî®Âà∞ÁöÑÁúüÁöÑ‰∏çÂ§öÔºåÂæóÂà∞ÁöÑÁªìÊûú‰πüÊØî‰∏ç‰∫ÜÂú®Á∫øÁöÑ

- ## üÜö [‰∏∫‰ªÄ‰πàÈÉΩÂú®Áî®ollamaËÄålm studioÂç¥Êõ¥Â∞ë‰∫∫‰ΩøÁî®? - Áü•‰πé](https://www.zhihu.com/question/654357364)
- ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºå‰∏çÂêå‰∫éÂÖ®ÈÉ®‰ª£Á†ÅÂú®githubÂºÄÊ∫ê„ÄÅÁîöËá≥ÂèØ‰ª•Ëá™Â∑±Âä®ÊâãÁºñËØëÁöÑollamaÔºålm studioËá≥‰ªä‰ªçÊòØÈó≠Ê∫êÂïÜ‰∏öËΩØ‰ª∂Ôºà‰ªÖ‰∏ÄÈÉ®ÂàÜÈùûÊ†∏ÂøÉ‰ª£Á†ÅÈô§Â§ñÔºâ

- Ollama ‰∫é 5 Êúà‰ªΩÊé®Âá∫ÁöÑÂÖ®Êñ∞Â§öÊ®°ÊÄÅÂºïÊìé„ÄÇÂü∫‰∫éÂ§öÊ®°ÊÄÅÂºïÊìéÔºåOllama ÂèØ‰ª•ÊîØÊåÅËøêË°åËÉΩÂêåÊó∂Â§ÑÁêÜÂõæÂÉè„ÄÅÊñáÊú¨ÁöÑÊ®°Âûã„ÄÇÊñ∞Êû∂ÊûÑ‰∏ç‰ªÖËß£ÂÜ≥ÂΩìÂâçÂ§öÊ®°ÊÄÅÊåëÊàòÔºå‰πü‰∏∫ÈõÜÊàêÊõ¥Â§çÊùÇÁöÑËÉΩÂäõÔºàËØ≠Èü≥„ÄÅÁîüÊàêÁ≠âÔºâÂíå‰ºòÂåñÊÄßËÉΩÔºàÊõ¥Èïø‰∏ä‰∏ãÊñá„ÄÅÊõ¥È´òÂπ∂ÂèëÔºâÊâì‰∏ãÂü∫Á°Ä„ÄÇ

- ÊàëÁöÑ‰Ωì‰ºöÔºö
  - ollamaÁî®Ëµ∑Êù•Âíådocker‰∏ÄÊ†∑ÁöÑÊÑüËßâÔºåpullÊ®°ÂûãÔºårunÊ®°ÂûãÔºålsÁúãÊ®°ÂûãÔºåpsÁúãËøêË°å„ÄÇÈùûÂ∏∏È°∫Êâã‰∏ùÊªëÔºåÂÖ•ÊâãÊó†Èó®Êßõ„ÄÇ
  - llamaÁöÑ‰∏≠ÊñáÔºåÂæÆË∞ÉÂêÑÁßçchatÔºåcodeÔºåÂ§üÁî®„ÄÇËÄå‰∏îÈÉΩÊòØÈáèÂåñÂ•ΩÁöÑÔºåÈöèÊãâÈöèÁî®Ôºå4090Â∞±Ë∑ëÁöÑËµ∑Êù•„ÄÇÂ∞§ÂÖ∂ÊòØÂú®ÂõΩÂÜÖÊãâÊ®°ÂûãÈÄüÂ∫¶ÊûÅÂø´ÔºåÊàëÁöÑÁéØÂ¢ÉÊúÄÈ´òÂèØËææ15m/s
  - ollamaÊòØllama.cppÂÆûÁé∞Ê®°ÂûãÊé®ÁêÜÔºåÊ®°ÂûãÂ∞èÔºåÈÄüÂ∫¶Âø´„ÄÇ
  - ollamaÊèê‰æõ11434Á´ØÂè£ÁöÑwebÊúçÂä°ÔºåÈáçË¶ÅÁöÑÊòØËøòÂÖºÂÆπopenaiÁöÑÁ´ØÁÇπÊé•Âè£ÔºåÂèØ‰ª•ÂíåÂêÑÁßçÂâçÁ´ØÈÖçÂêàÔºåÊØîÂ¶ÇollamaËá™Â∑±open webuiÔºåÂõΩ‰∫ßÁöÑchatboxÔºåËøûÂêéÁ´ØÂ∏¶ÁïåÈù¢Ôºå‰∏ÄÂ•óÊêûÂÆö
  - ollamaÊòØÁ≥ªÁªüÊúçÂä°ÂΩ¢ÂºèÔºà‰πüËÉΩÂÆπÂô®ËøêË°åÔºâÔºåÂâçÂêéÁ´ØÂàÜÁ¶ªÔºà ‰∏•Ê†ºÊù•ËØ¥Ê≤°ÊúâÂâçÁ´ØÔºåÂè™ÊúâÂëΩ‰ª§Ë°åÂÖ•Âè£ÔºâÔºåËÄ¶ÂêàÂ∞èÔºåÊê≠ÈÖçÁÅµÊ¥ª„ÄÇ
- ollamaÁöÑËø≠‰ª£ÂæàÂø´ÔºåÁé∞Âú®Â§öÊ®°ÂûãÂπ∂ÂèëÁöÑÈóÆÈ¢òÂ∑≤ÁªèËß£ÂÜ≥‰∫Ü

- ollamaÁé∞Âú®ÊîØÊåÅÂàÜÂ∏ÉÂºèÊé®ÁêÜÂêó
  - Áé∞Âú®‰∏çÊîØÊåÅÔºå‰ΩÜÊîØÊåÅÂçïÊú∫Â§ögpuËøêË°å

- ÊãâÊ®°ÂûãÈ∫ªÁÉ¶Â∞±Âõ∞Êâ∞ÂæàÂ§ö‰∫∫‰∫ÜÔºåollamaÁÖßÁùÄÊïôÁ®ãÂÅöÔºåÂü∫Êú¨Ê≤°ÊúâÈóÆÈ¢ò

- ollamaÈÖçÂêàopen webui‰∏çÈîôÁöÑ

- ollamaÁúãÁùÄÈÄüÂ∫¶Âø´ÔºåÂÖ∂ÂÆûÂÆÉÊúÄÂ§ßÈóÆÈ¢òÊòØ‰øÆÊîπ‰∏™‰∏ä‰∏ãÊñáÈÉΩÂæàÈ∫ªÁÉ¶
  - ÂèØ‰ª•Áî®chatbox‰πãÁ±ªÁöÑËÅäÂ§©Ê°Ü

- lm studio ‰πüÊèê‰æõopenai-likeÁöÑapiÂêéÁ´ØÔºåÊîØÊåÅÂâçÂêéÁ´ØÂàÜÁ¶ªÔºå‰πüÊîØÊåÅllama.cppÔºàÂú®macËøòÊîØÊåÅmlxÊ°ÜÊû∂Ôºâ. Ê®°ÂûãÂü∫Êú¨‰∏ä‰πüÈÉΩÊîØÊåÅÔºåËÄå‰∏îÈÉΩÊòØ‰ªéhf‰∏ä‰∏ÄÈîÆÂºè‰∏ãËΩΩÈÉ®ÁΩ≤Ôºà‰∏çËøáÁΩëÁªúÁ°ÆÂÆûÊòØÈóÆÈ¢òÔºâ
  - ÊÄªÁöÑËØ¥Ôºålinux‰∏äÈÉ®ÁΩ≤ËøòÊòØÂêàÈÄÇÔºå‰∏çÁî®ollamaÔºåËøòÊúâvllmÔºålmdeloyÔºå‰ª•ÂèäsglangÔºåÈÉΩÊòØÂºÄÊ∫êÁöÑÔºåÂæàÈÄÇÂêàÂºÄÂèë‰∫∫ÂëòÊàñËÄÖÊòØÁªÑÁªáÂÜÖÈÉ®ÈÉ®ÁΩ≤„ÄÇwindows‰∏äÈÉΩÂ∑ÆÁÇπÔºålmstudioÁïåÈù¢ËÄ¶ÂêàÔºåÈó≠Ê∫êÔºåÊãâÊ®°ÂûãËµ∞ÈïúÂÉèÁÇπÔºå‰∏çËÉΩÂàÜÂ∏ÉÂºèÊé®ÁêÜÔºå‰∏™‰∫∫Ëá™Â∑±ÂÆûÈ™åÊÄßË∑ëË∑ëÂ•Ω‰∏Ä‰∫õ„ÄÇ

- LMÂõæÂΩ¢ÂåñÁïåÈù¢‰πüÂèØ‰ª•ÂæÆË∞ÉÂ§ßÈáèÁöÑÊ®°ÂûãËøêË°åÂèÇÊï∞ÔºåollamaËøôÊñπÈù¢Â∞±ÂØπÂ∞èÁôΩ‰∏çÂ§üÂèãÂ•Ω‰∫ÜÔºåÂè™ËÉΩ‰ΩøÁî®ÂëΩ‰ª§Ë°åÂèÇÊï∞
  - LMÂÅöserverÊúçÂä°Âô®Êó∂ÔºåÁïåÈù¢ÊúâÂä®ÊÄÅÂèçÈ¶àÁöÑÊâßË°å‰ø°ÊÅØÂèØ‰ª•‰æõÁî®Êà∑ËßÇÂØüÂà∞ÔºåË∞ÉËØïÈóÆÈ¢òÊñπ‰æø„ÄÇ
  - LMÊúÄÊñ∞ÁöÑÁâàÊú¨Âú®chatÁïåÈù¢‰∏ãÔºåËøòÊîØÊåÅ‰∫Üollama‰∏çÊîØÊåÅÁöÑLaTexÊï∞Â≠¶ÂÖ¨ÂºèË°®ËææÔºåÂíåÊú¨Âú∞RAGÊñáÊ°£‰ø°ÊÅØÊ£ÄÁ¥¢ÔºåËøô‰∫õÈÉΩÊòØollamaËøò‰∏çÊèê‰æõÁöÑÁâπÊÄß„ÄÇ

- ollama Âíå LM studio ÂùáÂèØ‰ª•ÊîØÊåÅ‰∏çÂêåÂ§ßÊ®°ÂûãÁöÑÊú¨Âú∞ÈÉ®ÁΩ≤ÔºåÂπ∂‰∏î‰ºö‰ºòÂÖà‰ΩøÁî® GPU ËøõË°åÊé®ÁêÜ„ÄÇÂ¶ÇÊûúÊ≤°ÊúâÂèëÁé∞ GPUÔºåÂ∞±‰ºö‰ΩøÁî® CPU Êé®ÁêÜÔºåÂõ†Ê≠§‰πü‰ºöÂç†Áî®‰∏ÄÈÉ®ÂàÜÂÜÖÂ≠ò„ÄÇ‰ªéÂÆûÈôÖ‰ΩøÁî®Êù•ÁúãÔºåÁ¨îËÆ∞Êú¨ÂÜÖÂ≠òÂ∫îËØ•Ëá≥Â∞ë‰∏∫ 8GB ÊâçËÉΩÊ≠£Â∏∏ËøêË°å„ÄÇ

- ÂÜôÁ®ãÂ∫èÂíåollama‰∫§‰∫íÁúüÁöÑÊòØÈ°∫ÊªëÔºåÂá†Âè•‰ª£Á†ÅÊ®°ÂûãÂø´ÈÄüÂàáÊç¢„ÄÇ

- ollamaË∑üvllm„ÄÅsglang„ÄÅtrtllmÁõ∏ÊØîÊòØ‰∏™Áé©ÂÖ∑
  - lm studioËøûÁé©ÂÖ∑ÈÉΩ‰∏çÂ¶Ç

- ÊÑüÂèó‰∏äÂéüÂõ†ÊòØdsÊúÄÁÅ´ÁöÑÊó∂ÂÄôÔºåÈì∫Â§©ÁõñÂú∞ÁöÑËá™Â™í‰ΩìÊïôÁ®ãÈÉΩÊòØollamaÔºåÊèêÂà∞lmÁöÑÂ∞ëÁöÑÂ§ö„ÄÇËøô‰∏™‰∏úË•ø‰∏ÄÊó¶ÂΩ¢ÊàêÂÆ£‰º†ÊïàÂ∫î‰∫ÜÔºåÈô§‰∫Ü‰∏ìÈó®ÊêûËøô‰∏™ÁöÑÔºåÂ∞±‰∏ç‰ºöÊúâ‰∫∫Ê∑±ÂÖ•Á†îÁ©∂‰∫Ü„ÄÇ

- lm studioËÉΩÂÉèollamaÈÇ£Ê†∑ÂêåÊó∂‰ΩøÁî®chatÂíåembeddingÂêóÔºülm studioÊØèÊ¨°ÈÉΩË¶ÅÈ¢ÑÂÖàÂä†ËΩΩ„ÄÇ

- ## üçé [Â¶Ç‰ΩïÁúãÂæÖËãπÊûúÂèëÂ∏ÉÁöÑ MLX Êú∫Âô®Â≠¶‰π†Ê°ÜÊû∂Ôºü - Áü•‰πé _202312](https://www.zhihu.com/question/633585779)
- MLXÊòØ‰∏Ä‰∏™Á±ª‰ººNumPyÊï∞ÁªÑÁöÑÊ°ÜÊû∂ÔºåÁõÆÁöÑÊòØÂèØ‰ª•Âú®ËãπÊûúÁöÑËäØÁâá‰∏äÊõ¥Âä†È´òÊïàÂú∞ËøêË°åÂêÑÁßçÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÔºåÂΩìÁÑ∂ÊúÄ‰∏ªË¶ÅÁöÑÁõÆÁöÑÊòØÂ§ßÊ®°Âûã„ÄÇ
  - MLXÁöÑËÆæËÆ°ÂèóÂà∞PyTorch„ÄÅJaxÂíåArrayFileÁöÑÂêØÂèëÔºåÁõÆÁöÑÊòØËÆæËÆ°‰∏Ä‰∏™ÂØπÁî®Êà∑ÊûÅÂÖ∂ÂèãÂ•ΩÔºå‰ΩÜÂêåÊó∂Âú®ËÆ≠ÁªÉÂíåÈÉ®ÁΩ≤‰∏ä‰πüÈùûÂ∏∏È´òÊïàÁöÑÊ°ÜÊû∂„ÄÇ
  - ÊâÄ‰ª•ÔºåÂÆÉÁöÑÊé•Âè£‰Ω†‰ºöÈùûÂ∏∏ÁÜüÊÇâÔºåÂõ†‰∏∫ÂÆÉÁöÑPythonÊé•Âè£‰∏éNumPyÂæàÁõ∏‰ººÔºåËÄåÂÆÉÁöÑÁ•ûÁªèÁΩëÁªúÊ®°ÂûãÁöÑÊé•Âè£ÂíåPyTorchÈùûÂ∏∏Á±ª‰ºº„ÄÇ

- 2023Âπ¥‰∫ÜÔºåÁúüÁöÑËøòÈúÄË¶Å‰∏∫‰∫ÜËá™ÂÆ∂ËäØÁâá‰ªéÈõ∂ÂºÄÂßãÊûÑÂª∫‰∏Ä‰∏™Ê∑±Â∫¶Â≠¶‰π†ËÆ≠ÁªÉÊ°ÜÊû∂Âêó
  - ÊúâÁöÑÔºåÊÑüËßâÊòØ‰∏ìÈó®ÈíàÂØπuma‰ºòÂåñÁöÑ
- ÊØïËÆæÊòØÊ∑±Â∫¶Â≠¶‰π†ÂêéÈó®ÊîªÂáªÁõ∏ÂÖ≥ ËÆæÂ§áÊòØmacmini m4 ‰ªépytorchÊ°ÜÊû∂ÂàáÊç¢Âà∞mlxÊ°ÜÊû∂ ÂêåÊ†∑ÁöÑÂÆûÈ™åÂêåÊ†∑ÁöÑËÆ≠ÁªÉÈáè mlxÈÄüÂ∫¶Âø´‰∫ÜÂ•ΩÂá†ÂÄç ÂÆûÊâìÂÆûÁöÑÊèêÂçá
  - ‰ΩÜÊòØÈúÄË¶ÅÈáçÊûÑËÆ≠ÁªÉ‰ª£Á†ÅÔºåÊàë‰∏ÄÂ•ópytorch‰ª£Á†ÅÔºåÂè™ÈúÄÂÅöÂ∞ëÈáèÈÄÇÈÖçÔºåÂèØ‰ª•Ëá™Âä®Âú®ÊòáËÖæ„ÄÅnVidia„ÄÅmps‰πãÈó¥ÂàáÊç¢„ÄÇËÄå‰∏îÔºåmlxÁ°ÆÂÆûÊØîmpsÊèêÂçáÂ•ΩÂá†ÂÄçÔºå‰ΩÜÊòØÊàëÂ¶ÇÊûúÁî®nvidiaÊàñËÄÖÊòáËÖæ910BÔºåmlxÊÄßËÉΩËøòÊúâÂ∑ÆË∑ù„ÄÇ
- Âú® MacBook Air m3 ‰∏äÔºåmlx ÁöÑÁü©Èòµ‰πòÊ≥ï„ÄÅSVDÂàÜËß£ËøêË°åÈÄüÂ∫¶ÊòéÊòæÂø´‰∫é PyTorch
  - Â¶ÇÊûú‰∏ç‰ΩøÁî® mlx Êù•ËøõË°åÊ∑±Â∫¶Â≠¶‰π†Ôºå‰ªÖÁî®‰∫éÂ∏∏ËßÑÁßëÁ†îËÆ°ÁÆóÊàñ‰ºòÂåñ‰ªªÂä°ÔºåÂÆûÈôÖ‰∏äËøòÊòØËõÆÈÄÇÂêàÁöÑ„ÄÇ
  - ÁÑ∂ËÄåÔºåmlx ÁõÆÂâçÁîöËá≥‰∏çÊîØÊåÅ float64ÔºåÂºÄÂèëÂõ¢ÈòüÂΩìÂâçËÄÉËôëÁöÑÊòØÂÆÉ‰ª¨ GPU ÂèçÊ≠£Áî®‰∏çÂà∞ÔºåËøôÂØπ‰∏Ä‰∫õÈ´òÁ≤æÂ∫¶ËÆ°ÁÆóÈúÄÊ±ÇÂèàÊòØ‰∏ÄÁßçÈôêÂà∂ÔºåÊõ¥‰∏çÁî®ËØ¥‰∏Ä‰∫õÊú™ÂÆûÁé∞ÁöÑÁÆóÊ≥ïÊàñËÄÖÊï∞ÊçÆÁ±ªÂûãËøòÈúÄË¶Å‰ªéÈõ∂ÂÜô‰∫Ü„ÄÇ

- Ë¶ÅÊòØkerasÂêéÁª≠ËÉΩÂ§üÊîØÊåÅmlxÂêéÁ´ØÂ∞±Â•Ω‰∫ÜÔºåËøôÊ†∑ËãπÊûúËÆæÂ§á‰∏äÂÜôÁöÑ‰ª£Á†Å‰∏çËá≥‰∫éÂà∞Âà´ÁöÑÂπ≥Âè∞‰∏äË∑ë‰∏ç‰∫ÜËøòË¶ÅÈáçÊûÑ‰∫Ü„ÄÇ

- [ËãπÊûúÂêëËã±‰ºüËææÁîüÊÄÅÂ¶•Âçè‰∫ÜÔºÅMLXÊ°ÜÊû∂‰∏ªÂä®ÈÄÇÈÖçCUDA - Áü•‰πé _202507](https://zhuanlan.zhihu.com/p/1929178251202376826)
  - ËãπÊûú‰∏ÄÁõ¥‰ª•Êù•ÈÉΩ‰ª•‚ÄúÂ∞ÅÈó≠‚ÄùËëóÁß∞Ôºå‰ΩÜÈöèÁùÄËã±‰ºüËææCUDAÁîüÊÄÅÂú®AIÂºÄÂèëÈ¢ÜÂüüÂç†ÊçÆÁªùÂØπ‰∏ªÂØºÂú∞‰ΩçÔºåËãπÊûúËøô‰∏ã‰πü‰∏çÂæó‰∏çËΩ¨ÂèòÂßøÊÄÅ‰∫Ü„ÄÇ
  - ËøáËÆ©MLXÊ°ÜÊû∂‰∏ªÂä®ÈÄÇÈÖçCUDAÔºå‰ªäÂêéËãπÊûúÂºÄÂèëËÄÖ‰πüËÉΩÂà©Áî®Ëã±‰ºüËææGPUËÆ≠ÁªÉÊ®°Âûã„ÄÇÂÖ∂Êú¨Ë¥®ÊòØÂ¢ûÂä†‰∫ÜÂØπCUDAÁöÑÂêéÁ´ØÊîØÊåÅÔºåÊñπ‰æøÂºÄÂèëËÄÖÂú®Windows/LinuxÁöÑËã±‰ºüËææÊòæÂç°‰∏äËÆ≠ÁªÉÊ®°ÂûãÔºåÁÑ∂ÂêéÂÜçÈÉ®ÁΩ≤ÂõûMac„ÄÅiPhoneÁ≠âËÆæÂ§á„ÄÇ
  - ÊòØÁªôWin-vidia/Lin-vidiaÂÅöMLXÊîØÊåÅÔºå‰∏çÊòØÁªôMacÁâàMLXÂÅöCUDAÈ©±Âä®

- Ëøô‰∏§Â§©Â∞ùËØïÂéªÂ≠¶‰π†‰∫Ü‰∏Ä‰∏ã mlx Âíå mlx ÁöÑÊ∫êÁ†ÅÔºåÁ≤óÁï•ÁöÑÁúãÊ≥ïÊúâ 2 ÁÇπÔºö
  - ÂÆèËßÇ‰∏äÔºåÊ°ÜÊû∂ÁöÑËÆæËÆ°ÂæàÊ£íÔºåÊï¥‰ΩìËÆæËÆ°‰∏äÂæàÂ•ΩÂú∞Âê∏Âèñ‰∫ÜÂâç‰∫∫ÁöÑÁªèÈ™åÔºåÂú®‰øùËØÅÊòìÁî®ÊÄßÁöÑÂâçÊèê‰∏ãÔºå‰øùÁïô‰∫ÜË∂≥Â§üÈ´òÁöÑ‰ºòÂåñ‰∏äÈôêÔºõ
  - ÂæÆËßÇ‰∏äÔºåÈ°πÁõÆÁöÑ C++ Ê∞¥Âπ≥ÊØîËæÉ‰∏ÄËà¨ÔºåÊØîËæÉÂÉèÊòØÂá†‰∏™Â∑•Á®ãÂ∏àÁöÑ side projectÔºåÂ¶ÇÊûúË¶ÅÂèòÊàêÈïøÈùíÊ†ëÔºåÂÜÖÈÉ®Â§ßÈáèÁöÑÂÆûÁé∞ÈÉΩÈúÄË¶ÅÈáçÊûÑ
- ÂÖ≥‰∫éÊû∂ÊûÑ„ÄÇÊàëËßâÂæó‰πãÂâçÂá†Âπ¥ÔºåËÆ≠ÁªÉÊ°ÜÊû∂‰∏äÁöÑÂÆûË∑µÊÄªÁªì‰∫ÜËøôÊ†∑ÁöÑ‰∏Ä‰∫õÁªèÈ™åÔºö
  - pytorch Âíå tensorflow ÁöÑ‰∫âÊñóÁªìÊûúÂëäËØâÊàë‰ª¨ÔºåËÆ©Â§ßÂÆ∂ÂÜô pythonÔºåËÄå‰∏çÊòØÂú® python ÈáåÂÜô DSLÔºåÊèêÂçáÊòìÁî®ÊÄßÊòØÂæàÈáçË¶ÅÁöÑÔºõ
  - torchscript„ÄÅlibtorch ÁöÑÂêÑÁßçÂùëÂëäËØâÊàë‰ª¨ÔºåÂÆåÂÖ®‰æùËµñÂú® python ‰∏äÔºåÂõæ‰ºòÂåñÂæàÊ£òÊâãÔºåÊâîÊâãÊú∫ËøôÊ†∑Ê≤°Êúâ python runtime ÁöÑÂú∞Êñπ‰πüÊòØÁúüÁöÑÂ§¥Áñº„ÄÇÊúÄÂ•Ω‰∏çË¶ÅÁ∫Ø eager Ê®°ÂºèÔºåËÄåÊòØËÉΩËÆ©Ê°ÜÊû∂ÊèêÂèñÂá∫Êù•‰∏Ä‰∫õÂ≠êÂõæ„ÄÇÊàñËÄÖÊòØÊúâ‰∏™Âõ∫ÂÆöÁÇπÁöÑ c apiÔºåÁªô‰∏çËÆ≠Ê®°ÂûãÁöÑÂ∑•Á®ãÂ∏à‰ª¨ÁïôÊù°Ê¥ªË∑ØÔºõ
  - transformer ÂëäËØâÊàë‰ª¨Ôºå‰πüËÆ∏‰ªéÂ≠¶ÊúØËßíÂ∫¶Êù•ËØ¥ÔºåÊéßÂà∂ÊµÅ‰ºòÂåñËµ∑Êù•ÂæàÊÄßÊÑüÔºå‰ΩÜÊòØÂÆûÈôÖÂ∫îÁî®‰∏≠ÔºåÂÖ®ÊòØ‰∏ÄÊù°Ë∑ØËµ∞Âà∞ÈªëÔºõ
- ‰πãÂâçÊàëËßâÂæó jax Â∞±ÊòØËÆ≠ÁªÉÊ°ÜÊû∂‰∏äÁöÑÈõÜÂ§ßÊàêËÄÖÔºåÂêÑÁßçËÆæËÆ°ÁúüÁöÑÂæàÊºÇ‰∫Æ„ÄÇÂèØÊòØÂÆÉÊãñ‰∫Ü‰∏™ÈáçÈáçÁöÑ XLA ÁöÑÂ∞æÂ∑¥ÔºåÂèØËÉΩÊòØ‰∏∫‰∫ÜË∑ëÂú® TPU ‰∏äÔºå‰πüÂèØËÉΩÊòØ jax ÁöÑÊàêÂëò‰∏≠ÊúâÂæàÂ§ö tf ÈáåÂÅöÁºñËØë‰ºòÂåñÁöÑ‰∫∫Âêß„ÄÇËøô‰ΩøÂæóÊó©ÊúüÁöÑ jax ÂØπÁî® nv ÊòæÂç°ÁöÑÁî®Êà∑Âæà‰∏çÂèãÂ•ΩÔºåÁ≠â pytorch/huggingface Êàê‰∫ÜÊ∞îÂÄôÔºå‰πüÂ∞±Êó†ÂäõÂõûÂ§©‰∫Ü„ÄÇ
- Áé∞Âú® mlx Âü∫Êú¨Âê∏Êî∂‰∫Ü jax ÁöÑÁªèÈ™åÔºå
  - ÈááÁî®‰∫Ü numpy api + lazy evaluationÔºõ
  - Êúâ‰∏Ä‰∏™ÊØîËæÉÂπ≤ÂáÄÁöÑ c apiÔºåÁîöËá≥Â∑≤ÁªèÊúâ‰∫Ü swift bindingÔºõ
  - ÂÜÖÈÉ®ÁïôÂá∫‰∫ÜÁÆÄÂçïÁöÑÂõæ‰ºòÂåñÊé•Âè£ mlx.core.compile Ôºå‰øùËØÅ‰∫Ü‰ºòÂåñÁöÑ‰∏äÈôê„ÄÇ

- Áúã‰∫Ü‰∏ÄÁúºÊ°ÜÊû∂Ê∫êÁ†ÅÔºåÂÆåÂÖ®ÊòØÈáçÊñ∞Êêû‰∫Ü‰∏ÄÂ•óÊ°ÜÊû∂ÔºåAPIÂ∞ΩÈáèÂØπÊ†áPyTorchÔºåÁõÆÂâç‰πüÂè™ÊîØÊåÅÂ∞ëÈáèAPI„ÄÇËøôÊ†∑Ê°ÜÊû∂ÈÄÇÈÖçÊàêÊú¨ÊÑüËßâÊúâÁÇπÂ§ßÔºåËøûÂâçÁ´ØPython APIÈÉΩË¶ÅËá™Â∑±ÊûÑÂª∫ÔºåÂ§çÂàªÂ§ßÈáè‰ª£Á†ÅÈÄªËæë‰∏î‰∏çËØ¥ÔºåPyTorchÁ§æÂå∫ÁöÑÂÖ®ÈáèfeatureÁöÑË∑üË∏™ÊºîËøõÂ∞±‰ºöÊòØ‰∏Ä‰∏™ÈöæÈ¢ò„ÄÇ
  - ÊÑüËßâAppleÂíåÊòáËÖæËµ∞Âêë‰∫Ü‰∏§‰∏™ÊñπÂêëÁöÑÊûÅÁ´ØÔºåÊòáËÖæÊòØÂÆåÂÖ®‰æùËµñÂéüÁîüÁöÑÊèí‰ª∂ÂåñÔºå‰ºöÊÉ≥Â∞Ω‰∏ÄÂàáÂäûÊ≥ïÂ§çÁî®ÂéüÁîüÈÄªËæëÔºåÂÆûÂú®‰∏çË°åÂ∞±Â§çÂàªÔºõAppleÂàôÊòØÂÆåÂÖ®ÊäõÂºÉÂéüÁîüÈÄªËæëÔºåÁ´ØÂà∞Á´ØÈáçÊñ∞Êêû‰∏ÄÂ•ó„ÄÇ

- Áúã‰∫Ü‰∏ã‰ª£Á†ÅÔºåËãπÊûúÊòØÈáçÊñ∞ÈÄ†‰∫Ü‰∏™ËΩÆÂ≠êÔºåÂâçÁ´ØÂπ≤ÁöÑ‰∫ãÂíåPyTorchÂíåJaxÊ≤°Âï•Âå∫Âà´ÔºåÊÑüËßâÈáçÁÇπËøòÊòØÂú®Apple SiliconÁöÑÂêéÁ´Ø‰∏ä„ÄÇÁõÆÂâçÁöÑÂÆûÁé∞ËøòÊòØÊå∫Â∞èÂ∑ßÁöÑÔºåÁî®Êù•Â≠¶‰π†ÁöÑËØùËøò‰∏çÈîôÔºå‰ΩÜË¶ÅÂú®feature‰∏äÂØπÈΩêÁé∞ÊúâÁöÑÊ°ÜÊû∂ËøòÊúâ‰∏çÂ∞ëÁöÑÊ¥ªË¶ÅÂπ≤„ÄÇ

- ‰∏™‰∫∫ÊÑüËßâËøôÁé©ÊÑèÊúâÁÇπÈ∏°ËÇã„ÄÇ
  - macÂÜô‰ª£Á†Å„ÄÅË∞ÉËØïÔºåÁÑ∂ÂêéÊúçÂä°Âô®ÂÆåÊàêÊï¥‰∏™ËÆ≠ÁªÉ„ÄÇËøôÂ∞±ÊúâÈóÆÈ¢ò‰∫ÜÔºåÊúçÂä°Âô®‰∏çÊîØÊåÅMLXÊ°ÜÊû∂„ÄÇ
  - macÂÜô‰ª£Á†Å„ÄÅË∞ÉËØïÔºåÁÑ∂ÂêémacÂÆåÊàêÊï¥‰∏™ËÆ≠ÁªÉ„ÄÇ Ê≠£Â∏∏ÂÖ¨Âè∏Ë∑ëÊ®°ÂûãÔºåÂ¶ÇÊûúÂ∞ëÈáèÊï∞ÊçÆ‰ΩøÁî®Âá†Âº†Ê∑òÊ±∞‰∏ãÊù•ÁöÑ1080Â∞±ÂèØ‰ª•Ôºå‰ª∑Ê†ºËøò‰æøÂÆú„ÄÇÊï∞ÊçÆÈáèÂ§ßÔºåË¶Å‰ΩøÁî®ÂàÜÂ∏ÉÂºèÂπ∂Ë°åËÆ≠ÁªÉÔºåÈÇ£Â∞±Ê≤°mac‰ªÄ‰πà‰∫ãÊÉÖ‰∫Ü„ÄÇ
  - ÊúÄËøëÁÅ´ÁÉ≠ÁöÑÂ§ßÊ®°ÂûãËÆ≠ÁªÉÔºåÂì™ÂÆ∂ÂÖ¨Âè∏‰ºöÁî®macÊê≠Âª∫ÈõÜÁæ§ËÆ≠ÁªÉÂ§ßÊ®°ÂûãÔºåÂπ∂‰∏îËÆ≠ÁªÉÂÆåÊàêÂêé‰ΩøÁî®macÂÅöÊé®ÁêÜ„ÄÇÁî®‰∏çÂà∞macÂ∞±Ê≤°MLX‰ªÄ‰πà‰∫ãÊÉÖ‰∫Ü„ÄÇ

- ÊàëËßâÂæóÂèØ‰ª•ÂèÇËÄÉÊó©ÊúüswiftÔºå‰∏ÄÂπ¥‰∏Ä‰∏™ÁâàÊú¨ÔºåËÆ©‰Ω†ÊÑüÂà∞Â¥©Ê∫ÉÔºåËôΩÁÑ∂ËøôÊ¨°ÂºÄÊ∫êÔºå‰ΩÜÊòØ‰∏ªÂäõÂ∫îËØ•ËøòÊòØËãπÊûúËá™Â∑±

- [ÈòøÈáåÂ∑¥Â∑¥ÂèëÂ∏É‰∫Ü‰∏éËãπÊûú MLX Êû∂ÊûÑÂÖºÂÆπÁöÑ Qwen3 ÂçáÁ∫ßÁâàÔºåÊñ∞ÁâàÊú¨ÈÉΩÊúâÂì™‰∫õÊñ∞ÁâπÊÄßÔºü - Áü•‰πé _202506](https://www.zhihu.com/question/1918255662552582106)
  - MLXÊòØËãπÊûú‰∏ì‰∏∫Apple SiliconËäØÁâáÔºàMÁ≥ªÂàóÔºâËÆæËÆ°ÁöÑÂºÄÊ∫êÊú∫Âô®Â≠¶‰π†Ê°ÜÊû∂ÔºåÂÆÉÁöÑÊ†∏ÂøÉ‰ºòÂäøÂú®‰∫éÊ∑±Â∫¶Êï¥ÂêàËãπÊûúÁ°¨‰ª∂ÁâπÊÄßÔºåÂ∞§ÂÖ∂‰ª•‚ÄúÁªü‰∏ÄÂÜÖÂ≠òÊ®°Âûã‚Äù‰∏∫Ê†∏ÂøÉÂàõÊñ∞
  - Áªü‰∏ÄÂÜÖÂ≠òÊû∂ÊûÑÔºöÊï∞ÊçÆÂ≠òÂÇ®Âú®CPU‰∏éGPUÂÖ±‰∫´ÁöÑÂÜÖÂ≠òÁ©∫Èó¥‰∏≠ÔºåË∑®ËÆæÂ§áËÆ°ÁÆóÊó∂Êó†ÈúÄÊòæÂºèÊã∑Ë¥ùÊï∞ÊçÆÔºåÁõ∏ÊØî‰º†ÁªüÊ°ÜÊû∂ÔºàÂ¶ÇPyTorchÔºâÂáèÂ∞ë90%‰ª•‰∏äÁöÑÂÜÖÂ≠ò‰º†ËæìÂºÄÈîÄ„ÄÇ

- ## [Â¶Ç‰ΩïÁúãÂæÖ Google ÊúÄÊñ∞ÂºÄÊ∫êÁöÑ Gemma-3 Á≥ªÂàóÂ§ßÊ®°ÂûãÔºü - Áü•‰πé _202503](https://www.zhihu.com/question/14777841836)

- Gemma-3‰∏ªÊâìÁöÑÂçñÁÇπÊòØÂçïÂç°ÂèØË∑ë+Â§öÊ®°ÊÄÅ„ÄÇ
- Gemma-3ÁöÑÊ†∏ÂøÉÁ´û‰∫âÂäõÂú®‰∫éÊïàÁéáÈù©ÂëΩ„ÄÇÂÖ∂27BÁâàÊú¨‰ªÖÈúÄÂçïÂº†H100ÊòæÂç°Âç≥ÂèØËøêË°åÔºåËÄåÁ´ûÂìÅËææÂà∞ÂêåÁ≠âÊÄßËÉΩÂæÄÂæÄÈúÄË¶Å10ÂÄçÁÆóÂäõ„ÄÇËøôÁßçÊïàÁéáÊèêÂçáÂπ∂ÈùûÁÆÄÂçïÁöÑÂèÇÊï∞ÂéãÁº©ÔºåËÄåÊòØÊû∂ÊûÑÂ±ÇÈù¢ÁöÑÂàõÊñ∞
  - Áü•ËØÜËí∏È¶èÊäÄÊúØÔºöÂü∫‰∫éGemini 2.0ÁöÑ"ÊïôÂ∏àÊ®°Âûã"ËøõË°åËí∏È¶èÔºåÂ∞ÜÂ§ßÊ®°ÂûãËÉΩÂäõËøÅÁßªÂà∞Â∞èÊ®°ÂûãÔºåÂÆûÁé∞"Áî®1/10ÂèÇÊï∞ËææÂà∞80%ÊÄßËÉΩ"ÁöÑÊïàÊûú„ÄÇ
- Â∞ΩÁÆ°Gemma-3Ë°®Áé∞‰∫ÆÁúºÔºå‰ªçÈúÄÊ≥®ÊÑè‰ª•‰∏ãÈóÆÈ¢òÔºö 
  - 1. ÈïøÂ∞æ‰ªªÂä°Áº∫Èô∑ÔºöÂú®Êï∞Â≠¶Êé®ÁêÜÂíå‰º¶ÁêÜÂà§Êñ≠Á≠âÁâπÂÆöÂú∫ÊôØÔºåÂÆûÊµãÁªìÊûúÊòæÁ§∫ÂÖ∂Ë°®Áé∞‰∏çÂ¶ÇQwen2.5-Max„ÄÇ 
  - 2. Â§öÊ®°ÊÄÅÁ≤æÂ∫¶Áì∂È¢àÔºöÂ§ÑÁêÜÂ§çÊùÇÂõæÂÉèÊó∂ÔºåSigLIPÁºñÁ†ÅÂô®ÁöÑÁªÜËäÇËØÜÂà´ËÉΩÂäõ‰ªçËêΩÂêé‰∫é‰∏ì‰∏öCVÊ®°Âûã„ÄÇ 
  - 3. ËÆ≠ÁªÉÊï∞ÊçÆ‰æùËµñÔºöËôΩÁÑ∂ÊîØÊåÅ140ÁßçËØ≠Ë®ÄÔºå‰ΩÜÂÆûÈôÖË°®Áé∞Â≠òÂú®"Ëã±ËØ≠ÊúÄ‰ºòÔºåÂ∞èËØ≠ÁßçÊ¨°‰πã"ÁöÑÈóÆÈ¢ò„ÄÇ 
- Êú™Êù•ÂèØËÉΩÁöÑÁ™ÅÁ†¥ÊñπÂêëÂåÖÊã¨Ôºö 
  - Âä®ÊÄÅÊû∂ÊûÑÔºöÊ†πÊçÆËæìÂÖ•ÂÜÖÂÆπËá™Âä®ÂàáÊç¢ËÆ°ÁÆóËµÑÊ∫êÂàÜÈÖçÔºå‰æãÂ¶ÇËßÜÈ¢ëÂ§ÑÁêÜÊó∂‰∏¥Êó∂Ë∞ÉÁî®Êõ¥Â§öGPUÊ†∏ÂøÉ„ÄÇ 
  - Ë∑®Ê®°ÊÄÅÁîüÊàêÔºö‰ªé"ÁêÜËß£Â§öÊ®°ÊÄÅ"ÂçáÁ∫ßÂà∞"ÁîüÊàêÂ§öÊ®°ÊÄÅ"Ôºå‰æãÂ¶ÇÊ†πÊçÆÊñáÊú¨ÊèèËø∞Áõ¥Êé•ÁîüÊàê3DÊ®°Âûã„ÄÇ 
  - ËæπÁºòÁ´Ø‰ºòÂåñÔºöËøõ‰∏ÄÊ≠•ÂéãÁº©Ê®°Âûã‰ΩìÁßØÔºåÂú®ÊâãÊú∫Á´ØÂÆûÁé∞ÂÆûÊó∂Â§öÊ®°ÊÄÅ‰∫§‰∫í„ÄÇ

- ÂºÄÊ∫êGemmaÁ≥ªÊ®°ÂûãÊ≠£ÂºèËø≠‰ª£Âà∞Á¨¨‰∏â‰ª£ÔºåÂéüÁîüÊîØÊåÅÂ§öÊ®°ÊÄÅÔºå128k‰∏ä‰∏ãÊñá„ÄÇ
  - Gemma 3‰∏ÄÂÖ±ÂºÄÊ∫ê‰∫ÜÂõõÁßçÂèÇÊï∞Ôºå1B„ÄÅ4B„ÄÅ12BÂíå27B„ÄÇÊúÄÊúÄÊúÄÂÖ≥ÈîÆÁöÑÊòØÔºå‰∏ÄÂùóGPU/TPUÂ∞±ËÉΩË∑ëÊ®°Âûã„ÄÇ
  - Âú®LMArenaÁ´ûÊäÄÂú∫‰∏≠ÔºåGemma 3Êãø‰∏ã‰∫Ü1339 ELOÈ´òÂàÜÔºå‰ªÖ‰ª•27BÂèÇÊï∞ÂáªË¥•‰∫Üo1-preview„ÄÅo3-mini high„ÄÅDeepSeek V3ÔºåÂ†™Áß∞‰ªÖÊ¨°‰∫éDeepSeek R1ÊúÄ‰ºòÂºÄÊ∫êÊ®°Âûã„ÄÇ
  - Gemma3Á≥ª1B„ÄÅ4B„ÄÅ12B„ÄÅ27BÂàÜÂà´Âü∫‰∫é2T„ÄÅ4T„ÄÅ12T„ÄÅ14T tokenÊï∞ÊçÆÂÆåÊàêËÆ≠ÁªÉ„ÄÇ
  - ‰∏éÈó≠Ê∫êGemini 1.5Âíå2.0Áõ∏ÊØîÔºåGemma 3-27BÂü∫Êú¨‰∏äÁï•ÈÄäËâ≤‰∫éFlashÁâàÊú¨„ÄÇ
- ÈááÁî®‰∏éGemini 2.0Ê®°ÂûãÁõ∏ÂêåÁöÑÁ†îÁ©∂ÂíåÊäÄÊúØÊâìÈÄ†„ÄÇ

- ÊúÄÂ§ßÁöÑ17GBÊòØÁ®≥Á®≥ÂèØ‰ª•Ë∑ëÂú®Êã•Êúâ24GBÊòæÂ≠òÁöÑNvidia90Á≥ªÊòæÂç°ÁöÑ„ÄÇ

- Gemma 3n ÊòØ‰∏ÄÊ¨æ‰∏ì‰∏∫ÁßªÂä®ËÆæÂ§áÈáèË∫´ÊâìÈÄ†ÁöÑÂ§öÊ®°ÊÄÅ AI Êû∂ÊûÑÔºåÂÆÉ‰∏ç‰ªÖÊîØÊåÅÂõæÂÉè„ÄÅÈü≥È¢ë„ÄÅËßÜÈ¢ëÂíåÊñáÊú¨ÁöÑËæìÂÖ•ËæìÂá∫ÔºåËøòÈÄöËøáÂàõÊñ∞ÁöÑ MatFormer Êû∂ÊûÑÂíå Per-Layer Embeddings ÊäÄÊúØÔºåÂÆûÁé∞‰∫ÜÈ´òÊÄßËÉΩ‰∏é‰ΩéÂÜÖÂ≠òÂç†Áî®ÁöÑÂÆåÁæéÂπ≥Ë°°„ÄÇ
  - E2B Âíå E4B ‰∏§ÁßçÊ®°ÂûãÁâàÊú¨ÔºåÂàÜÂà´‰ªÖÈúÄ 2GB Âíå 3GB ÂÜÖÂ≠òÂç≥ÂèØËøêË°åÔºåËÄå‰∏îËøòËÉΩÈÄöËøá Mix-n-Match ÊñπÊ≥ïÁÅµÊ¥ªË∞ÉÊï¥Ê®°ÂûãÂ§ßÂ∞èÔºåÊª°Ë∂≥‰∏çÂêåÂú∫ÊôØÁöÑÈúÄÊ±Ç„ÄÇ

- Gemma 3n Êèê‰æõ‰∫ÜÂ§öÁßçÈÉ®ÁΩ≤ÈÄâÈ°πÔºåÂåÖÊã¨ Google GenAI API„ÄÅVertex AI„ÄÅSGLang„ÄÅvLLM Âíå NVIDIA API Catalog Á≠â„ÄÇ
  - Ê≠§Â§ñÔºåGemma 3n Ëøò‰∏é Hugging Face„ÄÅGoogle AI Edge„ÄÅOllama„ÄÅMLX Á≠âÂ§öÁßçÂ∑•ÂÖ∑ÂíåÂπ≥Âè∞Êó†ÁºùÈõÜÊàêÔºåËÆ©ÂºÄÂèëËÄÖÂèØ‰ª•Âø´ÈÄü‰∏äÊâãÔºåËΩªÊùæÈõÜÊàêÂà∞Áé∞ÊúâÈ°πÁõÆ‰∏≠„ÄÇ

- ### [Difference between Gemma and Gemini - Marvik _202407](https://blog.marvik.ai/2024/07/03/difference-between-gemma-and-gemini/)
- While Gemma is open source and lightweight, Gemini is proprietary and heavyweight. 

- ## [Â¶ÇÊûúÊú¨Âú∞Ë¶ÅË£ÖÂ§ßÊ®°ÂûãÔºåÂª∫ËÆÆÂì™‰∏™ÂºÄÊ∫êÂ§ßÊ®°ÂûãÔºü - Áü•‰πé](https://www.zhihu.com/question/653255605)
- ollamaÂéüÂßãÁöÑÁâàÊú¨ÊòØÂπ∂‰∏çÂÖºÂÆπ OpenAI Ê†ºÂºèÁöÑËØ∑Ê±ÇÔºåÂõ†Ê≠§Âú®‰ΩøÁî®ÊØîÂ¶Ç langchain„ÄÅllamaindex Á≠âÊ°ÜÊû∂ÁöÑÊó∂ÂÄôÔºå‰ºöÈÅáÂà∞‰∏Ä‰∫õÈ∫ªÁÉ¶ÁöÑÂú∞ÊñπÔºåÈúÄË¶ÅÂÜçËΩ¨Âåñ‰∏ã„ÄÇ
  - Áé∞Âú®ÔºåÊõ¥Êñ∞Âà∞Êñ∞ÁâàÊú¨ÁöÑÊó∂ÂÄôÔºåÂ∞±Êèê‰æõ‰∫Ü OpenAI Ê†ºÂºèÁöÑÊé•Âè£Ë∞ÉÁî®
  - ÁõÆÂâçËøô‰∏™ OpenAI-compatible API ËøòÂ§Ñ‰∫éÊµãËØïÈò∂ÊÆµÔºåÂπ∂‰∏çÊîØÊåÅ function calling ÁöÑÂäüËÉΩ„ÄÇ

- ÊàëÊú¨Âú∞ollamaË∑ë‰∏ÄÂº†P40/24GÊòæÂ≠òÔºåÁé©‰∫Ü‰∏ÄÂπ¥Â§ö‰∫Ü„ÄÇÊàë‰∏ÄËà¨Ë∑ëÈáèÂåñËøáÁöÑ32BÁöÑÊ®°ÂûãÔºåÂÜçÂæÄ‰∏äÈÄüÂ∫¶Â§™ÊÖ¢ÊàñËÄÖÈáèÂåñÂ§™Áã†ÔºåÈÉΩÊ≤°Â§™Â§ßÂÆûÁî®‰ª∑ÂÄº„ÄÇ

- Áúã‰Ω†ÊòæÂ≠òÂ§ßÂ∞èÈÄÇÂêàÂ§öÂ∞ëBÁöÑÊ®°Âûã„ÄÇËØÑ‰º∞Êó∂ÂÄôË¶ÅËÄÉËôëÈáèÂåñint4„ÄÅint8ËøòÊòØfloat16„ÄÇÂèØ‰ª•Âà©Áî®huggingface‰∏äÁöÑËÆ°ÁÆóÂô®ËØÑ‰º∞ https://hf-accelerate-model-memory-usage.hf.space
  - Èô§‰∫ÜollamaÂÆòÊñπÂàóË°® ‰πüÂèØ‰ª•ÂéªÊâæhugging face‰∏äÈáèÂåñÂ•ΩÁöÑÊ®°Âûã„ÄÇ
- ollamaÊòØÂü∫‰∫éllama.cpp, ÊâÄ‰ª•‰πüÂèØ‰ª•Âü∫‰∫éllama.cpp Ëá™Â∑±ÊäòËÖæ‰∏ÄÈÅçÔºå‰ªéËΩ¨Êç¢Ê®°Âûã‰∏∫ggufÂà∞ÈáèÂåñ‰∏∫int4 ËøòÊòØint8ÔºåÂèØ‰ª•ÊåâÁÖßÊàë‰πãÂâçÂÜôÁöÑÊïôÁ®ãËá™Â∑±Ëµ∞‰∏ÄÈÅçÔºåÂèØÁé©ÊÄßÊõ¥Âº∫‰∏ÄÁÇπ„ÄÇÊàëÁîµËÑëmac m1 Ë∑ëÈÄö‰πâÂçÉÈóÆÁöÑ7bÁöÑint4ÈáèÂåñÊ®°ÂûãÔºåÂ§ßÁ∫¶ÊØèÁßí12‰∏™token„ÄÇ

- Èùû‰∏ì‰∏ö‰∫∫Â£´ÁöÑËØùÔºåollamaÂíålocalaiÔºåËøô‰∏§Êó†ËÑëÈÉ®ÁΩ≤ËøêË°å„ÄÇÂêåÊó∂Ëøô‰∏§‰∏çÂ±ÄÈôê‰Ω†ÊúâÊ≤°ÊúâGPUÔºåcpu‰πüÂèØ‰ª•Êêû„ÄÇ

- Â§™Â∞èÁöÑÊ®°ÂûãÔºåÊÉ≥1.5bÂ§ßÂ∞èÁöÑÔºåÊÄßËÉΩËÇØÂÆöÂ∑Æ‰∏Ä‰∫õÔºå‰∏çËøáÁªìÂêàragËøô‰∫õÔºåÂÆûÁé∞Êú¨Âú∞Áü•ËØÜÂ∫ì‰πüÂ∑Æ‰∏çÂ§ö‰∫Ü„ÄÇÂÅöÂÆûÈ™åÔºåÊ≠£Â∏∏ËøòÊòØÁî®7bÂ∑¶Âè≥ÁöÑ„ÄÇ

- Ëá™Â∑±Êú¨Âú∞Êê≠Âª∫ÁöÑÊï∞ÊçÆÈáèÂ§™Â∞ë‰∫ÜÔºåËÅîÊÉ≥ÂÖ≥ÈîÆËØçÂõ∞Èöæ

- ## [ÁõÆÂâçÊúâ‰ªÄ‰πàÂèØ‰ª•Êú¨Âú∞ÈÉ®ÁΩ≤ÁöÑÂ§ßÊ®°ÂûãÊé®Ëçê? - Áü•‰πé](https://www.zhihu.com/question/648879790)
- ÂºÄÊ∫êÂ§ßÊ®°ÂûãÊõ¥Êñ∞Ëø≠‰ª£Â§™Âø´Ôºå‰ªäÂπ¥ÂàöÊé®Âá∫ÁöÑÊ®°ÂûãÂèØËÉΩËøáÂá†‰∏™ÊúàÂ∞±ËøáÊó∂‰∫Ü„ÄÇ
- ‰∏ÄÊòØÂ¶Ç‰ΩïÊâæÂà∞ÊúÄÊñ∞ÁöÑÂ§ßÊ®°ÂûãÔºå
  - huggingfaceÂèØ‰ª•ÁêÜËß£‰∏∫ÂØπ‰∫éAIÂºÄÂèëËÄÖÁöÑGitHubÔºåÊèê‰æõ‰∫ÜÊ®°Âûã„ÄÅÊï∞ÊçÆÈõÜÔºàÊñáÊú¨|ÂõæÂÉè|Èü≥È¢ë|ËßÜÈ¢ëÔºâ„ÄÅÁ±ªÂ∫ìÔºàÊØîÂ¶Çtransformers|peft|accelerateÔºâ„ÄÅÊïôÁ®ãÁ≠â„ÄÇ
  - huggingfaceÊúâÊó∂Â≠òÂú®ÁΩëÁªú‰∏çÁ®≥ÂÆöÁöÑÈóÆÈ¢ò, ËøôÈáåÊé®ËçêÂõΩÂÜÖÊØîËæÉÂ•ΩÁöÑÂπ≥Âè∞modelscope
- ‰∫åÊòØÂ¶Ç‰ΩïÂà§Êñ≠Êú¨Âú∞Á°¨‰ª∂ËµÑÊ∫êÊòØÂê¶Êª°Ë∂≥Â§ßÊ®°ÂûãÁöÑÈúÄÊ±ÇÔºå
  - Êú¨Âú∞ÂèØ‰ª•ÈÉ®ÁΩ≤‰ªÄ‰πàÂ§ßÊ®°ÂûãÔºåÂèñÂÜ≥‰∫é‰Ω†ÁöÑÁ°¨‰ª∂ÈÖçÁΩÆÔºàÂ∞§ÂÖ∂ÂÖ≥Ê≥®‰Ω†GPUÁöÑÊòæÂ≠òÔºâ„ÄÇ
  - Âú®Ê≤°ÊúâËÄÉËôë‰ªª‰ΩïÊ®°ÂûãÈáèÂåñÊäÄÊúØÁöÑÂâçÊèê‰∏ãÔºö ÂÖ¨ÂºèÔºöÊ®°ÂûãÊòæÂ≠òÂç†Áî®ÔºàGBÔºâ = Â§ßÊ®°ÂûãÂèÇÊï∞ÔºàBÔºâ*2
  - Âú®4bitÈáèÂåñÁöÑÊÉÖÂÜµÔºåÊª°Ë∂≥Êú¨Âú∞Êú∫Âô®GPUÊòæÂ≠òÔºàGBÔºâ >= Â§ßÊ®°ÂûãÂèÇÊï∞ÔºàBÔºâ/2ÔºåÂèØ‰ª•Â∞ùËØïÊú¨Âú∞ÈÉ®ÁΩ≤„ÄÇ
- ‰∏âÊòØÂ¶Ç‰ΩïÂø´ÈÄüÈÉ®ÁΩ≤Â§ßÊ®°Âûã„ÄÇ
  - ollama pull llama3

- ÊàëËØ¥ÁöÑÊú¨Âú∞ÔºåÊåáÁöÑ‰∏çÊòØ‰∏ÄÂè∞‰∏™‰∫∫ÁîµËÑë‰∏äÔºåË∑ë‰∏Ä‰∏™7B„ÄÅ13BÂèÇÊï∞ÁöÑÂ§ßÊ®°Âûã„ÄÇËÄåÊòØÂú®‰ºÅ‰∏öÊú¨Âú∞ÁÆóÂäõÊúçÂä°Âô®‰∏äÔºåÁßÅÊúâÂåñÈÉ®ÁΩ≤ÁöÑ700‰∫øÂèÇÊï∞‰ª•‰∏äËßÑÊ®°ÁöÑÂ§ßÊ®°ÂûãÔºåËøôÁßçÂèÇÊï∞ËßÑÊ®°ÁöÑÂ§ßÊ®°ÂûãÔºåÊâçÊúâÊõ¥Â•ΩÁöÑÊåá‰ª§‰æù‰ªéÊÄßÔºåÁªìÂêàRAG„ÄÅAgentÁ≠âÊäÄÊúØÔºåËÉΩÊúâÊïàÁöÑÂÆåÊàê‰Ω†ÂàÜÈÖçÁªô‰ªñÁöÑ‰ªªÂä°„ÄÇ 
# discuss-model-file-formats ‚öñÔ∏è 
- ## 

- ## 

- ## 

- ## There are two FP4 formats circulating right now: MXFP4 and NVFP4 (NV for Nvidia).
- https://x.com/awnihannun/status/1961500133990043967
  - Both formats quantize weights to 4-bit floating point (e2 m1) with a unique scale per group. 
  - The difference is the group size and how the scale for each group is encoded. 
  - MXFP4 uses an e8m0 scale (fixed-point, 8-bit) with a group size of 32. It gets raised to the power of 2 before multiplying the weight. 
  - NVFP4 uses an e4m3 (fp8) scale with a group size of 16. It is multiplied with the weight directly
  - The scale encoding in MXFP4 is pretty suboptimal because it doesn't have representations for a lot of values in the range we need.

- ah, the eternal battle of formats. MXFP4 might be the underdog, but NVFP4 sounds like the crowd favorite. Just wait till someone tries to quantify their existential dread, then we'll really see some innovation. 

- ## [Why does it seem like GGUF files are not as popular as others? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1ood0kn/why_does_it_seem_like_gguf_files_are_not_as/)
  - I feel like it‚Äôs the easiest to setup and it‚Äôs been around since the beginning I believe, why does it seem like HuggingFace mainly focuses on Transformers, vLLM, etc which don‚Äôt support GGUF

- It's pretty easy to support gguf, it's honestly weird that they haven't bothered.
  - The issue isn't really the format so much as all the possible quants. There's very little value in "supporting GGUF" if the code can't, say, multiply IQ3_XXS matrices. Supporting all those qants (which can be different per-matrix too, BTW) adds code complexity and can make optimizing difficult.
  - Also, I believe that GGUF doesn't support FP8 and only somewhat supports FP4. (IIRC some tenors are forced to FP32 in GGUF, but that might be wrong.) So for an engine is focused on only providing GPU floating point inference supporting gguf would be silly since it would basically be the bf16 .safetensors but worse.

- My guess because large scale inference providers need efficient inference for a multi-user environment and only care about GPU inference, and vLLM provides that. As of transformers, it is much easier to add support for new architectures there in most cases.
  - So, implementation complexity to support GGUF is generally higher. Vision models and Qwen Next are good example when it takes a while to get support for GGUF.
  - the main reason why I use GGUF is because `ik_llama.cpp` is good for CPU+GPU inference for a single user.
  - I am sure it will get there in time, but the point is, adding GGUF support is very hard, and requires either hiring professional programmer(s) or relying on the community to implement the support themselves.

- Transformers is going to be running the original, unmodified model; it was trained, tested, etc for this experience. You're getting the purest experience.
  - GGUF is a conversion to make it more compatible for the rest of us. The way inferencing occurs, there is a possibility of differences between gguf and the original intended experience, and also a possibility of speed differences as well.
  - ggufs are VERY popular amongst individual users and runners, and folks with limited/specific hardware. But if you're serving to other users, and you have the base hardware expected for Transformers, then you're probably doing that.
# discuss-vllm

```shell
vllm serve google/gemma-3-270m  --max-num-batched-tokens 32768 --chat-template /Users/yaoo/Documents/opt/configs/llm/chatemplates/template_chatml.jinja

vllm serve RUC-DataLab/DeepAnalyze-8B --max-num-batched-tokens 40000 --max-model-len 28000
```

- vllmÂ§ÑÁêÜpromptÁöÑÂÖ∏ÂûãÊó•Âøó

```log
[entrypoints/logger.py:37] Request chatcmpl-7ada460412e8448999ef5c40b74f9f3d details: prompt: '<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú># Instruction\nthe northwinds excel contains the sales data for

[entrypoints/logger.py:47] Received request chatcmpl-7ada460412e8448999ef5c40b74f9f3d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.4
127.0.0.1:55718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[v1/engine/async_llm.py:360] Added request chatcmpl-7ada460412e8448999ef5c40b74f9f3d
[v1/engine/core.py:880] EngineCore loop active.

[entrypoints/logger.py:76] Generated response chatcmpl-7ada460412e8448999ef5c40b74f9f3d (streaming delta): output: '<Understand>', output_token_ids: [151673], finish_reason: None
```

- ## 

- ## 

- ## 

- ## üÜö [Is vLLM worth it? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1p9t5c8/is_vllm_worth_it/)
- vLLM currently has way too many bugs related to gpt-oss (or even qwen3) and other important features (such as structured outputs). 
  - After trying multiple releases of their official docker images I gave up and moved onto sglang. And oh god I wish I did it much earlier because after an hour of setup it worked like a charm: 3x higher throughout and properly working structured outputs (for both qwen and gpt-oss). 
  - So my advice is to try sglang docker image, key thing is to set the correct reasoning parser and you're good to go

- ## ValueError: To serve at least one request with the models's max seq len (60000), (8.24 GiB KV cache is needed, which is larger than the available KV cache memory (4.00 GiB). 
  - Based on the available memory, the estimated maximum model length is 29056. 
  - Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.

- ## üÜö [Has vLLM made Ollama and llama.cpp redundant? : r/LocalLLaMA _202507](https://www.reddit.com/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/)
- vLLM: Extremely well optimized code. Made for enterprise, where latency and throughput is the highest importance. 
  - Only loads a single model per instance. 
  - Uses a lot of modern GPU features for speedup, so it doesn't work on older GPUs. 
  - It has great multi-GPU support (spreading model weights across the GPUs and acting as if they're one GPU with combined VRAM). 
  - Uses very fast caching techniques (its major innovation being a paged KV cache which massively reduces VRAM usage for long prompt contexts). 
  - It pre-allocates 90% of your VRAM to itself for speed regardless of how small the model is. 
  - üêõ It does NOT support VRAM offloading or CPU-split inference. It's designed to keep the ENTIRE model in VRAM. So if you are able to fit the models in your VRAM, then vLLM is better, but since it was made for dedicated enterprise servers it has the downside that you have to restart vLLM if you want to change model.
- Ollama: Can change models on the fly and automatically unloads the old model and loads the new one. 
  - It works on pretty much any GPU. 
  - It's able to do split inference and RAM offloading so that models which don't fit on the GPU will use offloading and still be able to run even if you have too little VRAM. 
  - And it's also very easy for beginners.
- they serve two totally different purposes. Ollama is way better and way more convenient for most home users. And vLLM is way better for servers and people who have tons of VRAM and want the fastest inference. That's it. 
  - I found a nice post where someone did a detailed benchmark of vLLM vs Ollama. The result was simple: vLLM was up to 3.23x faster than Ollama in an inference throughput/concurrency test

- vLLM has become the inference back-end of choice for the corporate world (Red Hat has based RHEAI on it). Because of that, hardware companies with a vested interest in making inference work well on their hardware have poured engineering-hours into making it maximally performant for corporate use-cases. It seems like the obvious back-end in which to invest one's time and efforts, if one expects to develop corporate LLM applications for a living.
  - Meanwhile, llama.cpp is becoming the swiss army pocketknife of LLM inference, the neatly self-contained do-it-all (with training/finetuning coming back to the project soon). It isn't always the most performant, and it's not great for concurrent inference, but it's very reliable.
  - vLLM by comparison is not very self-contained. It has many sprawling external dependencies, which can make it difficult to get working. 

- Does vLLM support switching models on the fly yet? if not, that‚Äôs a very important feature
  - That's a really good question and it seems the answer is no. I read something saying that vLLM is tightly optimized for high-throughput serving of a single model loaded into memory, using shared KV cache and GPU-accelerated paging (which allows it to support very long input contexts while reducing VRAM usage).
  - And that it therefore loads 1 model per instance and cannot switch without restarting vLLM.

- ## [Best frontend for vllm? : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1ldokl7/best_frontend_for_vllm/)
  - I use LM studio for an easy inference of llama.cpp but was wondering if there is a gui for more optimised inference.

- I've been using text-generation-webui as a combo back-end & front-end since I started with local models over two years ago now, and IMO nothing else comes close as an all-rounder for LLM work. You can also use it as a server, exposing an OpenAI API endpoint to utilize elsewhere if you want to use another front-end or need to make API calls from other programs.

- Been using Lmstudio as well but you could try https://jan.ai/docs as a free open source alternative.

- What we do is build Gradio UIs. Nowadays with LLMs it's super easy to create them and customize them to your liking.

- ## [TIL: For long-lived LLM sessions, swapping KV Cache to RAM is \~10x faster than recalculating it. Why isn't this a standard feature? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/)
  - TIL: For long-lived LLM sessions, swapping KV Cache to RAM is ~10x faster than recalculating it. Why isn't this a standard feature?
  - I was diving into how vLLM and similar inference servers work and had a thought about optimizing memory for long-lived but inactive chat sessions. 
  - The standard approach seems to be either keeping the KV Cache in precious VRAM or evicting it and recalculating from scratch when the user returns. I think there might be a better way.
  - Here's the core idea: Implement a swapping mechanism for the KV Cache of inactive sessions, moving it from VRAM to system RAM (and back), instead of deleting it.
  - We always focus on the high cost of moving data between CPU and GPU, but we often forget the cost of recalculating that data.
  - The math is pretty compelling. Swapping is roughly 7-15x faster than a full recalculation. For a user, waiting 200ms for their chat history to "wake up" is a much better experience than waiting 2+ seconds.
  - This wouldn't be for high-throughput, always-online inference, but specifically for managing many long-lived sessions (e.g., support chatbots, document analysis with breaks, multi-user systems with intermittent activity). It's a classic space-time tradeoff, but in this case, using slightly more "space" (system RAM) saves a huge amount of "time" (latency on reactivation).

- i had the same concern op. the cache saving should work. - https://docs.vllm.ai/en/stable/examples/others/lmcache.html?h=lmcache this exists i want to test it with moon shot ai db. i feel like kimi k2 guys uses the same

- Seems to be compatible with vllm now - https://docs.vllm.ai/en/stable/examples/others/lmcache.html?h=lmcache
  - awesome finding! I should have studied better how to Google properly

- It depends on the backend... vLLM has https://github.com/LMCache/LMCache for example (as someone already mentioned here), but it is mostly limited to GPU-only inference.
  - For CPU+GPU inference for a single user, I find ik_llama.cpp to be the best choice though.

- Llama.cpp does not work like that. It evicts cache only you suplly a prompt with no common prefix with what is in cache already.
  - "inactive" in this context means when the VRAM is not enough to store existing pages of KV cache when new requests are coming, with different prompts. As far as I understand this is how vllm works - it just evicts older pages. A sort of LRU ( least recently used ) buffer

- ## [Â§ßÊ®°ÂûãÊé®ÁêÜÊ°ÜÊû∂ÔºåSGLangÂíåvLLMÊúâÂì™‰∫õÂå∫Âà´Ôºü - Áü•‰πé](https://www.zhihu.com/question/666943660)
- PagedAttention vs RadixAttentionÔºöÂÜÖÂ≠òÁÆ°ÁêÜÁöÑ‰∏§ÁßçÂì≤Â≠¶
- ÂÖàËØ¥vLLMÁöÑPagedAttention„ÄÇ
  - ËøòËÆ∞ÂæóÊìç‰ΩúÁ≥ªÁªüËØæ‰∏äÁöÑËôöÊãüÂÜÖÂ≠òÂêóÔºüvLLMÂ∞±ÊòØÊääËøôÂ•óÁé©Ê≥ïÊê¨Âà∞‰∫ÜGPU‰∏ä„ÄÇÊàëÁ¨¨‰∏ÄÊ¨°ÁúãÂà∞Ëøô‰∏™ËÆæËÆ°Êó∂ÔºåÁõ¥Êé•ÊÉä‰∫Ü‚Äî‚ÄîÂ±ÖÁÑ∂ËÉΩÊääÂÜÖÂ≠òÂà©Áî®Áéá‰ªé20%Âπ≤Âà∞96%
- SGLangÁöÑRadixAttention„ÄÇ
  - Áî®ÁöÑÊòØRadixÊ†ëÔºàÂ∞±ÊòØÈÇ£‰∏™ÂéãÁº©ÂâçÁºÄÊ†ëÔºâ„ÄÇ
  - ÂÉèÊòØÁªôKV cacheÂª∫‰∫Ü‰∏™"ÊóèË∞±"
  - Âú®Â§öËΩÆÂØπËØùÂú∫ÊôØÔºåSGLangÁ°ÆÂÆûÁàΩ„ÄÇ
  - ÈÉ®ÁΩ≤Ëµ∑Êù•ÊòØÁúüÁöÑÈ∫ªÁÉ¶

- ÁºñÁ®ã‰ΩìÈ™åÔºö
  - vLLMÁöÑAPIËÆæËÆ°ÔºåÊÄé‰πàËØ¥Âë¢ÔºåÂ∞±ÊòØÈÇ£Áßç"ÂÇªÁìúÂºè"ÁöÑÔºàË§í‰πâÔºâ„ÄÇ‰∏âË°å‰ª£Á†ÅË∑ëËµ∑Êù•
  - SGLangÁöÑÁºñÁ®ãÊ®°Âûã, Ëøô‰∏™DSLÔºàÈ¢ÜÂüüÁâπÂÆöËØ≠Ë®ÄÔºâÂ≠¶‰π†Êõ≤Á∫øÊúâÁÇπÈô°

- ÂèØÊòØÔºåÈÄöÂ∏∏ÈÉ®ÁΩ≤ÈÉΩÊòØÂè™‰Ωú‰∏∫ÂêéÁ´ØÊèê‰æõ APIÔºåËÄå‰∏çÊòØÁ≠î‰∏ªËøôÊ†∑Âú® Python ÈáåÁõ¥Êé•Âä†ËΩΩÊ®°Âûã„ÄÇÊâÄ‰ª•Á≠î‰∏ªÁöÑÂÅöÊ≥ïÊØîËæÉÂ∞ëËßÅÂï¶„ÄÇ

- sglang‰∏çÁ®≥ÂÆöÂÖ∑‰ΩìÊåáÁöÑÊòØÔºü
  - ÂêÑÁßçÂÜÖÂ≠òÊ≥ÑÊºèÔºåoomÔºåcrashÔºå‰ª£Á†ÅËßÑÊ®°‰∏çÂ§ßÔºåÂ∞±ÂºÄÂßãÂ±éÂ±±Â†ÜÂ±éÔºåÁîü‰∫ßÂà´Áî®Ôºå‰πüÂà´ËøΩÊõ¥Ôºåmerge masterÂü∫Êú¨ÁöÑÂõûÂΩíÊµãËØïÈÉΩ‰ºöÁõ¥Êé•Ë∑≥ËøáÁöÑ„ÄÇÂÅöÂºÄÊ∫êÁöÑÔºåÂ∞±Ëøô

- vllm ‰ªÖÊîØÊåÅ NVIDIA GPU„ÄÅÈÉ®ÁΩ≤Â§çÊùÇ„ÄÅÊòæÂ≠òÈúÄÊ±ÇÂ§ß
# discuss-cpu-llm
- ## 

- ## 

- ## 

- ## [llama.cpp and llama-server VULKAN using CPU : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1ooyr6r/llamacpp_and_llamaserver_vulkan_using_cpu/)
  - llama.cpp and llama-server VULKAN appears to be using CPU. I only noticed when i went back to LM Studio and got double the speed and my Computer didnt sound like it was about to take off.
- First, it doesn't hurt to set ngl 99. Second, cache quantization hurts speed. Other than these two points, I don't know why llama.cpp could get slower than lms.

# discuss-ai-api/tools
- ## 

- ## 

- ## 

- ## [How is Cloud Inference so cheap : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1q2jwsn/how_is_cloud_inference_so_cheap/)
  - How do cloud inference companies like DeepInfra, Together, Chutes, Novita etc manage to be in profit regarding to the price of the GPUs/electricity and the fact that I guess it's difficult to have always someone to serve ?

- DeepInfra and Together are VC funded. Probably safe to assume they run at a loss for now.
  - Chutes runs on a decentralized network (bittensor), i.e. it's crowdsourced and they pay individual miners.
  - I haven't tried Novita but they're a tiny team of 10 and their blog has some posts about how they optimize e.g. quantization. They're also a little more expensive than the others.

- Scale is more efficient, batching is more efficient, horizontal scaling is more efficient, and a lot of them quantize

- Batching. One gpu can serve hundreds of uses at once. https://artificialanalysis.ai/benchmarks/hardware
- Good God the B200 is a beast!
  - I got a dgx spark and it has the same architecture. I‚Äôm getting 600tok/s doing batch inference with qwen3 next 80b. It‚Äôs not as good as the big boys but it‚Äôs doing batch data classification quite well.

- Inference providers are operating at a loss, hoping that their competitors run out of money before they do. Last man standing wins.

- Data is the new currency.

- same reason the USA is 38 trillion in debt. they built the datacenters to pump corporate profits sheets up. doesn't matter how much money they lose. as long as they are 'winning' uncle sam will cash out. money is about social control. it does not measure resources.

- ## openrouter ÊòØÁúüÊñπ‰æøÔºå‰∏Ä‰∏™ Key ÊâÄÊúâÊ®°ÂûãÈÉΩËÉΩÁî®„ÄÇ
- https://x.com/pengchujin/status/1894375539726803201
- Ê®°Âûã‰ª∑Ê†º‰∏ÄÊ†∑ÔºåÂÖÖÂÄº 5% ÁöÑÊâãÁª≠Ë¥π„ÄÇËøòÊúâ‰∏™Â•ΩÂ§ÑÊòØÊúâ‰∏ÄÈÉ®ÂàÜÂÖçË¥πÊ®°ÂûãÔºåÊØîÂ¶ÇAzÁöÑR1Âï•ÁöÑ
  - ‰ª∑Ê†º‰∏ÄÊ†∑ÁöÑÔºåÂÆòÊñπÈôç‰ª∑‰ªñ‰πüË∑üÁùÄÈôç
- QwenÁöÑkeyÔºåÁÅ´Â±±ÁöÑkey‰πüÊòØËøôÊ†∑ÁöÑ

- ## deepseek ÊúÄËøëÂºÄÊ∫êÁöÑÂâçÈù¢Âá†‰∏™È°πÁõÆÂèØËÉΩÊàë‰∏ç‰∫ÜËß£, ‰ªäÂ§©Ëøô‰∏™3fs, smallpond Ê≠£Â•ΩÊòØÊàë‰ª¨Ëøô‰∏™Ë°å‰∏öÁöÑ, ÊàëÁêÜËß£Âπ∂Ê≤°ÊúâÂÉèÊäÄÊúØÁÇ∏ÂºπÂêß, ÈÉΩÊòØÊØîËæÉÊàêÁÜüÁ®≥ÂÆöÁöÑÊäÄÊúØ‰∫Ü
- https://x.com/baotiao/status/1895395027129655691
- y1s1 ËøôÈáåÁöÑÈöæÁÇπÊòØÁ†çÈúÄÊ±ÇÔºå‰∏çÂÅö‰ªÄ‰πà

- ## DeepSeek ÂºÄÊ∫êÂë®ÁöÑ 5 Âè∑ÁÇ∏ÂºπÊù•Âï¶ÔºÅÂèàÊòØÈõÜÊùüÁÇ∏ÂºπÔºÅ3FS Âíå smallpondÔºÅ
- https://x.com/karminski3/status/1895280320989274560
  - Êàë‰∏çÊï¢Áõ∏‰ø°DeepSeekÁîöËá≥È¢†Ë¶Ü‰∫ÜÂ≠òÂÇ®Êû∂ÊûÑ...... Êàë‰∏äÊ¨°‰∏∫ÁΩëÁªúÊñá‰ª∂Á≥ªÁªüÈúáÊÉäËøòÊòØHDFSÂíåCEPH. ‰ΩÜËøô‰∫õÈÉΩÊòØÈù¢ÂêëÁ£ÅÁõòÁöÑÂàÜÂ∏ÉÂºèÊñá‰ª∂Á≥ªÁªü. Áé∞Âú®‰∏Ä‰∏™ÁúüÊ≠£ÊÑè‰πâ‰∏äÈù¢ÂêëÁé∞‰ª£SSDÂíåRDMAÁΩëÁªúÁöÑÊñá‰ª∂Á≥ªÁªüËØûÁîü‰∫ÜÔºÅ
  - È£ûÁÅ´ÊµÅÊòüÊñá‰ª∂Á≥ªÁªüÔºà3FSÔºâ- ‰∏ÄÁßçÂà©Áî®Áé∞‰ª£ SSD Âíå RDMA ÁΩëÁªúÂÖ®Â∏¶ÂÆΩÁöÑÂπ∂Ë°åÊñá‰ª∂Á≥ªÁªü
  - Ëøô‰∏™Êñá‰ª∂Á≥ªÁªüÂèØ‰ª•Âú® 180 ËäÇÁÇπÈõÜÁæ§‰∏≠ËææÂà∞6.6 TiB/s ÊÄªËØªÂèñÂêûÂêêÈáèÔºåÊØè‰∏™ÂÆ¢Êà∑Á´ØËäÇÁÇπ KVCache Êü•ÊâæÂ≥∞ÂÄºÂêûÂêêÈáè 40+ GiB„ÄÇ
  - Âè¶‰∏Ä‰∏™ smallpondÔºàÂ∞èÊ±†Â°òÔºâÊòØÂü∫‰∫é 3FS ÁöÑÊï∞ÊçÆÂ§ÑÁêÜÊ°ÜÊû∂ÔºÅ
  - Ëøô‰∏™Ê°ÜÊû∂Áî± DuckDB Êèê‰æõÁöÑÈ´òÊÄßËÉΩÊï∞ÊçÆÂ§ÑÁêÜÔºåÂèØÊâ©Â±ï‰ª•Â§ÑÁêÜ PB Á∫ßÊï∞ÊçÆÈõÜÔºÅ
  - ÊàëÁúã‰∫Ü‰∏ãÂ∫îËØ•ËøòÊòØKVÂ≠òÂÇ®ÁöÑÔºàÊØïÁ´üÈù¢ÂêëÊú∫Âô®Â≠¶‰π†ÔºâÔºåÂπ∂‰∏çÊòØÂùóÂ≠òÂÇ®„ÄÇÂõ†Ê≠§NAS‰Ω¨ËøòÊòØ‰∏çÂ§™ËÉΩÁî®Âæó‰∏äÁöÑ„ÄÇ  ‰∏ÄËá¥ÊÄßÂçèËÆÆÂü∫‰∫éCRAQÔºåÊØïÁ´üKVÂ≠òÂÇ®ÔºåÂü∫‰∫éÈìæÂºèÂ§çÂà∂ÁöÑÔºåÂÜôÊìç‰Ωú‰ªçÁÑ∂ÈúÄË¶ÅÈÄöËøáÊï¥‰∏™ÈìæÔºåÊâÄ‰ª•ÂÜôÂª∂ËøüÂ§ß„ÄÇ‰ΩÜ‰º∞ËÆ°ÂÖ∂ÂÆûÁªôËÆ≠ÁªÉÂΩíÊ°£Áî®ÔºåÂÜôÂª∂ËøüÂ§ßÊó†ÊâÄË∞ì„ÄÇÂºÇÊ≠•ÂΩíÊ°£ËÄåÂ∑≤„ÄÇ
- ÊòØÊñá‰ª∂Á≥ªÁªüÔºåÂÖÉÊï∞ÊçÆÊîæfoundationdbÈáåÔºåÊï∞ÊçÆÊîæxfsÈáå

- Âè¶Â§ñIntelÁöÑDAOSÂÖ∂ÂÆûÊàëÊÑüËßâÂÅöÁöÑÊõ¥ÊûÅËá¥ÔºåÁõ¥Êé•Áî®SPDKÊìç‰ΩúË£∏ÁõòÔºåËøòÂà©Áî®NVMe-oFÊäÄÊúØÂÖÖÂàÜÂèëÊå•RDMAÈõ∂Êã∑Ë¥ùÁöÑÁâπÊÄßÔºåÂè™ÂèØÊÉúÂÖÉÊï∞ÊçÆÁ≥ªÁªü‰æùËµñ‰∫éNVMÁ°¨‰ª∂ÔºåËÄåNVMÁ°¨‰ª∂Â∑≤ÁªèÂáâÂáâÔºåDAOS‰πüÂçñÁªôDell‰∫Ü

- cephÊúâobject storeÂëÄ‚Ä¶ Â§¥‰∏ÄÊ¨°ËßÅÂà∞ÁúüÁöÑÊúâ‰∫∫Áî®chain replicationÁöÑ‚Ä¶latency‰∏çË¶ÅÂï¶ÔºürdmaÁ°¨‰ª∂Ëøô‰πàË¥µÁõ¥Êé•Èù†Á°¨‰ª∂Á†∏ÂêóÔºü
- Êï∞ÊçÆ‰∏≠ÂøÉÔºåÂúüË±™Áé©Ê≥ïÔºåÈ¢†Ë¶Ü‰∏çËá≥‰∫éÂêß

- ## ÊàëÊâçÁü•ÈÅì Â∑•ÂÖ∑Ë∞ÉÁî®ÊòØ‰∏™ÂçïÁã¨ÁöÑ call ‰∏ÄÊó¶Ë∞ÉÁî®ÂÖ∂‰ªñ‰ªÄ‰πà‰∫ãÊÉÖÈÉΩÂπ≤‰∏ç‰∫Ü ÊÄé‰πàËøô‰∏™ llm Áé∞Âú®Ëøô‰πàËçâÂè∞Áè≠Â≠ê„ÄÇ„ÄÇ„ÄÇ
- https://x.com/Lakr233/status/1894950577416901018
  - ËøòÊúâ Agent ËøôÁßçÊ¶ÇÂøµÔºåÂè´ÂÖ®Ëá™Âä®ÁºñÊéíÊ£ÄÁ¥¢Â∑•ÂÖ∑‰ΩøÁî®‰∏çÂ•Ω‰πàÊêûÁöÑËøô‰πàÈ´òÂ§ß‰∏äÂà∞Â§¥Êù•ËøòÊòØ GPT 3.5 Â∞±ËÉΩÂπ≤ÁöÑÊ¥ª„ÄÇ„ÄÇ„ÄÇ

- tools Â∞±ÊòØ‰∏ÄËΩÆÊñ∞ÁöÑÂØπËØùÔºåÂëäËØâ AI ÂèØ‰ª•Ë∞ÉËøô‰∏™ÔºåÁÑ∂ÂêéÊääË∞ÉÁî®ÁªìÊûúÊãºÊé•Âà∞‰∏ä‰∏ÄËΩÆÂØπËØùÂêéÈù¢„ÄÇ
  - agent Â∞±ÊòØ pipeline/orchestration/scheduler„ÄÇ
  - embedding/RAG Â∞±ÊòØ search„ÄÇ
  - LLM framework Â∞±ÊòØ HTTP API calls„ÄÇ
- ‰Ω†ËøôÊ†∑ÔºåPPTËøòÊÄé‰πàÁºñÔºåÂõ¢ÈòüÈ¢ÑÁÆóÊÄé‰πàÊâπ

- ‰πüÊúâ‰∏Ä‰∫õÂºÇÊ≠•Ë∞ÉÁî® +event ÁöÑÁé©Ê≥ï, ‰ΩÜÊòØÊï¥‰∏™ Agent Á°ÆÂÆûÂ∞±ÊòØÂª∫Á´ãÂú®Êó†ÈôêÁöÑÊñáÊú¨ÁîüÊàê‰∏äÁöÑ
  - Agent = FunctionCall + LLM + LOOP

- Â∑•ÂÖ∑Ë∞ÉÁî®ÂÖ∂‰ªñ‰ªÄ‰πà‰∫ãÊÉÖÈÉΩÂπ≤‰∏ç‰∫ÜÊåáÁöÑÊòØÔºüÂèØ‰ª•Âπ∂ÂèëÂ§ÑÁêÜÂÖ∂‰ªñÈóÆÈ¢òÂëÄÔºü

- ## Êµ∑Â§ñÂ§ßÊ®°ÂûãÊé•ÂÖ•‰ΩøÁî®openrouter Â∑≤ÁªèÊòØÊúÄ‰Ω≥ÂÆûË∑µ‰∫ÜÂòõÔºü
- https://x.com/leeoxiang/status/1887714022327525797
  1„ÄÅÊîØÊåÅ‰∫ÜÂ∏ÇÈù¢‰∏äÂ§ßÈÉ®ÂàÜÊ®°ÂûãÔºõ
  2„ÄÅÂèØ‰ª•ËÆæÁΩÆÊ®°ÂûãÁöÑÊ∂àËÄóÈ¢ùÂ∫¶Ôºõ
  3„ÄÅËÉΩÊîØÊåÅÊ®°ÂûãÁöÑË¥üËΩΩÂùáË°°Ôºõ
  4„ÄÅÊîØÊåÅ‰∏ÄÂ•óapiÈÄÇÈÖçÂ§ßÂ§öÊï∞Ê®°ÂûãÔºõ
  5„ÄÅÊîØÊåÅÂ§ö‰∏™Ê®°ÂûãÁöÑÂ§±Ë¥•ÈáçËØïÔºõ

- Â§™Ë¥µ‰∫ÜÔºÅËÄå‰∏îÁâπÊÆäÊ®°ÂûãËøòÊòØË¶ÅÊ∑ªÂä†Ëá™Â∑±ÁöÑAPI Key

- ## We ( @jamesmurdza ) have been building Open Computer Use - 100% open source computer use agent.
- https://x.com/mlejva/status/1877054558481813799
  - The agent is using @e2b_dev 's Desktop Sandbox as virtual computer.
  - The agent is using 3 different LLMs: üî∏Llama 3.2 ( @AIatMeta ) üî∏Llama 3.3 üî∏OS-Atlas ( @Alibaba_Qwen )
  - It's slow and makes mistakes but this is a big milestone for OS AI community!

- ## ÊÉ≥Ë¶ÅÈÉ®ÁΩ≤Êú¨Âú∞Ê®°Âûã‰ΩÜÊòØ‰∏ç‰ºöËÆ°ÁÆó vRAM Âç†Áî® 
- https://x.com/tuturetom/status/1842492423848804686
  - https://huggingface.co/spaces/hf-accelerate/model-memory-usage

- ## ÁúãÊù•Â§ßÂÆ∂Áªà‰∫éËææÊàêÂÖ±ËØÜ‰∫ÜÔºölangchain ÊòØÁé©ÂÖ∑ÔºåÂ¶ÇÊûúÈùûË¶ÅÂú®Áîü‰∫ßÁéØÂ¢ÉÁî®ÂÆÉÔºåÈÇ£ÂÆÉÂ∞±‰ºöÂèòÊàêÂ∑•‰∏öÂûÉÂúæ„ÄÇ
- https://x.com/beihuo/status/1840058205768167699
  - ÁúãËøô‰ª£Á†ÅÊØîËæÉÔºålangchain Â∞±ÂÉè‰∏Ä‰∏™Ê≤°ÊúâÂ§™Â§öÂ∑•Á®ãÁªèÈ™åÔºå‰ΩÜÊòØÂèàÁúã‰∫ÜÂ§™Â§öËÆæËÆ°Ê®°ÂºèÊïôÁ®ãÁöÑ‰∫∫ÂÜôÂá∫Êù•ÁöÑ‰∏úË•ø„ÄÇ‰ΩøÁî®ÂÆÉÊù•ÂÆûÁé∞‰∏Ä‰∏™Áîü‰∫ßÁ≥ªÁªüÔºåÂ∞±ÊòØ‰∏Ä‰∏™ÁÅæÈöæ„ÄÇ
- ÂêåÊÑü longchainÊûÑÂª∫ÊÄùÁª¥Èìæ‰∏çÂ¶ÇÁõ¥Êé•ÊåâÁÖßÂ∑•‰ΩúÈÄªËæë‰∫∫Â∑•ÊûÑÂª∫ÊÄùÁª¥Èìæ„ÄÇÁúüÊ≠£ÁöÑËøûÊÄùÁª¥ÈìæÈÉΩ‰∏çÊ∏ÖÊ•öÁöÑÂàõÈÄ†ÂèëÊòéÔºåÁé∞Âú®Áî®AIÊù•ÂÅö‰∏∫Êó∂Â∞öÊó©„ÄÇ

- ## üíÑ ÁîüÊàêÂºèÁü•ËØÜ UI ÊúÄÊ†∏ÂøÉÁöÑÂü∫Á°ÄËÆæÊñΩÔºåÁõÆÂâçÂõ¥ÁªïÊ≠§Á±ªÂΩ¢ÊÄÅËÆæËÆ°ÁöÑ http://Me.bot ‰πüÊØîËæÉÂèóÊ¨¢Ëøé
- https://x.com/tuturetom/status/1835349759848333340

- ## Hugging Face ÂÆ£Â∏ÉÊäïÂÖ• 1000 ‰∏áÁæéÂÖÉÁî®‰∫éÂÖçË¥πÂÖ±‰∫´ GPUÔºåÊó®Âú®Â∏ÆÂä©Â∞èÂûãÂºÄÂèëËÄÖ„ÄÅÂ≠¶ÊúØÁïåÂíåÂàùÂàõÂÖ¨Âè∏ÂºÄÂèëÊñ∞ÁöÑ AI ÊäÄÊúØÔºåÊäóË°° AI ËøõÊ≠•ÁöÑÈõÜ‰∏≠Âåñ„ÄÇ
- https://x.com/glow1n/status/1791488036259434749
  - CEO Clem Delangue Ë°®Á§∫ÔºåËøô‰∏Ä‰∏æÊé™Â∞ÜÈÄöËøá ZeroGPU ËÆ°ÂàíÂÆûÁé∞Ôºå‰øÉËøõ AI ÊäÄÊúØÁöÑÂéª‰∏≠ÂøÉÂåñÂèëÂ±ï
  - ZeroGPU ‰ΩøÁî® Nvidia A100 GPU ËÆæÂ§áÔºåÊèê‰æõÈ´òÊïàÁöÑËÆ°ÁÆóËµÑÊ∫ê„ÄÇ
  - Hugging Face ÁöÑ Spaces Âπ≥Âè∞Â∑≤ÊúâË∂ÖËøá 30 ‰∏á‰∏™ AI ÊºîÁ§∫„ÄÇ

- ## Cloudflare ÁöÑ Workers AI ÊØèÂ§©ÂèØ‰ª•ÂÖçË¥π‰ΩøÁî® 10, 000 NeuronsÔºàÁõ∏ÂΩì‰∫éÁîüÊàê100-200‰∏™LLMÂìçÂ∫îÔºå500Ê¨°ÁøªËØëÔºå500ÁßíÁöÑËØ≠Èü≥ËΩ¨ÊñáÂ≠óÈü≥È¢ëÔºâ ÔºåË∞ÉÁî®ÊñπÂºèÂÖºÂÆπ OpenAI 
- https://x.com/scomper/status/1791804644332908646
- Â•ΩÂÉèÈÉΩÊòØÂ∞èÊ®°Âûã‰∏∫‰∏ªÂêß

# discuss-model-tuning/internals
- ## 

- ## 

- ## [Transformer Lab - Download, interact, and train models locally : r/LocalLLaMA _202402](https://www.reddit.com/r/LocalLLaMA/comments/1awer80/transformer_lab_download_interact_and_train/)
  - We built Transformer Lab to be an open source tool that lets you interact, train, and evaluate LLMs on your own machine or in the cloud (currently Mac or Linux, Windows soon) through a simple GUI.
  - One of our goals is to make it as easy as possible to get started experimenting with LLMs regardless of what platform you have (even if you don‚Äôt have a GPU). You can do everything through the UI but it is built on an API that can be extended using plugins. When you start the app it will try to figure out what kind of system you have and install the right set of initial plugins to get you going optimally.
  - Recently we've been putting a lot of effort in trying to make it work really well for folks without a GPU but who have an Apple Silicon Mac and want to finetune models through a GUI.

- Since this is written in electron, will there be an option to host the entire app as a web server in the future?
  - Interesting. We‚Äôve thought about this a bit. Mostly the app is just a plain React app so it could just be a website. It only does a bit of non web work in the installer which could be removed.
  - We chose to make it an app in the hope that it‚Äôs easier to install but if people really want a web version it‚Äôs something we could work on (or could use help with).

- The core Llama Training library uses the out of the box Hugging Face Trainer library which supports AMD. So at worst, it might require a bit of tweaking to the TrainingArguments when doing training in order to optimize for AMD.

- ## [Looking for a Base Model : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1q9si66/looking_for_a_base_model/)

- ## [Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pi8z74/which_small_model_is_best_for_finetuning_we/)
  - TL; DR: We fine-tuned 12 small models to find which ones are most tunable and perform best after fine-tuning. Surprise finding: Llama-3.2-1B showed the biggest improvement (most tunable), while Qwen3-4B delivered the best final performance - matching a 120B teacher on 7/8 tasks and outperforming by 19 points on the SQuAD 2.0 dataset.
  - 12 models total - Qwen3 (8B, 4B, 1.7B, 0.6B), Llama (3.1-8B, 3.2-3B, 3.2-1B), SmolLM2 (1.7B, 135M), Gemma (1B, 270M), and Granite 8B.
  - Tested on 8 benchmarks: classification tasks (TREC, Banking77, Ecommerce, Mental Health), document extraction, and QA (HotpotQA, Roman Empire, SQuAD 2.0).
  - I guess the title is a little click-baity. To get the total number, you have to account for the synthetic data we generated for each benchmark. That's the reason we could easily scale the training to many tasks and domains.
  - You are right that the training itself was not 10k, but running the whole pipeline adds up to the number. We didnt think that _that_ would be the main discussion point.

- i did this test back then with qwen 2.5 models for our work project, and interally we found that qwen2.5 7b was the most consistent model with the best result for its size, where the 2.5 3b had issues following instructions at times. did the qwen 3 4b model ever had this happen or was it able to execute all tasks without fail?
  - I wouldn't say "without fail", it's definitely not perfect, but it's pretty good at following instructions overall. On some tasks there was little difference between the 8B & 4B Qwen3 models (the harder the task, the more important param count is).

- Can this also apply to VL models? Qwen3 VL personally.
  - Not yet, but doing this same for vision models is on our roadmap!

- The Qwen team hit it out of the park with Qwen3-4B-Instruct-2507. This model keeps coming up all over the place and it deserves the praise it gets. I hope whatever secret sauce they used for that model is further propagated to their other models. Actually what I really would like is a deep dissection and a full whitepaper on why the hell this little model hits so much above its belt.

- I recommend strongly HuggingFaceTB/SmolLM3-3B. Got the same perfs as Qwen3-4B-Instruct-2507 for my task but with faster inference 

- [Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found : r/LocalLLM](https://www.reddit.com/r/LocalLLM/comments/1pi98l9/which_small_model_is_best_for_finetuning_we/)

- ## [Convert Dense into MOE model? : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pfxrv5/convert_dense_into_moe_model/)
- Contrary to many of the other answers here, it's definitely possible but usually not worth the trade-offs.
  - The thing is, you are better off using a model that was trained from scratch to be an MoE (since MoE are cheaper to train anyway) or just using a smaller dense model with more predictable performance.
  - They were able to convert a dense Llama model into an MoE with 25% activation and still have non-trivial performance without any retraining (and of course it does better with retraining). However, the MoE is not nearly as good as the original dense model.

- ## [In light of Kimi Linear, reposting Minimax's article on Linear Attention : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1oorvd0/in_light_of_kimi_linear_reposting_minimaxs/)
  - MiniMax M2 Tech Blog 3: Why Did M2 End Up as a Full Attention Model?

- until we have some gated delta net models with way more active parameters, i don't think we can really come to any solid conclusions right now. gpt-oss is 100% transformer and also sucks the same way as these really sparse hybrid models.

- Feels like "You never really know what‚Äôs going to happen until you scale up." might be one big reason why qwen will keep with Next type models for the time being. The reasoning could have been: their some performance penalty at inference, but for the gpu/time cost of one 32b dense, we can train 10+ different "good enough" 80b hybrid MoE. Allows us to iterate faster, detect issues faster, experiment more (more parameters? more experts? different training methods?). All of which could benefit full attention down the line.

- ## [What‚Äôs the training cost for models like Qwen3 coder 30b and is the code for training it is open source or close source? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1njws3n/whats_the_training_cost_for_models_like_qwen3/)
- You can do fine tunes on the 4B real easy with Unsloth. LoRAs and QLoRAs, too. It won‚Äôt take much VRAM. You can fine tune a LoRA for Qwen3 30B A3B Instruct 2507 in BF16 on a single 96GB 6000 Pro in just a few hours.
  - That depends entirely on what you are training and the size of your dataset. True fine tuning, on a really small dataset will take a few hours, but if you are trying to train in new capabilities, that is going to take a much larger dataset, and could be months or over a year on a single RTX pro 6000.

- Their exact code isn't open source, but many companies train using Pytorch and Megatron-Core open source code.
  - I estimate GPU-time needed for training Qwen 30B A3B to be around 800 000 hours for H100 SXM5, so around 1.6 million dollars. That's for pretraining on 20T tokens, assuming 40% MFU.

- Stick to finetunes, you don't have the hardware or budget to pretrain models of practical size (believe me I have tried).
# discuss-terminal/fs
- ## 

- ## 

- ## 

- ## Introducing SQLite-Agent, a groundbreaking SQLite extension that allows your database to run autonomous AI agents directly inside SQLite itself. _202512
- https://x.com/_marcobambini/status/2000599613271892369
  - With SQLite-Agent, your data becomes active: agents can read, reason, act, and write back results without leaving the database engine. This turns SQLite into a fully self-contained automation and intelligence platform for apps, devices, and edge environments.
- https://github.com/sqliteai/sqlite-agent /c
  - A SQLite extension that enables SQLite databases to run autonomous AI agents, using other SQLite AI extensions. 

- ## at @llama_index we came up with ùóÆùó¥ùó≤ùóªùòÅùó≥ùòÄ-ùó∞ùóπùóÆùòÇùó±ùó≤, a small demo that combines @claudeai and AgentFS by @tursodatabase with our Agent Workflows  _202512
- https://x.com/itsclelia/status/2000623632629002579
  - to create a harnessed and secure environment where the agent can perform all operations in a virtual filesystem without affecting your real one
  - Coding Agents are great, but giving them unfiltered access to your filesystem might be a very bad idea
  - Bonus: we also gave the agent understanding of unstructured documents, by parsing them with LlamaParse

# discuss
- ## 

- ## 

- ## 

- ## üÜö [[Experiment] Three identical Qwen2.5:7b runs, three distinct behavioral strategies. One hit max reported metrics and started failing to execute actions. [Full data + code] : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1obkn0d/experiment_three_identical_qwen257b_runs_three/)
- 
- 
- 

- ## üß© [Qwen3-30B-A3B 2507 Instruct vs Thinking : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1o8529x/qwen330ba3b_2507_instruct_vs_thinking/)
- My understanding is that 
  - GPQA is general knowledge, 
  - AIME25 is Math, 
  - LiveCodeBench is coding, 
  - Arena-Hard is a predictor of how well it will perform at LMArena 
  - and BCFL is about tool calling.

- i'm guessing the t/s will be low if you use CPU (~5t/s) so just use instruct if you don't want to wait like 5 minutes for an answer with reasoning

- Reasoning only helps on logic-intensive tasks. On anything else, it‚Äôs a waste of time at best, and can actually make things worse if the chain of thought focuses on the wrong thing.

- ## [Why do people like Ollama more than LM Studio? : r/LocalLLaMA _202501](https://www.reddit.com/r/LocalLLaMA/comments/1icta5y/why_do_people_like_ollama_more_than_lm_studio/)
- I think many of us are programmers, personally I prefer open-source applications. Of course as a programmer, I have no problems with using command line.
- You can search for models using the the website of Ollama. Downloading models is just one command ollama pull `<model_name>` . The same for removing or serving a model.
- And you can use Ollama with HuggingFace GGUF now too.

- Ollama depends on llama.cpp which was created by the guy (Georgi Gerganov) who invented GGUF.
  - Ollama does have its own registry though that stores the GGUF files in its own MODELFILE format.
  - You can use any GGUF with Ollama

- As a programmer I don't understand why more people don't use koboldcpp
  - It was a pain in the ass to get going properly. Also it might be just me, but I couldn't figure out how to serve multiple models at the same time with kobold.
- KoboldCPP seems oriented towards Windows. A lot of programmers are on Mac/Linux. (At least compared to the average population). Sure KoboldCPP seems to work on Mac/Linux. But, the docs are very Windows-centric. And so I'd rather use something more focused on my environment.

- Does llama-server allow serving multiple models at once? I couldn't figure out how to get that going, but it was a killer feature for me since I swap models on a task by task basis.
  - Well, kind of. You can start multiple llama-server instances and have multiple models loaded simultaneously, resources permitting. Otherwise, you can always hot load llama-server in your code.

- I use LM Studio even in production. You can start it as service. When product is new you need to babysit it , testing different params and models in pretty OK in LM Studio. I was using it from 0.1 to new 0.3 , new interface and multi-model support is pretty good.

- you can use ollama commercially without problems. lm studio writes that you have to contact them.

- Headless is a feature not a bug. I'm using multiple frontends to talk to the same server (Tabby, OpenWebUI, custom programs, etc). And, I'm running the inference on a separate machine from where I'm accessing it anyways.
  - Basically, programmers like Ollama, Llama.cpp Server, and other programmer oriented servers and libraries.

- Why don't people talk about OOBABOOGA? It's better than all listed, yes, including LM Studio!
  - Python programs take like 5 hours to install to then fail on some dependency conflict. Then you start again. I used OOBA for a bit but my python env and everything broke with every update.

- For me, the biggest advantage of Ollama is loading models on demand. I have a few automation scripts that use different models, plus it serves OpenWebUI, cline and aider, and dynamic loading/unloading is the way to go for me, otherwise I‚Äôd just use pure llama.cpp.

- ## [Are there any better offline/local LLMs for computer illiterate folk than Llama? Specifically, when it's installed using Ollama? : r/LocalLLaMA _202411](https://www.reddit.com/r/LocalLLaMA/comments/1gnjev5/are_there_any_better_offlinelocal_llms_for/)
- LMStudio uses llama.cpp backend on PC, and can additionally use MLX on Mac, so the model selection is somewhat broader on Mac. 
- MSTY uses bundled Ollama and can connect to other servers, including ChatGPT. It will also pick up models already downloaded by another Ollama installation.
- Msty is quite easy to set up and use: https://msty.app/ But which model they run depends on the computer. If necessary need to use some online API service like OpenRouter, which Msty also supports.

- ## [LM Studio vs Ollama vs Jan vs Llama.cpp vs GPT4All : r/LocalLLaMA _202502](https://www.reddit.com/r/LocalLLaMA/comments/1isazyj/lm_studio_vs_ollama_vs_jan_vs_llamacpp_vs_gpt4all/)
- llama-swap, which I use to proxy llama.cpp and koboldcpp.
  - llama-swap provides two important features: on-demand model load/unload; manage the parameters for launching the models
  - I am now able to "save" the way I launch llama.cpp for each model I want to use. I no longer need to keep a CLI open to stop/start llama.cpp or ollama when I want to change models.
- Agree. I started with ollama but slowly shifting towards llama-swap because it's simpler and provides better control over parameters.
  - I know I can create alter parameters in ollama to a great extent but having everything in a single config file is just better.

- LM Studio, mainly because it now has the "Auto unload unused JIT loaded models" feature.

- LM Studio as it just works. I really don‚Äôt care about open/closed source. I want something that looks nice and just works well on my hardware.
  - same here. lm studio outperforms ollama anythingllm msty gpt4all in terms of speed and ease and model intelligence actually. I tried them all.
- Until it doesnt and you could do nothing about it.

- Quick note: Cortex powers Jan, and we're adding backend-level features to Jan through Cortex. Once Cortex becomes multi-engine, Jan will also support multiple engines alongside llama.cpp.

- KoboldCpp for obvious reasons, but the main advantages are that its not limited to just instruct models as it also has regular text completion modes. Its a server rather than just a local app so it can be used remotely, and for those that the bundled UI isn't suitable for it can be used with third party UI's trough the multiple API's it supports and emulates.

- GPT4ALL is my favorite on paper, but the devs refuse to fix a big issue with response streaming. I want to couple it with Open WebUI and the responses don't show up.

- Ooba because you can use quantized KV cache with Context Shift (LLM_Streaming). It's a must have if you roleplay and have low VRAM.
  - LLM Streaming to my knowledge is not the same as Context Shift like in KoboldCpp. Context Shift is compatible with things that should stay in memory, it shifts after the tokens that persisted. While traditional LLM Streaming implementations tend to fade out the old bits including context you may need. If I am out of the loop and ooba's implementation does now work like Kobolds feel free to correct me but this was not originally the case.

- LMStudio but their REST API is kinda of restrictive. I use hugging models loaded on ray sometimes for better API functionality. Wish i could try vLLM but not working on windows

- I use llama.cpp, because it is relatively simple and self-contained (not a lot of external dependencies), and something for which I could develop features and maintain.
  - It has the potential to be the "do-everything tool" for LLM technology; 
  - it used to have training and fine-tuning example code, but it was taken out and is in the process of being re-implemented so it can take advantage of any supported back-end (CUDA, SYCL, Vulkan, etc). 
  - The GGUF file format is well thought-out and people are taking good advantage of its metadata features, so you don't have to chase down inference-time parameters from obscure corners of the web.
  - I might be forced to learn vLLM eventually if my employer decides to start using Red Hat Enterprise AI, but until then llama.cpp is all I want or need.

- ## üÜö [What's the point of OpenUI Web vs Piniochio or Jan.ai? The ladder of which being substantially easier to install : r/LocalLLaMA _202412](https://www.reddit.com/r/LocalLLaMA/comments/1hc20u7/whats_the_point_of_openui_web_vs_piniochio_or/)
- Lmstudio and Jan are examples of kitchen sink LLM apps, they merge model management with backend and frontend.
- OpenWebUI is just a frontend, so it makes most sense for folks that already have both a backend (and thus already have docker, etc all configured) and a way to manage models loaded on that backend.
- There are also several middle grounds:
  - Ollama has backend and model management, but no frontend.
  - Koboldcpp has backend and frontend, no model management.

- The kitchen sinks are a great place to start but if you're wondering what their downsides are: you are stuck to their model management system and whatever quants they support. Batch/multi-user performance is also not a priority for these apps.
  - To pick a concrete example, to my knowledge there's no all in one's support EXL2 and can spin me up a tabbyAPI with tensor parallel and a big context cache. So how easy it is to install doesn't matter to me, vs doing what I want.

- I don't use any of these and instead sometimes have to resort to fixing bugs in python myself in ooga/text-generation-webui, 
  - because I want to use top_a, tail_free or mirostat sampling, use GBNF-grammar restrictions, use DRY penalty, define custom stop tokens or token bias, control the order the samplers are applied, toggle if the BOS token is prefixed or not, edit the Models responses on the fly, change the chat template, adjust the RoPe-scale, choose the quantization datatype, etc...
- Most interfaces have some subset of these but rarely all. Some are pretty stubborn when I want to use the folder where I store all my models and not have them download the model again together with some stupid specific template file.
- text-generation-webui is a pretty crappy(Â∑ÆÂä≤ÁöÑÔºåÂæàÁ≥üÁöÑ) UI and can be quite the hassle(È∫ªÁÉ¶, ‰∏çÊñπ‰æø) to install and update, but the closest feature set relative to writing custom python code using llama.cpp or hf-transformers.

- Simplest "chat to LLM" is llama.cpp's server. The newest release.
  - It's 5 MB in size, has a well-looking web frontend, stores past conversations in browser memory. And is super lightweight and works fast.
- You say "simplest", but... one of these you can just download an app and double click it, and it's not llama.cpp. I have no trouble compiling llama.cpp myself, and I can also navigate HuggingFace to find a good model in the right quant and download that myself, but this is asking a lot of the average person.

- ## [What is your preferred front-end/back-end these days? (Q2 2024) : r/LocalLLaMA _202406](https://www.reddit.com/r/LocalLLaMA/comments/1dad0rh/what_is_your_preferred_frontendbackend_these_days/)
- Personal (single user, power-user) use: SillyTavern frontend, oobabooga's text-generation-webui backend (EXL2, HF). KoboldCpp backend if I need to run a GGUF for some reason (I prefer EXL2 for speed, especially with big contexts).
  - Professional (multi user, end-user) use: Open WebUI frontend, Ollama backend (simple) or vLLM/Aphrodite Engine (fast). Aphrodite Engine is a fork of vLLM that I prefer, supports more formats and is more customizable.
- I like to call SillyTavern the LLM IDE for power users: it gives you complete control over generation settings and prompt templates, editing chat history or forking entire chats, and offers many other useful options. There are also extensions for advanced features such as RAG and web search, real-time voice chat, etc. I love it and use it all the time. Once you learn it, you can use any backend, both local and online.
  - But it's not for everyone. Some just want a ChatGPT alternative, a simple chat interface, and advanced options would just confuse them. That's true for most users who aren't AI developers or enthusiasts, and that's who Open WebUI is ideal for. I run it as a local AI chat interface at work for my colleagues, while I prefer to use SillyTavern myself.

- I currently use koboldcpp for both back and front ends. I love how fast the team integrates upstream llamacpp fixes/additions. Has a multiuser batching function with nice GUI for the Mrs to use, and "just works" no matter what I throw at it.

- jan failed for me with codestral. It never answered back, tried multiple times, and different PCs as well. I can get llama3 8B just fine tho.

- I started with lm-studio but now i prefer librechat as frontend and ollama as backend. Supports pretty much all online apis and ollama. Supports a rag database and search. Is very simple to setup and i dont need to manage all the model setup (ollama just works) im just limited to the available models unless i write a modelfile myself.

- What i didnt like about lm studio but got with librechat:
  - using Local ai and online apis seamlessly in the same chat
  - not have to manage presets to run models correctly and efficiently
  - Search in chats
  - Rag database
  - TTS
- What i miss from using lm-studio:
  - accessing huggingface models directly
  - performance metrics
