---
title: lib-ai-app-community-model-toolchain
tags: [community, large-language-model, toolchain]
created: 2025-09-16T12:35:55.935Z
modified: 2025-09-16T12:36:12.968Z
---

# lib-ai-app-community-model-toolchain

# guide

- model with UD(unsloth dynamic quants)

- models-config
  - å¤§æ¨¡å‹çš„æµ‹è¯•ç»å¸¸éœ€è¦ä¿®æ”¹å‚æ•°ï¼Œæ”¯æŒä¸€é”®æ¢å¤é»˜è®¤é…ç½®æ›´å¥½

- tips-ai-tools
  - lm studioåº•å±‚ç”¨çš„ä¹Ÿæ˜¯llama.cpp, ä¸å¿…å¯»æ‰¾æ›¿ä»£ï¼Œæ·±å…¥åº•å±‚æ›´å®¹æ˜“æ›¿ä»£å’Œæ‰©å±•

- mlx
  - mlxçš„ä¼˜ç‚¹ä¹‹ä¸€æ˜¯æ–¹ä¾¿æ”¯æŒåœ¨iphoneä¸Šè¿è¡Œ

- ollama-pros
  - stable and rich models
  - å¯¹ embedding æ¨¡å‹é¢æ”¯æŒæ›´å¥½

- ollama-cons
  - å¯¹ mlx æ¨¡å‹çš„æ”¯æŒè½å

- lmstudio-pros
  - uiæ“ä½œä½“éªŒå‹å¥½

- lmstudio-cons
  - å¯¹mlxæ ¼å¼çš„ embedding æ¨¡å‹é¢æ”¯æŒæœ‰é—®é¢˜, ä½†ggufæ ¼å¼æ¨¡å‹æ”¯æŒå¥½
# lmstudio-xp
- not-yet
  - èŠå¤©å†…æœç´¢
  - æ ‡é¢˜åæœç´¢ï¼Œä¾¿äºæŸ¥çœ‹åŒ…å«æŸå…³é”®å­—çš„chats

- 
- 
- 
- 

# discuss-stars
- ## 

- ## 

- ## 

- ## [Is Q8 KV cache alright for vision models and high context : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1py4xp6/is_q8_kv_cache_alright_for_vision_models_and_high/)
- I don't have a mathematical evidence, but with KV at Q8 in all my cases qwen3 Vision couldn't do any tasks correctly in browser use cases with very low consistency, the same for picture tagging and descriptions.
  - The difference when observing logs is accuracy, where the model interprets small differences in inputs so differently they give the wrong outcome in terms of tool calls.

- Ruins the output even at a really short context (with gemma-3, pixtral-large, qwen2-72b-vl)

- I cant say anything about vision models as i haven't tested but when it comes to kv cache and llm's i never go below fp 16 because lowering the precision wrecks it. seems that trend would transfer over as well. but who knows maybe someone here sees things differently after experiments..

- ## ğŸ  [Scaling with Open WebUI + Ollama and multiple GPUs? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o9ssqd/scaling_with_open_webui_ollama_and_multiple_gpus/)
  - At our organization, I am in charge of our local RAG System using Open WebUI and Ollama. So far, we only use a single GPU, and provide access to only our own department with 10 users. Because it works so well
  - The final goal will be to provide all our around 1000 users access to Open WebUI (and LLMs like Mistral 24b, Gemma3 27b, or Qwen3 30b, 100% on premises).
  - To provide sufficient VRAM and compute for this, we are going to buy a dedicated GPU server, for which currently the Dell Poweredge XE7745 in a configuration with 8x RTX 6000 Pro GPUs (96GB VRAM each) looks most appealing atm.
  - However, I am not sure how well Ollama is going to scale over several GPUs. Is Ollama going to load additional instances of the same model into additional GPUs automatically to parallelize execution when e.g. 50 users perform inference at the same time? Or how should we handle the scaling?
  - Would it be beneficial to buy a server with H200 GPUs and NVLink instead?

- Instead of complicating and getting overkill hardware, just check and learn out vLLM. Its much suitable for these usecases.
  - yep vLLM just `--data-parallel-size 8` out the entire system, and works well with Ray cluster so it has internal model router and LB for multiple model. and it has KV store backend, so multiple GPU can access the same already compute KV Cache
  - although you can use NGINX as model router (as in api router, not internal of MoE model)

- We just went through this exact scaling problem at Anthromind - started with one GPU for our internal team, then had to figure out multi-GPU serving when other departments wanted in. Ollama doesn't automatically parallelize across GPUs the way you're thinking.. it'll load one model instance per GPU but won't split a single request across multiple cards.
  - For 1000 users you're gonna need to run multiple ollama instances behind a load balancer, each managing different GPUs. 
  - The H200s with NVLink would be overkill for just inference honestly - those RTX 6000s should handle it fine if you set up proper request routing. 
  - We ended up using a simple nginx setup to distribute requests across ollama instances, each pinned to specific GPUs.

- I use LiteLLM to distribute my LLM requests to different ollama instances, running on different servers. This will solve your scaling challenge of the GPU. Also make sure, your embedding is also using a LLM instance, afaik default is cpu embedding in openweb zu instance

- Maybe vllm or sglang with some load balancer, although a single RTX 6000 Blackwell may have enough FLOPS for 50 concurrent requests.

- ## [What is your primary reason to run LLMâ€™s locally : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1nsjwv1/what_is_your_primary_reason_to_run_llms_locally/)
- For me, it is independence from Big Tech and venture capital. Open weight models can never be taken away from us.

- New open weight models come out like every week and I want to be able to try the ones that can can run on my PC without waiting for a subscription service to add them or paying per use.

- ## ğŸ†š [what is the bottom line difference between GGUF and FP8? : r/comfyui _202512](https://www.reddit.com/r/comfyui/comments/1pu6t8e/what_is_the_bottom_line_difference_between_gguf/)
  - You should ask about regular FP8, scaled FP8, and mixed FP8, instead of comparing it to GGUF because those FP8 differences can affects the quality/accuracy vs the base BF16 model.
  - Even FP8_e4 vs FP8_e5 can generates a different things on the same prompt & seed.
  - Meanwhile, GGUF are designed for low VRAM with the ability to offload some part of it on RAM/CPU.

- [Comparison of Qwen-Image-Edit GGUF models : r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/1my3lq0/comparison_of_qwenimageedit_gguf_models/)
  - Based on those result FP8 is close to Q4_K_M

- so what is the difference between fp8 and scaled fp8 (have not come across weights labeled mixed fp8 yet)?

- FP8 (8-bit Floating Point): Uses 8 bits divided into a sign, exponent, and mantissa. It is a "native" format for modern GPUs (RTX 40-series and newer), meaning the hardware can do the math directly in this format without converting it first.
- GGUF (e.g., Q5/Q6): These use integer-based quantization with "K-quants" (block-wise scaling). The model is broken into small blocks, and each block has its own scaling factor. This allows Q5 (5-bit) or Q6 (6-bit) to often retain more intelligence than a simple 8-bit float because it "allocates" precision more intelligently where it's needed most.
- does it mean we should go for G8 over FP8 if we can, considering they're around the same size? I mean, if G5/G6 is better than FP8, then G8 should be even better, right?
  - yes

- from my personal experience, i'd go for a higher precision gguf over an fp8â€¦ i can't give u a technical explanation, but i think in a like for like quants, the gguf is a better option.
  - It may be better Quality but it will likely be a lot slower, as thats how .gguf works.

- there's no real difference between an fp8 safetensor and a Q8 gguf except that your gpu might natively accelerate the fp8. A gguf will unpack to the native data type your gpu supports (bf16/fp16/fp32). So that unpacking is negligible but not insignificant.

-Depends on your GPU if it's 40xx or 50xx series with fp8 native acceleration fp8 is better (at lest for Wan 2.2 and ZiT), despite the model being over your VRAM it would be faster. If you're with older Nvidia card Q8 and Q6 are faster. I've done tests on RTX 3060 12GB and 5070ti... and 5070ti is faster with the fp8 models. 

- ## ğŸ§© [The difference between quantization methods for the same bits : r/LocalLLaMA _202307](https://www.reddit.com/r/LocalLLaMA/comments/159nrh5/the_difference_between_quantization_methods_for/)
  - Using GGML quantized models, let's say we are going to talk about 4bit
  - I see a lot of versions suffixed with either 0, 1, k_s or k_m
- [k-quants Â· Pull Request Â· ggml-org/llama.cpp _202306](https://github.com/ggml-org/llama.cpp/pull/1684)

- K-quantization options are labeled "S", "M", and "L" and stand for small, medium, and large model sizes, respectively. 
  - Option "0" represents baseline quantization without extra calibration. 
  - In terms of quality and speed: 0 (lowest quality, fastest speed) < S < M < L (highest quality, slowest speed).

- k models are k-quant models and generally have less perplexity loss relative to size. A q4_K_M model will have much less perplexity loss than a q4_0 or even a q4_1 model.
- Generally, the K_M models have the best balance between size and PPL, so q3_K_M, q4_K_M, q5_K_M, etc. I like q5, and q4 best usually. 

- Aside from the number of bits per weight in a scaling group being obvious, here are the main differences:
  - Type _0 compression gives each group of weights a shared scale, but they are "symmetric" weights about 0. Type _1 weights add in a "bias" - an offset for each group of weights which allows them to be better resolved if they are mainly shifted away from zero. Type K is an enhancement in the way hierarchal groups are encoded to squeeze a little more compression into the mix. Finally, after the K we now have nothing, M, S and L variants - these actually refer to which tensors have the base precision. In the K_S models, all weight tensors have the stated precision. The simple K, K_M and K_L models specify varying amounts of weight tensors that will actually use higher precision to improve accuracy, typically 4-6 bits. This will no doubt keep expanding over time. Note that the PR referred to by u/lemon07r also contains descriptions of all the formats.

- With the older quantisation method, 4_0 is 4.5 bits per weight and 4_1 is 5 bits per weight.
  - The K quantisation methods are newer. Hopefully, they will get slightly better accuracy for roughly the same file size compared to the old methods.
- Thatâ€™s not correct. You will get the best speed with q4_K_S oder q4_K_M. This is because 3-bit and 2-bit needs more calculations.
  - If memory bandwidth is your bottleneck and not processor speed, then smaller is faster.

- [Is IQ4_XS closer to Q4 or Q3 in terms of quality? : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pk1zpq/is_iq4_xs_closer_to_q4_or_q3_in_terms_of_quality/)
- it was accepted that IQ4_XS beat all Q3 quants, as well as the legacy Q4_0 quants. Bartowski has a table on every model that he uploads where he mentions that IQ4_XS is comparable in quality to Q4_K_S but smaller.

- All IQ4_XS I've tried were worse than Q4_K_M at creative writing.

- i1 means imatrix, IQ is the quantization type you are asking about, don't mix these concepts up. You can see that anything about about 5 bits with imatrix is getting quite close to max possible accuracy, and imatrix IQ4_something is very good as well.

- ## [AMA with the Unsloth team : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/)
- Whatâ€™s your go-to quant for most models? I usually pick Q4_K_XL dynamic, but if I have enough VRAM, is there another Q4 youâ€™d recommend for better accuracy?
  - Yes correct, usually always got for the K_XL quants as they have the best ratios in terms of accuracy/speed/size etc 
  - My goto is probably Q3_K_XL as my laptop is incapable of handling anything larger

- ğŸ Do you plan to support Apple/MLX?
  - Yes definitely, it has been a super high request and we know there are soooo many Mac users out there so we'd be silly to not to. As for when, mmm to be honest maybe late this year? Unfortunately we are team constrained at the moment

- Any plannings on the TPU full integration?
  - It is possible yes but probably after MLX/AMD/Intel etc first
- I think they have it in the roadmap but I do not think anytime soon. I think it would be better for Unsloth if they are support Apple/MLX first and then TPU
  - Yes, AMD/Intel/ MLX will come first then TPU

- Iâ€™d like to use your model in a distributed llama cluster using all my old computer at home. Any planning?
  - We support multiGPU which might help with your setup but won't be officially announcing multigpu until maybe later this year as it's not up to the standard we would like!

- 
- 

- ## [Poor performance qwen3 235B 2507 mlx vs. unsloth variant : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mj7m20/poor_performance_qwen3_235b_2507_mlx_vs_unsloth/)
  - I just downloaded the Qwen3 235B 2507 instruct model for LmStudio on a M2 studio ultra. 
  - I got the MLX 4bit and Unsloth q4_0 versions. 
  - I am getting very low generation speeds on the MLX version ~0.3 tokens/s while on the other hand I am getting ~27 tokens/s on the Unsloth variant.

- Unsloth is highly optimized with advanced quantization for massive models, while MLX is known to have performance issues with models larger than ~22B parameters, which explains the speed difference you're seeing.
  - Where can I read more about MLX having problems with >22B parameters? First time hearing about it

- For unsloth would recommend q3_k_xl or q4_k_xl, should be more accurate than simple q4_o quantâ€¦ MLX should be faster, maybe the full model is not loaded to GPU?
  - Thanks. I double checked if all layers were loaded onto the GPU and that's the case, so I don't know what is the problem

- [Performance Qwen3 30BQ4 and 235B Unsloth DQ2 on MBP M4 Max 128GB : r/LocalLLaMA _202504](https://www.reddit.com/r/LocalLLaMA/comments/1kbacf2/performance_qwen3_30bq4_and_235b_unsloth_dq2_on/)
  - I was wondering what performance I could get out of the Mac MBP M4 Max 128GB
  - LMStudio Qwen3 30BQ4 MLX: 100tokens/s
  - LMStudio Qwen3 30BQ4 GUFF: 65tokens/s
  - LMStudio Qwen3 235B USDQ2: 2 tokens per second?
  - So I tried llama-server with the models, 30B same speed as LMStudio but the 235B went to 20 t/s!!! So starting to become usable

- ## ğŸ†š [MLX vs. UD GGUF : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1kpqrzz/mlx_vs_ud_gguf/)
- I benchmarked Unsloth's Qwen3-30B-A3B Dynamic 2.0 GGUF against the MLX version. Both models are the 8-bit quantization. Both are running on LM Studio with the recommended Qwen 3 settings for samplers and temperature.
  - MLX: 3, 516 tokens generated, 1.0s to first token, 70.6 tokens/second
  - UD GGUF: 3, 321 tokens generated, 0.12s to first token, 23.41 tokens/second
  - This is on an MacBook M4 Max with 128 GB of RAM, all layers offloaded to the GPU.

- UD q8 xl is not efficient for Mac. Use normal q8_0

- Isn't the entire point of using the UD XL GGUF for higher quality responses? If you were comparing for speed alone why not use the normal Q8 GGUF?
  - UD also uses less VRAM. I was hoping if it was comparable in speed, UD would be more resource efficient.
- They use less vram than 16 bit, not 8 bit. The goal is better accuracy through selective quantization so it should use more memory than standard q8. This is also why thereâ€™s XL in the name.

- Turn on flash attention if you havenâ€™t. I wish I could use MLX, it it faster but the output by comparison was worse by a margin Iâ€™ve never seen before between the formats. I have an M1 ultra 64gb. Itâ€™s even worse with Qwen3 32B

- In my experience MLX is only marginally faster, less than 10% usually; it has an edge when it comes to speculative decoding, though.

- ## ğŸ¤” [Genuine question: Why are the Unsloth GGUFs more preferred than the official ones? : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1ksw070/genuine_question_why_are_the_unsloth_ggufs_more/)
- They often write "getting started" blog posts along with their quants of popular models where they share insights. That's valuable to newcomers. 

- They waste their time on stuff so you don't have to. When some meta data is wrong or a model outputs gibberish for some reason, they check it out and update it with a fix. The other top uploaders aren't bad either and do the same I imagine if an issue get raised. But random uploaders, who knows. And the official model creators do weird shit on their uploads like require login tokens on HF because they don't want you to download from them.

- There was a nicely done test recently that showed that they (quants by unsloth, bartowski, mrademacher) are all good. There is no clear winner. 
  - However, the "official" quants were often released without imatrix or broken / different in some other way. That's why those unofficial quants are usually preferred.
  - Also, unsloth made large MoE models usable on non-server machines with their dynamic Q2_XXS quants.
- The biggest difference I would say isn't the quants, but rather our bug fixes for every model
# discuss-model-speed âš¡ï¸
- resources
  - [LocalScore - Local AI Benchmark](https://www.localscore.ai/)
    - LocalScore is an open benchmark which helps you understand how well your computer can handle local AI tasks.

- ## 

- ## ğŸ†š [When should you choose F16 over Q8_0 quantization? : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1q0ci23/when_should_you_choose_f16_over_q8_0_quantization/)
  - We've all read about how Q8_0 is "virtually indistinguishable" from F16 when doing inference.

- Every time I download native safetensors I'm amazed at just how good my Q4KL was the entire time. I don't ever recall being impressed by Q8 or BF16 over Q4KL.

- If it's a VLM I will choose F16, if it's text only model - Q8.
  - VLMs in higher precision have significantly better image understanding and bounding box accuracy.
- Do you mean the `mmproj`-part should be f16? Or also the LLM part?
  - I personally found a lot of success with even harsh quantization of the base model, but unquantized vision encoder, without much degradation in image understanding personally.
- I can't upvoter this enough. In llama.cpp I found huge accuracy improvements for some of my use cases by swapping the mmproj file to the F16 or F32 version, and increasing the number of tokens generated from each image. With those tweaks I can get away with running Qwen3-VL 8b instead of a much larger VL model. Especially when I'm using it as part of an agent system to just handle visual descriptions and letting another larger text only LLM handle final analysis and writing. This lets me balance my local agent deployments better and get much better token rates with almost no loss of analytical quality.

- PDE stuff is often done in FP64
  - Highly recurrent/compounding math, such as interest rate calculations or number theory, is often done in higher than 64 but such as 128 bit, 512 bit or 1024 bit etc

- In Agentic Coding you will ALWAYS see differences between FP16 and Q8.

- Got-oss should be left at f16 since itâ€™s already quantized
  - Vision encoders should also be left at f16
  - Otherwise q8 is great and even q4kl is very good.

- ## [Most used models and performance on M3u 512 gb : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1ouogiq/most_used_models_and_performance_on_m3u_512_gb/)
  - Overall GLM 4.6 is queen right now.
- Model: GLM 4.6
  - Token Gen speed: generally 10-20
Use Case: vibe coding (slow but actually can create working software semi-autonomously with Cline); creative writing; expository/professional writing; general quality-sensitive use
PP Speed: 4 bit MLX 50-70 t/s at large context sizes (greater than 40k)
- Model: Kimi K2 thinking
  - Token gen speed: 3ish to 2
Use case: idk it's just cool having a huge model running local. I guess I will use it for brainstorming stuff, medical stuff, other questionable activities like academic writing. PP speed/context size is too limited for a lot of agentic workflows but it's a modest step above other open source models for pure smarts
PP speed: Q3 GGUF 19 t/s (26k context) faster with lower context; 
- Model: Minimax-m2
  - Token gen speed: 40-50 at modest sizes
Use case: Document review, finance, math. Like a smarter OSS 120.
PP Speed: MLX 4 bit 3-400 at modest sizes (10k ish)
- Model: GPT-OSS-120
  - Token gen speed: about 80 at medium context sizes
Use case: Agentic searching, large document ingesting; general medium-quality, fast use
PP speed: 4 bit MLX near 1000 at modest context sizes. But context caching doesn't work, so has to reprocess every turn.
- Model: Deepseek 3.1:
  - Token gen speed: 3-20 depending on context size
Use case: Used to be for roleplay, long context high quality slow work. Might be obsoleted by glm 4.6... not sure it can do anything better
PP Speed: Q3 GGUF: 50 t/s

- Iâ€™m ngl 4.6 running around 10-20tps is kinda disappointing for a $10k+ computer when you can run the same model on cpu for 2-3tps on a $1500~ ddr5 rig (pre price jump).
  - Don't get me wrong Iâ€™d still love those speeds, but idk if thatâ€™s worth spending an extra $500 per token in extra speed (at least for my use case); definitely reshapes my perspective of everything.
  - if the main reason of buying an expensive computer is to run big models faster, itâ€™s a valid metric; how much value are you getting out of a $10k computer when a computer at 10% of the cost can do the same thing, just barely slower. Iâ€™d want to see at least 30-40tps out of a $10k computer.

- For comparison 12 3090 gets me 12k prompt processing with VLLM and 20 tokens per second for GLM and Minimax.

- ## [M2 Ultra Mac Studio with 64GB RAM and Ilama3.1 70b : r/ollama _202410](https://www.reddit.com/r/ollama/comments/1g0gycc/m2_ultra_mac_studio_with_64gb_ram_and_ilama31_70b/)
- I have M3 Max with 64 GB RAM and it can handle 70B (4 bit quantized) models. However, in my case, inference with 70B models isnâ€™t at a comfortable speed.
  - Hereâ€™s the benchmark for Qwen2.5:72B with Ollama and MLX-ML:
  - â€¢ MLX (mlx-community/Qwen2.5-72B-Instruct-4bit): 8.14 t/s
  - â€¢ Ollama (Qwen2.5:72B): 6.95 t/s
  - â€¢ Ollama (Gemma2:27B): 19.39 t/s
- How much RAM does it use to run that 70B model?
  - About 55 out of 64Gb. I have the browser open at the same time, but it doesnâ€™t slow down web browsing.

- ## [Any m3 ultra test requests for MLX models in LM Studio? : r/LocalLLaMA _202503](https://www.reddit.com/r/LocalLLaMA/comments/1jdvk7c/any_m3_ultra_test_requests_for_mlx_models_in_lm/)
  - Got my 512 gb. Happy with it so far. Prompt processing is not too bad for 70b models -- with about 7800 tokens of context, 8 bit MLX Llama 3.3 70b processes at about 145 t/s per second. It then generates at about 8.5 t/s.
  - 70b 8 bit llama 3 at 7800 context: 8.5 t/s generation; 150 t/s prompt eval; 
  - 70b 4 bit llama 3 at 7800 context: 15.5 t/s generation; 150 t/s prompt eval; 
  - 70b 4 bit llama 3 fine-tune at low context with speculative decoding (and coding related prompt): 23.89 t/s generation; 
  - Gemma 3 27b 4 bit at 2317 context: 331 t/s processing; 29.20 t/s generation
  - Gemma 3 27b 8 bit at 3310 context: 255 t/s processing; 18.71 t/s generation 

- ## [M3 Ultra Mac Studio Benchmarks (96gb VRAM, 60 GPU cores) : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1kvd0jr/m3_ultra_mac_studio_benchmarks_96gb_vram_60_gpu/)
  - I loaded each model freshly in LMStudio, and input 30-40k tokens of Lorem Ipsum text (the text itself shouldn't matter, all that matters is token counts)
  - Input Context Size (tokens): 32k ~ 40k
  - Model Name & Size	Time to First Token (s)	Tokens / Second
  - Mistral Large 123B (4-bit/dense)	900.61	7.75
  - Gemma 3 27B (4-bit)	108.15	29.55
  - Qwen3 30b-a3b (8-bit)	67.74	34.62

- I have the same machine as the OP (96GB, 60 cores) and am running Qwen3-30B-A3B 8bit and Qwen3-32b 6bit concurrently - great combo to use in Aider architect mode. Which two models have you chosen to work with in Roo Code? What has been your experience?
  - I typically use Qwen3 32b as Orchestrator and Architect, Qwen 2.5 32b 128 K as coder and debugger. I use Unsloth versions of all. They can handle certain projects just fine. Especially languages like python. If I run into issues, I mix in deepseek r1 or v3 from openrouter.

- Are you generally satisfied? Or would you rather have the 256GB version? Or the one with 80 GPU?
  - I have the 256 GB version with 60 cores. I would go for the 80 core if given the choice again. Every little bit helps with inference speed. However I can load several 32b 8 bit models concurrently which is great for things like orchestrator mode in Roo Code. Everything works, just could be faster.
- I have the 256 / 60 too. Iâ€™m not sure that the 80 would make such a big diffÃ©rence. With lama4 scout it would probably amount to 25 secs of pp time gain in the exemple given. But you still have to wait 80 seconds, which makes it too slow for conversational infÃ©rence.

- I also have the 96GB/60 core. I am just a casual user and I couldn't justify another 2000â‚¬ for 256GB Ram or 80 core. And I think 256GB is not worth it for my purpose. I can use dense models up to 70B (at Q5) for chatting. Mistral Large and Command A (at Q4) are okayish but everything larger will be way too slow. So the only benefit of 256GB is for MoE models.
  - Shortly after I bought mine, Qwen3 235B A22B came out. Right now, this is the only reason (for me) wanting 256GB. But is it worth 2000â‚¬? No, not right now. If that model becomes everybodies darling for finetuning, then maybe.

- Can you explain how to load several models?
  - I use LM Studio, and you just keep loading models until youâ€™ve either loaded all of the ones that you need, or when you reach 85% of your memory capacity. Itâ€™s a good practice not to fill more than that.

- That's quite slow, on my 2x3090 I have
  - google_gemma-3-12b-it-Q8_0 - 30.68 t/s
  - Qwen_Qwen3-30B-A3B-Q8_0 - 90.43 t/s

- ## ğŸ†š [[Benchmark] Quickâ€‘andâ€‘dirty test of 5 models on a Macâ€¯Studioâ€¯M3â€¯Ultraâ€¯512â€¯GB (LMâ€¯Studio) â€“ Qwen3 runs away with it : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1kfi8xh/benchmark_quickanddirty_test_of_5_models_on_a_mac/)
  - M3â€¯Ultra, 128â€¯CPUâ€¯/â€¯80â€¯GPU cores, 512â€¯GB unified RAM
- Model	Quant. / RAM footprint 	Speed (tok/s)	Tokens out	1stâ€‘token latency
  - MLX Deepseekâ€‘R1â€‘4bit	402.17â€¯GB	16.55	 2â€¯062	 15.01â€¯s
  - MLX deepseekâ€‘V3â€‘0324â€‘4bit	355.95â€¯GB	19.34	 755	17.29â€¯s
  - MLX Qwen3â€‘235â€‘A22Bâ€‘8bit	233.79â€¯GB	18.86	 3â€¯096	 9.02â€¯s
  - GGFU Qwen3â€‘235â€‘A22Bâ€‘8bit	â€¯233.72â€¯GB	14.35	 2â€¯883	 4.47â€¯s
  - MLX Gemmaâ€‘3â€‘27bâ€‘itâ€‘bf16	 52.57â€¯GB	11.19	 1â€¯317	 1.72â€¯s
- TL; DR
  - Qwen3â€‘8bit dominates â€“ PhDâ€‘level answers, fast enough, reasoning quick.
  - Thinking time isnâ€™t the bottleneck; quantization + memory bandwidth are (if any expert wants to correct or improve this please do so).
  - Macâ€¯Studio M3â€¯Ultra is a silenceâ€‘loving, powerâ€‘sipping, tiny beastâ€”just not the rig for GPU fiends or upgrade addicts.

- Useful test as far as the raw numbers, but not an eval. Not even for quick and dirty. You can give a model the same prompt and get a "bad answer" once, then an amazing answer the next time. This is why some of the tests will run the same prompt 3 or 5 times.

- One thing I'd like to know is how you compare the MLX vs GGFU performance (for same models)?
  - I just run a simple test, based only on that I would only use ggfu if a MLX doesn't exist. It gave me almost the same output, definitely same quality, but almost 30% more fast.

- ## [M2 Ultra 192 gb + GLM Air 4.5/4.6 for local coding agents? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o7k78o/m2_ultra_192_gb_glm_air_4546_for_local_coding/)
- M2 Ultra 192 GB Running GLM 4.5 Air Q8_0 with flashattention in llama.cpp. 10k token prompt, responding with ~400 tokens.
  - prompt eval time =   29693.60 ms / 10565 tokens (2.81 ms per token, 355.80 tokens per second)
  - eval time =   14221.61 ms /   374 tokens ( 38.03 ms per token, 26.30 tokens per second)

- 4.6 at 4bit will need more than 192gb vram, you could use a 3 bit quant though.

- ## [GLM 4.6 already runs on MLX : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1nujx4x/glm_46_already_runs_on_mlx/)
- My Epyc workstation has 12 RAM channels and I have 8 sticks of 16GB each so I'll max at 192 GB sadly. To run this you'll want 12 sticks of 32 GB to get to 384GB. The RAM will cost roughly $2400.
  - I have DDR5-4800 which is the slowest DDR-5 (base JDEC standard) does 38.4GB/s
  - DDR4-3200, the highest supported speed on EPYC 7003 Milan, does 25.6 GB/s.
  - If you use DDR5-6400 on a 9005 series CPU it is roughly twice as fast. But the new EPYC processors support 12 channels vs 8 with DDR4, so you get an additional 50% bump.

- what's the prompt-processing speed? It sucks to wait 10 minutes every request.
  - As lim context->infinity, pp rate is proportional to attention speed, which is O(n2) and dominates the equation
  - Attention is usually tensor fp16 non-sparse, so 142 TFLOPs on a RTX 3090, or 57.3 TFLOPs on the M3 Ultra.
  - So about 40% the perf of a 3090. In practice, since FFN performance does matter, you'd get ~50% performance.
- Here is my M2 Ultraâ€™s performance: context/prompt: 69780 tokens Result: 31.43tokens/second, 6574 tokens, 151.24s to first token. 
  - Model: Qwen-Next 80B at FP16
  - That is 500/s, but using full precision sparse MoE.
  - About 300/s for a dense 70b model, which you are not using to code. It will be faster for a 30b dense model which many use to code. Same for a 235billion sparse MoE, or in the case of GLM4.6 taking up 165gb, it is about 400/s. 

- CLine/Roo regularly uses up to 100k tokens on the context, it's slow even with GPUs.

- ## [I know nobody has asked before, but what is GLM 4.5 like on an M3 Ultra? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1n80nxn/i_know_nobody_has_asked_before_but_what_is_glm_45/)
  - Google claims m3 ultra ram bandwidth is barely slower than 3090 bandwidth. Does this mean I would get barely lower tps compared to a machine with 20 x 3090?
- I just tested 4bit MLX quant with simple task of creating calculator in single html page. This is a standard test I run to see how the model works with one shot tasks that are very well specified.
  - Compared to Air (I tested 4, 6 and 8 bit MLX quants there), it provided better working result on the first prompt.
  - Mac Studio M3 ultra with 256GB ram
  - 19.08 tok/sec â€¢ 10013 tokens â€¢ 14.29s to first token

- People suggesting Macs for LLMs are often not mentioning that they can have prompt processing times of ten minutes for every single large prompt. Personally I think that it is rather negligent to not mention this to people.
  - I have an M3 Ultra and based on how slow things are with any non-trivially sized context length, I am not even bothering to try a model as big as GLM 4.5
- Not to mention switching models to test different parameters. It is only strictly better than pure RAM setup

- 10 minutes would imply a prompt of around 200, 000 tokens. Is that actually a common use case for you?
- You need to factor in the prompt processing speed of the model/gpu and how big the context is, because pp speed slows down as context gets bigger.
  - Reaching 10 minutes of pp time is pretty easy on my 512 m3u, for example 60k of context with gguf deepseek.
  - But for smaller models it can be quite decent. Mlx gpt oss will be maybe only 1-2 minutes for that same prompt.

- M3 Ultra RAM bandwidth is 819.2 Gb/s while 3090 RAM bandwidth is 936 Gb/s 
  - So itâ€™s probably true that for token generation, which is largely RAM bandwidth limited, that the M3 Ultra is â€œcomparableâ€ to a 3090
  - Whatâ€™s actually different here is prompt processing speed which is compute bound.  In this realm a 3090 would be many multiples faster.  

- M3 Ultra, 512 GB RAM, 32/80-core variant.
  - I have a script processing files roughly 30-50k tokens in length, which I cache, and then ask subsequent questions of. I just fired it up on GLM4.5 4 bit MLX quant. For a 35000 token document, prompt processing took ~247 seconds. In subsequent turns of conversation generation speed was 10 tokens per second roughly speaking.
  - GLM4.5 Air, same document was ~104 seconds for prompt processing, and then generation is at 30 tokens per second or so.
# discuss-quantized
- ## 

- ## 

- ## 

- ## 

- ## ğŸ†š [MiniMax M2.1 quantization experience (Q6 vs. Q8) : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1q3579f/minimax_m21_quantization_experience_q6_vs_q8/)
  - I was using Bartowski's Q6_K quant of MiniMax M2.1 on llama.cpp's server with Opencode and it was giving me some very strange results.
  - The usual way I test coding models is by having them write some of the many, many missing unit tests.
  - I stepped up to Q8 just to see and it nailed everything on the first try with a tiny fraction of the tokens.
  - That's a small sample size and there's always the possibility of a random outcome. But, wow, yikes, I won't be trying Q6 again in a hurry.

- Minimax does not quantize like other models, and is native FP8. You can try my NVFP4 if you have Blackwell GPUâ€™s

- I had that happen with Q8 KV cache. Not worth it.

- Did you try tweaking the settings, such as setting a repetition penalty, or other settings that are designed to reduce the amount of rambling a model does. A q6 should be virtually identical to a q8, there's a good chance it was just random. I always use rep pen of 1.05 for the qwen model I use, and have never encountered rambling with it set.
  - Also you're using a quant with an imatrix, and imatrices can mess up reasoning chains. I don't ever use imatrix quants for reasoning models. And at q6 or q8 you don't need an imatrix quant, it won't make the model any better than without.
- Im pretty sure unsloth uses imatrices. I don't have any references, it's from my own experience and I also remember bartowski talking about it in a thread once, which suggests it's a known issue. I normally use mradermachers non imatrix quants.

- ## ğŸ†š [Benchmarks for Quantized Models? (for users locally running Q8/Q6/Q2 precision) : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pyrjke/benchmarks_for_quantized_models_for_users_locally/)
- Nope! Definitely a need for such a thing. Quantization has been around for a while but is still the wild-west of LLM's in terms of documenting the results/impact.

- Most of the benchmarks are too annoying or slow to run, unfortunately. MMLU-Pro compsci only isn't too terrible and Aider 'Polyglot' limited to python only (or whatever your preferred language is) is ok, too.
- [Unsloth Dynamic GGUFs on Aider Polyglot | Unsloth Documentation](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot)

- Devstral-Small-2-24B-Instruct-2512-Q8_0.gguf Lm-studio community VS Devstral-Small-2-24B-Instruct-2512-UD-Q4_K_XL.gguf Unsloth
  - It's a dense model, but it all fits comfortably in a 96GB RTX 6000, even with very long runs. The card is set to 450W instead of 600W.
  - The problem is simple: The Q4_K_XL version of unsloth returned the modified HTML file but without some of the required fixes, while the Q8 version solved every problem.
  - Obviously, it's only one test, but I definitely prefer the slower execution speed and greater precision. Now I'll try the Q6.

- I found MXFP4 definitely superior to Q4 across a variety of models considering their applied weights (MXFP4+F32), even better than Q8 in the few cases I tried - at the exception where GPT-OSS-20B MXFP4 was not as good as Unsloth slightly heavier F16.

- Primary CUDA maintainer for llama.cpp/ggml here, given enough time I'll eventually do it for quality control and publish the results here https://github.com/JohannesGaessler/elo_hellm
# discuss-llm-tools-tips/tricks
- ## 

- ## 

- ## 

- ## [New functiongemma model: not worth downloading : r/ollama](https://www.reddit.com/r/ollama/comments/1pq69o5/new_functiongemma_model_not_worth_downloading/)
  - I have a valid MCP toolset that works great with other very small models such as qwen3:1.7b. I obtain quite reliable function calls. So, an even smaller model that could do this with the same quality sounds great. 
  - I downloaded the functiongemma:270m-it-fp16 version of 552MB and deleted after the second test
- I believe the purpose of this model is to be fine-tuned further on your specific tool use case.

- In practice Iâ€™ve had the best luck with models that were explicitly trained for tool use / JSON (and I still add strict schemas + validation + retry).
  - If you want to stay local, Iâ€™d try Qwen2.5-Instruct (7B) or Llama 3.1 Instruct (8B) as a baseline and then see how far down you can compress without breaking tool calls.

- Itâ€™s a 270m parameter model. MCP dumps massive JSONs. Comparing it to Qwen1.7b is literally 6x its size. I think this model is good for small, focused use cases - not something like MCP.

- ## ğŸ’¡ [Suggestions to improve RAG with 200+ pdfs : r/Rag _202412](https://www.reddit.com/r/Rag/comments/1h8205b/suggestions_to_improve_rag_with_200_pdfs/)
- Iâ€™m working on a company where we are running a RAG for a company with more than 4.000.000 documents (more than 200.000.000 embeddings) so this might help you.
  - We start with an ingestion process where all PDFs or documents (regardless of source) are converted into markdown using LLMSherpa, an open-source library weâ€™ve fine-tuned for our needs. The markdown is then subdivided into sections, which are further broken down into smaller chunks. These chunks undergo a context retrieval process to add additional relevant information before being stored.
  - Next, we generate embeddings for these chunks using BAAI/bge-m3 and store them in a PostgreSQL database managed via pgvector.
  - When a user interacts with our playground, they send a message, which includes the chat history. To handle this, we first make an initial LLM call to rephrase the prompt and incorporate additional context. This step is crucial for questions referencing prior chat history, ensuring clarity and proper fine-tuning before the main LLM call.
  - The rephrased prompt is then matched against all the stored chunks in our database using inner product similarity, with a predefined threshold to filter the most relevant chunks. After retrieving these, a reranker further narrows the results to the top 50 most relevant chunks.
  - Finally, the rephrased prompt and the refined chunks are sent to the LLM, which generates the userâ€™s final answer. To ensure fast and efficient database queries, we use HNSW (Hierarchical Navigable Small World) indexing, enabling low-latency and scalable retrieval.

- Converting the PDFs to images and using an LLM like Claude 3.5 Sonnet to convert it with custom instructions works quite well, though it's slower and pricier. I convert all my PDFs like that.

- ## [Best AI to search in large folder of PDFs : r/AI_Agents _202503](https://www.reddit.com/r/AI_Agents/comments/1j1pum1/best_ai_to_search_in_large_folder_of_pdfs/)
- You donâ€™t want an AI, you want a robust indexing and search engine that feeds relevant docs/chunks to an LLM for a simple summarization. You can do that with RAG, vector DB, or just old school ELK stack. Plenty of options.

- Notebooklm can take up to 50 files. And it has the great podcast summary.

- I have efficient semantic chunking + STORM + meilisearch integration specifically for this problem.

- ## [How does having a very long context window impact performance? : r/LocalLLaMA _202507](https://www.reddit.com/r/LocalLLaMA/comments/1lxuu5m/how_does_having_a_very_long_context_window_impact/)
- Basically, the longer the context window the more memory is needed to store the model and the KV cache. The more memory the model takes up, the longer it will take for the GPU to read the model from memory.
  - keep in mind many models claim 128K context length, but their ability to recall information deep within a block of text will degrade as it fills up. QWQ appears to be one of best models for long context recall and creative writing.

- You should not max out the context window. You should be extracting what's valuable and reinjection it when necessary.
  - This is a lot more complex than it seems. But context summarization into RAG and MCP servers probably help get you there.

- My experience with Gemma3 27B is that speed fell off the cliff once the context is more than 4 windows (ie over 4K). Iâ€™m using a 5070ti and I get 30t/s using Q3KM model under 4K. Beyond 4K context it requires a lot more compute to look back and remember things is my understanding. So you get loooong prompt processing time. In my case itâ€™s 5t/sâ€¦ Probably better off using Mistral Small which is what Iâ€™m doing, something a bit longer context and less cliff.
  - You can also write in auto-summarizing function just before it hits context limit. Or use something that has it. And when the sessions get very long use RAG.

- Gemma 3 had a very memory heavy context. Q4 quant with 32k context was barely fitting into 32gb vram. But people praised it for good attention to context. Then Google "fixed" it, introducing SWA, which made it lightweight, but after this I've seen many complaints about the model forgetting things very fast.
- Gemma3 does not understand long context that well. Even at 8k it is becoming quite inconsistent (contradicting what was in context before).

- ## [LLM with large context : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1kdnbhj/llm_with_large_context/)
- The current mainstream open-source LLMs have a context length of around 128K, but there are already some options that support longer contexts (Llama4, Minimax-Text, Qwen2.5-1M). 
  - However, the GPU memory overhead for long contexts is substantial. For example, Qwen2.5-1M-7B mentions that it requires approximately 120GB of GPU memory to deploy a model supporting a 1M context. 
  - It's difficult to fully run a model with a 1M context locally. However, such models might perform better than regular models in tasks requiring longer inputs(64K-128K) refer to Qwen2.5-1M.
- A significant issue with using long-context LLMs is that most LLMs' long contexts are extrapolated (for instance, Qwen-2.5 has a pre-training length of 4K â†’ long context training of 32K â†’ Yarn extrapolation to 128K, and Llama3.1 has pre-training up to 8K â†’ Rope scaling extrapolation to 128K), and only a small amount of long-context data is used during training. 
  - As a result, performance may degrade in actual long conversations (I believe most models start to degrade above 8K length, and performance notably worsens beyond 32K). 
  - Of course, if you only aim to extract some simple information from a long text, this performance degradation might be acceptable.

- Attention mechanism right now just don't handle large context very well. If there's too much hard distractors within context, the model just won't do too well.

- But fan of Qwen 3 8B or 32B. You can fit 128K with model in 24GB of VRAM, but you will have to trade Q8 for Q4 for KVcache on the 32B model. 

- 32k is where all models degrade, even if stated otherwise.
  - Qwen 3 are better ones though.
  - There is also Llama 3.1 8b Nemotron 1M, 2M and 4M; I had mixed success with them - they are strange, weird models, but handle context well.

- 1m context's kv cache takes too much vram for local use case. https://www.reddit.com/r/LocalLLaMA/comments/1jta5vj/vram_requirement_for_10m_context/

- [How large is your local LLM context? : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1j6xpvt/how_large_is_your_local_llm_context/)
  - Most local LLMs are massively degraded by 32K context. Both token quality and generation speed. I would say there's no point going over that, and you should try not to even get close. You have to do more work to fit in only the relevant context but that's the tradeoff with going local
- No I don't unless it's required. Having large input tokens impacts the throughput, so it's better to optimize the input tokens length.

- ## ğŸ†š [Comparing Unsloth's GLM-4.6 IQ2_M -vs- GLM-4.6-REAP-268B Q2_K_XL : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1ozq14d/comparing_unsloths_glm46_iq2_m_vs_glm46reap268b/)
  - During my limited coding testing I'm seeing:
  - REAP_Q2_K_XL sometimes perform better but fail more often, including sometimes looping and some broken code outputs.
  - Full_IQ2_M retains more general and contextual knowledge and seems more consistent, but perhaps less chance of a great output.

- Tested REAP vs IQ a lot. IQ always better. Minimax M2 ---> same
- I had the same experience with GLM 4.6 and GLM 4.5 air. Eventually I think reap will work but at the current stage I view it as a tech demo.

- The tradeâ€‘offs you describe mirror the differences between quantization and Mixtureâ€‘ofâ€‘Experts pruning at a systems level.
  - A Runpod overview notes that moving from FP32 to INT8 or 4â€‘bit quantization can cut memory use by 60â€“80â€¯% while preserving over 95â€¯% of model accuracy. 
  - By contrast, REAP (Routerâ€‘weighted Expert Activation Pruning) evaluates the routerâ€™s gate values and expert activation norms to remove lowâ€‘impact experts, achieving nearâ€‘lossless compression even after pruning 50â€¯% of experts on codeâ€‘generation tasks.

- All the reaps I tried were slower and dumber. Then again i didn't use them for code as intended.

- Far as roleplay goes, I personally find that REAP loses a ton of flavor and personality. It just doesn't feel good.

- ## [What tools or workflows save you hours every week? : r/PKMS](https://www.reddit.com/r/PKMS/comments/1p00u1x/what_tools_or_workflows_save_you_hours_every_week/)
- A digital file cabinet (PKMS) to store/organize my notes/documents/files
  - For enhanced features, I use pkms app Devonthink; accessed with a Mac and iPad
  - Integrated scripting for workflow automations
  - I use AppleScript on my Mac

- Knowledge management: Mostly Note app + NotebookLM for long and hard PDF (I put that into the app and it turns that to podcasts!)
  - Daily planning: Brain dump + Saner: I basically just offload my thoughts and the app turn it to tasks with reminders automatically

- Zettelkasten for idea incubation/research/serendipity, PARA as archiving method, GTD/ZTD for running tasks. A personal combination of them

- ## [Best small LLM (â‰¤4B) for function/tool calling with llama.cpp? : r/LocalLLM _202505](https://www.reddit.com/r/LocalLLM/comments/1kdva3y/best_small_llm_4b_for_functiontool_calling_with/)
- So far qwen3 is really the only game in town for consistent tool calling for me at small sizes. 
  - when I test I do not tell the model exactly what tools to use and try to keep my prompts sort of vague because I want to be able to ask for something without for example knowing a table name in a database.
  - Iâ€™ve only really tried 4b and up. I had downloaded 1.7b and it worked like once out of the 3 runs I tried with it. Iâ€™d imagine a smaller model would do worse. If youâ€™re very instructionally verbose it may work better though.
  - 4b, 8b, 14b, 32b all call functions really well and consistently.
  - 8b, 14b, 32b can digest the returned agent information and transform it.
  - 14b, 32b can transform it well and provide better context.
  - 32b is not noticeably better for agentic at least for my use cases than 14b
  - Sweet spot for me is 8b/14b. Iâ€™ve used 8b extensively. It fails like 10% depending on instruction vagueness and how strict I am with temperature.

- Everyoneâ€™s talking about Qwen which makes sense due to its recent release but for an alternative, Iâ€™ve had good success with the Gemma 3 4B and 12B models. Once you get around the Google ReAct logic itâ€™s pretty manageable and it seems to be smart enough for my use cases

- ## [Cerebras REAP'd GLM4.6: 25%, 30%, 40% pruned FP8 checkpoints on HF! : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/)
  - The models deployed in Cerebras prod inference API are not pruned, and we don't have such plans for GLM4.6. The REAP pruning work is for research purposes and to give more efficient models to the community

- The MoE is sparse, yes. But the REAP technique appears to leverage the fact that â€œexpertsâ€ are more or less just certain parameters highly correlated with other parameters. Itâ€™s not like thereâ€™s a programming expert, a language expert, etc. itâ€™s more like you have some number of tokens that seem to be the strongest correlated to other sets of tokens, so an invocation of that token or closely related tokens results in that particular part of the neural net lighting up.

- I actually have the opposite question. Can we reduce the number of experts from 8 to 6 or 4 and still maintain good performance. Experts are largely CPU bound and would go a long way in speeding up the models.
  - my experience with qwen3 30b a3b
  - with 7 things are ok , but it fails sometimes (i would say 10% of the times you noticed is worse than the default)
  - with 6 it fails more often
  - with 4 it lose coherence in the majority of times
  - but increasing also doesnt help a lot
  - with 9-10 i barely see an improvement over 8
  - with 12-16 the output gets worse

- ## [I did not realize how easy and accessible local LLMs are with models like Qwen3 4b on pure CPU. : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o1z3hj/i_did_not_realize_how_easy_and_accessible_local/)
  - I hadn't tried running LLMs on my laptop until today. I thought CPUs were too slow and getting the old igpu working (AMD 4650U, so Vega something) would be driver hell. So I never bothered.
  - I downloaded LM Studio, downloaded Qwen3 4b q4, and I was getting 5 tok/sec generation with no hassle at all with the automatic Vulkan setup. Not bad
  - Then, just to be sure, I disabled the GPU and was surprised to get 10 tok/sec generation with CPU only! Wow! Very usable.
  - This means I can just buy any used laptop off ebay, install linux, and go wild??? It has a built in monitor, low power draw, everything for $200-300? My laptop only has DDR4-3200, so anything at that speed or above should be golden. Since async processing is fine I could do even more if I dared. Maybe throw in whisper.

- Qwen3-4B is really a good choice for 16GB laptops (common choice for general consumers). I use it for local PDF rag and it can provide me with accurate in-line citations + clear structured report.

- Or you can just run it much faster with a $60 GPU and have your low power kitchen computer connect to that via wifi.

- ## ğŸ†š [8B models at full-size, or 32B models at Q4? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1o13oyn/30b_models_at_fullsize_or_120b_models_at_q4/)
- Q4 isn't much of a degradation and the base is so much stronger.
- The degradation depends on the models original size and architecture as MOE can be more sensitive but overall I would always pick the larger models when possible so long as you still have enough space for context.

- ## [Mixed precision KV cache quantization, Q8 for K / Q4 for V : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1kddcdp/mixed_precision_kv_cache_quantization_q8_for_k_q4/)
- Plenty of KV cache quantization literature suggest K is more sensitive to quantization than V (SKVQ, QAQ, etc.
  - No one can really answer whether K8V4 is good enough as this is highly model-task-setting-whatever else dependent, 
  - but conventional wisdom suggests: Granting Key cache more budget is generally the right move.
  - You probably need some kind of "fancy" KV cache quantization techniques â€” like our KIVI, 

- Yes.. I try it all the time. 4/4 is really bad, 8/4 is as low as I'm willing to go. Don't forget to compile llama.cpp with all kernels.

- ## [Does Q4-8 'KV cache' quantization have any impact on quality with GGUF? : r/LocalLLaMA _202409](https://www.reddit.com/r/LocalLLaMA/comments/1flw4of/does_q48_kv_cache_quantization_have_any_impact_on/)
- Yes in my test. I was testing the summarizing ability of llama3.1 8B q5_ks on oobabooga a while back on youtube transcripts.
  - With q4 kv cache the accuracy was about 82%. (something like that) I think even q8 had a performance drop that was unacceptable. (if I remember correctly)
  - Without cache quantisation bullet point accuracy went up to 97.6% accuracy. This was over at least 5 youtube videos of 2-12 minutes each. (If I remember)

- Q8 is pretty painless, but Q4 can be pretty rough, although is usually usable. Smaller models will feel it worse. Just like with model quantization.

- Yes, I noticed some larger models suffer even with q8 so I don't even bother using it

- ## [Recommendation for getting the most out of Qwen3 Coder? : r/LocalLLM _202508](https://www.reddit.com/r/LocalLLM/comments/1mrzi5x/recommendation_for_getting_the_most_out_of_qwen3/)
- Setting K quantization to q4_0 has a pretty big impact on modelâ€™s quality of output. 
  - K is much more sensitive to quantization than V. 
  - You can get away with q4_0 for V but for K the lowest iâ€™d go is q5_1 and ideally q8_0

- [Why is my llama so dumb? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ljn4h8/why_is_my_llama_so_dumb/)
  - I know a lot of models don't like going that small. Try upping that to Q8_0 or even fp16/bf16.
  - Q4_0 is a fairly very aggressive quantization. Quantization noise leads to loops.
  - Ive personally never used a model that didnt have a noticable decrease in quality at q4, often even at q8. Just leave it at fp16.

- [What's the best models available today to run on systems with 8 GB / 16 GB / 24 GB / 48 GB / 72 GB / 96 GB of VRAM today? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1k4avlq/whats_the_best_models_available_today_to_run_on/)
  - I wouldn't go as low as q4_0 for K cache, quality impact is quite noticeable. q4_0 on V cache seems fine though.
  - I use in larger models (nemotron 253B, deepseekv3 03-25) ctk q8_0 and ctv q4_0, haven't noticed much differences vs fp16 (on deepseek I had to test at like 4k ctx lol)

- ## [LM Studio released new version with Flash Attention - 0.2.22 : r/LocalLLaMA _202405](https://www.reddit.com/r/LocalLLaMA/comments/1cir98j/lm_studio_released_new_version_with_flash/)
- Must be because llama.cpp merged it recently.
  - Yes, from the changelog
- ROCm version already released you need to get the link from LMstudio discord sever

- The flash attention benefit becomes noticeable at LONG LONG contexts - at 26k, its the diff between 30 tok/sec and 10 tok/sec w/ LLama 3 8B on an M3 Max.

- the difference shows up at long contexts.

- Quick check for me didn't see any difference between flash attention on and off for llama3 8b. On a Macbook pro
  - Made huge difference for me on m2 max Mac studio in terms of ftimr to first token and general response rate for decently sized contexts. Details are also remembered much better
- Try with more context, or models like Mixtral. In llama.cpp (which LMStudio uses) I get about 4+ tokens a second higher and no real downside. I can store a lot more context at the same time.

- Flash Attention works on Mac? I always thought CUDA only.
  - Not only does it work it seems to be giving a bigger performance boost, at least on some quick tests

- ## ğŸ†š [weighted/imatrix VS static quants? : r/LocalLLaMA _202405](https://www.reddit.com/r/LocalLLaMA/comments/1ck76rk/weightedimatrix_vs_static_quants/)
- Imatrix quants were introduced a couple of months ago and are recommended over static quants because they have better output quality. 
  - For example, a Q4_K_M quant made with imatrix should be closer to a Q5_K_M non-imatrix quant in quality.

- Importance matrix is used to choose vocabulary to preserve precision for.
  - Both normal Q_k quants and IQ quants can use imatrix.
  - IQ quants, I don't know the expansion of the I, but they attempt to approximate the original value at runtime. They do extra math, and slow down CPU inference but GPUs generally are still memory speed limited.

- Are there any disadvantages? I usually go for Q4k_m and tried iq4_nl or something, the IQ is slightly smaller in file size but inference speed seems to be basically the same. If imatrix is better why do people still release/use static? 

- [Static vs imatrix? : r/SillyTavernAI](https://www.reddit.com/r/SillyTavernAI/comments/1ggfpo8/static_vs_imatrix/)
  - imatrix is distinct from i-quants.
  - i-quants are better if your hardware(CPU in this case, not GPU/RAM/VRAM) is beefy enough to run them at a speed you find usable.
  - K-quants if your CPU is struggling with i-quants.

- [How does any of this work? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1p8up6n/how_does_any_of_this_work/)
  - i-matrix quants use special calibration matrix, they essentially maintain quality on selected type of tasks sacrificing anything else. 
  - My experience suggest that imatrix quants are ever so slightly messed up, especially for creative writing.

- ## [Getting counter-intuitive results with local KV Cache Quantization Benchmark - am I doing something wrong? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nn2nqz/getting_counterintuitive_results_with_local_kv/)
  - I've been running some benchmarks on KV cache quantization for long-context tasks, and I'm getting results that don't make much sense to me. 
  - The Weird Results: I was expecting to see a clear trend where higher quantization (like q4_0) would lead to a drop in accuracy compared to the f16 baseline. Instead, I'm seeing the opposite. My best performing combination is k-f16_v-q5_0 with 16.79% accuracy, while the f16-f16 baseline only gets 13.74%.

- You see scattered reports of quantized KV increasing accuracy because it â€œfuzzesâ€ attention in a way that actually benefits (specifically) low bit weight quants. Basically it acts as an implicit smoothing function. Iâ€™ve not had amazing luck with llama.cppâ€™s implementation, but EXllamaâ€™s KV cache quants seem to perform exceptionally well at even 4-bits. 

- ## [å›½å†…å¤–å¤§æ¨¡å‹APIå¹³å°ä½“éªŒå¯¹æ¯” - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1914700194517345472)

- å¦‚æœæƒ³ä½¿ç”¨å›½å†…çš„æ¨¡å‹APIï¼Œæ¨èä½¿ç”¨å›½å†…äº‘å‚å•†æä¾›çš„APIæœåŠ¡æˆ–ç¬¬ä¸‰æ–¹ä»£ç†å¹³å°ã€‚
  - å¦‚æœæƒ³åœ¨å›½å†…ä½¿ç”¨å›½å¤–çš„æ¨¡å‹APIï¼Œæˆ–è€…æƒ³åŒæ—¶ä½¿ç”¨å¾ˆå¤šä¸åŒç³»åˆ—çš„æ¨¡å‹ï¼Œæ¨èä½¿ç”¨ç¬¬ä¸‰æ–¹ä»£ç†å¹³å°ã€‚
  - å¦‚æœæœåŠ¡å™¨åœ¨å›½å¤–ï¼Œæƒ³è°ƒç”¨å›½å¤–çš„æŸä¸ªAPIæ¨¡å‹ï¼Œåˆ™æ¨èä½¿ç”¨å›½å¤–äº‘å‚å•†ï¼Œæˆ–ç¬¬ä¸‰æ–¹ä»£ç†å¹³å°ã€‚

- 2025/9/1 æ›´æ–°ï¼š
  - 1. å¢åŠ DMXAPIï¼Œä¸€ä¸ªä¸é”™çš„APIä¸­è½¬å•†ï¼Œæ¨¡å‹æ¯”è¾ƒå…¨ã€‚
  - 2. åˆ é™¤åšé˜…ï¼Œå› ä¸ºå…¶ä¸å¼€æ”¾æ–°ç”¨æˆ·å……å€¼ã€‚
  - 3. é™ä½openrouterçš„æ¨èæŒ‡æ•°ï¼Œå› ä¸ºä½¿ç”¨è¿‡ç¨‹é‡åˆ°ç¨³å®šæ€§é—®é¢˜æ¯”è¾ƒå¤š 
  - 4. é™ä½ç¡…åŸºæµåŠ¨çš„æ¨èæŒ‡æ•°ï¼Œå› ä¸ºé™æµTPMå¤ªå°ï¼Œå½±å“ä½¿ç”¨ã€‚
  - 5. å¢åŠ GLMã€Kimiã€Minimaxæ¨¡å‹çš„æ¨èå‚å•†

- openrouter èƒŒåçš„ä¾›åº”å•†æ¯”è¾ƒä¹±, æœ‰äº›æ¨¡å‹ç‰ˆæœ¬ä¸æ˜¯æœ€æ–°çš„. è¿˜éœ€è¦è‡ªå·±æ’é™¤ä¾›åº”å•†

- ## ğŸ”§ [Open WebUI vs. LM Studio vs. MSTY vs. _insert-app-here_... What's your local LLM UI of choice? : r/LocalLLM _202502](https://www.reddit.com/r/LocalLLM/comments/1ij3j8m/open_webui_vs_lm_studio_vs_msty_vs_insertapphere/)
- Ollama vanilla CLI in tmux with vim copy/paste between terminals. I like pain

- All of them; donâ€™t lock into one solution.

- Open WebUI + LibreChat. LibreChat mainly for creating agents for RAG. Most painless interface for RAG.

- Open Web UI. MSTY is no alternative because it is an all-in-one solution.
  - Closed source right?
- Yep, they are selling it for businesses.

- KoboldAI Lite running on KoboldCpp. Most others aren't as flexible and just focused on instruct. This one can do instruct, but it can also do regular text generation for example. 
  - KoboldCpp meanwhile is a single executable with text gen, image gen, image recognition, speech to text and text to speech support. And it emulates the most popular API's if you prefer another UI (KoboldAI LIte doesn't need the backend to have any UI code so if its not open in the browser it does not effect you).

- ## [intelçš„cpuè¿å¤§æ¨¡å‹éƒ½æ²¡æ³•è·‘, æ€ä¹ˆè¿˜å¤©å¤©åœ¨æ¨aipc? - çŸ¥ä¹](https://www.zhihu.com/question/668042879/answers/updated)
- å¯¹äºç«¯ä¾§AIï¼Œæˆ‘ä¸ªäººçš„æƒ³æ³•ï¼Œæœ€å¤§çš„ä»·å€¼åº”è¯¥æ˜¯æ‹‰é«˜ä¸Šä¸‹æ–‡çª—å£ï¼Œåœ¨æœ¬åœ°åšä¸ªäººçŸ¥è¯†åº“ï¼Œä»¥åŠæœ¬åœ°æ‰¹é‡æ¨ç†ï¼Œæ¯”å¦‚åšç§‘ç ”çš„ï¼Œæ‡’å¾—è¯»è®ºæ–‡ï¼Œè®©AIæ‰¹é‡æ€»ç»“å†™ä¸ªç»¼è¿°ã€‚è¿™ä¸¤ç§åšæ³•å¦‚æœè°ƒç”¨çº¿ä¸Šçš„APIï¼Œå…¶å®æŒºè´µçš„ã€‚é˜…è¯»ä¸€ç¯‡è®ºæ–‡å°‘åˆ™å‡ åƒtokensï¼Œå¤šåˆ™ä¸¤ä¸‰ä¸‡tokensã€‚æœ¬åœ°ä½¿ç”¨32768çš„ä¸Šä¸‹æ–‡é•¿åº¦çš„Qwen3 8Bï¼Œä¹Ÿèƒ½å®Œæˆå¾—ä¸é”™
  - åšé•¿çª—å£æœ¬èº«å°±éœ€è¦èµ„æºï¼Œtransformerçš„kqvè®¡ç®—æ˜¯ä¸ªo(n^2)çš„å¤æ‚åº¦

- ç«¯ä¾§ AI ç°åœ¨çš„åº”ç”¨åœºæ™¯å®åœ¨æ˜¯å¤ªå°ã€‚å°¤å…¶æ˜¯è®¾å¤‡æ— æ—¶æ— åˆ»åœ¨çº¿çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä¸ºä»€ä¹ˆæ”¾ç€å…è´¹çš„å¼ºå¤§çš„åœ¨çº¿ AI ä¸ç”¨ï¼Œè½¬è€Œå»ç”¨ä¸èƒ½è”ç½‘æœç´¢ã€æ›´å¼±ï¼Œæ›´å¡ã€æ›´è€—ç”µçš„ç«¯ä¾§çš„ AI å‘¢ï¼Ÿ
  - æˆ‘æœ¬åœ°éƒ¨ç½²äº†chatgml çš„ 6B å’Œç§‹å¶çš„ [SD æ•´åˆåŒ…] ï¼Œä½†æ˜¯ç”¨åˆ°çš„çœŸçš„ä¸å¤šï¼Œå¾—åˆ°çš„ç»“æœä¹Ÿæ¯”ä¸äº†åœ¨çº¿çš„

- ## ğŸ†š [ä¸ºä»€ä¹ˆéƒ½åœ¨ç”¨ollamaè€Œlm studioå´æ›´å°‘äººä½¿ç”¨? - çŸ¥ä¹](https://www.zhihu.com/question/654357364)
- éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸åŒäºå…¨éƒ¨ä»£ç åœ¨githubå¼€æºã€ç”šè‡³å¯ä»¥è‡ªå·±åŠ¨æ‰‹ç¼–è¯‘çš„ollamaï¼Œlm studioè‡³ä»Šä»æ˜¯é—­æºå•†ä¸šè½¯ä»¶ï¼ˆä»…ä¸€éƒ¨åˆ†éæ ¸å¿ƒä»£ç é™¤å¤–ï¼‰

- Ollama äº 5 æœˆä»½æ¨å‡ºçš„å…¨æ–°å¤šæ¨¡æ€å¼•æ“ã€‚åŸºäºå¤šæ¨¡æ€å¼•æ“ï¼ŒOllama å¯ä»¥æ”¯æŒè¿è¡Œèƒ½åŒæ—¶å¤„ç†å›¾åƒã€æ–‡æœ¬çš„æ¨¡å‹ã€‚æ–°æ¶æ„ä¸ä»…è§£å†³å½“å‰å¤šæ¨¡æ€æŒ‘æˆ˜ï¼Œä¹Ÿä¸ºé›†æˆæ›´å¤æ‚çš„èƒ½åŠ›ï¼ˆè¯­éŸ³ã€ç”Ÿæˆç­‰ï¼‰å’Œä¼˜åŒ–æ€§èƒ½ï¼ˆæ›´é•¿ä¸Šä¸‹æ–‡ã€æ›´é«˜å¹¶å‘ï¼‰æ‰“ä¸‹åŸºç¡€ã€‚

- æˆ‘çš„ä½“ä¼šï¼š
  - ollamaç”¨èµ·æ¥å’Œdockerä¸€æ ·çš„æ„Ÿè§‰ï¼Œpullæ¨¡å‹ï¼Œrunæ¨¡å‹ï¼Œlsçœ‹æ¨¡å‹ï¼Œpsçœ‹è¿è¡Œã€‚éå¸¸é¡ºæ‰‹ä¸æ»‘ï¼Œå…¥æ‰‹æ— é—¨æ§›ã€‚
  - llamaçš„ä¸­æ–‡ï¼Œå¾®è°ƒå„ç§chatï¼Œcodeï¼Œå¤Ÿç”¨ã€‚è€Œä¸”éƒ½æ˜¯é‡åŒ–å¥½çš„ï¼Œéšæ‹‰éšç”¨ï¼Œ4090å°±è·‘çš„èµ·æ¥ã€‚å°¤å…¶æ˜¯åœ¨å›½å†…æ‹‰æ¨¡å‹é€Ÿåº¦æå¿«ï¼Œæˆ‘çš„ç¯å¢ƒæœ€é«˜å¯è¾¾15m/s
  - ollamaæ˜¯llama.cppå®ç°æ¨¡å‹æ¨ç†ï¼Œæ¨¡å‹å°ï¼Œé€Ÿåº¦å¿«ã€‚
  - ollamaæä¾›11434ç«¯å£çš„webæœåŠ¡ï¼Œé‡è¦çš„æ˜¯è¿˜å…¼å®¹openaiçš„ç«¯ç‚¹æ¥å£ï¼Œå¯ä»¥å’Œå„ç§å‰ç«¯é…åˆï¼Œæ¯”å¦‚ollamaè‡ªå·±open webuiï¼Œå›½äº§çš„chatboxï¼Œè¿åç«¯å¸¦ç•Œé¢ï¼Œä¸€å¥—æå®š
  - ollamaæ˜¯ç³»ç»ŸæœåŠ¡å½¢å¼ï¼ˆä¹Ÿèƒ½å®¹å™¨è¿è¡Œï¼‰ï¼Œå‰åç«¯åˆ†ç¦»ï¼ˆ ä¸¥æ ¼æ¥è¯´æ²¡æœ‰å‰ç«¯ï¼Œåªæœ‰å‘½ä»¤è¡Œå…¥å£ï¼‰ï¼Œè€¦åˆå°ï¼Œæ­é…çµæ´»ã€‚
- ollamaçš„è¿­ä»£å¾ˆå¿«ï¼Œç°åœ¨å¤šæ¨¡å‹å¹¶å‘çš„é—®é¢˜å·²ç»è§£å†³äº†

- ollamaç°åœ¨æ”¯æŒåˆ†å¸ƒå¼æ¨ç†å—
  - ç°åœ¨ä¸æ”¯æŒï¼Œä½†æ”¯æŒå•æœºå¤šgpuè¿è¡Œ

- æ‹‰æ¨¡å‹éº»çƒ¦å°±å›°æ‰°å¾ˆå¤šäººäº†ï¼Œollamaç…§ç€æ•™ç¨‹åšï¼ŒåŸºæœ¬æ²¡æœ‰é—®é¢˜

- ollamaé…åˆopen webuiä¸é”™çš„

- ollamaçœ‹ç€é€Ÿåº¦å¿«ï¼Œå…¶å®å®ƒæœ€å¤§é—®é¢˜æ˜¯ä¿®æ”¹ä¸ªä¸Šä¸‹æ–‡éƒ½å¾ˆéº»çƒ¦
  - å¯ä»¥ç”¨chatboxä¹‹ç±»çš„èŠå¤©æ¡†

- lm studio ä¹Ÿæä¾›openai-likeçš„apiåç«¯ï¼Œæ”¯æŒå‰åç«¯åˆ†ç¦»ï¼Œä¹Ÿæ”¯æŒllama.cppï¼ˆåœ¨macè¿˜æ”¯æŒmlxæ¡†æ¶ï¼‰. æ¨¡å‹åŸºæœ¬ä¸Šä¹Ÿéƒ½æ”¯æŒï¼Œè€Œä¸”éƒ½æ˜¯ä»hfä¸Šä¸€é”®å¼ä¸‹è½½éƒ¨ç½²ï¼ˆä¸è¿‡ç½‘ç»œç¡®å®æ˜¯é—®é¢˜ï¼‰
  - æ€»çš„è¯´ï¼Œlinuxä¸Šéƒ¨ç½²è¿˜æ˜¯åˆé€‚ï¼Œä¸ç”¨ollamaï¼Œè¿˜æœ‰vllmï¼Œlmdeloyï¼Œä»¥åŠsglangï¼Œéƒ½æ˜¯å¼€æºçš„ï¼Œå¾ˆé€‚åˆå¼€å‘äººå‘˜æˆ–è€…æ˜¯ç»„ç»‡å†…éƒ¨éƒ¨ç½²ã€‚windowsä¸Šéƒ½å·®ç‚¹ï¼Œlmstudioç•Œé¢è€¦åˆï¼Œé—­æºï¼Œæ‹‰æ¨¡å‹èµ°é•œåƒç‚¹ï¼Œä¸èƒ½åˆ†å¸ƒå¼æ¨ç†ï¼Œä¸ªäººè‡ªå·±å®éªŒæ€§è·‘è·‘å¥½ä¸€äº›ã€‚

- LMå›¾å½¢åŒ–ç•Œé¢ä¹Ÿå¯ä»¥å¾®è°ƒå¤§é‡çš„æ¨¡å‹è¿è¡Œå‚æ•°ï¼Œollamaè¿™æ–¹é¢å°±å¯¹å°ç™½ä¸å¤Ÿå‹å¥½äº†ï¼Œåªèƒ½ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
  - LMåšserveræœåŠ¡å™¨æ—¶ï¼Œç•Œé¢æœ‰åŠ¨æ€åé¦ˆçš„æ‰§è¡Œä¿¡æ¯å¯ä»¥ä¾›ç”¨æˆ·è§‚å¯Ÿåˆ°ï¼Œè°ƒè¯•é—®é¢˜æ–¹ä¾¿ã€‚
  - LMæœ€æ–°çš„ç‰ˆæœ¬åœ¨chatç•Œé¢ä¸‹ï¼Œè¿˜æ”¯æŒäº†ollamaä¸æ”¯æŒçš„LaTexæ•°å­¦å…¬å¼è¡¨è¾¾ï¼Œå’Œæœ¬åœ°RAGæ–‡æ¡£ä¿¡æ¯æ£€ç´¢ï¼Œè¿™äº›éƒ½æ˜¯ollamaè¿˜ä¸æä¾›çš„ç‰¹æ€§ã€‚

- ollama å’Œ LM studio å‡å¯ä»¥æ”¯æŒä¸åŒå¤§æ¨¡å‹çš„æœ¬åœ°éƒ¨ç½²ï¼Œå¹¶ä¸”ä¼šä¼˜å…ˆä½¿ç”¨ GPU è¿›è¡Œæ¨ç†ã€‚å¦‚æœæ²¡æœ‰å‘ç° GPUï¼Œå°±ä¼šä½¿ç”¨ CPU æ¨ç†ï¼Œå› æ­¤ä¹Ÿä¼šå ç”¨ä¸€éƒ¨åˆ†å†…å­˜ã€‚ä»å®é™…ä½¿ç”¨æ¥çœ‹ï¼Œç¬”è®°æœ¬å†…å­˜åº”è¯¥è‡³å°‘ä¸º 8GB æ‰èƒ½æ­£å¸¸è¿è¡Œã€‚

- å†™ç¨‹åºå’Œollamaäº¤äº’çœŸçš„æ˜¯é¡ºæ»‘ï¼Œå‡ å¥ä»£ç æ¨¡å‹å¿«é€Ÿåˆ‡æ¢ã€‚

- ollamaè·Ÿvllmã€sglangã€trtllmç›¸æ¯”æ˜¯ä¸ªç©å…·
  - lm studioè¿ç©å…·éƒ½ä¸å¦‚

- æ„Ÿå—ä¸ŠåŸå› æ˜¯dsæœ€ç«çš„æ—¶å€™ï¼Œé“ºå¤©ç›–åœ°çš„è‡ªåª’ä½“æ•™ç¨‹éƒ½æ˜¯ollamaï¼Œæåˆ°lmçš„å°‘çš„å¤šã€‚è¿™ä¸ªä¸œè¥¿ä¸€æ—¦å½¢æˆå®£ä¼ æ•ˆåº”äº†ï¼Œé™¤äº†ä¸“é—¨æè¿™ä¸ªçš„ï¼Œå°±ä¸ä¼šæœ‰äººæ·±å…¥ç ”ç©¶äº†ã€‚

- lm studioèƒ½åƒollamaé‚£æ ·åŒæ—¶ä½¿ç”¨chatå’Œembeddingå—ï¼Ÿlm studioæ¯æ¬¡éƒ½è¦é¢„å…ˆåŠ è½½ã€‚

- ## ğŸ [å¦‚ä½•çœ‹å¾…è‹¹æœå‘å¸ƒçš„ MLX æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Ÿ - çŸ¥ä¹ _202312](https://www.zhihu.com/question/633585779)
- MLXæ˜¯ä¸€ä¸ªç±»ä¼¼NumPyæ•°ç»„çš„æ¡†æ¶ï¼Œç›®çš„æ˜¯å¯ä»¥åœ¨è‹¹æœçš„èŠ¯ç‰‡ä¸Šæ›´åŠ é«˜æ•ˆåœ°è¿è¡Œå„ç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå½“ç„¶æœ€ä¸»è¦çš„ç›®çš„æ˜¯å¤§æ¨¡å‹ã€‚
  - MLXçš„è®¾è®¡å—åˆ°PyTorchã€Jaxå’ŒArrayFileçš„å¯å‘ï¼Œç›®çš„æ˜¯è®¾è®¡ä¸€ä¸ªå¯¹ç”¨æˆ·æå…¶å‹å¥½ï¼Œä½†åŒæ—¶åœ¨è®­ç»ƒå’Œéƒ¨ç½²ä¸Šä¹Ÿéå¸¸é«˜æ•ˆçš„æ¡†æ¶ã€‚
  - æ‰€ä»¥ï¼Œå®ƒçš„æ¥å£ä½ ä¼šéå¸¸ç†Ÿæ‚‰ï¼Œå› ä¸ºå®ƒçš„Pythonæ¥å£ä¸NumPyå¾ˆç›¸ä¼¼ï¼Œè€Œå®ƒçš„ç¥ç»ç½‘ç»œæ¨¡å‹çš„æ¥å£å’ŒPyTorchéå¸¸ç±»ä¼¼ã€‚

- 2023å¹´äº†ï¼ŒçœŸçš„è¿˜éœ€è¦ä¸ºäº†è‡ªå®¶èŠ¯ç‰‡ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªæ·±åº¦å­¦ä¹ è®­ç»ƒæ¡†æ¶å—
  - æœ‰çš„ï¼Œæ„Ÿè§‰æ˜¯ä¸“é—¨é’ˆå¯¹umaä¼˜åŒ–çš„
- æ¯•è®¾æ˜¯æ·±åº¦å­¦ä¹ åé—¨æ”»å‡»ç›¸å…³ è®¾å¤‡æ˜¯macmini m4 ä»pytorchæ¡†æ¶åˆ‡æ¢åˆ°mlxæ¡†æ¶ åŒæ ·çš„å®éªŒåŒæ ·çš„è®­ç»ƒé‡ mlxé€Ÿåº¦å¿«äº†å¥½å‡ å€ å®æ‰“å®çš„æå‡
  - ä½†æ˜¯éœ€è¦é‡æ„è®­ç»ƒä»£ç ï¼Œæˆ‘ä¸€å¥—pytorchä»£ç ï¼Œåªéœ€åšå°‘é‡é€‚é…ï¼Œå¯ä»¥è‡ªåŠ¨åœ¨æ˜‡è…¾ã€nVidiaã€mpsä¹‹é—´åˆ‡æ¢ã€‚è€Œä¸”ï¼Œmlxç¡®å®æ¯”mpsæå‡å¥½å‡ å€ï¼Œä½†æ˜¯æˆ‘å¦‚æœç”¨nvidiaæˆ–è€…æ˜‡è…¾910Bï¼Œmlxæ€§èƒ½è¿˜æœ‰å·®è·ã€‚
- åœ¨ MacBook Air m3 ä¸Šï¼Œmlx çš„çŸ©é˜µä¹˜æ³•ã€SVDåˆ†è§£è¿è¡Œé€Ÿåº¦æ˜æ˜¾å¿«äº PyTorch
  - å¦‚æœä¸ä½¿ç”¨ mlx æ¥è¿›è¡Œæ·±åº¦å­¦ä¹ ï¼Œä»…ç”¨äºå¸¸è§„ç§‘ç ”è®¡ç®—æˆ–ä¼˜åŒ–ä»»åŠ¡ï¼Œå®é™…ä¸Šè¿˜æ˜¯è›®é€‚åˆçš„ã€‚
  - ç„¶è€Œï¼Œmlx ç›®å‰ç”šè‡³ä¸æ”¯æŒ float64ï¼Œå¼€å‘å›¢é˜Ÿå½“å‰è€ƒè™‘çš„æ˜¯å®ƒä»¬ GPU åæ­£ç”¨ä¸åˆ°ï¼Œè¿™å¯¹ä¸€äº›é«˜ç²¾åº¦è®¡ç®—éœ€æ±‚åˆæ˜¯ä¸€ç§é™åˆ¶ï¼Œæ›´ä¸ç”¨è¯´ä¸€äº›æœªå®ç°çš„ç®—æ³•æˆ–è€…æ•°æ®ç±»å‹è¿˜éœ€è¦ä»é›¶å†™äº†ã€‚

- è¦æ˜¯kerasåç»­èƒ½å¤Ÿæ”¯æŒmlxåç«¯å°±å¥½äº†ï¼Œè¿™æ ·è‹¹æœè®¾å¤‡ä¸Šå†™çš„ä»£ç ä¸è‡³äºåˆ°åˆ«çš„å¹³å°ä¸Šè·‘ä¸äº†è¿˜è¦é‡æ„äº†ã€‚

- [è‹¹æœå‘è‹±ä¼Ÿè¾¾ç”Ÿæ€å¦¥åäº†ï¼MLXæ¡†æ¶ä¸»åŠ¨é€‚é…CUDA - çŸ¥ä¹ _202507](https://zhuanlan.zhihu.com/p/1929178251202376826)
  - è‹¹æœä¸€ç›´ä»¥æ¥éƒ½ä»¥â€œå°é—­â€è‘—ç§°ï¼Œä½†éšç€è‹±ä¼Ÿè¾¾CUDAç”Ÿæ€åœ¨AIå¼€å‘é¢†åŸŸå æ®ç»å¯¹ä¸»å¯¼åœ°ä½ï¼Œè‹¹æœè¿™ä¸‹ä¹Ÿä¸å¾—ä¸è½¬å˜å§¿æ€äº†ã€‚
  - è¿‡è®©MLXæ¡†æ¶ä¸»åŠ¨é€‚é…CUDAï¼Œä»Šåè‹¹æœå¼€å‘è€…ä¹Ÿèƒ½åˆ©ç”¨è‹±ä¼Ÿè¾¾GPUè®­ç»ƒæ¨¡å‹ã€‚å…¶æœ¬è´¨æ˜¯å¢åŠ äº†å¯¹CUDAçš„åç«¯æ”¯æŒï¼Œæ–¹ä¾¿å¼€å‘è€…åœ¨Windows/Linuxçš„è‹±ä¼Ÿè¾¾æ˜¾å¡ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œç„¶åå†éƒ¨ç½²å›Macã€iPhoneç­‰è®¾å¤‡ã€‚
  - æ˜¯ç»™Win-vidia/Lin-vidiaåšMLXæ”¯æŒï¼Œä¸æ˜¯ç»™Macç‰ˆMLXåšCUDAé©±åŠ¨

- è¿™ä¸¤å¤©å°è¯•å»å­¦ä¹ äº†ä¸€ä¸‹ mlx å’Œ mlx çš„æºç ï¼Œç²—ç•¥çš„çœ‹æ³•æœ‰ 2 ç‚¹ï¼š
  - å®è§‚ä¸Šï¼Œæ¡†æ¶çš„è®¾è®¡å¾ˆæ£’ï¼Œæ•´ä½“è®¾è®¡ä¸Šå¾ˆå¥½åœ°å¸å–äº†å‰äººçš„ç»éªŒï¼Œåœ¨ä¿è¯æ˜“ç”¨æ€§çš„å‰æä¸‹ï¼Œä¿ç•™äº†è¶³å¤Ÿé«˜çš„ä¼˜åŒ–ä¸Šé™ï¼›
  - å¾®è§‚ä¸Šï¼Œé¡¹ç›®çš„ C++ æ°´å¹³æ¯”è¾ƒä¸€èˆ¬ï¼Œæ¯”è¾ƒåƒæ˜¯å‡ ä¸ªå·¥ç¨‹å¸ˆçš„ side projectï¼Œå¦‚æœè¦å˜æˆé•¿é’æ ‘ï¼Œå†…éƒ¨å¤§é‡çš„å®ç°éƒ½éœ€è¦é‡æ„
- å…³äºæ¶æ„ã€‚æˆ‘è§‰å¾—ä¹‹å‰å‡ å¹´ï¼Œè®­ç»ƒæ¡†æ¶ä¸Šçš„å®è·µæ€»ç»“äº†è¿™æ ·çš„ä¸€äº›ç»éªŒï¼š
  - pytorch å’Œ tensorflow çš„äº‰æ–—ç»“æœå‘Šè¯‰æˆ‘ä»¬ï¼Œè®©å¤§å®¶å†™ pythonï¼Œè€Œä¸æ˜¯åœ¨ python é‡Œå†™ DSLï¼Œæå‡æ˜“ç”¨æ€§æ˜¯å¾ˆé‡è¦çš„ï¼›
  - torchscriptã€libtorch çš„å„ç§å‘å‘Šè¯‰æˆ‘ä»¬ï¼Œå®Œå…¨ä¾èµ–åœ¨ python ä¸Šï¼Œå›¾ä¼˜åŒ–å¾ˆæ£˜æ‰‹ï¼Œæ‰”æ‰‹æœºè¿™æ ·æ²¡æœ‰ python runtime çš„åœ°æ–¹ä¹Ÿæ˜¯çœŸçš„å¤´ç–¼ã€‚æœ€å¥½ä¸è¦çº¯ eager æ¨¡å¼ï¼Œè€Œæ˜¯èƒ½è®©æ¡†æ¶æå–å‡ºæ¥ä¸€äº›å­å›¾ã€‚æˆ–è€…æ˜¯æœ‰ä¸ªå›ºå®šç‚¹çš„ c apiï¼Œç»™ä¸è®­æ¨¡å‹çš„å·¥ç¨‹å¸ˆä»¬ç•™æ¡æ´»è·¯ï¼›
  - transformer å‘Šè¯‰æˆ‘ä»¬ï¼Œä¹Ÿè®¸ä»å­¦æœ¯è§’åº¦æ¥è¯´ï¼Œæ§åˆ¶æµä¼˜åŒ–èµ·æ¥å¾ˆæ€§æ„Ÿï¼Œä½†æ˜¯å®é™…åº”ç”¨ä¸­ï¼Œå…¨æ˜¯ä¸€æ¡è·¯èµ°åˆ°é»‘ï¼›
- ä¹‹å‰æˆ‘è§‰å¾— jax å°±æ˜¯è®­ç»ƒæ¡†æ¶ä¸Šçš„é›†å¤§æˆè€…ï¼Œå„ç§è®¾è®¡çœŸçš„å¾ˆæ¼‚äº®ã€‚å¯æ˜¯å®ƒæ‹–äº†ä¸ªé‡é‡çš„ XLA çš„å°¾å·´ï¼Œå¯èƒ½æ˜¯ä¸ºäº†è·‘åœ¨ TPU ä¸Šï¼Œä¹Ÿå¯èƒ½æ˜¯ jax çš„æˆå‘˜ä¸­æœ‰å¾ˆå¤š tf é‡Œåšç¼–è¯‘ä¼˜åŒ–çš„äººå§ã€‚è¿™ä½¿å¾—æ—©æœŸçš„ jax å¯¹ç”¨ nv æ˜¾å¡çš„ç”¨æˆ·å¾ˆä¸å‹å¥½ï¼Œç­‰ pytorch/huggingface æˆäº†æ°”å€™ï¼Œä¹Ÿå°±æ— åŠ›å›å¤©äº†ã€‚
- ç°åœ¨ mlx åŸºæœ¬å¸æ”¶äº† jax çš„ç»éªŒï¼Œ
  - é‡‡ç”¨äº† numpy api + lazy evaluationï¼›
  - æœ‰ä¸€ä¸ªæ¯”è¾ƒå¹²å‡€çš„ c apiï¼Œç”šè‡³å·²ç»æœ‰äº† swift bindingï¼›
  - å†…éƒ¨ç•™å‡ºäº†ç®€å•çš„å›¾ä¼˜åŒ–æ¥å£ mlx.core.compile ï¼Œä¿è¯äº†ä¼˜åŒ–çš„ä¸Šé™ã€‚

- çœ‹äº†ä¸€çœ¼æ¡†æ¶æºç ï¼Œå®Œå…¨æ˜¯é‡æ–°æäº†ä¸€å¥—æ¡†æ¶ï¼ŒAPIå°½é‡å¯¹æ ‡PyTorchï¼Œç›®å‰ä¹Ÿåªæ”¯æŒå°‘é‡APIã€‚è¿™æ ·æ¡†æ¶é€‚é…æˆæœ¬æ„Ÿè§‰æœ‰ç‚¹å¤§ï¼Œè¿å‰ç«¯Python APIéƒ½è¦è‡ªå·±æ„å»ºï¼Œå¤åˆ»å¤§é‡ä»£ç é€»è¾‘ä¸”ä¸è¯´ï¼ŒPyTorchç¤¾åŒºçš„å…¨é‡featureçš„è·Ÿè¸ªæ¼”è¿›å°±ä¼šæ˜¯ä¸€ä¸ªéš¾é¢˜ã€‚
  - æ„Ÿè§‰Appleå’Œæ˜‡è…¾èµ°å‘äº†ä¸¤ä¸ªæ–¹å‘çš„æç«¯ï¼Œæ˜‡è…¾æ˜¯å®Œå…¨ä¾èµ–åŸç”Ÿçš„æ’ä»¶åŒ–ï¼Œä¼šæƒ³å°½ä¸€åˆ‡åŠæ³•å¤ç”¨åŸç”Ÿé€»è¾‘ï¼Œå®åœ¨ä¸è¡Œå°±å¤åˆ»ï¼›Appleåˆ™æ˜¯å®Œå…¨æŠ›å¼ƒåŸç”Ÿé€»è¾‘ï¼Œç«¯åˆ°ç«¯é‡æ–°æä¸€å¥—ã€‚

- çœ‹äº†ä¸‹ä»£ç ï¼Œè‹¹æœæ˜¯é‡æ–°é€ äº†ä¸ªè½®å­ï¼Œå‰ç«¯å¹²çš„äº‹å’ŒPyTorchå’ŒJaxæ²¡å•¥åŒºåˆ«ï¼Œæ„Ÿè§‰é‡ç‚¹è¿˜æ˜¯åœ¨Apple Siliconçš„åç«¯ä¸Šã€‚ç›®å‰çš„å®ç°è¿˜æ˜¯æŒºå°å·§çš„ï¼Œç”¨æ¥å­¦ä¹ çš„è¯è¿˜ä¸é”™ï¼Œä½†è¦åœ¨featureä¸Šå¯¹é½ç°æœ‰çš„æ¡†æ¶è¿˜æœ‰ä¸å°‘çš„æ´»è¦å¹²ã€‚

- ä¸ªäººæ„Ÿè§‰è¿™ç©æ„æœ‰ç‚¹é¸¡è‚‹ã€‚
  - macå†™ä»£ç ã€è°ƒè¯•ï¼Œç„¶åæœåŠ¡å™¨å®Œæˆæ•´ä¸ªè®­ç»ƒã€‚è¿™å°±æœ‰é—®é¢˜äº†ï¼ŒæœåŠ¡å™¨ä¸æ”¯æŒMLXæ¡†æ¶ã€‚
  - macå†™ä»£ç ã€è°ƒè¯•ï¼Œç„¶åmacå®Œæˆæ•´ä¸ªè®­ç»ƒã€‚ æ­£å¸¸å…¬å¸è·‘æ¨¡å‹ï¼Œå¦‚æœå°‘é‡æ•°æ®ä½¿ç”¨å‡ å¼ æ·˜æ±°ä¸‹æ¥çš„1080å°±å¯ä»¥ï¼Œä»·æ ¼è¿˜ä¾¿å®œã€‚æ•°æ®é‡å¤§ï¼Œè¦ä½¿ç”¨åˆ†å¸ƒå¼å¹¶è¡Œè®­ç»ƒï¼Œé‚£å°±æ²¡macä»€ä¹ˆäº‹æƒ…äº†ã€‚
  - æœ€è¿‘ç«çƒ­çš„å¤§æ¨¡å‹è®­ç»ƒï¼Œå“ªå®¶å…¬å¸ä¼šç”¨macæ­å»ºé›†ç¾¤è®­ç»ƒå¤§æ¨¡å‹ï¼Œå¹¶ä¸”è®­ç»ƒå®Œæˆåä½¿ç”¨macåšæ¨ç†ã€‚ç”¨ä¸åˆ°macå°±æ²¡MLXä»€ä¹ˆäº‹æƒ…äº†ã€‚

- æˆ‘è§‰å¾—å¯ä»¥å‚è€ƒæ—©æœŸswiftï¼Œä¸€å¹´ä¸€ä¸ªç‰ˆæœ¬ï¼Œè®©ä½ æ„Ÿåˆ°å´©æºƒï¼Œè™½ç„¶è¿™æ¬¡å¼€æºï¼Œä½†æ˜¯ä¸»åŠ›åº”è¯¥è¿˜æ˜¯è‹¹æœè‡ªå·±

- [é˜¿é‡Œå·´å·´å‘å¸ƒäº†ä¸è‹¹æœ MLX æ¶æ„å…¼å®¹çš„ Qwen3 å‡çº§ç‰ˆï¼Œæ–°ç‰ˆæœ¬éƒ½æœ‰å“ªäº›æ–°ç‰¹æ€§ï¼Ÿ - çŸ¥ä¹ _202506](https://www.zhihu.com/question/1918255662552582106)
  - MLXæ˜¯è‹¹æœä¸“ä¸ºApple SiliconèŠ¯ç‰‡ï¼ˆMç³»åˆ—ï¼‰è®¾è®¡çš„å¼€æºæœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œå®ƒçš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºæ·±åº¦æ•´åˆè‹¹æœç¡¬ä»¶ç‰¹æ€§ï¼Œå°¤å…¶ä»¥â€œç»Ÿä¸€å†…å­˜æ¨¡å‹â€ä¸ºæ ¸å¿ƒåˆ›æ–°
  - ç»Ÿä¸€å†…å­˜æ¶æ„ï¼šæ•°æ®å­˜å‚¨åœ¨CPUä¸GPUå…±äº«çš„å†…å­˜ç©ºé—´ä¸­ï¼Œè·¨è®¾å¤‡è®¡ç®—æ—¶æ— éœ€æ˜¾å¼æ‹·è´æ•°æ®ï¼Œç›¸æ¯”ä¼ ç»Ÿæ¡†æ¶ï¼ˆå¦‚PyTorchï¼‰å‡å°‘90%ä»¥ä¸Šçš„å†…å­˜ä¼ è¾“å¼€é”€ã€‚

- ## [å¦‚ä½•çœ‹å¾… Google æœ€æ–°å¼€æºçš„ Gemma-3 ç³»åˆ—å¤§æ¨¡å‹ï¼Ÿ - çŸ¥ä¹ _202503](https://www.zhihu.com/question/14777841836)

- Gemma-3ä¸»æ‰“çš„å–ç‚¹æ˜¯å•å¡å¯è·‘+å¤šæ¨¡æ€ã€‚
- Gemma-3çš„æ ¸å¿ƒç«äº‰åŠ›åœ¨äºæ•ˆç‡é©å‘½ã€‚å…¶27Bç‰ˆæœ¬ä»…éœ€å•å¼ H100æ˜¾å¡å³å¯è¿è¡Œï¼Œè€Œç«å“è¾¾åˆ°åŒç­‰æ€§èƒ½å¾€å¾€éœ€è¦10å€ç®—åŠ›ã€‚è¿™ç§æ•ˆç‡æå‡å¹¶éç®€å•çš„å‚æ•°å‹ç¼©ï¼Œè€Œæ˜¯æ¶æ„å±‚é¢çš„åˆ›æ–°
  - çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼šåŸºäºGemini 2.0çš„"æ•™å¸ˆæ¨¡å‹"è¿›è¡Œè’¸é¦ï¼Œå°†å¤§æ¨¡å‹èƒ½åŠ›è¿ç§»åˆ°å°æ¨¡å‹ï¼Œå®ç°"ç”¨1/10å‚æ•°è¾¾åˆ°80%æ€§èƒ½"çš„æ•ˆæœã€‚
- å°½ç®¡Gemma-3è¡¨ç°äº®çœ¼ï¼Œä»éœ€æ³¨æ„ä»¥ä¸‹é—®é¢˜ï¼š 
  - 1. é•¿å°¾ä»»åŠ¡ç¼ºé™·ï¼šåœ¨æ•°å­¦æ¨ç†å’Œä¼¦ç†åˆ¤æ–­ç­‰ç‰¹å®šåœºæ™¯ï¼Œå®æµ‹ç»“æœæ˜¾ç¤ºå…¶è¡¨ç°ä¸å¦‚Qwen2.5-Maxã€‚ 
  - 2. å¤šæ¨¡æ€ç²¾åº¦ç“¶é¢ˆï¼šå¤„ç†å¤æ‚å›¾åƒæ—¶ï¼ŒSigLIPç¼–ç å™¨çš„ç»†èŠ‚è¯†åˆ«èƒ½åŠ›ä»è½åäºä¸“ä¸šCVæ¨¡å‹ã€‚ 
  - 3. è®­ç»ƒæ•°æ®ä¾èµ–ï¼šè™½ç„¶æ”¯æŒ140ç§è¯­è¨€ï¼Œä½†å®é™…è¡¨ç°å­˜åœ¨"è‹±è¯­æœ€ä¼˜ï¼Œå°è¯­ç§æ¬¡ä¹‹"çš„é—®é¢˜ã€‚ 
- æœªæ¥å¯èƒ½çš„çªç ´æ–¹å‘åŒ…æ‹¬ï¼š 
  - åŠ¨æ€æ¶æ„ï¼šæ ¹æ®è¾“å…¥å†…å®¹è‡ªåŠ¨åˆ‡æ¢è®¡ç®—èµ„æºåˆ†é…ï¼Œä¾‹å¦‚è§†é¢‘å¤„ç†æ—¶ä¸´æ—¶è°ƒç”¨æ›´å¤šGPUæ ¸å¿ƒã€‚ 
  - è·¨æ¨¡æ€ç”Ÿæˆï¼šä»"ç†è§£å¤šæ¨¡æ€"å‡çº§åˆ°"ç”Ÿæˆå¤šæ¨¡æ€"ï¼Œä¾‹å¦‚æ ¹æ®æ–‡æœ¬æè¿°ç›´æ¥ç”Ÿæˆ3Dæ¨¡å‹ã€‚ 
  - è¾¹ç¼˜ç«¯ä¼˜åŒ–ï¼šè¿›ä¸€æ­¥å‹ç¼©æ¨¡å‹ä½“ç§¯ï¼Œåœ¨æ‰‹æœºç«¯å®ç°å®æ—¶å¤šæ¨¡æ€äº¤äº’ã€‚

- å¼€æºGemmaç³»æ¨¡å‹æ­£å¼è¿­ä»£åˆ°ç¬¬ä¸‰ä»£ï¼ŒåŸç”Ÿæ”¯æŒå¤šæ¨¡æ€ï¼Œ128kä¸Šä¸‹æ–‡ã€‚
  - Gemma 3ä¸€å…±å¼€æºäº†å››ç§å‚æ•°ï¼Œ1Bã€4Bã€12Bå’Œ27Bã€‚æœ€æœ€æœ€å…³é”®çš„æ˜¯ï¼Œä¸€å—GPU/TPUå°±èƒ½è·‘æ¨¡å‹ã€‚
  - åœ¨LMArenaç«æŠ€åœºä¸­ï¼ŒGemma 3æ‹¿ä¸‹äº†1339 ELOé«˜åˆ†ï¼Œä»…ä»¥27Bå‚æ•°å‡»è´¥äº†o1-previewã€o3-mini highã€DeepSeek V3ï¼Œå ªç§°ä»…æ¬¡äºDeepSeek R1æœ€ä¼˜å¼€æºæ¨¡å‹ã€‚
  - Gemma3ç³»1Bã€4Bã€12Bã€27Båˆ†åˆ«åŸºäº2Tã€4Tã€12Tã€14T tokenæ•°æ®å®Œæˆè®­ç»ƒã€‚
  - ä¸é—­æºGemini 1.5å’Œ2.0ç›¸æ¯”ï¼ŒGemma 3-27BåŸºæœ¬ä¸Šç•¥é€Šè‰²äºFlashç‰ˆæœ¬ã€‚
- é‡‡ç”¨ä¸Gemini 2.0æ¨¡å‹ç›¸åŒçš„ç ”ç©¶å’ŒæŠ€æœ¯æ‰“é€ ã€‚

- æœ€å¤§çš„17GBæ˜¯ç¨³ç¨³å¯ä»¥è·‘åœ¨æ‹¥æœ‰24GBæ˜¾å­˜çš„Nvidia90ç³»æ˜¾å¡çš„ã€‚

- Gemma 3n æ˜¯ä¸€æ¬¾ä¸“ä¸ºç§»åŠ¨è®¾å¤‡é‡èº«æ‰“é€ çš„å¤šæ¨¡æ€ AI æ¶æ„ï¼Œå®ƒä¸ä»…æ”¯æŒå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘å’Œæ–‡æœ¬çš„è¾“å…¥è¾“å‡ºï¼Œè¿˜é€šè¿‡åˆ›æ–°çš„ MatFormer æ¶æ„å’Œ Per-Layer Embeddings æŠ€æœ¯ï¼Œå®ç°äº†é«˜æ€§èƒ½ä¸ä½å†…å­˜å ç”¨çš„å®Œç¾å¹³è¡¡ã€‚
  - E2B å’Œ E4B ä¸¤ç§æ¨¡å‹ç‰ˆæœ¬ï¼Œåˆ†åˆ«ä»…éœ€ 2GB å’Œ 3GB å†…å­˜å³å¯è¿è¡Œï¼Œè€Œä¸”è¿˜èƒ½é€šè¿‡ Mix-n-Match æ–¹æ³•çµæ´»è°ƒæ•´æ¨¡å‹å¤§å°ï¼Œæ»¡è¶³ä¸åŒåœºæ™¯çš„éœ€æ±‚ã€‚

- Gemma 3n æä¾›äº†å¤šç§éƒ¨ç½²é€‰é¡¹ï¼ŒåŒ…æ‹¬ Google GenAI APIã€Vertex AIã€SGLangã€vLLM å’Œ NVIDIA API Catalog ç­‰ã€‚
  - æ­¤å¤–ï¼ŒGemma 3n è¿˜ä¸ Hugging Faceã€Google AI Edgeã€Ollamaã€MLX ç­‰å¤šç§å·¥å…·å’Œå¹³å°æ— ç¼é›†æˆï¼Œè®©å¼€å‘è€…å¯ä»¥å¿«é€Ÿä¸Šæ‰‹ï¼Œè½»æ¾é›†æˆåˆ°ç°æœ‰é¡¹ç›®ä¸­ã€‚

- ### [Difference between Gemma and Gemini - Marvik _202407](https://blog.marvik.ai/2024/07/03/difference-between-gemma-and-gemini/)
- While Gemma is open source and lightweight, Gemini is proprietary and heavyweight. 

- ## [å¦‚æœæœ¬åœ°è¦è£…å¤§æ¨¡å‹ï¼Œå»ºè®®å“ªä¸ªå¼€æºå¤§æ¨¡å‹ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/653255605)
- ollamaåŸå§‹çš„ç‰ˆæœ¬æ˜¯å¹¶ä¸å…¼å®¹ OpenAI æ ¼å¼çš„è¯·æ±‚ï¼Œå› æ­¤åœ¨ä½¿ç”¨æ¯”å¦‚ langchainã€llamaindex ç­‰æ¡†æ¶çš„æ—¶å€™ï¼Œä¼šé‡åˆ°ä¸€äº›éº»çƒ¦çš„åœ°æ–¹ï¼Œéœ€è¦å†è½¬åŒ–ä¸‹ã€‚
  - ç°åœ¨ï¼Œæ›´æ–°åˆ°æ–°ç‰ˆæœ¬çš„æ—¶å€™ï¼Œå°±æä¾›äº† OpenAI æ ¼å¼çš„æ¥å£è°ƒç”¨
  - ç›®å‰è¿™ä¸ª OpenAI-compatible API è¿˜å¤„äºæµ‹è¯•é˜¶æ®µï¼Œå¹¶ä¸æ”¯æŒ function calling çš„åŠŸèƒ½ã€‚

- æˆ‘æœ¬åœ°ollamaè·‘ä¸€å¼ P40/24Gæ˜¾å­˜ï¼Œç©äº†ä¸€å¹´å¤šäº†ã€‚æˆ‘ä¸€èˆ¬è·‘é‡åŒ–è¿‡çš„32Bçš„æ¨¡å‹ï¼Œå†å¾€ä¸Šé€Ÿåº¦å¤ªæ…¢æˆ–è€…é‡åŒ–å¤ªç‹ ï¼Œéƒ½æ²¡å¤ªå¤§å®ç”¨ä»·å€¼ã€‚

- çœ‹ä½ æ˜¾å­˜å¤§å°é€‚åˆå¤šå°‘Bçš„æ¨¡å‹ã€‚è¯„ä¼°æ—¶å€™è¦è€ƒè™‘é‡åŒ–int4ã€int8è¿˜æ˜¯float16ã€‚å¯ä»¥åˆ©ç”¨huggingfaceä¸Šçš„è®¡ç®—å™¨è¯„ä¼° https://hf-accelerate-model-memory-usage.hf.space
  - é™¤äº†ollamaå®˜æ–¹åˆ—è¡¨ ä¹Ÿå¯ä»¥å»æ‰¾hugging faceä¸Šé‡åŒ–å¥½çš„æ¨¡å‹ã€‚
- ollamaæ˜¯åŸºäºllama.cpp, æ‰€ä»¥ä¹Ÿå¯ä»¥åŸºäºllama.cpp è‡ªå·±æŠ˜è…¾ä¸€éï¼Œä»è½¬æ¢æ¨¡å‹ä¸ºggufåˆ°é‡åŒ–ä¸ºint4 è¿˜æ˜¯int8ï¼Œå¯ä»¥æŒ‰ç…§æˆ‘ä¹‹å‰å†™çš„æ•™ç¨‹è‡ªå·±èµ°ä¸€éï¼Œå¯ç©æ€§æ›´å¼ºä¸€ç‚¹ã€‚æˆ‘ç”µè„‘mac m1 è·‘é€šä¹‰åƒé—®çš„7bçš„int4é‡åŒ–æ¨¡å‹ï¼Œå¤§çº¦æ¯ç§’12ä¸ªtokenã€‚

- éä¸“ä¸šäººå£«çš„è¯ï¼Œollamaå’Œlocalaiï¼Œè¿™ä¸¤æ— è„‘éƒ¨ç½²è¿è¡Œã€‚åŒæ—¶è¿™ä¸¤ä¸å±€é™ä½ æœ‰æ²¡æœ‰GPUï¼Œcpuä¹Ÿå¯ä»¥æã€‚

- å¤ªå°çš„æ¨¡å‹ï¼Œæƒ³1.5bå¤§å°çš„ï¼Œæ€§èƒ½è‚¯å®šå·®ä¸€äº›ï¼Œä¸è¿‡ç»“åˆragè¿™äº›ï¼Œå®ç°æœ¬åœ°çŸ¥è¯†åº“ä¹Ÿå·®ä¸å¤šäº†ã€‚åšå®éªŒï¼Œæ­£å¸¸è¿˜æ˜¯ç”¨7bå·¦å³çš„ã€‚

- è‡ªå·±æœ¬åœ°æ­å»ºçš„æ•°æ®é‡å¤ªå°‘äº†ï¼Œè”æƒ³å…³é”®è¯å›°éš¾

- ## [ç›®å‰æœ‰ä»€ä¹ˆå¯ä»¥æœ¬åœ°éƒ¨ç½²çš„å¤§æ¨¡å‹æ¨è? - çŸ¥ä¹](https://www.zhihu.com/question/648879790)
- å¼€æºå¤§æ¨¡å‹æ›´æ–°è¿­ä»£å¤ªå¿«ï¼Œä»Šå¹´åˆšæ¨å‡ºçš„æ¨¡å‹å¯èƒ½è¿‡å‡ ä¸ªæœˆå°±è¿‡æ—¶äº†ã€‚
- ä¸€æ˜¯å¦‚ä½•æ‰¾åˆ°æœ€æ–°çš„å¤§æ¨¡å‹ï¼Œ
  - huggingfaceå¯ä»¥ç†è§£ä¸ºå¯¹äºAIå¼€å‘è€…çš„GitHubï¼Œæä¾›äº†æ¨¡å‹ã€æ•°æ®é›†ï¼ˆæ–‡æœ¬|å›¾åƒ|éŸ³é¢‘|è§†é¢‘ï¼‰ã€ç±»åº“ï¼ˆæ¯”å¦‚transformers|peft|accelerateï¼‰ã€æ•™ç¨‹ç­‰ã€‚
  - huggingfaceæœ‰æ—¶å­˜åœ¨ç½‘ç»œä¸ç¨³å®šçš„é—®é¢˜, è¿™é‡Œæ¨èå›½å†…æ¯”è¾ƒå¥½çš„å¹³å°modelscope
- äºŒæ˜¯å¦‚ä½•åˆ¤æ–­æœ¬åœ°ç¡¬ä»¶èµ„æºæ˜¯å¦æ»¡è¶³å¤§æ¨¡å‹çš„éœ€æ±‚ï¼Œ
  - æœ¬åœ°å¯ä»¥éƒ¨ç½²ä»€ä¹ˆå¤§æ¨¡å‹ï¼Œå–å†³äºä½ çš„ç¡¬ä»¶é…ç½®ï¼ˆå°¤å…¶å…³æ³¨ä½ GPUçš„æ˜¾å­˜ï¼‰ã€‚
  - åœ¨æ²¡æœ‰è€ƒè™‘ä»»ä½•æ¨¡å‹é‡åŒ–æŠ€æœ¯çš„å‰æä¸‹ï¼š å…¬å¼ï¼šæ¨¡å‹æ˜¾å­˜å ç”¨ï¼ˆGBï¼‰ = å¤§æ¨¡å‹å‚æ•°ï¼ˆBï¼‰*2
  - åœ¨4bité‡åŒ–çš„æƒ…å†µï¼Œæ»¡è¶³æœ¬åœ°æœºå™¨GPUæ˜¾å­˜ï¼ˆGBï¼‰ >= å¤§æ¨¡å‹å‚æ•°ï¼ˆBï¼‰/2ï¼Œå¯ä»¥å°è¯•æœ¬åœ°éƒ¨ç½²ã€‚
- ä¸‰æ˜¯å¦‚ä½•å¿«é€Ÿéƒ¨ç½²å¤§æ¨¡å‹ã€‚
  - ollama pull llama3

- æˆ‘è¯´çš„æœ¬åœ°ï¼ŒæŒ‡çš„ä¸æ˜¯ä¸€å°ä¸ªäººç”µè„‘ä¸Šï¼Œè·‘ä¸€ä¸ª7Bã€13Bå‚æ•°çš„å¤§æ¨¡å‹ã€‚è€Œæ˜¯åœ¨ä¼ä¸šæœ¬åœ°ç®—åŠ›æœåŠ¡å™¨ä¸Šï¼Œç§æœ‰åŒ–éƒ¨ç½²çš„700äº¿å‚æ•°ä»¥ä¸Šè§„æ¨¡çš„å¤§æ¨¡å‹ï¼Œè¿™ç§å‚æ•°è§„æ¨¡çš„å¤§æ¨¡å‹ï¼Œæ‰æœ‰æ›´å¥½çš„æŒ‡ä»¤ä¾ä»æ€§ï¼Œç»“åˆRAGã€Agentç­‰æŠ€æœ¯ï¼Œèƒ½æœ‰æ•ˆçš„å®Œæˆä½ åˆ†é…ç»™ä»–çš„ä»»åŠ¡ã€‚ 
# discuss-model-file-formats âš–ï¸ 
- ## 

- ## 

- ## 

- ## There are two FP4 formats circulating right now: MXFP4 and NVFP4 (NV for Nvidia).
- https://x.com/awnihannun/status/1961500133990043967
  - Both formats quantize weights to 4-bit floating point (e2 m1) with a unique scale per group. 
  - The difference is the group size and how the scale for each group is encoded. 
  - MXFP4 uses an e8m0 scale (fixed-point, 8-bit) with a group size of 32. It gets raised to the power of 2 before multiplying the weight. 
  - NVFP4 uses an e4m3 (fp8) scale with a group size of 16. It is multiplied with the weight directly
  - The scale encoding in MXFP4 is pretty suboptimal because it doesn't have representations for a lot of values in the range we need.

- ah, the eternal battle of formats. MXFP4 might be the underdog, but NVFP4 sounds like the crowd favorite. Just wait till someone tries to quantify their existential dread, then we'll really see some innovation. 

- ## [Why does it seem like GGUF files are not as popular as others? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1ood0kn/why_does_it_seem_like_gguf_files_are_not_as/)
  - I feel like itâ€™s the easiest to setup and itâ€™s been around since the beginning I believe, why does it seem like HuggingFace mainly focuses on Transformers, vLLM, etc which donâ€™t support GGUF

- It's pretty easy to support gguf, it's honestly weird that they haven't bothered.
  - The issue isn't really the format so much as all the possible quants. There's very little value in "supporting GGUF" if the code can't, say, multiply IQ3_XXS matrices. Supporting all those qants (which can be different per-matrix too, BTW) adds code complexity and can make optimizing difficult.
  - Also, I believe that GGUF doesn't support FP8 and only somewhat supports FP4. (IIRC some tenors are forced to FP32 in GGUF, but that might be wrong.) So for an engine is focused on only providing GPU floating point inference supporting gguf would be silly since it would basically be the bf16 .safetensors but worse.

- My guess because large scale inference providers need efficient inference for a multi-user environment and only care about GPU inference, and vLLM provides that. As of transformers, it is much easier to add support for new architectures there in most cases.
  - So, implementation complexity to support GGUF is generally higher. Vision models and Qwen Next are good example when it takes a while to get support for GGUF.
  - the main reason why I use GGUF is because `ik_llama.cpp` is good for CPU+GPU inference for a single user.
  - I am sure it will get there in time, but the point is, adding GGUF support is very hard, and requires either hiring professional programmer(s) or relying on the community to implement the support themselves.

- Transformers is going to be running the original, unmodified model; it was trained, tested, etc for this experience. You're getting the purest experience.
  - GGUF is a conversion to make it more compatible for the rest of us. The way inferencing occurs, there is a possibility of differences between gguf and the original intended experience, and also a possibility of speed differences as well.
  - ggufs are VERY popular amongst individual users and runners, and folks with limited/specific hardware. But if you're serving to other users, and you have the base hardware expected for Transformers, then you're probably doing that.
# discuss-mac-mlx ğŸ
- ## 

- ## 

- ## 

- ## [Any experience serving LLMs locally on Apple M4 for multiple users? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ov5bpe/any_experience_serving_llms_locally_on_apple_m4/)
- vLLM, or any production grade multi user inference engine, does not support this platform.. I think the Apple multi user story is effectively "buy each user their own mac"
  - vllm works on metal now, sort of. not currently worth the trade offs in my opinion, though

- I have no experience with it, but considering how poorly it handles context I think it's safe to say it isn't going to work well at all with multiuser concurrency. It's really designed to be a single user device.

- Prompt processing is significantly slower on the Mac platform compared to nvidia so if the users are sending large contexts, they're going to see a big delay before they see output tokens. 
  - I use an m3 512 with qwen vl 30ba3b which is 1-2 seconds before outputting even with 2-3k contexts, but then also use deepseek 3.1 q4 (370 gigs) which can take up to 30 seconds to prompt process a 400-500 token input. Subsequent same requests (literally identical because of many text to image prompts) are much faster. Doing that with multiple users who are all submitting unique requests would be unbearably slow unless you submit and walk away. This is with lm studio and the mlx versions of the models. 
  - Vllm isn't correctly supported on mac, and you get a 25% speed reduction if you use ggufs compared to the mlx versions.

- ## [MLX added support for MXFP8 and NVFP4 : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1ojpfwl/mlx_added_support_for_mxfp8_and_nvfp4/)
- I dont think native fp4 supportwill come until m6 or m7. M5 didnt have fp4 or fp8 accelerators. maybe m5 max will have dedicated fp8 support, if not then m6

- ## ğŸ”§ [There is a space on HF where you can convert models to MLX without downloading them : r/LocalLLaMA _202503](https://www.reddit.com/r/LocalLLaMA/comments/1j3k8o8/there_is_a_space_on_hf_where_you_can_convert/)
  - Well I just learned that there is a space on HF where you can convert models, you just enter the model name and the quant and it uploads it straight to your HF profile! No need to download or do anything at all. This also means I'm not limited by my ram, I can convert any model now and so should you :)
  - https://huggingface.co/spaces/mlx-community/mlx-my-repo

- ## [Best Local LLM for MacBook Air M3 with 24GB RAM : r/LocalLLaMA _202404](https://www.reddit.com/r/LocalLLaMA/comments/1bu65s6/best_local_llm_for_macbook_air_m3_with_24gb_ram/)
- Rough rule of thumb is 2xParam Count in B = GB needed for model in FP16.
  - From there reducing precision scales more or less linearly. 
  - So 1B param model = 1GB RAM needed INT8, or 0.5GB RAM needed INT4. 

- ## [Any experiences running LLMs on a MacBook? : r/LocalLLaMA _202507](https://www.reddit.com/r/LocalLLaMA/comments/1m1t19r/any_experiences_running_llms_on_a_macbook/)
- Pretty good experience on M4 Max with 128Gb, Qwen3-30B-A3B (8bit quants). Speed on small inputs is around 40-50 toks/s, which is very very usable.
  - I have the same setup I would say not very good. lol. But thatâ€™s because I try to use models for things like cline and opencode. Itâ€™s just soooo slow on initial prompt and even later on as well. For chats with 24bâ€™s itâ€™s great though
- I feel that almost any thinking models feel pretty slow in coding assistants. I would prefer them to answer faster with the same quality

- Qwen3-30B-A3B is the goat on my M3 Ultra (96 gb). I asked a few coding/ML questions, and I'm getting between 55 - 70 tok/s

- I get vastly better results with qwen3 32B in 4bit than qwen3 30B in 8bit. 

- Prompt: Write a binary search function in javascript
  - All models loaded w/ max context window, lm studio, mlx backend.
- M4 Max MacBook Pro 128GB (mostly 4bit):
  kimi-dev-72b-dwq: 11 tps
  qwen3-53b-a3b: 45tps
  qwen3-30b-a3b-dwq: 96tps
  devstral-small-2507-dwq: 33tps
  gemma-3n-e4b-it: 75tps
  jan-nank-128k (8bit): 90tps
  qwen3-4b-dwq-053125: 142tps
  qwen3-1.7b-dwq-053125: 252tps
- M2 MacBook Air 24GB:
  qwen3-30b-a3b (3bit): 32tps
  qwen3-4b-dwq-053125: 30tps
  qwen3-1.7b-dwq-053125: 66tps
  devstral-small-2507-dwq: 6tps
  gemma-3n-e4b-it: 25tps
  jan-nano-128k (8bit): 16tps
  jan-nano-128k (4bit): 31tps

- I have M4 pro with 48GB RAM. I can run local models maximum 32B (4/6Q). Gemma 3 27b/Qwen 3 32B is good enough to use for general QnA purpose. For dev assistance, it lacks accuracy and generation speed on Mac M4. I would choose R1 or else in Openrouter. However definitely battery runs out faster with local models.
  - iâ€™d recommend giving devstral small a shot itâ€™s surprisingly really good and punches above its weight.

- Get the pro and with as much unified memory as you can afford. Iâ€™m running a M4 Max with 64GB and I regret not getting the 128
  - you might want to increase the portion of unified memory allocated to the GPU
  - LM Studio has awful default settings when you download a model, always check online what are the modelâ€™s recommended settings (Unsloth blog or model pages on huggingface are great sources for this).
  - donâ€™t just max out context length, set one that makes sense for your hardware. You can find calculators online, or resort to the good old trial and error process. Be aware that, even if models support 128k or more tokens in the context, most of them degrade after 40/50k.

- I wouldn't recommend fine tuning on Macs - took 9hrs to train phi 3 mini on the guanaco dataset with autotrain.

- ## [MacBook Air M4/32gb Benchmarks : r/LocalLLaMA _202503](https://www.reddit.com/r/LocalLLaMA/comments/1jklk5y/macbook_air_m432gb_benchmarks/)
  - Phi4-mini (3.8b)- 34 t/s, 
  - Gemma3 (4b)- 35 t/s, 
  - Granite 3.2 (8b)- 18 t/s, 
  - Llama 3.1 (8b)- 20 t/s, 
  - Gemma3 (12b)- 13 t/s, 
  - Phi4 (14b)- 11 t/s, 
  - Gemma (27b)- 6 t/s, 
  - QWQ (32b)- 4 t/s

- I suspect it's heating up quite a bit like my 24 gb one does
  - Sure does, being completely silent is a nice tradeoff though. My old MacBook Pro would sound like a jet engine preparing to take off.

- Those figures are close to what I'm getting using accelerated ARM CPU inference on a Snapdragon X1 Elite with 12 cores. That's on a ThinkPad with fans and big cooling vents. It's incredible that the M4 Air has that much performance in a fanless design.
  - It definitely gets warm when inferencing with the larger models and longer contexts but being completely silent is pretty amazing. Models tested were Q4.

- People are angry with the MacBook Air M4. Without fans, the benchmarks drop by half compared to the Mac Mini with the same M4 chip.

- [Tested local LLMs on a maxed out M4 Macbook Pro so you don't have to : r/ollama _202503](https://www.reddit.com/r/ollama/comments/1j0by7r/tested_local_llms_on_a_maxed_out_m4_macbook_pro/)
  - æµ‹è¯•äº†qwen2.5çš„å„ç§é‡åŒ–ç‰ˆ
  - lmstudioçš„mlxæ¯”ollamaå¿«å¾ˆå¤š

- ## âš¡ï¸ [Is M4 MacBook Air 32 + 512 good for AI/LLM? : r/macbookair _202503](https://www.reddit.com/r/macbookair/comments/1jd2cbc/is_m4_macbook_air_32_512_good_for_aillm/)
  - Some preliminary benchmark tests show that the token per second is as follows in model M4 MBA 32 GB: 
  - 7b 20.8 token/s 
  - 14b 11.0 token/s 
  - 32b 4.9 token/s 
  - Is this acceptable or good enough for playing with local AI/LLM?

- deepseek-r114b on M4 is near 14 tokens but on the 4070 it is 50!!!
  - Then deepseek 32b does like 4.7 tokens on M4 32/512 and 5.7 on the 4070 ti.
  - For LLM running i'd go for the M4 PRO with more cores and 48gb ram but that configuration is like x2 expensive over the 32/512 M4

- The problem is LLM running locally are very CPU intensive. Which equals heat, which the Air has no internal fans to handle it, which equals the CPU throttling (under clocking) itself to prevent overheating. The AIR will get Hot AF!
  - The memory and storage is in the range of what is needed. Personally I would go for a 1TB of storage but you need something with a fan which would be a MacBook Pro or mini.

- The base M4 MBP would probably be best because of the active cooling, itâ€™ll allow for better sustained performance and also comes with a slightly bigger battery.
- Pro model recommended so the active cooling system, the MacBook Air doesnâ€™t handled loads well.

- ## [How are people running an MLX-compatible OpenAI API server locally? : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mg26g0/how_are_people_running_an_mlxcompatible_openai/)
- If you are on a Mac then LM Studio is about your only choice for a mature, stable, fast, reliable, supported, maintained MLX server.

- ## [Adjust VRAM/RAM split on Apple Silicon Â· ggml-org/llama.cpp _202307](https://github.com/ggml-org/llama.cpp/discussions/2182)
- just do: `sudo sysctl iogpu.wired_limit_mb=<mb>` from Terminal. Youâ€™d have to do it every boot as itâ€™s not sticky

- ## ğŸ¤” [MLX now has MXFP4 quantization support for GPT-OSS-20B, a 6.4% faster toks/sec vs GGUF on M3 Max. : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1n4mxrj/mlx_now_has_mxfp4_quantization_support_for/)
- The speed increase is pretty noticeable, tested with short prompts, on M4 MAX 128GB
  - mlx-community's gpt-oss-120b-MXFP4-Q8: 80tps
  - ggml 120b: 75tps, with fa enabled
  - unsloth 120b f16: 61tps, with fa enabled
- Agree with your observation, the new gpt-oss-120b-MXFP4-Q8 that was released just several days ago is indeed faster by ~25% based on my testing than the unsloth full precision (fp16) - even full FA enabled. Quality appears to be the same, because it doesnâ€™t really matter Q8 or fp16 for that model, as 95% of the params were trained with native MXFP4 â€œfull precisionâ€, that is why also all unsloth quants and MLX Q4 and Q8 weights almost the same!

- itâ€™s still has to be software optimisation right since M series chip doesnâ€™t natively support this quantization

- ## [Upcoming MLX support news? : r/unsloth _202508](https://www.reddit.com/r/unsloth/comments/1mlsoar/upcoming_mlx_support_news/)
- will there be UD MLX quants?
  - Oh maybe if demand is more!

- That would be so amazing! Love unsloth quants and I always feel like I need to make a tough choice between them and MLX ones. 

- ## ğŸ [Apple users: Unsloth's quants could be coming to MLX - if we show interest : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1moqhvf/apple_users_unsloths_quants_could_be_coming_to/)
- Mac user.  I would be very interested to see how they could benefit the MLX community.  
  - I highly respect Llama.cpp and GGUF but MLX DWQ quants are very high quality and faster than their GGUF cousins on this hardware. I find myself unwilling to go back unless I have to.  Thankfully MLX support, unless first party, often comes faster than Llama.cpp so we have been well served.
- Last I heard the performance delta was <10%. Id much prefer them just supporting the llama.cpp ecosystem rather than diluting their time
  - Then you've heard a while ago because the performance delta is huge now. I get 16t/s on MLX and 9t/s on llama.

- Would very much like to run unsloth dynamic mlx quants on my mac studio

- That would be amazing! MLX is great but still lack the support of other inference engines imo.

- ## ğŸ¤” [MLX you're using in release LM Studio seems to be outdated and broken for Q8 and larger models Â· Issue Â· lmstudio-ai/mlx-engine _202411](https://github.com/lmstudio-ai/mlx-engine/issues/34)
- The next version of our app should help remedy this, since we'll be shipping the latest version of MLX.

- The guidance is incomplete and solves nothing. The correct kernel tweaks to make these work are actually:
  - Without the `disable_wired_collector` setting, the problem remains.

```sh
sudo sysctl iogpu.disable_wired_collector=1
sudo sysctl iogpu.wired_limit_mb=n
```

- To make these persistent across reboots:
  - [Persistent sysctl Settings - Apple Community _202204](https://discussions.apple.com/thread/253840320?sortBy=rank)

- ## ğŸ’¡ [Run LLMs on macOS using llm-mlx and Appleâ€™s MLX framework | Lobsters _202502](https://lobste.rs/s/1reyhf/run_llms_on_macos_using_llm_mlx_apple_s_mlx)
  - By default, MacOS allows 2/3rds of this RAM to be used by the GPU on machines with up to 36GB / RAM and up to 3/4s to be used on machines with >36GB RAM.

- I havenâ€™t tried mlx the library yet, but when I was using LM Studio with their MLX backend, I had to run `sudo sysctl iogpu.wired_limit_mb=<something>` to increase the VRAM limit.
  - However, be aware that the system would panic when you try to load a big model or use a large context size when `<something>` is too big.

- By the way, I found that if you have models in` ~/.cache/huggingface/hub` already, due to having used mlx-lm directly, the llm mlx download-model mlx-community/â€¦ command will skip the download and just add them to llmâ€™s index.

- When comparing popular quant q4_K_M on Llama.cpp to MLX-4bit, in average, MLX processes tokens 1.14x faster and generates tokens 1.12x faster. This is what most people would be using.

- ## [Can't run Mixtral on GPU with M2 Pro 32 gb VRAM : r/LocalLLaMA _202312](https://www.reddit.com/r/LocalLLaMA/comments/18l9kal/cant_run_mixtral_on_gpu_with_m2_pro_32_gb_vram/)
  - I was able to run Mixtral Q5 on my macbook on CPU, but when I choose Apple Metal (GPU) in LM Studio I get this error:
  - GGML Metal Error: command buffer 4 failed with status 'Error'. Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory)
  - Worked for Q4_K_M when increasing the VRAM limit

- Gpu only has access to about 22gigs on a 32gig macbook, check the Llama.cpp or this community for a thread where setting a few command line arguments disables this limit.

- By default you can have up to 22GB of RAM allocated to the GPU but you can increase that amount up to 29GB with the system running fine. Try 27-28GB if you want to run a web browser on the side. In the terminal type the following (mb=0 to reset to default settings):
  - sudo sysctl iogpu.wired_limit_mb=29256
  - sudo sysctl iogpu.wired_limit_mb=0
  - Youâ€™d have to do it every boot as itâ€™s not sticky .
- 32GB might be too tight for 32k ctx and would usually crash the machine. 12k ctx seems to be the limit for 32GB.

- Why no one talks about the new ml framework mlx from apple? Is it not applicable here?
  - It would hit the same limit that a q5 quant of dolphin-mixtral needs more GPU memory than can be allocated on a 32GB Mac, even after tweaking the kernel limit.

- ## ğŸ§© [First time using MLX and MacOS for inference: getting poor results : r/LocalLLaMA _202409](https://www.reddit.com/r/LocalLLaMA/comments/1fpy26p/first_time_using_mlx_and_macos_for_inference/)
- Set sysctls `sudo sysctl iogpu.wired_lwm_mb=400000 && sudo sysctl iogpu.wired_limit_mb=150000` , reboot
- iogpu.wired_limit_mb=150000 sets the maximum amount of wired memory the GPU is allowed to allocate to 150, 000 MB (~150 GB)
  - sets the maximum GPU memory allocation to 150GB
- iogpu.wired_lwm_mb=400000 sets the low-water mark (LWM) for the GPUâ€™s wired memory to 400, 000 MB (~400 GB).
  - In memory management, a low water mark typically represents a threshold. When the amount of free memory drops below this mark, the system might take certain actions to free up more memory.

- iogpu.disable_wired_collector=1
  - tells macOS to disable the â€œwired memory collectorâ€ for the GPU.
  - Normally, the collector periodically trims/reclaims GPU wired memory (VRAM allocations backed by unified RAM) when the system is under pressure.
  - Disabling it (=1) means the GPU can hold onto large wired allocations without the OS trying to recycle them. This is sometimes done when running ML/LLM workloads on Apple Silicon, because the collector can otherwise kill big GPU allocations and crash jobs.
  - Setting this to 1 disables this collector, potentially leaving GPU memory allocations "locked" and unadjusted even when the system needs memory.

- MLX is lazy so it won't load models until it needs to. So the time-to-first-token should be a lot lower on the second request.

- Run the command to expand the max VRAM allocation (needs to be done after each reboot):
  - sudo sysctl iogpu.wired_limit_mb=120000

- ## [Can the iogpu.wired_limit_mb setting on MacOS be persisted? ğŸ“£ YES! : r/LocalLLaMA _202405](https://www.reddit.com/r/LocalLLaMA/comments/1cncgqe/can_the_iogpuwired_limit_mb_setting_on_macos_be/)
- Can CPU intensive, RAM demanding tasks take RAM seemlessly when not running GPU inference?
  - Yes, this sets the higher limit. Clearly the problem is that under heavy load, GPU will take it all leaving just 8GB to the other apps and OS.

- è®¾ç½®sysctlå¹¶æ‰§è¡Œ

```sh
sudo mv io.github.sysctl.plist /Library/LaunchDaemons/
sudo chown root:wheel /Library/LaunchDaemons/io.github.sysctl.plist
sudo launchctl load /Library/LaunchDaemons/io.yaoo.sysctl.plist
```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
 <key>Label</key>
 <string>io.github.sysctl</string>
 <key>ProgramArguments</key>
 <array>
 <string>/usr/sbin/sysctl</string>
 <string>-w</string>
 <string>iogpu.wired_limit_mb=27648</string>
 </array>
 <key>RunAtLoad</key>
 <true/>
</dict>
</plist>

```

- Rather than dealing with plist syntax, add `iogpu.wired_limit_mb=122800` on a line by itself to `/etc/sysctl.conf`. Create the file if it doesn't already exist. It should probably be owned by `root:wheel` and mode should be 644.

- ## ğŸ’¡ [M1/M2/M3: increase VRAM allocation with `sudo sysctl iogpu.wired\_limit\_mb=12345` (i.e. amount in mb to allocate) : r/LocalLLaMA _202311](https://www.reddit.com/r/LocalLLaMA/comments/186phti/m1m2m3_increase_vram_allocation_with_sudo_sysctl/)
  - One note on this ... All macos systems would be happiest to have at least 8gb available for OS stuff.
  - this is not a permanent change and it automatically resets on reboot.
  - PS: to get the current value use: sudo sysctl iogpu

- I looked at what wired memory (memory that can't be swapped) was without having an LLM loaded/running and then added a margin to that. I ended up allocating 26.5GB, up from 22.8GB default.
  - It worked, but it didn't work great because I still had a bunch of other stuff running on my Mac, so (not surprisingly) swapping slowed it down. For anything more than a proof of concept test I'd be shutting all the unnecessary stuff down.

- On my 32GB Mac, I allocate 30GB. [Macs with 32GB of memory can run 70B models with the GPU. : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/18674zd/macs_with_32gb_of_memory_can_run_70b_models_with/)
  - That's what I do and I have no swapping at all. I listed the two big things to turn off to save RAM. 
  - I don't use the GUI. Just simply logging in and doing nothing uses a fair amount of RAM. I run my Mac headless. I ssh in.
  - I stopped the `mds_stores` process from running

- consider llama.cpp instead of LM Studio to reduce RAM requirements a bit (still need to run terminal.app but this is pretty light)

- As per the latest developments in that discussion, "iogpu.wired_limit_mb" only works on Sonoma. So if you are on an older version of Mac OS, try "debug.iogpu.wired_limit" instead.

- How can I check if the changed worked? I did the intial code and said it was initially set to 0
  - One way I've verified is through the llama.cpp diagnostic output. It reports the available vram as well as the size of the model and how much vram it requires.
  - I've got 32gb total and I think the default availability is approx 22gb. So I can easily increase to 26gb and I see the difference immediately when I launch llama.cpp - the available vram will be reported as 26gb.

- ## ğŸ’¡ [Macs with 32GB of memory can run 70B models with the GPU. : r/LocalLLaMA _202311](https://www.reddit.com/r/LocalLLaMA/comments/18674zd/macs_with_32gb_of_memory_can_run_70b_models_with/)
  - I recently got a 32GB M1 Mac Studio. I was excited to see how big of a model it could run. It turns out that's 70B. It is a Q3_K_S model so the 2nd smallest for 70B in GGUF format
  - Mac shouldn't be able to dedicate that much RAM to the GPU. Apple limits it to 67%, which is about 21GB. This model is 28GB. So it shouldn't fit. But there's a solution to that thanks to these smart people here.
  - This new method is just setting the value of a variable. llama.cpp even tells you the value, how much RAM, it needs.
  - > sudo sysctl iogpu.wired_limit_mb=57344 ( I did that for my 64GB, you'd want to change the 57344 )

- â‰¥64GB allows 75% to be used by GPU. â‰¤32 its ~66%. Not sure about the 36GB machines.

- I also do these couple of things to save RAM.
  - I don't use the GUI. Just simply logging in and doing nothing uses a fair amount of RAM. I run my Mac headless. I ssh in.
  - I stopped the mds_stores process from running. I saw that it was using up between 500MB and 1GB of RAM. Its the processes that indexes the drives for faster search. 
  - With all that set, the highest I've seen in use memory is 31.02GB while running a 70B Q3_K_S model. So there's headroom. 

- You may try and run one of Q4 models without problems: because llama.cpp uses mmap to map files into memory, you can go above available RAM and because many models are sparse it will not use all mapped pages and even if it needs it, it will swap it out with other pages on demand... I was able to run falcon-180b-chat. Q6_K which uses about 141GB on a 128GB Windows PC with less than 1% SSD reads during inference... I could even run falcon-180b-chat. Q8 which uses about 182GB but in this case SSD was working heavily during inference and it was unbearably slow (0.01 tokens/second)...
  - Yes. I've done that before on my other machines. Llama.cpp in fact defaults to that. The hope for me was that since the models are sparse that the OS would cache the relevant parts of the models in RAM. So the first run through would be slow but subsequent runs would be fast since those pages are cached in RAM. 

- ## ğŸ§  [Instantly allocate more graphics memory on your Mac VRAM Pro : r/LocalLLaMA _202504](https://www.reddit.com/r/LocalLLaMA/comments/1k1tdpa/instantly_allocate_more_graphics_memory_on_your/)
  - I built a tiny macOS utility that does one very specific thing: It unlocks additional GPU memory on Apple Silicon Macs.
  - Why? Because macOS doesnâ€™t give you any control over VRAM â€” and hard caps it, leading to swap issues in certain use cases.
  - Do you need this app? No! You can do this with various commands in terminal. But wanted a nice and easy GUI way to do this.

- https://github.com/Sub-Soft/Siliv /NALic/202504/python
  - A simple macOS menuâ€‘bar utility to adjust Apple Silicon GPU VRAM allocation
  - Siliv provides a convenient way to view and set GPU VRAM allocation on Apple Silicon Macs via `sysctl` .
  - Persistence: `sysctl` changes may reset after reboot; 

- ğŸ¤” did you know that MLX ignores the VRAM limit global? do you know a way to make MLX use the extra VRAM allocated?
  - If you're on MacOS 15 or higher: Just run `sudo sysctl iogpu.wired_limit_mb=14336` on the terminal and replace 14336 (14GB) with your desired VRAM allocation in MB
- That's the global I use, and MLX ignores what's set for it. Using LM Studio, I can load and use q8 quants of 70B ggufs, but MLX models the same size, run out of memory.

- don't bother, this app just does this: `sudo sysctl iogpu.wired_limit_mb=24576`.
  - Yeah I already have a version of this aliased in my .zshrc file whenever I feel I need it (or to reset). 
  - Dont work , its a very old command for intel igpus ... 

- ## ğŸš€ [LM Studio ships an MLX backend! Run any LLM from the Hugging Face hub on Mac blazingly fast! âš¡ : r/LocalLLaMA _202410](https://www.reddit.com/r/LocalLLaMA/comments/1fz6z79/lm_studio_ships_an_mlx_backend_run_any_llm_from/)
- M1 Max 32GB, Qwen2.5-14B-Instruct
  - Q4_K_M: 15.15 t/s
  - 4 bit MLX: 23.10 t/s

- M2 Max 64GB, Qwen2.5-32B-Instruct
  - Q4_K_M: 14.78 tok/sec
  - 4 bit MLX: 17.62 tok/sec

- One bonus from MLX I hadn't anticipated but is nice is not the speed difference, but the VRAM savings
  - Running on an M2 Ultra Mac Studio 192GB Ram.
  - mlx-Llama-3.1-70B-Instruct-8Bit - 82GB RAM/VRAM used - 53 Watts Peak Power - 8.62 tk/s
  - Llama-3.1-70B-Instruct-GGUF Q8_0 - 123.15GB RAM/VRAM used (Flash Attention enabled) - 53.2 Watts Peak Power - 8.2 tk/s
- Yes, this! Plus (potentially in a future LMStudio update) with the circular buffer + infini cache, it means we can fit much stronger models in and not worry about memory footprint increases with conversation length! I can finally get 8bit 70b models comfortably in my 64gb and use them all the way up to the 100k token limit 

- just FYI the reduction in VRAM is because MLX doesn't pre-allocate memory, whereas llama.cpp does.
  - Simple calculation: Model size at Q8 is ~75 GB, KV cache for 128k context is 40GB
  - Required memory to load the model and fill the cache is at least 75+40 = 115 GB.
# discuss-llama.cpp
- ## 

- ## 

- ## 

- ## [llama.cpp: Multi-host inference slower than single-host? : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pwmpcn/llamacpp_multihost_inference_slower_than/)
  - I have two computers running locally and I want to see how I can get faster generation speeds by combining them instead of running the models separately on each computer.
  - I've built a very recent version of llama.cpp on both hosts (jetson using CUDA12 and Dekstop using ROCm 6.7). I use the unsloth Qwen3 80B Q8. This model is 87GBs and hence it's larger than both hosts individually, but the entire model fits into RAM when combined.
  - Using both combined yields a generation speed of 1.1 t/s. However, if I use the desktop llama-cli command exactly the same as above but remove the --rpc "$JETSON_IP_ADDR:12400" (hence disabling multi-host), then I'm at double the speed of 2.2 t/s.

- Even with no latency, you won't get faster speed with llama.cpp, because it can't do tensor parallel, only layer splitting. It allows to serve larger models, but doesn't increase speed.
  - You can do tensor parallel with vllm, but your interconnect will be a bottleneck, unless you use RDMA capable NIC (ConnectX from NVidia/Mellanox).
  - I see you have ROCm/CUDA mix and uneven VRAM distribution, so vllm won't work either.

- ## ["Router mode is experimental" | llama.cpp now has a router mode and I didn't know. : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pfssoo/router_mode_is_experimental_llamacpp_now_has_a/)
- While this is cool, I still do think llama-swap is very mature and worth using rn.

- ## [Mastering llama.cpp: A Comprehensive Guide to Local LLM Integration : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ov2ll9/mastering_llamacpp_a_comprehensive_guide_to_local/)
- no mention of override tensor, CPU MoE offloading, API keys, and a lot more stuff. To the people who found this Reddit post, this guide pretty surface level.

- ## [Question about "./llama-server" prompt caching : r/LocalLLaMA _202507](https://www.reddit.com/r/LocalLLaMA/comments/1lujz2h/question_about_llamaserver_prompt_caching/)
  - Does `./llama-server` support prompt caching (like `--prompt-cache` in the CLI), and if not, whatâ€™s the correct way to persist or reuse context between chat turns to avoid recomputing the full prompt each time in API-based usage (e.g., with Open WebUI)?
- Yes, it's enabled by default but only for one chat: If you have stuff in Open WebUI that makes use of the same model (title and tag generation, autocomplete, etc.) then it will be sending different requests which invalidates the main chat.

- Yes, llama-server supports it, no parameter needed. It's up to the client to make use of it. Works fine with Open WebUI (GUI) but you're asking for API. I guess you'll have to initiate a chat and send the chat_id in all your subsequent API calls. I did not test this yet.

- ğŸ§‘â€ğŸ« [Tutorial: KV cache reuse with llama-server Â· ggml-org/llama.cpp _202505](https://github.com/ggml-org/llama.cpp/discussions/13606)
  - This tutorial demonstrates how to use the slots management feature in llama-server to optimize repeated prompt processing through KV cache reuse.

- ## ğŸ†šâš¡ï¸ [Performance of llama.cpp on Apple Silicon M-series Â· ggml-org/llama.cpp _202311](https://github.com/ggml-org/llama.cpp/discussions/4167)
  - æä¾›äº†å„ç§mac, airçš„æ¨¡å‹æµ‹è¯•æ•°æ®

# discuss-lmstudio-roadmap
- ## 

- ## 

- ## [Phi 3.5 Vision Instruct Fails to Load "Trust remote code" error Â· Issue Â· lmstudio-ai/mlx-engine _202411](https://github.com/lmstudio-ai/mlx-engine/issues/29)
- ğŸ‘·: If you're seeing a `Trust remote code` issue, it means that the model is not (yet) supported by `mlx-engine` . Specifically, support for that model's tokenizer and/or config has not been merged into huggingface transformers.
  - We choose to block loading a model instead of enabling remote code because we are wary of letting users download and run potentially malicious code.

- [when load mlx-community/Kimi-VL-A3B-Thinking-4bit, it prompts trust_remote_code=True. Â· Issue #607 Â· lmstudio-ai/lmstudio-bug-tracker](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/607)
- 
- 
- 
- 
- 

- ## ğŸ–¼ï¸ [Qwen3-VL kinda sucks in LM Studio : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1ock0lc/qwen3vl_kinda_sucks_in_lm_studio/)
- LM Studio apparently downscales to 500x500 ish. llama.cpp is better for multimodal for now until LM Studio fixes this.
- LM Studio downscales images to just 1024x and gives you no way to increase it.
- LM Studio used to downscale the image to just 512x512 and it was giving terrible results at times. Now they have increased it to 1024x1024. But I still find it not as good as just running it directly on MLX.
- I believe lmstudio (currently) downscales/resizes images, while qwen3-vl can accept (and performs better with) arbitrary image sizes. There was a post a few days ago about how they were going to make it optional, but I don't know if they've done that yet.

- ## [LM Studio Fails to Load 32B 4bit Models on macOS M4 (Freezes System) While Ollama Works Normally Â· Issue Â· lmstudio-ai/lmstudio-bug-tracker _202503](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/484)
- It seems like LM Studio isn't correctly utilizing swap memory, as there's a small spike in usage when the model is initially loaded, but afterwards it doesnâ€™t exceed 24GB of memory.

- Only fix for this inside LM Studio that I found is to set Model loading guardrails to Custom and a memory limit to 23GB, because if it goes over 23GB it crashes macOS as LM Studio doesn't use swap memory if it uses over the total available 24GB of memory of the Mac Mini M4 Pro. With this guard setup if the model is too big and exceeds over 23GB of memory, it unloads the model and not crash macOS. For models 30B+, it's better to use Q3 or MLX 3bit because Q4/4bit is too big for 24GB of RAM. I wish I had known about this before buying the 24GB version

- ## [Incompatible type of `tool_choice` Â· Issue Â· lmstudio-ai/lmstudio-bug-tracker _202505](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/670)
- 
- 

- [[Bug]: Invalid value for 'tool_choice' Â· Issue #7039 Â· All-Hands-AI/OpenHands](https://github.com/All-Hands-AI/OpenHands/issues/7039)
  - When I switch to use OpenAI (gpt-4o), it working fine.

- ## [Qwen3 Models not Recognized as Embedding Types for MLX Format Â· Issue Â· lmstudio-ai/lmstudio-bug-tracker _202507](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/808)
  - Qwen3 models not recognized as embedding models in MLX format (but same models are ok in GGUF)

- From what i can understand, while the chat models are using mlx as an inference engine, LMStudio would need to use a package like `mlx-embeddings` to properly handle MLX Embeddings, and integrate it in there LM Studio MLX Runtime.
  - This would explain why the `Override Domain Type` option is not available for MLX: Only the 'LLMs' models are supported for mlx.
- there is no support for mlx embeddings in LMStudio
- The only workaround for now using LMStudio is to use the GGUF versions and take the performance hit

- [[Models] Qwen3 Embedding shown as LLM instead of an embedding model Â· Issue Â· lmstudio-ai/lmstudio-bug-tracker](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/696)
# discuss-lmstudio
- ## 

- ## 

- ## ğŸ”€ is it possible in MLX-LM to have inference from multiple LLMs at the same time? 
- https://x.com/OrganicGPT/status/1982460752507085256
  - I'd like to serve multiple AI agents on MStudio simultaneously but last I checked with llama.cpp the GPU could handle only one request at a time even with multiple threads/processes
  - Mac Studio M3 Ultra's massive 512GB memory is perfect for parallel agent calling! Here I used @lmstudio to run *40* instances of @openai 's gpt-oss-20b-mlx and got 35% performance boost compared to sequential API calling. @awnihannun , next I'll do it with batching like you said!

- Running 40 instances in parallel sounds very promising for performance.

- 2 LM Studio on 2 different ports and you get it.

- Yea, you can do this with different processes. But it's much better to batch requests with the same model using a single process.
  - Each request uses up most of the memory bandwidth so having two models run simultaneously on two processes won't be much faster than running them sequentially. 
  - When you batch properly with the same model (as in `mlx_lm.batch_generate`) you don't double the memory i/o because the model weights are the same.
  - Note, this is not specific to MLX or Apple silicon. This is a general property of LLM inference on GPUs. If you have a low latency implementation it should be using most of the memory bandwidth even at batch size 1 so multi-process with multiple models *running at the same time* on the same GPU is not optimal.

- If youâ€™re using Linux OS in production you can use vLLM which maximizes hardware utilization to handle many concurrent requests with low latency using an OpenAI-compatible API.
  - vLLM performs automatic continuous (dynamic) batching of incoming requests via its scheduler to maximize throughput.

- Yesterday I have written python script for M3 Ultra which tries to do that with batch_generate function, depends on the batch size, you can process requests in batch, other requests have to wait until batch is finished, and then another batch is processedâ€¦ and so on

- If you need a load balancer, you can try https://github.com/autolocalize/lm-studio-loadbalancer

- ## [LM Studio not reading document correctly. But why? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1o98vqd/lm_studio_not_reading_document_correctly_but_why/)
- So rag-v1 plugin LM Studio is using has a simple rule: if file fits into 70% of remaining context, it will be fully injected - and this is what you want. Otherwise it resorts to RAG retrieval.

- ## [In LM Studio + MoE Model, if you enable this setting with low VRAM, you can achieve a massive context length at 20 tok/sec. : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o6dnzc/in_lm_studio_moe_model_if_you_enable_this_setting/)
  - enable: Force Model Expert Weights onto CPU, Flash Attention
  - Worked for my 3060
- Since you offload on cpu ram the speed decrease significantly.

- If you use llamacpp directly, you can further fine tune the number of expert layers pushed to CPU rather than pushing them all out. With careful adjustment and if there is enough headroom on your GPU, you might double your t/s vs the option used by lm studio.

- You can set KVcache to Q8 and double the context size. Q8 has a very small loss in quality but might be worth it to you. 

- ## [Is there any all-in-one app like LM Studio, but with the option of hosting a Web UI server? : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1larzxz/is_there_any_allinone_app_like_lm_studio_but_with/)
- llama.cpp's llama-server hosts a very basic webUI by default. It's hosted at the server root, without the API endpoint.

- LM Studio + OpenWebUI work quite well together and you can share it via a VPN like Tailscale if you want to access it from anywhere. 

- Anything can be anything with docker. Make a docker compose once that launches all the things

- ## [Best Opensource LM Studio alternative : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mrxuwd/best_opensource_lm_studio_alternative/)
- llama.cpp: `llama-server -m model.gguf` http://localhost:8080
- Add Llama-Swap to make it hot swap models. Open WebUI is a sleek interface.
- what is llama swap
  - Itâ€™s a small proxy server (portable, no installation needed) that runs your Llama Cpp instance but offers its OpenAI compatible API to any client you have. Once you connect to this proxy and request any model by name, it will load it up and serve.
- This is the only correct answer. Start here. You will not be dependent on some company that wants to make money at some point.

- The problem with the llama.cpp / llama swap configuration is that the easy install is Vulcan only and if you buy newer hardware, aka, 50 series stuff, you have to build it from source. Most of the people using lm studio or ollama are not set up for that.

- Really wanted to use jan.ai since its fully open source but its lacking many features of lmstudio 
  - Jan doesn't have RAG to chat with document files 
  - qwen 3 30b a4b runs at 3 t/s instead of 17 on lm studio. using 2080 super
  - No projects folder option like in lm studio/ chatgpt 

- Jan AI if you want an all-in-one desktop app that runs both the AI and the GUI. Open source and looks very nice. Best LM Studio alternative IMO.

- KoboldCPP feels more like LM Studio because it's available as a single binary.
  - If only it wasn't ugly as all hell. Really needs.. some.. no.. A LOT.. of UI work.
- Agreed. They should invest some in creating a new UI. Lots of good backend stuff... There is a lot I love about Oobabooga that I wish they would adopt.

- I like gpustack, they run llamabox which is based on llama.cpp

- GPT4ALL was nice but it's a dead project now.

- ## [Why do people say LM Studio isn't open-sourced? : r/LocalLLaMA _202405](https://www.reddit.com/r/LocalLLaMA/comments/1cvawmz/why_do_people_say_lm_studio_isnt_opensourced/)
- LMstudio is free in cost but it's proprietary software.

- Surely not all freeware are open source.
- The power of LM Studio is 4 things:
  - model discovery is incredibly easy, directly to huggingface gguf repositories
  - it's a direct inferencing app, can load models itself
  - able to work as a standalone endpoint server
  - it can loads multiple model on available GPUs

- i'm currently building https://kolosal.ai it's a free and opensource platform to run LLM on device, 
  - and the best part? it's only 20mb and can run on both CPU and GPUs, 
  - and it's also using `llama.cpp` as backend so the performance wouldn't be any different with lmstudio
- Is this windows only? 
  - Unfortunately currently yes. But, we're using framework that mostly is crossplatform

- because it's not open source, you cannot view their code or fork it to change it. the open source direct alternative to lmstudio is jan ai

- ## ğŸ [Is there an alternative to LM Studio with first class support for MLX models? : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1l0ct34/is_there_an_alternative_to_lm_studio_with_first/)
  - I've been using LM Studio for the last few months on my Macs due to it's first class support for MLX models (they implemented a very nice MLX engine which supports adjusting context length etc.
- While it works great, there are a few issues with it:
  - it doesn't work behind a company proxy, which means it's a pain in the ass to update the MLX engine etc when there is a new release, on my work computers
  - it's closed source, which I'm not a huge fan of
  - I can run the MLX models using `mlx_lm.server` and using open-webui or Jan as the front end; but running the models this way doesn't allow for adjustment of context window size (as far as I know)

- Now I use MLX more because of it's GPU usage is not blocking macOS visual fluidity. My Mac screen rendering (especially when doing multitasking with Stage Manager) a lot stutter when inferencing with llama.cpp, but still fluid with MLX. Yes, there are not as mature as llama.cpp, but this factor made me swith to MLX only. I run it using LM Studio as an endpoint.

- ## ğŸ¤” [LM Studio incredibly slow (1.2 tokens/sec) on a 3090, despite model (Qwen 2.5 32B 4xs) fitting entirely into VRAM and not yet hitting the context length. : r/LocalLLaMA _202411](https://www.reddit.com/r/LocalLLaMA/comments/1gqa5xx/lm_studio_incredibly_slow_12_tokenssec_on_a_3090/)
- increasing context size also increases Vram overhead. From what I remember it's roughly something to the tune of each 4k = 1gb vram.
  - Yeah. It's just that LM Studio shows VRAM usage and it's not yet hitting the limit. Unless it doesn't show if it goes over for some reason?
  - Halving the context to 16k allows me to fit everything.

- If you never figured it out its probably the Guardrails just go to hardware and turn them off, also bigger the context the more vram it needs, turn mmap and dont keep in ram off and turn flash attention adn K cache on.

- ## ğŸ§© [Does the number of bits in KV cache quantization affect quality/accuracy? : r/LocalLLaMA _202502](https://www.reddit.com/r/LocalLLaMA/comments/1iuw1kx/does_the_number_of_bits_in_kv_cache_quantization/)
- Setting the KV cache to Q8 has only a minimal influence on the results. 
  - Setting the KV cache to Q4 has quite an impact though. 
  - Setting K to F16 or Q8 and V to Q4 still achieves decent results though.
  - Just the extensive test that the author of the KV quantization in llama.cpp did that I linked above. The results make sense, as the keys are used to find the right value, and mismatching keys due to higher quantization will lead to incorrect values, whereas correctly looked up values that have been quantized will still be somewhat related to the original information.

- ## [LMStudio, KV cache and context length : r/LocalLLaMA _202502](https://www.reddit.com/r/LocalLLaMA/comments/1iyv8t6/lmstudio_kv_cache_and_context_length/)
  - I've turned on KV cache quantization because it "may lead to faster generation"
  - LMStudio's UI also mentions that when this feature is on, the context length value doesn't matter. So, I'm wondering how big the actual context is with this setting enabled?

- Multiplicative. Normal is fp16. Q4 quadruples your context length with minimal loss (except qwen)

- under the KV Cache Quantization option that "Context Length setting is ignored when using KV Cache Quantization" and unsurprisingly completely ignores whatever you have in the field.
  - Based on pasting a 10k-ish token article into a conversation, and LM Studio filling the context to 257.8%, it seems like it's set to 4096 by default... which really seems like a huge waste of being able to quantize the cache, but again there is no additional field or slider to adjust context size when quantization is enabled.
# discuss-ollama-roadmap
- ## 

- ## 

- ## 

- ## [FR: Meaningful names of models in models/blobs dir _202501](https://github.com/ollama/ollama/issues/8466)
  - Please make models to have meaningful filenames (like user/modelname-quantization.gguf) in models/blobs directory, so they can be (easier) used with other model inference software.

- The blobs are content addressable which means that if you have two or more models which share the same content they will share the same blob which saves space on your disk and also means you don't have to pull down that data if you already have it. The format is also changing with the new engine though so it probably won't be useful with other tools unless they support the new format.

- You use less disk space because the many of the quantizations of the tensors share the same tensors.

- ## [MLX backend Â· Issue Â· ollama/ollama _202312](https://github.com/ollama/ollama/issues/1730)
- Add mlx-vlm backend also.

- Lack of MLX support is the only reason I don't use Ollama. In some cases MLX Q8 is 20% faster than GGUF, and memory usage is better handled.

- ğŸ“¡ [Draft MLX go backend for new engine by dhiltgen Â· Pull Request Â· ollama/ollama _202502](https://github.com/ollama/ollama/pull/9118)
# discuss-ollama

```sh
# ollamaåŸå§‹çš„ç‰ˆæœ¬æ˜¯å¹¶ä¸å…¼å®¹ OpenAI æ ¼å¼çš„è¯·æ±‚
curl http://localhost:11434/api/chat -d '{
  "model": "gemma3:4b",
  "messages": [
    { "role": "user", "content": "why is the sky blue?" }
  ]
}'
# æ¨¡å‹å“åº”è¾“å‡ºå¦‚ä¸‹ 
{"model":"gemma3:4b","created_at":"2025-07-31T16:54:00.804772Z","message":{"role":"assistant","content":"That"},"done":false}

```

- ## 

- ## 

- ## 

- ## [How does Ollama truncate the context when it's too long? : r/ollama _202512](https://www.reddit.com/r/ollama/comments/1pe18c7/how_does_ollama_truncate_the_context_when_its_too/)
  - I'm trying to understand how prompts are truncated then they exceed the context limit. Let's say I have the num_ctx parameter set to 1000 tokens. I then send in a system prompt, and a whole bunch of user and assistant messages, but in total there are 2000 tokens in the context. What happens?

- if your system prompt is 100 tokens, and your message history is 1000 tokens, and your context limit is 1000, Ollama will process the full system prompt and the most recent 900 message history tokens.
  - You're supposed to manage your context yourself/using whatever app you may use Ollama with, to ensure it is within the limit.

- ## [feat: add trace log level by mxyng Â· Pull Request Â· ollama/ollama _202505](https://github.com/ollama/ollama/pull/10650)
  - OLLAMA_DEBUG= or OLLAMA_DEBUG=0 or OLLAMA_DEBUG=false: unset or empty or falsy values sets default INFO level
  - OLLAMA_DEBUG=1 or OLLAMA_DEBUG=true: set DEBUG level
  - OLLAMA_DEBUG=2: set TRACE level

- [OLLAMA_DEBUG=1 Not Showing Prompts in Logs ](https://github.com/ollama/ollama/issues/10950)
  - OLLAMA_DEBUG=2

- ## ğŸ  [Web front end for Ollama? Is llama.cpp what I'm looking for? : r/ollama](https://www.reddit.com/r/ollama/comments/1p0p1pm/web_front_end_for_ollama_is_llamacpp_what_im/)
- Ollama is a wrapper and somewhat of a fork of llama.cpp. Ollama runs on a core version of llama.cpp but doesn't have the same capabilities or features as far as granular controls.
  - But they're at their heart inference engines first thst happen to have their own options front ends. I've used both with Open WebUI for about 8 months.
  - Ollama has a chat interface now that is not cli. But it's just chat and some basic rag. If you want pipelines, oauth, in depth RAG etc., you need something like OWUI, Libre Chat, AnythingLLM or LocalAI

- ## [502 Bad Gateway Â· Issue Â· ollama/ollama _202407](https://github.com/ollama/ollama/issues/5437)
- you're right, I ignored the proxy.

- [Running without network error: ollama._types. ResponseError Â· Issue #85 Â· ollama/ollama-python](https://github.com/ollama/ollama-python/issues/85)
  - Met similar issue that direct me here, fixed by turning off the global proxy.
  - Tried turning off the wifi, and it still works (on MacOS).

- [ValueError: Ollama call failed with status code 502. Details Â· Issue #20742 Â· langchain-ai/langchain](https://github.com/langchain-ai/langchain/issues/20742)

- ## [ä½¿ç”¨OllamaEmbeddingsæ—¶ï¼ŒæŠ¥é”™ollama._types. ResponseError(status code:502)å’Œ(status code:500)_ollama 502-CSDNåšå®¢](https://blog.csdn.net/m0_70647377/article/details/146829186)
- 1. å…³é—­VPNä»£ç†ï¼Œé‡è¯•

- 2. åœ¨cmdä¸­è®¾ç½®ä¸¤æ­¥
  - set OLLAMA_HOST=0.0.0.0
  - ollamaÂ serve

- ## ğŸ’¡ [Electron & root priveleges Â· Issue Â· ollama/ollama _202311](https://github.com/ollama/ollama/issues/1076)
  - This project has 84% of the codebase written in Go, but for some reason it uses Electron, which is very heavy (500MB+) both for persistent memory and RAM usage.
- 202404: I successfully migrated the ollama mac app from electron to Tauri. The app package size is only 30M and the memory usage is only 100M.

- ## ğŸï¸ [Can add Stable Diffusion 3.5 modelï¼Ÿ Â· Issue Â· ollama/ollama _202412](https://github.com/ollama/ollama/issues/8047)
  - Does Olama not support the Diffusion 3.5 model? Can we add an Diffusion 3.5 model?

- Short answer is no, you shouldn't be able to as Ollama is dedicated to Large LANGUAGE models.
  - Longer answer is that in general, Stable diffusion and Large Language Models require different architectures, and while some llava models (and more recently Llama models) do have the ability to reason about images, being able to identify an image / the contents of an image is inherently different from being able to generate one, and ollama isn't designed or equiped for that.
- For stable diffusion you should be looking into something like Automatic1111, forge, comfy, or swarm (to name a few of the more popular projects).

- ### [Image generation models Â· Issue Â· ollama/ollama](https://github.com/ollama/ollama/issues/786)
- Ollama currently uses llama.cpp to do a lot of the work of actually supporting a range of large language models. This choice allowed the team to focus on delivering value in other ways. Llama.cpp's reasons for not supporting text-to-image models are probably for similar reasons.

- There are plenty of Stable Diffusion UIs and all of them are drowning in issues and features because of this:
  - Automatic1111: most popular, most fully-integrated environment
  - ComfyUI: very modular, usually has the newest features first, allows flexible workflows
  - Fooocus: restricted and opinionated but focused on providing a simple UI which produces good output out of the box (original author is the inventor of ControlNets and has a deep understanding of the inner workings of Stable Diffusion). probably the closest to Dall-E 3 (Fooocus provides a GPT2 prompt rewrite engine in the background)
  - SD. Next: fork and overhaul of Automatic1111 because everything took sooo long to implement
  - (I think all of them provide API endpoints)

- I'd recommend Stable Diffusion 1.5 because it has the lowest hardware requirements and integration is simpler and will be faster. Later models have more specifics which require more attention to detail.

- ## [image generation : r/ollama _202410](https://www.reddit.com/r/ollama/comments/1g5lp9s/image_generation/)
  - is there any model that allows image generation?

- nope, but there are some vision models that capable of taking images as input

- You can add ComfyUI, Automatic1111 or LocalAI via API to generate images in OpenWebUI, though itâ€™s somewhat finicky. It creates images from the LLMâ€˜s answer, not the Edit: prompt

- I am running Flux locally. It does a decent job in the dev model, there are 2 higher levels that arenâ€™t for commercial use.

- [Image generator : r/ollama](https://www.reddit.com/r/ollama/comments/1iuzv2v/image_generator/)
  - No llm are able to generate image... However, you can let your llm reply with tool_call to fill a image promot to be sent to another ai model that generate images (chatgpt4o-dalle3 for exemple)

- [How can I generate images with ollama? : r/ollama](https://www.reddit.com/r/ollama/comments/18laoe6/how_can_i_generate_images_with_ollama/)
- Ollama doesn't yet support stable diffusion or creating images. It does support image to text though.

- If you are using Mac you can either use diffuser or diffustionbee.

- ## ğŸ¯ Ollama 0.10! Ollama's new app is here for macOS and Windows _20250731
- https://x.com/ollama/status/1950670503376761133
- [Ollama's new app](https://ollama.com/blog/new-app)
- Ollamaâ€™s macOS and Windows now include a way to download and chat with models.
  - Ollamaâ€™s new app supports file drag and drop, making it easier to reason with text or PDFs.
  - For processing large documents, Ollamaâ€™s context length can be increased in the settings. Note: this will require more memory.
- Building on Ollamaâ€™s new multimodal engine, images can be sent to models that support them, such as Google DeepMindâ€™s Gemma 3 model.
- Code files can be processed by models for understanding.

- Is this ollama CLI with its own GUI?
  - Ollamaâ€™s new app includes the CLI. You can also use Ollamaâ€™s API or try Ollamaâ€™s Python/JavaScript libraries.

- It would be nice it it recommended models for my hardware? or is it already doing that?
  - Not yet, but we have hand selected the top models that will run on most users' hardware.

- What would be the use case over LMStudio on Mac?
  - quality, ease of use, model support directly with the major labs, hardware support
- ğŸ› I understand and appreciate ease of use and minimalism. It's really great when you'd want to chat with a model. However, as far as I understand there is still no MLX support. I'm strictly talking about MacOS version here.

- All that's missing now is MCP support.

- Can i choose where to save my models?
  - Yes, check out Ollama settings

- Feature request: Allow the user to set the Ollama endpoint. I run it on a beefy desktop computer but would like to use the app as a client.

- can you use it if the models are a other pc as a server
  - not yet. In the future, we'll add the support.

- Ollama natively support new AMD gpus?
  - We are working with @AMD to support the new GPUs

- We need our own completions API â€” I have the feeling that tool calling doesn't work properly with the OpenAI SDK. The tool tags need improvement; for example, DeepSeek R1 shows tool support, but the 8B version doesnâ€™t, even though itâ€™s based on Qwen3, so it should.

- å…¶å®æ—©å°±æœ‰ç¬¬ä¸‰æ–¹çš„äº†...... ä¸è¿‡æ„Ÿè§‰å®˜æ–¹åˆå¹¶åŠŸèƒ½äº†åæ›´æ–¹ä¾¿äº†å±äºæ˜¯

- ## Ollama 0.2 is here! Concurrency is now enabled by default.  _20240709
- https://x.com/ollama/status/1810480544976626159
  - Parallel requests: Ollama can now serve multiple requests at the same time, using only a little bit of additional memory for each request.
  - Run multiple models: Ollama now supports loading different models at the same time

- ## So this is an LLM running locally? `ollama run llama3` .
- https://x.com/eatonphil/status/1797039865470570942
  - Asking it to help me with some Postgres C code. It's complete nonsense, but it's impressive complete nonsense
  - `ollama run codestral` looks about right though.
- How much RAM am I supposed to have for the 70B model?
  - B Ã— Q / 8 â†’ RAM requirement for llm inference in GB
  - B: number of parameters
  - Q: quantization (16 = no quantization)
  - useful rule of thumb for RAM requirement for llm inference via hn user CobaltFire
- 3bit quantized should work with 32 GB RAM with other apps closed

- based on the download size that looks like the 7b model (smallest), which are pretty hopeless at anything beyond basic coding in my experience (codestral is a 22b iirc)
  - also if you like using models from the command line i highly rec @simonw 's `llm` cli! i believe it supports ollama

- ## ğŸ› [Error: pull model manifest Â· ollama/ollama](https://github.com/ollama/ollama/issues/3434)
  - Error: pull model manifest: Get "https://registry.ollama.ai/v2/library/codellama/manifests/70b": read tcp 192.168.3.79:64976->172.67.182.229:443: read: operation timed out
- Error: max retries exceeded: Get "https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/14/1436d66b69757a245f02d000874c670507949d11ad5c188a623652052c6aa508/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%! F(MISSING)20240529%! F(MISSING)auto%! F(MISSING)s3%! F(MISSING)aws4_request&X-Amz-Date=20240529T155900Z&X-Amz-Expires=1200&X-Amz-SignedHeaders=host&X-Amz-Signature=cd4472bad19931e399f39a352a4a1b0902857996b7b784b8138f168d70532277": dial tcp 104.18.8.90:443: i/o timeout

- ## [ä¸ºä»€ä¹ˆLlama2å¤§æ¨¡å‹å¯ä»¥åœ¨ä¸ªäººç”µè„‘ä¸Šéƒ¨ç½² ï¼Ÿ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/646939066)
- æˆ‘åœ¨Meatçš„å®˜ç½‘ä¸Šçœ‹åˆ° llama2 æ˜¯æ„å»ºåœ¨PyTorchä¹‹ä¸Šçš„ï¼Œè€ŒChatGPTæ˜¯åŸºäºTensorFlow Probabilityæ¡†æ¶çš„ï¼Œæœ¬æ–‡é‡Œé¢å°±ç®€ç§°TFPã€‚

- ## [Meta AI ä¸ºä»€ä¹ˆä¼šå¼€æº Llama2 å‘¢? - çŸ¥ä¹](https://www.zhihu.com/question/613072688/answers/updated)
- å› ä¸ºæ‰€è°“çš„LLMå¼€æºåªæ˜¯å…¬å¸ƒè®­ç»ƒå¥½çš„ç»“æ„å’Œå‚æ•°è€Œå·²ï¼ŒçœŸæ­£é‡è¦çš„æ•°æ®å’Œè®­ç»ƒä»£ç å¹¶æ²¡æœ‰å¼€æºï¼Œæ›´åˆ«è¯´å¤§éƒ¨åˆ†äººè¿˜æ²¡æœ‰è¶³å¤Ÿçš„GPUã€‚
  - å³ä½¿å¦‚æ­¤ï¼Œç›®å‰mistralè¿™æ ·çš„ä¹Ÿåªå¼€æº7bä¸å¼€æºlargeï¼Œllamaåç»­è¿˜å¾—ç»§ç»­è§‚å¯Ÿ

- Llama2 å¼€æºä½†ä¸æ˜¯å¯ä»¥éšä¾¿ç”¨çš„å•†ç”¨è®¸å¯ã€‚ ç”¨æˆ·æ•°åˆ°äº†ä¸€å®šç¨‹åº¦å°±ä¸æ˜¯å…è´¹çš„ã€‚
  - 7äº¿æœˆæ´»

- ## [Allow listening on all local interfaces _202310](https://github.com/ollama/ollama/issues/703)
- If youâ€™re running Ollama directly from the command line, use the
`OLLAMA_HOST=0.0.0.0 ollama serve` command

- Edit the service file: Open `/etc/systemd/system/ollama.service` and add the following line inside the [Service] section:
 `Environment="OLLAMA_HOST=0.0.0.0"`

- sudo systemctl daemon-reload
- sudo systemctl restart ollama
# discuss-janai-roadmap
- ## 

- ## 

- ## [idea: MLX support Â· Issue Â· janhq/jan _202506](https://github.com/janhq/jan/issues/5485)
  - Jan does not currently support MLX as an inference engine
- Once we finish with the llama.cpp extension, I think supporting MLX won't be too difficult. We already bundle `uvx` with Jan.

# discuss-janai
- ## 

- ## 

- ## 

- ## ğŸ†š [LM Studio no new runtimes since weeks..? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o4m7yt/lm_studio_no_new_runtimes_since_weeks/)
- Given Jan is actually open source and development is progressing more rapidly AND it's consistently more up-to-date, I don't see a reason to use LM Studio anymore other than nostalgia.
  - Do you know why Jan lags behind in making the latest models accessible through their model hub? GOtta give it to LM Studio for their super neat integration that lists models as soon as they appear on HF, whether you can run them or not...

- LM Studio gets kickbacks for promoting certain models in their model hub. For example, OpenAI paid them to promote the GPT-OSS rollout and it was one of the largest chunks of money they've ever made for anything they've ever done.
  - So Jan doesn't care as much about funneling people to their "hub" to download specific hand-picked models. I agree it's something LM Studio does much better and Jan could do better though. Until Jan allows actual search of Hugging Face and a similar compatibility screen to help new users, it will continue to be worse in this area.

- I was one of the first supporters of Jan and love to hear those great news. I saw that it is possible to import .ggufs into Jan, but with super large models such as GLM-4.6 that I downloaded through LMStudio, it is split into three .gguf files. Do you know how I can reuse them instead of re-downloading them?
  - cat model.gguf-split-a model.gguf-split-b model.gguf-split-c > model.gguf
  - [or on windows powershell] Get-Content model.gguf-split-a, model.gguf-split-b, model.gguf-split-c -Raw | Set-Content model.gguf -NoNewline
- If they're legitimately split models, I don't think that works. That method was for manual split.

- ## [Is it possible to use Jan AI server instead of LM Studio server in Obsidian Copilot? : r/LocalLLaMA _202409](https://www.reddit.com/r/LocalLLaMA/comments/1f5vswt/is_it_possible_to_use_jan_ai_server_instead_of_lm/)
- You could also try changing Jan port to 1234 to attempt to use original (unchanged) LM Studio option. If Obsidian doesn't use any uncommon API features, maybe it could work.

- Yep, there's a local API server. In the left ribbon, click the icon right above settings. Turn on the server. Visit: http://127.0.0.1:1337 to verify (there should be an api playground)
  - Make sure you are on v0.5.3+

- ## [Jan vs LM Studio : r/LocalLLaMA _202503](https://www.reddit.com/r/LocalLLaMA/comments/1j7qol7/jan_vs_lm_studio/)
- LM studio does have Rocm support, but only for RX6800 or higer or newer variants(7600, 7600XT, 7800....).
- I recommend you to stay with LM Studio, which responds more quickly to newer models than other frontends. Jan was just a full of frustrating experience as I tried it.

- Check settings in Jan i think it doesnâ€™t offload all layers to gpu fully by default

- ## [Jan. AI with Ollama (working solution) : r/LocalLLaMA _202507](https://www.reddit.com/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/)
  - I already have lot of downloaded local llms in my system via ollama.

- Go to Settings > Model Providers
  - Click OpenAI
  - In API Key field add `ollama` as api key & in Base URL enter `http://localhost:11434/v1` equivalent endpoint.
  - In Models list you can see many already available models of OpenAI you can keep it or delete it (but it's optional)
  - In Models there is and option to add new model (there is plus icon), so click on it and add you local ollama model name in my case it was `qwen2.5-coder:14b` (to see available models with exact name run ollama list command in terminal)

- ## ğŸ¯ [Jan now auto-optimizes llama.cpp settings based on your hardware for more efficient performance : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1nvzeuh/jan_now_autooptimizes_llamacpp_settings_based_on/)
  - It works with Mac too! Although it is still experimental, so do let us know how it works for you.
  - We don't support MLX yet (only gguf and llama.cpp), but we will be looking into it in the near future.

- Can the Jan server serve multiple models (swapping them in/out as required) similar to Ollama?
  - You can definitely serve multiple models similar to Ollama. Although the only caveat is that you would also need to have enough VRAM to run both model at the same time also, if not you would need to manually switch out the model on Jan.
  - Under the hood we are basically just proxying `llama.cpp` server as Local API Server to you with an easier to use UI
- The manual switching out of the models is what Iâ€™m trying to avoid. It would be great if Jan could automatically swap out the models based on the requests.
  - We used to have this, but it makes us deviate(èƒŒç¦», åç¦») too much away from llama.cpp and make it hard to maintain, so we have to deprecate it for now.
  - We are looking into how to bring it back in a more compartmentalize way, so that it is easier for us to manage. Do stay tune tho, it should be coming relative soon!
- I believe this is what llama-swap does?

- What is the use case for a chat tool without RAG? How is this better than the llama.cpp integrated Webserver? 
  - Jan supports MCP so you can have it call a search tool for example
  - It can reason - use tool - reason just like chatgpt
  - As for the use case, it's the only open source AIO(All-In-One) solution that nicely wraps llama.cpp with multiple models
- You can actually run MCP that search your own files too! A lot of our user do that through Filesystem MCP that come pre-config with Jan
  - Any file over 5MB will flood the context and become truncated. It is not an alternative. 

- RAG is definitely on our roadmap, however, like other user has pointed out, implementing RAG with a smooth UX is actually a non-trivial task. A lot of our users don't have access to high compute power, so balance between functionality and usability has always been a huge pain point for us.

- I really wish Jan would be a capable RAG-system (like GPT4all) but with regular updates and support of any gguf-models (unlike GPT4all).

- What â€œAll-In-Oneâ€ practically means for Jan
  - Runtime + engine: includes llama.cpp (local inference) plus cloud provider options so you can run models locally
  - Model management / hub
  - UI: chat UI, model hub, model settings, downloads
  - Tool / integration layer: supports MCP
  - Server/API & multi-tenant features

- Does it support multi-GPU optimization?
  - Yes, it does!
  - It does handle multi-GPU setups, but not automatically yet. 
- I found the optimizer doesn't check if the model fits in a single GPU without layer offloading to CPU. It should put -1

- Does Jan allow one to create their own agents and/or agent routing?
  - Not yet, but soon! Right now, we only have Assistant, which is a combination of custom prompt and model temperature settings

- ## ğŸ¯ [Jan now runs fully on llama.cpp & auto-updates the backend : r/LocalLLaMA _202507](https://www.reddit.com/r/LocalLLaMA/comments/1mdy1at/jan_now_runs_fully_on_llamacpp_autoupdates_the/)
  - Jan v0.6.6 is out. Over the past few weeks we've ripped out Cortex, the backend layer on top of llama.cpp. It's finally gone, every local model now runs directly on llama.cpp.
  - Plus, you can switch to any llama.cpp build under Settings, Model Providers, llama.cpp (see the video above).
- We had different plans for Cortex, that's why we insisted on keeping it for a while, but maintaining it with the new plans became pretty tough. It just made more sense to support llama.cpp directly instead of going through an extra layer.
  - Well I'm glad the team changed direction. This more polished, cleaner, leaner, faster way of Jan development is bound to result in great progress!
- Cortex was the engine behind Jan. It ran on llama.cpp and worked kind of like an alternative to Ollama. We used to update it whenever llama.cpp got updated. Since we've changed our plans, we removed it with Jan v0.6.6.

- I see that the Linux version became 850MB lighter. Is that because of the Cortex removal?
  - Yes, Cortex removal is part of it. We also trimmed down the app by moving out unused dependencies like CUDA 11/12 and extra llama.cpp builds. 
  - Jan now fetches the right llama.cpp version after install, so no need to be bundled in the app itself, which makes the download much lighter
- Although it would be great to keep at least 1 version of llama.cpp, so the install would be portable.

- Does Jan allow selectively offloading model tensors to CPU? (For MoE models) If yes, I would migrate from LM Studio to Jan just for that!
  - oh - not yet, but we're adding it to the roadmap. Thanks for the request!
- this is the killer feature all the slick frontends are missing.

- Allowing ik_llama as a backend would be cool too!

- Llama cpp has so many options, there really should be a "custom arguments" option (input field that gets passed through to llama).

- Koboldcpp have this feature iirc

- Would be great if we could also use llama.cpp forks such as ik_llama.cpp or the Unsloth fork
  - llama.cpp forks: Each fork comes with its own changes, and integrating them cleanly would add a lot of maintenance overhead. We're keeping an eye on where things go, but sticking with official llama.cpp keeps Jan more stable for now.

- MLX support has been on the table for a while... it's something we revisit from time to time. We made several roadmap changes around it, but no final decision yet

- ## ğŸ  [Jan got an upgrade to v0.6.0: New design, switched from Electron to Tauri, custom assistants, and 100+ fixes - it's faster & more stable now : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1lf5yog/jan_got_an_upgrade_new_design_switched_from/)
  - [Release 0.6.0 Â· menloresearch/jan _20250619](https://github.com/menloresearch/jan/releases/tag/v0.6.0)
  - Switched from Electron to Tauri for lighter and more efficient performance
  - You can create your own assistants with instructions & custom model settings
  - Fully redesigned UI

- Tauri helps us bring Jan to new platforms like mobile. It's also lighter and faster, which gives us more room to improve performance in upcoming updates.
  - Tauri allows us to reuse our existing React and JavaScript codebase, which helps us move faster while keeping the design clean and modern. 
  - Qt and similar native toolkits don't integrate well with the web stack.

- Wails doesn't support mobile, and we're planning to bring Jan to mobile too - so we went with Tauri.

- As someone who is finalizing a port of a small app (atv-desktop-remote) from electron to tauri: If most of your code is in the renderer, the transition is easy. If you have lots of stuff in main.js, youâ€™ll need to reimplement that in rust. The IPC line is still there (between main and renderer processes), you can move stuff around if you need to.

- what makes Jan different from / better than LM Studio? It seems you have same backend and similar frontend?
  - Jan is open-source 
  - We're experimenting with MCP support using our own native model, Jan-nano that outperforms DeepSeek V3 671B in tool use. It's available now in the Beta build.

- Been looking for some opensource UI for openrouter for a long time. It looks very promising, but I haven't figured out how to output reasoning yet.
  - reasoning output isn't supported yet - it's still on the todo list to check.

- Can we edit the responses?
  - No, you can't. why do you want to edit the responses?
- That's an easy way to steer the story in the direction you want it to go. Say, you draft a chapter and ask the model to develop the story; it starts well, but in the middle of generation makes an unwanted twist, and the story starts going in the wrong direction. Sure, you could just delete the response and try again, or try editing your prompt, but it is much, much easier to just stop generation, edit the last line of text, and hit "resume". So, editing AI responses is a must-have for writing novels.

- Jan supports OpenAI-compatible setup for APIs. So if it's empty, it won't work, even if the remote endpoint itself doesn't require one. We should add a clearer indicator for this, but in the meantime, try entering any placeholder key and see if that works.
  - Plus, Ollama doesn't support OPTIONS requests on all endpoints (like /models), which breaks some standard web behavior. We're working on improving compatibility.

- Jan doesn't work with Ollama llms out of the box is my biggest grip . Still unable to add ollama to Jan. Anyone succeeded?
- can it be used with lmstudio ?
  - i tried it and it doesnt work. not sure which one is wrong
  - jan is using OPTION request to list model while lmstudio only serve /v1/models on GET

- When using Ollama if the model does not fit in my GPU it will use the GPU and offload to the CPU but the same did not work with jan.
  - you can update the number of GPU layers manually in model settings (Model Providers -> llama.cpp).
  - We're planning to add automatic GPU offloading soon, once it's merged upstream in llama.cpp

- Please add MLX as a future inference engine

- The app is huge in size. I thought Tauri didn't ship the whole browser.
  - Totally fair. The app's size is due to the universal bundle - we're working on slimming it down soon.
  - Universal bundle on Mac means the app includes native code for both Apple Silicon & Intel Macs, so it's basically twice the size.

- I love Jan! Is there an option for it to autostart on boot with the API server enabled? I couldn't find any way to do that with the previous versions of Jan so I went with LM Studio for my backend unfortunately.

- Jan doesn't support chatting with files or image generation yet. We're going to add chatting with files feature soon. As for image generation, it's not available right now - we'll take a look at it as well.

- What do you feel differentiates your app from all the others? I keep downloading chat apps and they all pretty much offer the same functionality and interface. What is Jan's approach for separating yourself from the herd?

- Is chat history formatted into some portable standard?
  - We're using the same format as previous versions. It loosely follows OpenAI's thread structure but isn't a strict match. It's extension-based, so it can be replaced or exported to other formats if needed.

- on Linux Tauri uses WebKitGTK for now, so performance isn't as good as Chromium. We're refactoring to reduce app size, and once that's done, we'll look into bundling a better browser. Tauri gives us flexibility there, unlike Electron.

- ## [Cortex.cpp: llama.cpp Engine Â· menloresearch/cortex.cpp _202409](https://github.com/menloresearch/cortex.cpp/discussions/1156)
- We submodule llama.cpp
  - How we handle different llama.cpp versions (e.g. AVX, CUDA etc)
  - Documents which llama.cpp versions we bundle into Cortex/Jan
  - Switching logic for CPU/GPU inference

- ## [Are people not using JAN a lot? : r/LocalLLaMA _202408](https://www.reddit.com/r/LocalLLaMA/comments/1eza2kl/are_people_not_using_jan_a_lot/)
- I think that Jan still lacks a lot of â€œnice havesâ€ that are present in LM Studio
  - Built in RAG support

- I'm Emre from Jan. Really appreciate this post - it's going to be super helpful as we keep improving Jan! Let me share a few things we're working on:
  - Improving Jan's performance across different hardware (with a new engine that is compatible with various hardware)
  - Revamping the new model hub, providing better
  - Experimenting with something that we'll share soon in homebrew.ltd, our R&D lab which is also the company behind Jan.
- Responding only to features and ideas without addressing the questions of struggling users is called exploitation(å¹¿å‘Šï¼Œå®£ä¼ ).

- Jan has a lot of potential, but is let down by its model management when itâ€™s not the server. Please invest in Ollama and OpenAI compatible APIs. Jan has a lot of potential but not being able to simply connect to your server and select models from a list makes it a pain to use when you already have Ollama / OpenAI compatible servers other clients use.

- I donâ€™t know if Im Studio has it, but I am waiting for tts and stt support in Jan. As for the online based tts it would nice to have different options like elevenlabs, oai whisper, google tts, amazon poly etc For a local tts it would be cool to have at least something that is friendly to limited hardware like piper.
  - Msty for example has implemented stt in the meantime. They just use oaiâ€˜s whisper api.

- It's probably due to the "competition" from other excellent easy-to-use software such as Koboldcpp, LM Studio, Ollama, etc. Additionally, Jan has historically been lacking in some functionality that the alternatives have. Maybe this is not the case anymore, as it was a few months ago I last tried Jan.

- I love using JAN but there are a few things that made me switch to openwebui.
  - First I need a more robust API. Running the same time as my threads. Why does it not let me chat in threads if I turn on the API? 
  - I also tried to do some simple things like get the currently loaded model and swap the loaded model for another one. I still have not been able to do this.
  - Why is there so many manual steps involved in adding a model that is not included in the hub? Can't we just provide the huggingface link and it pull the info it needs?
  - I could not find a way to add a custom extension. I want to add image generation using SearmUI. But I don't think I'm able to create extensions yet.
  - I would really love to come back to Jan but the features I need are already there on openwebui.

- JAN is poorly build and thought out unfortunately. 
  - They really need to address support of already existing GGUF models, new quantization types (IQ quants), proper and easy ways to switch prompt/system messages and have presets for things like ChatML, LLama 2/3/3.1, Mistral, etc.
  - Also things like longer context, Flash Attention, KV Cache, More useful debugging/error messages and proper Apple Silicon support is needed badly.
  - It generates a bunch of random and weird output on a lot of new models and quant types.
  - It's too bad that after all this time it is still very buggy and clunky.. I had high hopes for it

- jan has stopped supporting the WebUI, so browser functions are not available.

- ## [Jan: An open source alternative to ChatGPT that runs on the desktop | Hacker News _202403](https://news.ycombinator.com/item?id=39782876)
- GPT4All makes it annoyingly difficult to run any other than their "approved" models. I'd like to kick the tires on a whole host of random GGUF quantizations on Hugging Face, please.
  - I use text-gen-ui (Oobabooga) as a back-end and have it run with `--api` to use the front end of my choice.

- ## [Jan: an open-source alternative to LM Studio providing both a frontend and a backend for running local large language models : r/LocalLLaMA _202401](https://www.reddit.com/r/LocalLLaMA/comments/193m27u/jan_an_opensource_alternative_to_lm_studio/)
- A Big problem all these LLM tools have is that they all have their own way of reading Models folders. I have a huge collection of GGUF's from llama.cpp usage that I want to use in different models. Symlinking isn't user friendly, why can't apps just make their Models folder a plain folder and allow people to point their already existing LLM folders to it
  - This is salient(æ˜¾è‘—çš„, çªå‡ºçš„) criticism, thank you. At the core, we're just an application framework. We should not be so opinionated about HOW users go about their filesystem.
  - [/prå·²åˆå¹¶ - epic: Better files & links Â· Issue Â· menloresearch/jan _202401](https://github.com/menloresearch/jan/issues/1494)

- Ollama being the biggest offender, with that fake docker syntax for modelfiles, model import and renaming using sha hashes.

- The Stable Diffusion UI variants also had this problem - until Stability Matrix came along and resolved a number of inconveniences with model management. Wonder if something similar could be viable here too.
  - invokeai çš„é€‰æ‹©æ–‡ä»¶ä¹Ÿæ˜¯ä¸€ç§æ–¹æ¡ˆ

- Its why Koboldcpp just has a file selector popup, it doesn't make sense to tie people to a location.
  - Koboldcpp also has an OpenAI compatible server on by default, so if the main thing you wish for is an OpenAI endpoint (or KoboldAI API endpoint) with bigger context processing enhancements its worth a look.
  - Koboldcpp is a bit of a hybrid since it also has its own bundled QT UI. We also have GGUF support as well as every single version of GGML.

- This is the best example of why LLMs wont replace devs.
  - IMO, work is the tedious processes of begrudgingly implementing common design patterns. 
  - Did anyone building LLM frameworks/dev tools think they'd be building model library browsers drawing from itunes and calibre? If they're smart. How many people used itunes just because it had better browsing/searching than winamp? (Jumping back to hugging face for the model card and details is already less frequent.)
  - We all want different things.

- Is it better to use llama.cpp instead of LM Studio? 
  - Absolutely! KoboldCpp and Oobabooga are also worth a look. 
  - I'm trying out Jan right now, but my main setup is KoboldCpp's backend combined with SillyTavern on the frontend. 
  - They all have their pros and cons of course, but one thing they have in common is that they all do an excellent job of staying on the cutting edge of the local LLM scene (unlike LM Studio).

- It really needs a custom folder and scan directory function to incorporate already available local GGUF files. I also don't understand the weird implementation of needing a config/JSON file for each model. Why not just use the GGUF metadata and filename to determine the proper settings like other apps are doing?
  - local configs give you an ability to override specific parameters for every model - like to have a custom prompt, custom context length, rope settings etc. Without having local configs there would be no such place to put all your overrides. 
  - But of course no inference software should require them by default and should take everything it can from metadata (where applicable). 
  - And generate those Configs automatically only if you change some parameter from its default value.

- it's nice and simple, but as a consequence of being so new it lacks a lot of QoL stuff that I would expect with more mature apps. 
  - Also I find that the app loads/unloads LLM models into the RAM with every query, unlike LLMStudio which leaves it in RAM until you eject the model. I don't know which is better, but I am a bit concerned about that constant load on my computer.

- I don't like ollama. It keeps loading and unloading models Everytime you do Inference.

- It looks to have an openai API compatibility, wonder if I can use oobabooga textgen as a backend

- I am looking for a GUI that lets me set a -ve CFG without hassle. Does this GUI let me do that?

- Is it some kind of electron app bundled with llama.cpp? Can it be used as a web UI over network?
# discuss-vllm

```shell
vllm serve google/gemma-3-270m  --max-num-batched-tokens 32768 --chat-template /Users/yaoo/Documents/opt/configs/llm/chatemplates/template_chatml.jinja

vllm serve RUC-DataLab/DeepAnalyze-8B --max-num-batched-tokens 40000 --max-model-len 28000
```

- vllmå¤„ç†promptçš„å…¸å‹æ—¥å¿—

```log
[entrypoints/logger.py:37] Request chatcmpl-7ada460412e8448999ef5c40b74f9f3d details: prompt: '<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ># Instruction\nthe northwinds excel contains the sales data for

[entrypoints/logger.py:47] Received request chatcmpl-7ada460412e8448999ef5c40b74f9f3d: params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.4
127.0.0.1:55718 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[v1/engine/async_llm.py:360] Added request chatcmpl-7ada460412e8448999ef5c40b74f9f3d
[v1/engine/core.py:880] EngineCore loop active.

[entrypoints/logger.py:76] Generated response chatcmpl-7ada460412e8448999ef5c40b74f9f3d (streaming delta): output: '<Understand>', output_token_ids: [151673], finish_reason: None
```

- ## 

- ## 

- ## 

- ## ğŸ†š [Is vLLM worth it? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1p9t5c8/is_vllm_worth_it/)
- vLLM currently has way too many bugs related to gpt-oss (or even qwen3) and other important features (such as structured outputs). 
  - After trying multiple releases of their official docker images I gave up and moved onto sglang. And oh god I wish I did it much earlier because after an hour of setup it worked like a charm: 3x higher throughout and properly working structured outputs (for both qwen and gpt-oss). 
  - So my advice is to try sglang docker image, key thing is to set the correct reasoning parser and you're good to go

- ## ValueError: To serve at least one request with the models's max seq len (60000), (8.24 GiB KV cache is needed, which is larger than the available KV cache memory (4.00 GiB). 
  - Based on the available memory, the estimated maximum model length is 29056. 
  - Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.

- ## ğŸ†š [Has vLLM made Ollama and llama.cpp redundant? : r/LocalLLaMA _202507](https://www.reddit.com/r/LocalLLaMA/comments/1mb6i7x/has_vllm_made_ollama_and_llamacpp_redundant/)
- vLLM: Extremely well optimized code. Made for enterprise, where latency and throughput is the highest importance. 
  - Only loads a single model per instance. 
  - Uses a lot of modern GPU features for speedup, so it doesn't work on older GPUs. 
  - It has great multi-GPU support (spreading model weights across the GPUs and acting as if they're one GPU with combined VRAM). 
  - Uses very fast caching techniques (its major innovation being a paged KV cache which massively reduces VRAM usage for long prompt contexts). 
  - It pre-allocates 90% of your VRAM to itself for speed regardless of how small the model is. 
  - ğŸ› It does NOT support VRAM offloading or CPU-split inference. It's designed to keep the ENTIRE model in VRAM. So if you are able to fit the models in your VRAM, then vLLM is better, but since it was made for dedicated enterprise servers it has the downside that you have to restart vLLM if you want to change model.
- Ollama: Can change models on the fly and automatically unloads the old model and loads the new one. 
  - It works on pretty much any GPU. 
  - It's able to do split inference and RAM offloading so that models which don't fit on the GPU will use offloading and still be able to run even if you have too little VRAM. 
  - And it's also very easy for beginners.
- they serve two totally different purposes. Ollama is way better and way more convenient for most home users. And vLLM is way better for servers and people who have tons of VRAM and want the fastest inference. That's it. 
  - I found a nice post where someone did a detailed benchmark of vLLM vs Ollama. The result was simple: vLLM was up to 3.23x faster than Ollama in an inference throughput/concurrency test

- vLLM has become the inference back-end of choice for the corporate world (Red Hat has based RHEAI on it). Because of that, hardware companies with a vested interest in making inference work well on their hardware have poured engineering-hours into making it maximally performant for corporate use-cases. It seems like the obvious back-end in which to invest one's time and efforts, if one expects to develop corporate LLM applications for a living.
  - Meanwhile, llama.cpp is becoming the swiss army pocketknife of LLM inference, the neatly self-contained do-it-all (with training/finetuning coming back to the project soon). It isn't always the most performant, and it's not great for concurrent inference, but it's very reliable.
  - vLLM by comparison is not very self-contained. It has many sprawling external dependencies, which can make it difficult to get working. 

- Does vLLM support switching models on the fly yet? if not, thatâ€™s a very important feature
  - That's a really good question and it seems the answer is no. I read something saying that vLLM is tightly optimized for high-throughput serving of a single model loaded into memory, using shared KV cache and GPU-accelerated paging (which allows it to support very long input contexts while reducing VRAM usage).
  - And that it therefore loads 1 model per instance and cannot switch without restarting vLLM.

- ## [Best frontend for vllm? : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1ldokl7/best_frontend_for_vllm/)
  - I use LM studio for an easy inference of llama.cpp but was wondering if there is a gui for more optimised inference.

- I've been using text-generation-webui as a combo back-end & front-end since I started with local models over two years ago now, and IMO nothing else comes close as an all-rounder for LLM work. You can also use it as a server, exposing an OpenAI API endpoint to utilize elsewhere if you want to use another front-end or need to make API calls from other programs.

- Been using Lmstudio as well but you could try https://jan.ai/docs as a free open source alternative.

- What we do is build Gradio UIs. Nowadays with LLMs it's super easy to create them and customize them to your liking.

- ## [TIL: For long-lived LLM sessions, swapping KV Cache to RAM is \~10x faster than recalculating it. Why isn't this a standard feature? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1olouiw/til_for_longlived_llm_sessions_swapping_kv_cache/)
  - TIL: For long-lived LLM sessions, swapping KV Cache to RAM is ~10x faster than recalculating it. Why isn't this a standard feature?
  - I was diving into how vLLM and similar inference servers work and had a thought about optimizing memory for long-lived but inactive chat sessions. 
  - The standard approach seems to be either keeping the KV Cache in precious VRAM or evicting it and recalculating from scratch when the user returns. I think there might be a better way.
  - Here's the core idea: Implement a swapping mechanism for the KV Cache of inactive sessions, moving it from VRAM to system RAM (and back), instead of deleting it.
  - We always focus on the high cost of moving data between CPU and GPU, but we often forget the cost of recalculating that data.
  - The math is pretty compelling. Swapping is roughly 7-15x faster than a full recalculation. For a user, waiting 200ms for their chat history to "wake up" is a much better experience than waiting 2+ seconds.
  - This wouldn't be for high-throughput, always-online inference, but specifically for managing many long-lived sessions (e.g., support chatbots, document analysis with breaks, multi-user systems with intermittent activity). It's a classic space-time tradeoff, but in this case, using slightly more "space" (system RAM) saves a huge amount of "time" (latency on reactivation).

- i had the same concern op. the cache saving should work. - https://docs.vllm.ai/en/stable/examples/others/lmcache.html?h=lmcache this exists i want to test it with moon shot ai db. i feel like kimi k2 guys uses the same

- Seems to be compatible with vllm now - https://docs.vllm.ai/en/stable/examples/others/lmcache.html?h=lmcache
  - awesome finding! I should have studied better how to Google properly

- It depends on the backend... vLLM has https://github.com/LMCache/LMCache for example (as someone already mentioned here), but it is mostly limited to GPU-only inference.
  - For CPU+GPU inference for a single user, I find ik_llama.cpp to be the best choice though.

- Llama.cpp does not work like that. It evicts cache only you suplly a prompt with no common prefix with what is in cache already.
  - "inactive" in this context means when the VRAM is not enough to store existing pages of KV cache when new requests are coming, with different prompts. As far as I understand this is how vllm works - it just evicts older pages. A sort of LRU ( least recently used ) buffer

- ## [å¤§æ¨¡å‹æ¨ç†æ¡†æ¶ï¼ŒSGLangå’ŒvLLMæœ‰å“ªäº›åŒºåˆ«ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/666943660)
- PagedAttention vs RadixAttentionï¼šå†…å­˜ç®¡ç†çš„ä¸¤ç§å“²å­¦
- å…ˆè¯´vLLMçš„PagedAttentionã€‚
  - è¿˜è®°å¾—æ“ä½œç³»ç»Ÿè¯¾ä¸Šçš„è™šæ‹Ÿå†…å­˜å—ï¼ŸvLLMå°±æ˜¯æŠŠè¿™å¥—ç©æ³•æ¬åˆ°äº†GPUä¸Šã€‚æˆ‘ç¬¬ä¸€æ¬¡çœ‹åˆ°è¿™ä¸ªè®¾è®¡æ—¶ï¼Œç›´æ¥æƒŠäº†â€”â€”å±…ç„¶èƒ½æŠŠå†…å­˜åˆ©ç”¨ç‡ä»20%å¹²åˆ°96%
- SGLangçš„RadixAttentionã€‚
  - ç”¨çš„æ˜¯Radixæ ‘ï¼ˆå°±æ˜¯é‚£ä¸ªå‹ç¼©å‰ç¼€æ ‘ï¼‰ã€‚
  - åƒæ˜¯ç»™KV cacheå»ºäº†ä¸ª"æ—è°±"
  - åœ¨å¤šè½®å¯¹è¯åœºæ™¯ï¼ŒSGLangç¡®å®çˆ½ã€‚
  - éƒ¨ç½²èµ·æ¥æ˜¯çœŸçš„éº»çƒ¦

- ç¼–ç¨‹ä½“éªŒï¼š
  - vLLMçš„APIè®¾è®¡ï¼Œæ€ä¹ˆè¯´å‘¢ï¼Œå°±æ˜¯é‚£ç§"å‚»ç“œå¼"çš„ï¼ˆè¤’ä¹‰ï¼‰ã€‚ä¸‰è¡Œä»£ç è·‘èµ·æ¥
  - SGLangçš„ç¼–ç¨‹æ¨¡å‹, è¿™ä¸ªDSLï¼ˆé¢†åŸŸç‰¹å®šè¯­è¨€ï¼‰å­¦ä¹ æ›²çº¿æœ‰ç‚¹é™¡

- å¯æ˜¯ï¼Œé€šå¸¸éƒ¨ç½²éƒ½æ˜¯åªä½œä¸ºåç«¯æä¾› APIï¼Œè€Œä¸æ˜¯ç­”ä¸»è¿™æ ·åœ¨ Python é‡Œç›´æ¥åŠ è½½æ¨¡å‹ã€‚æ‰€ä»¥ç­”ä¸»çš„åšæ³•æ¯”è¾ƒå°‘è§å•¦ã€‚

- sglangä¸ç¨³å®šå…·ä½“æŒ‡çš„æ˜¯ï¼Ÿ
  - å„ç§å†…å­˜æ³„æ¼ï¼Œoomï¼Œcrashï¼Œä»£ç è§„æ¨¡ä¸å¤§ï¼Œå°±å¼€å§‹å±å±±å †å±ï¼Œç”Ÿäº§åˆ«ç”¨ï¼Œä¹Ÿåˆ«è¿½æ›´ï¼Œmerge masteråŸºæœ¬çš„å›å½’æµ‹è¯•éƒ½ä¼šç›´æ¥è·³è¿‡çš„ã€‚åšå¼€æºçš„ï¼Œå°±è¿™

- vllm ä»…æ”¯æŒ NVIDIA GPUã€éƒ¨ç½²å¤æ‚ã€æ˜¾å­˜éœ€æ±‚å¤§
# discuss-cpu-llm
- ## 

- ## 

- ## 

- ## [llama.cpp and llama-server VULKAN using CPU : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1ooyr6r/llamacpp_and_llamaserver_vulkan_using_cpu/)
  - llama.cpp and llama-server VULKAN appears to be using CPU. I only noticed when i went back to LM Studio and got double the speed and my Computer didnt sound like it was about to take off.
- First, it doesn't hurt to set ngl 99. Second, cache quantization hurts speed. Other than these two points, I don't know why llama.cpp could get slower than lms.

# discuss-ai-api/tools
- ## 

- ## 

- ## 

- ## [How is Cloud Inference so cheap : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1q2jwsn/how_is_cloud_inference_so_cheap/)
  - How do cloud inference companies like DeepInfra, Together, Chutes, Novita etc manage to be in profit regarding to the price of the GPUs/electricity and the fact that I guess it's difficult to have always someone to serve ?

- DeepInfra and Together are VC funded. Probably safe to assume they run at a loss for now.
  - Chutes runs on a decentralized network (bittensor), i.e. it's crowdsourced and they pay individual miners.
  - I haven't tried Novita but they're a tiny team of 10 and their blog has some posts about how they optimize e.g. quantization. They're also a little more expensive than the others.

- Scale is more efficient, batching is more efficient, horizontal scaling is more efficient, and a lot of them quantize

- Batching. One gpu can serve hundreds of uses at once. https://artificialanalysis.ai/benchmarks/hardware
- Good God the B200 is a beast!
  - I got a dgx spark and it has the same architecture. Iâ€™m getting 600tok/s doing batch inference with qwen3 next 80b. Itâ€™s not as good as the big boys but itâ€™s doing batch data classification quite well.

- Inference providers are operating at a loss, hoping that their competitors run out of money before they do. Last man standing wins.

- Data is the new currency.

- same reason the USA is 38 trillion in debt. they built the datacenters to pump corporate profits sheets up. doesn't matter how much money they lose. as long as they are 'winning' uncle sam will cash out. money is about social control. it does not measure resources.

- ## openrouter æ˜¯çœŸæ–¹ä¾¿ï¼Œä¸€ä¸ª Key æ‰€æœ‰æ¨¡å‹éƒ½èƒ½ç”¨ã€‚
- https://x.com/pengchujin/status/1894375539726803201
- æ¨¡å‹ä»·æ ¼ä¸€æ ·ï¼Œå……å€¼ 5% çš„æ‰‹ç»­è´¹ã€‚è¿˜æœ‰ä¸ªå¥½å¤„æ˜¯æœ‰ä¸€éƒ¨åˆ†å…è´¹æ¨¡å‹ï¼Œæ¯”å¦‚Azçš„R1å•¥çš„
  - ä»·æ ¼ä¸€æ ·çš„ï¼Œå®˜æ–¹é™ä»·ä»–ä¹Ÿè·Ÿç€é™
- Qwençš„keyï¼Œç«å±±çš„keyä¹Ÿæ˜¯è¿™æ ·çš„

- ## deepseek æœ€è¿‘å¼€æºçš„å‰é¢å‡ ä¸ªé¡¹ç›®å¯èƒ½æˆ‘ä¸äº†è§£, ä»Šå¤©è¿™ä¸ª3fs, smallpond æ­£å¥½æ˜¯æˆ‘ä»¬è¿™ä¸ªè¡Œä¸šçš„, æˆ‘ç†è§£å¹¶æ²¡æœ‰åƒæŠ€æœ¯ç‚¸å¼¹å§, éƒ½æ˜¯æ¯”è¾ƒæˆç†Ÿç¨³å®šçš„æŠ€æœ¯äº†
- https://x.com/baotiao/status/1895395027129655691
- y1s1 è¿™é‡Œçš„éš¾ç‚¹æ˜¯ç éœ€æ±‚ï¼Œä¸åšä»€ä¹ˆ

- ## DeepSeek å¼€æºå‘¨çš„ 5 å·ç‚¸å¼¹æ¥å•¦ï¼åˆæ˜¯é›†æŸç‚¸å¼¹ï¼3FS å’Œ smallpondï¼
- https://x.com/karminski3/status/1895280320989274560
  - æˆ‘ä¸æ•¢ç›¸ä¿¡DeepSeekç”šè‡³é¢ è¦†äº†å­˜å‚¨æ¶æ„...... æˆ‘ä¸Šæ¬¡ä¸ºç½‘ç»œæ–‡ä»¶ç³»ç»Ÿéœ‡æƒŠè¿˜æ˜¯HDFSå’ŒCEPH. ä½†è¿™äº›éƒ½æ˜¯é¢å‘ç£ç›˜çš„åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ. ç°åœ¨ä¸€ä¸ªçœŸæ­£æ„ä¹‰ä¸Šé¢å‘ç°ä»£SSDå’ŒRDMAç½‘ç»œçš„æ–‡ä»¶ç³»ç»Ÿè¯ç”Ÿäº†ï¼
  - é£ç«æµæ˜Ÿæ–‡ä»¶ç³»ç»Ÿï¼ˆ3FSï¼‰- ä¸€ç§åˆ©ç”¨ç°ä»£ SSD å’Œ RDMA ç½‘ç»œå…¨å¸¦å®½çš„å¹¶è¡Œæ–‡ä»¶ç³»ç»Ÿ
  - è¿™ä¸ªæ–‡ä»¶ç³»ç»Ÿå¯ä»¥åœ¨ 180 èŠ‚ç‚¹é›†ç¾¤ä¸­è¾¾åˆ°6.6 TiB/s æ€»è¯»å–ååé‡ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯èŠ‚ç‚¹ KVCache æŸ¥æ‰¾å³°å€¼ååé‡ 40+ GiBã€‚
  - å¦ä¸€ä¸ª smallpondï¼ˆå°æ± å¡˜ï¼‰æ˜¯åŸºäº 3FS çš„æ•°æ®å¤„ç†æ¡†æ¶ï¼
  - è¿™ä¸ªæ¡†æ¶ç”± DuckDB æä¾›çš„é«˜æ€§èƒ½æ•°æ®å¤„ç†ï¼Œå¯æ‰©å±•ä»¥å¤„ç† PB çº§æ•°æ®é›†ï¼
  - æˆ‘çœ‹äº†ä¸‹åº”è¯¥è¿˜æ˜¯KVå­˜å‚¨çš„ï¼ˆæ¯•ç«Ÿé¢å‘æœºå™¨å­¦ä¹ ï¼‰ï¼Œå¹¶ä¸æ˜¯å—å­˜å‚¨ã€‚å› æ­¤NASä½¬è¿˜æ˜¯ä¸å¤ªèƒ½ç”¨å¾—ä¸Šçš„ã€‚  ä¸€è‡´æ€§åè®®åŸºäºCRAQï¼Œæ¯•ç«ŸKVå­˜å‚¨ï¼ŒåŸºäºé“¾å¼å¤åˆ¶çš„ï¼Œå†™æ“ä½œä»ç„¶éœ€è¦é€šè¿‡æ•´ä¸ªé“¾ï¼Œæ‰€ä»¥å†™å»¶è¿Ÿå¤§ã€‚ä½†ä¼°è®¡å…¶å®ç»™è®­ç»ƒå½’æ¡£ç”¨ï¼Œå†™å»¶è¿Ÿå¤§æ— æ‰€è°“ã€‚å¼‚æ­¥å½’æ¡£è€Œå·²ã€‚
- æ˜¯æ–‡ä»¶ç³»ç»Ÿï¼Œå…ƒæ•°æ®æ”¾foundationdbé‡Œï¼Œæ•°æ®æ”¾xfsé‡Œ

- å¦å¤–Intelçš„DAOSå…¶å®æˆ‘æ„Ÿè§‰åšçš„æ›´æè‡´ï¼Œç›´æ¥ç”¨SPDKæ“ä½œè£¸ç›˜ï¼Œè¿˜åˆ©ç”¨NVMe-oFæŠ€æœ¯å……åˆ†å‘æŒ¥RDMAé›¶æ‹·è´çš„ç‰¹æ€§ï¼Œåªå¯æƒœå…ƒæ•°æ®ç³»ç»Ÿä¾èµ–äºNVMç¡¬ä»¶ï¼Œè€ŒNVMç¡¬ä»¶å·²ç»å‡‰å‡‰ï¼ŒDAOSä¹Ÿå–ç»™Delläº†

- cephæœ‰object storeå‘€â€¦ å¤´ä¸€æ¬¡è§åˆ°çœŸçš„æœ‰äººç”¨chain replicationçš„â€¦latencyä¸è¦å•¦ï¼Ÿrdmaç¡¬ä»¶è¿™ä¹ˆè´µç›´æ¥é ç¡¬ä»¶ç ¸å—ï¼Ÿ
- æ•°æ®ä¸­å¿ƒï¼ŒåœŸè±ªç©æ³•ï¼Œé¢ è¦†ä¸è‡³äºå§

- ## æˆ‘æ‰çŸ¥é“ å·¥å…·è°ƒç”¨æ˜¯ä¸ªå•ç‹¬çš„ call ä¸€æ—¦è°ƒç”¨å…¶ä»–ä»€ä¹ˆäº‹æƒ…éƒ½å¹²ä¸äº† æ€ä¹ˆè¿™ä¸ª llm ç°åœ¨è¿™ä¹ˆè‰å°ç­å­ã€‚ã€‚ã€‚
- https://x.com/Lakr233/status/1894950577416901018
  - è¿˜æœ‰ Agent è¿™ç§æ¦‚å¿µï¼Œå«å…¨è‡ªåŠ¨ç¼–æ’æ£€ç´¢å·¥å…·ä½¿ç”¨ä¸å¥½ä¹ˆæçš„è¿™ä¹ˆé«˜å¤§ä¸Šåˆ°å¤´æ¥è¿˜æ˜¯ GPT 3.5 å°±èƒ½å¹²çš„æ´»ã€‚ã€‚ã€‚

- tools å°±æ˜¯ä¸€è½®æ–°çš„å¯¹è¯ï¼Œå‘Šè¯‰ AI å¯ä»¥è°ƒè¿™ä¸ªï¼Œç„¶åæŠŠè°ƒç”¨ç»“æœæ‹¼æ¥åˆ°ä¸Šä¸€è½®å¯¹è¯åé¢ã€‚
  - agent å°±æ˜¯ pipeline/orchestration/schedulerã€‚
  - embedding/RAG å°±æ˜¯ searchã€‚
  - LLM framework å°±æ˜¯ HTTP API callsã€‚
- ä½ è¿™æ ·ï¼ŒPPTè¿˜æ€ä¹ˆç¼–ï¼Œå›¢é˜Ÿé¢„ç®—æ€ä¹ˆæ‰¹

- ä¹Ÿæœ‰ä¸€äº›å¼‚æ­¥è°ƒç”¨ +event çš„ç©æ³•, ä½†æ˜¯æ•´ä¸ª Agent ç¡®å®å°±æ˜¯å»ºç«‹åœ¨æ— é™çš„æ–‡æœ¬ç”Ÿæˆä¸Šçš„
  - Agent = FunctionCall + LLM + LOOP

- å·¥å…·è°ƒç”¨å…¶ä»–ä»€ä¹ˆäº‹æƒ…éƒ½å¹²ä¸äº†æŒ‡çš„æ˜¯ï¼Ÿå¯ä»¥å¹¶å‘å¤„ç†å…¶ä»–é—®é¢˜å‘€ï¼Ÿ

- ## æµ·å¤–å¤§æ¨¡å‹æ¥å…¥ä½¿ç”¨openrouter å·²ç»æ˜¯æœ€ä½³å®è·µäº†å˜›ï¼Ÿ
- https://x.com/leeoxiang/status/1887714022327525797
  1ã€æ”¯æŒäº†å¸‚é¢ä¸Šå¤§éƒ¨åˆ†æ¨¡å‹ï¼›
  2ã€å¯ä»¥è®¾ç½®æ¨¡å‹çš„æ¶ˆè€—é¢åº¦ï¼›
  3ã€èƒ½æ”¯æŒæ¨¡å‹çš„è´Ÿè½½å‡è¡¡ï¼›
  4ã€æ”¯æŒä¸€å¥—apié€‚é…å¤§å¤šæ•°æ¨¡å‹ï¼›
  5ã€æ”¯æŒå¤šä¸ªæ¨¡å‹çš„å¤±è´¥é‡è¯•ï¼›

- å¤ªè´µäº†ï¼è€Œä¸”ç‰¹æ®Šæ¨¡å‹è¿˜æ˜¯è¦æ·»åŠ è‡ªå·±çš„API Key

- ## We ( @jamesmurdza ) have been building Open Computer Use - 100% open source computer use agent.
- https://x.com/mlejva/status/1877054558481813799
  - The agent is using @e2b_dev 's Desktop Sandbox as virtual computer.
  - The agent is using 3 different LLMs: ğŸ”¸Llama 3.2 ( @AIatMeta ) ğŸ”¸Llama 3.3 ğŸ”¸OS-Atlas ( @Alibaba_Qwen )
  - It's slow and makes mistakes but this is a big milestone for OS AI community!

- ## æƒ³è¦éƒ¨ç½²æœ¬åœ°æ¨¡å‹ä½†æ˜¯ä¸ä¼šè®¡ç®— vRAM å ç”¨ 
- https://x.com/tuturetom/status/1842492423848804686
  - https://huggingface.co/spaces/hf-accelerate/model-memory-usage

- ## çœ‹æ¥å¤§å®¶ç»ˆäºè¾¾æˆå…±è¯†äº†ï¼šlangchain æ˜¯ç©å…·ï¼Œå¦‚æœéè¦åœ¨ç”Ÿäº§ç¯å¢ƒç”¨å®ƒï¼Œé‚£å®ƒå°±ä¼šå˜æˆå·¥ä¸šåƒåœ¾ã€‚
- https://x.com/beihuo/status/1840058205768167699
  - çœ‹è¿™ä»£ç æ¯”è¾ƒï¼Œlangchain å°±åƒä¸€ä¸ªæ²¡æœ‰å¤ªå¤šå·¥ç¨‹ç»éªŒï¼Œä½†æ˜¯åˆçœ‹äº†å¤ªå¤šè®¾è®¡æ¨¡å¼æ•™ç¨‹çš„äººå†™å‡ºæ¥çš„ä¸œè¥¿ã€‚ä½¿ç”¨å®ƒæ¥å®ç°ä¸€ä¸ªç”Ÿäº§ç³»ç»Ÿï¼Œå°±æ˜¯ä¸€ä¸ªç¾éš¾ã€‚
- åŒæ„Ÿ longchainæ„å»ºæ€ç»´é“¾ä¸å¦‚ç›´æ¥æŒ‰ç…§å·¥ä½œé€»è¾‘äººå·¥æ„å»ºæ€ç»´é“¾ã€‚çœŸæ­£çš„è¿æ€ç»´é“¾éƒ½ä¸æ¸…æ¥šçš„åˆ›é€ å‘æ˜ï¼Œç°åœ¨ç”¨AIæ¥åšä¸ºæ—¶å°šæ—©ã€‚

- ## ğŸ’„ ç”Ÿæˆå¼çŸ¥è¯† UI æœ€æ ¸å¿ƒçš„åŸºç¡€è®¾æ–½ï¼Œç›®å‰å›´ç»•æ­¤ç±»å½¢æ€è®¾è®¡çš„ http://Me.bot ä¹Ÿæ¯”è¾ƒå—æ¬¢è¿
- https://x.com/tuturetom/status/1835349759848333340

- ## Hugging Face å®£å¸ƒæŠ•å…¥ 1000 ä¸‡ç¾å…ƒç”¨äºå…è´¹å…±äº« GPUï¼Œæ—¨åœ¨å¸®åŠ©å°å‹å¼€å‘è€…ã€å­¦æœ¯ç•Œå’Œåˆåˆ›å…¬å¸å¼€å‘æ–°çš„ AI æŠ€æœ¯ï¼ŒæŠ—è¡¡ AI è¿›æ­¥çš„é›†ä¸­åŒ–ã€‚
- https://x.com/glow1n/status/1791488036259434749
  - CEO Clem Delangue è¡¨ç¤ºï¼Œè¿™ä¸€ä¸¾æªå°†é€šè¿‡ ZeroGPU è®¡åˆ’å®ç°ï¼Œä¿ƒè¿› AI æŠ€æœ¯çš„å»ä¸­å¿ƒåŒ–å‘å±•
  - ZeroGPU ä½¿ç”¨ Nvidia A100 GPU è®¾å¤‡ï¼Œæä¾›é«˜æ•ˆçš„è®¡ç®—èµ„æºã€‚
  - Hugging Face çš„ Spaces å¹³å°å·²æœ‰è¶…è¿‡ 30 ä¸‡ä¸ª AI æ¼”ç¤ºã€‚

- ## Cloudflare çš„ Workers AI æ¯å¤©å¯ä»¥å…è´¹ä½¿ç”¨ 10, 000 Neuronsï¼ˆç›¸å½“äºç”Ÿæˆ100-200ä¸ªLLMå“åº”ï¼Œ500æ¬¡ç¿»è¯‘ï¼Œ500ç§’çš„è¯­éŸ³è½¬æ–‡å­—éŸ³é¢‘ï¼‰ ï¼Œè°ƒç”¨æ–¹å¼å…¼å®¹ OpenAI 
- https://x.com/scomper/status/1791804644332908646
- å¥½åƒéƒ½æ˜¯å°æ¨¡å‹ä¸ºä¸»å§

# discuss-model-internals/tuning
- ## 

- ## 

- ## 

- ## 

- ## [Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pi8z74/which_small_model_is_best_for_finetuning_we/)
  - TL; DR: We fine-tuned 12 small models to find which ones are most tunable and perform best after fine-tuning. Surprise finding: Llama-3.2-1B showed the biggest improvement (most tunable), while Qwen3-4B delivered the best final performance - matching a 120B teacher on 7/8 tasks and outperforming by 19 points on the SQuAD 2.0 dataset.
  - 12 models total - Qwen3 (8B, 4B, 1.7B, 0.6B), Llama (3.1-8B, 3.2-3B, 3.2-1B), SmolLM2 (1.7B, 135M), Gemma (1B, 270M), and Granite 8B.
  - Tested on 8 benchmarks: classification tasks (TREC, Banking77, Ecommerce, Mental Health), document extraction, and QA (HotpotQA, Roman Empire, SQuAD 2.0).
  - I guess the title is a little click-baity. To get the total number, you have to account for the synthetic data we generated for each benchmark. That's the reason we could easily scale the training to many tasks and domains.
  - You are right that the training itself was not 10k, but running the whole pipeline adds up to the number. We didnt think that _that_ would be the main discussion point.

- i did this test back then with qwen 2.5 models for our work project, and interally we found that qwen2.5 7b was the most consistent model with the best result for its size, where the 2.5 3b had issues following instructions at times. did the qwen 3 4b model ever had this happen or was it able to execute all tasks without fail?
  - I wouldn't say "without fail", it's definitely not perfect, but it's pretty good at following instructions overall. On some tasks there was little difference between the 8B & 4B Qwen3 models (the harder the task, the more important param count is).

- Can this also apply to VL models? Qwen3 VL personally.
  - Not yet, but doing this same for vision models is on our roadmap!

- The Qwen team hit it out of the park with Qwen3-4B-Instruct-2507. This model keeps coming up all over the place and it deserves the praise it gets. I hope whatever secret sauce they used for that model is further propagated to their other models. Actually what I really would like is a deep dissection and a full whitepaper on why the hell this little model hits so much above its belt.

- I recommend strongly HuggingFaceTB/SmolLM3-3B. Got the same perfs as Qwen3-4B-Instruct-2507 for my task but with faster inference 

- [Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found : r/LocalLLM](https://www.reddit.com/r/LocalLLM/comments/1pi98l9/which_small_model_is_best_for_finetuning_we/)

- ## [Convert Dense into MOE model? : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pfxrv5/convert_dense_into_moe_model/)
- Contrary to many of the other answers here, it's definitely possible but usually not worth the trade-offs.
  - The thing is, you are better off using a model that was trained from scratch to be an MoE (since MoE are cheaper to train anyway) or just using a smaller dense model with more predictable performance.
  - They were able to convert a dense Llama model into an MoE with 25% activation and still have non-trivial performance without any retraining (and of course it does better with retraining). However, the MoE is not nearly as good as the original dense model.

- ## [In light of Kimi Linear, reposting Minimax's article on Linear Attention : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1oorvd0/in_light_of_kimi_linear_reposting_minimaxs/)
  - MiniMax M2 Tech Blog 3: Why Did M2 End Up as a Full Attention Model?

- until we have some gated delta net models with way more active parameters, i don't think we can really come to any solid conclusions right now. gpt-oss is 100% transformer and also sucks the same way as these really sparse hybrid models.

- Feels like "You never really know whatâ€™s going to happen until you scale up." might be one big reason why qwen will keep with Next type models for the time being. The reasoning could have been: their some performance penalty at inference, but for the gpu/time cost of one 32b dense, we can train 10+ different "good enough" 80b hybrid MoE. Allows us to iterate faster, detect issues faster, experiment more (more parameters? more experts? different training methods?). All of which could benefit full attention down the line.

- ## [Whatâ€™s the training cost for models like Qwen3 coder 30b and is the code for training it is open source or close source? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1njws3n/whats_the_training_cost_for_models_like_qwen3/)
- You can do fine tunes on the 4B real easy with Unsloth. LoRAs and QLoRAs, too. It wonâ€™t take much VRAM. You can fine tune a LoRA for Qwen3 30B A3B Instruct 2507 in BF16 on a single 96GB 6000 Pro in just a few hours.
  - That depends entirely on what you are training and the size of your dataset. True fine tuning, on a really small dataset will take a few hours, but if you are trying to train in new capabilities, that is going to take a much larger dataset, and could be months or over a year on a single RTX pro 6000.

- Their exact code isn't open source, but many companies train using Pytorch and Megatron-Core open source code.
  - I estimate GPU-time needed for training Qwen 30B A3B to be around 800 000 hours for H100 SXM5, so around 1.6 million dollars. That's for pretraining on 20T tokens, assuming 40% MFU.

- Stick to finetunes, you don't have the hardware or budget to pretrain models of practical size (believe me I have tried).
# discuss-terminal/fs
- ## 

- ## 

- ## 

- ## Introducing SQLite-Agent, a groundbreaking SQLite extension that allows your database to run autonomous AI agents directly inside SQLite itself. _202512
- https://x.com/_marcobambini/status/2000599613271892369
  - With SQLite-Agent, your data becomes active: agents can read, reason, act, and write back results without leaving the database engine. This turns SQLite into a fully self-contained automation and intelligence platform for apps, devices, and edge environments.
- https://github.com/sqliteai/sqlite-agent /c
  - A SQLite extension that enables SQLite databases to run autonomous AI agents, using other SQLite AI extensions. 

- ## at @llama_index we came up with ğ—®ğ—´ğ—²ğ—»ğ˜ğ—³ğ˜€-ğ—°ğ—¹ğ—®ğ˜‚ğ—±ğ—², a small demo that combines @claudeai and AgentFS by @tursodatabase with our Agent Workflows  _202512
- https://x.com/itsclelia/status/2000623632629002579
  - to create a harnessed and secure environment where the agent can perform all operations in a virtual filesystem without affecting your real one
  - Coding Agents are great, but giving them unfiltered access to your filesystem might be a very bad idea
  - Bonus: we also gave the agent understanding of unstructured documents, by parsing them with LlamaParse

# discuss
- ## 

- ## 

- ## 

- ## ğŸ†š [[Experiment] Three identical Qwen2.5:7b runs, three distinct behavioral strategies. One hit max reported metrics and started failing to execute actions. [Full data + code] : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1obkn0d/experiment_three_identical_qwen257b_runs_three/)
- 
- 
- 

- ## ğŸ§© [Qwen3-30B-A3B 2507 Instruct vs Thinking : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1o8529x/qwen330ba3b_2507_instruct_vs_thinking/)
- My understanding is that 
  - GPQA is general knowledge, 
  - AIME25 is Math, 
  - LiveCodeBench is coding, 
  - Arena-Hard is a predictor of how well it will perform at LMArena 
  - and BCFL is about tool calling.

- i'm guessing the t/s will be low if you use CPU (~5t/s) so just use instruct if you don't want to wait like 5 minutes for an answer with reasoning

- Reasoning only helps on logic-intensive tasks. On anything else, itâ€™s a waste of time at best, and can actually make things worse if the chain of thought focuses on the wrong thing.

- ## [Why do people like Ollama more than LM Studio? : r/LocalLLaMA _202501](https://www.reddit.com/r/LocalLLaMA/comments/1icta5y/why_do_people_like_ollama_more_than_lm_studio/)
- I think many of us are programmers, personally I prefer open-source applications. Of course as a programmer, I have no problems with using command line.
- You can search for models using the the website of Ollama. Downloading models is just one command ollama pull `<model_name>` . The same for removing or serving a model.
- And you can use Ollama with HuggingFace GGUF now too.

- Ollama depends on llama.cpp which was created by the guy (Georgi Gerganov) who invented GGUF.
  - Ollama does have its own registry though that stores the GGUF files in its own MODELFILE format.
  - You can use any GGUF with Ollama

- As a programmer I don't understand why more people don't use koboldcpp
  - It was a pain in the ass to get going properly. Also it might be just me, but I couldn't figure out how to serve multiple models at the same time with kobold.
- KoboldCPP seems oriented towards Windows. A lot of programmers are on Mac/Linux. (At least compared to the average population). Sure KoboldCPP seems to work on Mac/Linux. But, the docs are very Windows-centric. And so I'd rather use something more focused on my environment.

- Does llama-server allow serving multiple models at once? I couldn't figure out how to get that going, but it was a killer feature for me since I swap models on a task by task basis.
  - Well, kind of. You can start multiple llama-server instances and have multiple models loaded simultaneously, resources permitting. Otherwise, you can always hot load llama-server in your code.

- I use LM Studio even in production. You can start it as service. When product is new you need to babysit it , testing different params and models in pretty OK in LM Studio. I was using it from 0.1 to new 0.3 , new interface and multi-model support is pretty good.

- you can use ollama commercially without problems. lm studio writes that you have to contact them.

- Headless is a feature not a bug. I'm using multiple frontends to talk to the same server (Tabby, OpenWebUI, custom programs, etc). And, I'm running the inference on a separate machine from where I'm accessing it anyways.
  - Basically, programmers like Ollama, Llama.cpp Server, and other programmer oriented servers and libraries.

- Why don't people talk about OOBABOOGA? It's better than all listed, yes, including LM Studio!
  - Python programs take like 5 hours to install to then fail on some dependency conflict. Then you start again. I used OOBA for a bit but my python env and everything broke with every update.

- For me, the biggest advantage of Ollama is loading models on demand. I have a few automation scripts that use different models, plus it serves OpenWebUI, cline and aider, and dynamic loading/unloading is the way to go for me, otherwise Iâ€™d just use pure llama.cpp.

- ## [Are there any better offline/local LLMs for computer illiterate folk than Llama? Specifically, when it's installed using Ollama? : r/LocalLLaMA _202411](https://www.reddit.com/r/LocalLLaMA/comments/1gnjev5/are_there_any_better_offlinelocal_llms_for/)
- LMStudio uses llama.cpp backend on PC, and can additionally use MLX on Mac, so the model selection is somewhat broader on Mac. 
- MSTY uses bundled Ollama and can connect to other servers, including ChatGPT. It will also pick up models already downloaded by another Ollama installation.
- Msty is quite easy to set up and use: https://msty.app/ But which model they run depends on the computer. If necessary need to use some online API service like OpenRouter, which Msty also supports.

- ## [LM Studio vs Ollama vs Jan vs Llama.cpp vs GPT4All : r/LocalLLaMA _202502](https://www.reddit.com/r/LocalLLaMA/comments/1isazyj/lm_studio_vs_ollama_vs_jan_vs_llamacpp_vs_gpt4all/)
- llama-swap, which I use to proxy llama.cpp and koboldcpp.
  - llama-swap provides two important features: on-demand model load/unload; manage the parameters for launching the models
  - I am now able to "save" the way I launch llama.cpp for each model I want to use. I no longer need to keep a CLI open to stop/start llama.cpp or ollama when I want to change models.
- Agree. I started with ollama but slowly shifting towards llama-swap because it's simpler and provides better control over parameters.
  - I know I can create alter parameters in ollama to a great extent but having everything in a single config file is just better.

- LM Studio, mainly because it now has the "Auto unload unused JIT loaded models" feature.

- LM Studio as it just works. I really donâ€™t care about open/closed source. I want something that looks nice and just works well on my hardware.
  - same here. lm studio outperforms ollama anythingllm msty gpt4all in terms of speed and ease and model intelligence actually. I tried them all.
- Until it doesnt and you could do nothing about it.

- Quick note: Cortex powers Jan, and we're adding backend-level features to Jan through Cortex. Once Cortex becomes multi-engine, Jan will also support multiple engines alongside llama.cpp.

- KoboldCpp for obvious reasons, but the main advantages are that its not limited to just instruct models as it also has regular text completion modes. Its a server rather than just a local app so it can be used remotely, and for those that the bundled UI isn't suitable for it can be used with third party UI's trough the multiple API's it supports and emulates.

- GPT4ALL is my favorite on paper, but the devs refuse to fix a big issue with response streaming. I want to couple it with Open WebUI and the responses don't show up.

- Ooba because you can use quantized KV cache with Context Shift (LLM_Streaming). It's a must have if you roleplay and have low VRAM.
  - LLM Streaming to my knowledge is not the same as Context Shift like in KoboldCpp. Context Shift is compatible with things that should stay in memory, it shifts after the tokens that persisted. While traditional LLM Streaming implementations tend to fade out the old bits including context you may need. If I am out of the loop and ooba's implementation does now work like Kobolds feel free to correct me but this was not originally the case.

- LMStudio but their REST API is kinda of restrictive. I use hugging models loaded on ray sometimes for better API functionality. Wish i could try vLLM but not working on windows

- I use llama.cpp, because it is relatively simple and self-contained (not a lot of external dependencies), and something for which I could develop features and maintain.
  - It has the potential to be the "do-everything tool" for LLM technology; 
  - it used to have training and fine-tuning example code, but it was taken out and is in the process of being re-implemented so it can take advantage of any supported back-end (CUDA, SYCL, Vulkan, etc). 
  - The GGUF file format is well thought-out and people are taking good advantage of its metadata features, so you don't have to chase down inference-time parameters from obscure corners of the web.
  - I might be forced to learn vLLM eventually if my employer decides to start using Red Hat Enterprise AI, but until then llama.cpp is all I want or need.

- ## ğŸ†š [What's the point of OpenUI Web vs Piniochio or Jan.ai? The ladder of which being substantially easier to install : r/LocalLLaMA _202412](https://www.reddit.com/r/LocalLLaMA/comments/1hc20u7/whats_the_point_of_openui_web_vs_piniochio_or/)
- Lmstudio and Jan are examples of kitchen sink LLM apps, they merge model management with backend and frontend.
- OpenWebUI is just a frontend, so it makes most sense for folks that already have both a backend (and thus already have docker, etc all configured) and a way to manage models loaded on that backend.
- There are also several middle grounds:
  - Ollama has backend and model management, but no frontend.
  - Koboldcpp has backend and frontend, no model management.

- The kitchen sinks are a great place to start but if you're wondering what their downsides are: you are stuck to their model management system and whatever quants they support. Batch/multi-user performance is also not a priority for these apps.
  - To pick a concrete example, to my knowledge there's no all in one's support EXL2 and can spin me up a tabbyAPI with tensor parallel and a big context cache. So how easy it is to install doesn't matter to me, vs doing what I want.

- I don't use any of these and instead sometimes have to resort to fixing bugs in python myself in ooga/text-generation-webui, 
  - because I want to use top_a, tail_free or mirostat sampling, use GBNF-grammar restrictions, use DRY penalty, define custom stop tokens or token bias, control the order the samplers are applied, toggle if the BOS token is prefixed or not, edit the Models responses on the fly, change the chat template, adjust the RoPe-scale, choose the quantization datatype, etc...
- Most interfaces have some subset of these but rarely all. Some are pretty stubborn when I want to use the folder where I store all my models and not have them download the model again together with some stupid specific template file.
- text-generation-webui is a pretty crappy(å·®åŠ²çš„ï¼Œå¾ˆç³Ÿçš„) UI and can be quite the hassle(éº»çƒ¦, ä¸æ–¹ä¾¿) to install and update, but the closest feature set relative to writing custom python code using llama.cpp or hf-transformers.

- Simplest "chat to LLM" is llama.cpp's server. The newest release.
  - It's 5 MB in size, has a well-looking web frontend, stores past conversations in browser memory. And is super lightweight and works fast.
- You say "simplest", but... one of these you can just download an app and double click it, and it's not llama.cpp. I have no trouble compiling llama.cpp myself, and I can also navigate HuggingFace to find a good model in the right quant and download that myself, but this is asking a lot of the average person.

- ## [What is your preferred front-end/back-end these days? (Q2 2024) : r/LocalLLaMA _202406](https://www.reddit.com/r/LocalLLaMA/comments/1dad0rh/what_is_your_preferred_frontendbackend_these_days/)
- Personal (single user, power-user) use: SillyTavern frontend, oobabooga's text-generation-webui backend (EXL2, HF). KoboldCpp backend if I need to run a GGUF for some reason (I prefer EXL2 for speed, especially with big contexts).
  - Professional (multi user, end-user) use: Open WebUI frontend, Ollama backend (simple) or vLLM/Aphrodite Engine (fast). Aphrodite Engine is a fork of vLLM that I prefer, supports more formats and is more customizable.
- I like to call SillyTavern the LLM IDE for power users: it gives you complete control over generation settings and prompt templates, editing chat history or forking entire chats, and offers many other useful options. There are also extensions for advanced features such as RAG and web search, real-time voice chat, etc. I love it and use it all the time. Once you learn it, you can use any backend, both local and online.
  - But it's not for everyone. Some just want a ChatGPT alternative, a simple chat interface, and advanced options would just confuse them. That's true for most users who aren't AI developers or enthusiasts, and that's who Open WebUI is ideal for. I run it as a local AI chat interface at work for my colleagues, while I prefer to use SillyTavern myself.

- I currently use koboldcpp for both back and front ends. I love how fast the team integrates upstream llamacpp fixes/additions. Has a multiuser batching function with nice GUI for the Mrs to use, and "just works" no matter what I throw at it.

- jan failed for me with codestral. It never answered back, tried multiple times, and different PCs as well. I can get llama3 8B just fine tho.

- I started with lm-studio but now i prefer librechat as frontend and ollama as backend. Supports pretty much all online apis and ollama. Supports a rag database and search. Is very simple to setup and i dont need to manage all the model setup (ollama just works) im just limited to the available models unless i write a modelfile myself.

- What i didnt like about lm studio but got with librechat:
  - using Local ai and online apis seamlessly in the same chat
  - not have to manage presets to run models correctly and efficiently
  - Search in chats
  - Rag database
  - TTS
- What i miss from using lm-studio:
  - accessing huggingface models directly
  - performance metrics
