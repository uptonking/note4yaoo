---
title: lib-office-docling-examples
tags: [docling, examples, format, office]
created: 2025-09-21T13:58:28.944Z
modified: 2025-09-21T13:58:46.548Z
---

# lib-office-docling-examples

# guide

- tips
  - è¿˜å¯ä»¥åœ¨ ocr benchmark/leaderboard ä¸­å¯»æ‰¾å‚è€ƒå’Œä»£ç 
# popular
- https://github.com/docling-project/docling /39.3kStar/MIT/202509/python
  - https://github.com/DS4SD/docling
  - https://docling-project.github.io/docling
  - Docling simplifies document processing, parsing diverse formats
  - Transform PDF to JSON or Markdown with ease and speed
  - Parsing of multiple document formats incl. PDF, DOCX, PPTX, XLSX, HTML, WAV, MP3, images (PNG, TIFF, JPEG, ...), and more
  - æ¨¡å—åŒ–è®¾è®¡ï¼Œé›†æˆ Unstructuredã€LayoutParser ç­‰åº“ï¼Œæ”¯æŒæœ¬åœ°åŒ–å¤„ç†
  - ~~éœ€ CUDA ç¯å¢ƒï¼Œéƒ¨åˆ†åŠŸèƒ½ä¾èµ–å•†ä¸šæ¨¡å‹~~
  - Advanced PDF understanding incl. page layout, reading order, table structure, code, formulas, image classification, and more
  - Unified, expressive DoclingDocument representation format
  - export formats: Markdown, HTML, DocTags and lossless JSON
  - Plug-and-play integrations incl. LangChain, LlamaIndex, Crew AI & Haystack for agentic AI
  - Optionally applies OCR, Extensive OCR support for scanned PDFs and images
  - Support of Visual Language Models (SmolDocling)
  - Works on macOS, Linux and Windows environments. Both x86_64 and arm64 architectures.
  - https://huggingface.co/ibm-granite/granite-docling-258M /apache2
    - a multimodal Image-Text-to-Text model engineered for efficient document conversion

- https://huggingface.co/spaces/ibm-granite/granite-docling-258M-WebGPU/tree/main /apache2/202510/html/vanillajs
  - a demo which showcases the model running entirely in your browser with WebGPU acceleration
  - âœ¨ æ­¤ç±»demoå¯è€ƒè™‘å®ç°ç±»ä¼¼å›¾ç‰‡å¯¹æ¯”çš„æ–°æ—§å¯¹æ¯”æ»‘å—
  - [Granite Docling WebGPU: State-of-the-art document parsing 100% locally in your browser. : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o0php3/granite_docling_webgpu_stateoftheart_document/)
    - I had a very good experience with granite-docling as my goto pdf processor for RAG knowledge base.
    - Love this. WebGPU seems to be underutilized in general and could provide a better alternative to BYOK + cloud inference.
    - If someone could add translation feature on top of this, it would be perfect
    - It is first time I am seeing someone using Transformers.js
  - https://github.com/PDFMathTranslate/PDFMathTranslate /29.8kStar/AGPLv3/202511/python 
    - âœ¨ å¯å‚è€ƒç¿»è¯‘åpdfå¸ƒå±€ä¸å˜çš„å®ç°æ–¹å¼, ç‰¹åˆ«æ˜¯è¡¨æ ¼ä¸­è‹±æ–‡å˜ä¸­æ–‡ä½†å¸ƒå±€ä¸å˜
    - æ”¯æŒåŒæ å¸ƒå±€æ˜¾ç¤ºåŸæ–‡å’Œç¿»è¯‘ï¼Œä½“éªŒéå¸¸å¥½
    - [Upload your documents and see how OmniAI reads scans, PDFs](https://getomni.ai/ocr-demo)
  - https://github.com/pdfme/pdfme /MIT
    - æä¾›äº†æœºé‡jsonæ¨¡ç‰ˆç”Ÿæˆpdfçš„æ–¹æ¡ˆ

- https://huggingface.co/spaces/mozilla-ai/document-to-markdown/tree/main
  - https://huggingface.co/spaces/mozilla-ai/document-to-markdown
  - å¯é€‰: easyocr, tesseract, rapidocr, ocrmac

- https://huggingface.co/spaces/PaddlePaddle/PaddleOCR/tree/main /apache2/202504/python
  - and i tested handwritten text for english. it got the numbers right...

- https://github.com/NanoNets/docstrange /460Star/MIT/202508/python
  - https://docstrange.nanonets.com/
  - Extract and convert data from any document, images, pdfs, word doc, ppt or URL into multiple formats (Markdown, JSON, CSV, HTML) with intelligent content extraction and advanced OCR.
  - Cloud Processing (Default): Instant free conversion with cloud API 
  - Local Processing: CPU/GPU options for complete privacy - no data sent anywhere
  - ğŸ› ä»£ç ä¸­æœªçœ‹åˆ°æµå¼å¤„ç†æ–‡ä»¶çš„é€»è¾‘, å¯¹å¤„ç†å¤§æ–‡ä»¶ä¸å‹å¥½
  - Universal Input: PDFs, Word docs, Excel, PowerPoint, images, URLs, and raw text
  - URL Processing: Direct conversion from web pages
  - Smart Output: Markdown, JSON, CSV, HTML, and plain text formats
  - Advanced OCR: Multiple OCR engines with automatic fallback
  - MCP Server: Integrate with Claude Desktop for intelligent document navigation
  - [Package for converting PDF, images and docs to structured data like JSON, markdown, HTML : r/node _202509](https://www.reddit.com/r/node/comments/1nqxada/package_for_converting_pdf_images_and_docs_to/)
    - I've published a Node.js client for DocStrange 
  - [Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes & More : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/)
    - models designed for advanced image-to-markdown conversion and Visual Question Answering (VQA).
    - 1.5b--apache2, 3b--Qwen2.5-VL-3B-research-only-lic

- https://github.com/docling-project/docling-serve /725Star/MIT/202509/python
  - Running Docling as an API service.
  - The server is available at http://127.0.0.1:5001
  - An image for AMD ROCm 6.3 (docling-serve-rocm) is supported but not published due to its large size.

- https://github.com/drmingler/docling-api /693Star/MIT/202503/python/inactive
  - a highly scalable and lightweight backend for docling.
  - deployable and scalable backend server that efficiently converts various document formats (pdf, docx, pptx, html, images, etc) into Markdown. 
  - it offers text/table extraction, OCR, and batch processing with sync/async endpoints.
  - Powered by Docling (IBM's advanced document parser), this service is built with FastAPI, Celery, and Redis, ensuring fast, efficient processing.
  - Optimized for both CPU and GPU modes, with GPU highly recommended for production environments

- https://github.com/docling-project/docling-mcp /210Star/MIT/202508/python
  - Making docling agentic through MCP
  - a service that provides tools for document conversion, processing and generation. 
  - It uses the Docling library to convert PDF documents into structured formats and provides a caching mechanism to improve performance.
  - The service exposes functionality through a set of tools that can be called by client applications.
  - Local document caching for improved performance
  - Support for local files and URLs as document sources
  - RAG applications with Milvus upload and retrieval

- https://github.com/AKSarav/pdfstract /108Star/apache2/202601/python/js
  - The Extraction and Chunking Layer in Your RAG Pipeline - Available as CLI - WEBUI - API
  - Extract structured text, tables, and metadata from PDFs using various libraries (PyMuPDF4LLM, MarkItDown, Marker, Docling, PaddleOCR, DeepSeek-OCR, Tesseract, MinerU, Unstructured, and more)
  - Chunk the text into smaller chunks using various libraries (Token, Sentence, Recursive, Table, Semantic, Code, Late, Neural, Slumber, and more)
    - 10+ chunking methods powered by `Chonkie`.
  - Embed the chunks using various libraries (Sentence Transformers, OpenAI, etc.)
  - Multiple Output Formats: Markdown, JSON, and Plain Text
  - On-Demand Model Downloads: Download ML models only when needed
  - Batch Processing: Parallel conversion of 100+ PDFs with detailed reporting
  - ä¾èµ–fastapiã€Chonkieã€PyMuPDF, Marker, Docling
  - ğŸ†š [Built a small tool to compare PDF â†’ Markdown libraries (for RAG / LLM workflows) : r/Rag _202507](https://www.reddit.com/r/Rag/comments/1m1j10e/built_a_small_tool_to_compare_pdf_markdown/)
    - Iâ€™ve been exploring different libraries for converting PDFs to Markdown to use in a Retrieval-Augmented Generation (RAG) setup.
    - But testing each library turned out to be quite a hassle â€” environment setup, dependencies, version conflicts, etc.
    - Currently, it supports: docling pymupdf4llm markitdown marker
# examples
- https://github.com/mozilla-ai/document-to-markdown /MIT/202506/python/inactive
  - Convert unstructured documents to markdown using the Docling.
  - This blueprint guides you to convert various unstructured documents (PDFs, DOCX, HTML, etc.) to markdown, or other, formats using the Docling CLI or a locally-hosted demo UI
  - We have built a simple Graphical Interface demo of Docling to showcase some basic functionality 

- https://huggingface.co/spaces/AmineAce/pdf-tables-rag-demo/tree/main /202512/python/å•æ–‡ä»¶
  - https://amineace-pdf-tables-rag-demo.hf.space/
    - I built a free demo using IBM's Docling â€“ it handles merged cells and footnotes way better than most open-source options.
    - Built with Gradio
  - [Free PDF-to-Markdown demo that finally extracts clean tables from 10-Ks (Docling) : r/Rag](https://www.reddit.com/r/Rag/comments/1pupkkf/free_pdftomarkdown_demo_that_finally_extracts/)

- https://github.com/paazmaya/docling-japanese-books /MIT/202511/python
  - A streamlined document processing tool that uses Docling to extract, process, and store Japanese books and documents for LLM training workflows.
  - Extract and annotate images with SHA-256 hashing for deduplication
  - Vector Storage: Milvus database with enhanced metadata
  - LLM Ready: Multiple embedding models (Jina v4, BGE-M3, Snowflake Arctic, MiniLM) with Late Chunking optimization

- https://github.com/AI-Engineer-Skool/booktutor-ai /AGPL/202507/python/å•æ–‡ä»¶
  - https://www.skool.com/ai-engineer/
  - Using Docling to create an AI assistant out of any PDF book in minutes
  - ä¾èµ–langchain
  - å¯ä½¿ç”¨æœ¬åœ°llm
- https://github.com/fahdmirza/doclingwithollama /apache2/å•æ–‡ä»¶
  - Docling with Ollama - RAG on Local Files with Local Models

- https://github.com/HossamEldinx/docling-in-action /202411
  - This is a simple Question-Answering system using Retrieval Augmented Generation (RAG) with Groq's LLM.

- https://github.com/genieincodebottle/parsemypdf /MIT/202508/python
  - A Comprehensive example codes for extracting content from complex PDFs with mixed elements, including text and image data extraction

- https://github.com/lesteroliver911/docling-pdf-processor /MIT/202412/python/inactive
  - A simple UI wrapper around Docling for document processing. 
  - I built this to make document analysis more accessible and thought others might find it useful.
  - Presents results in a clean Streamlit interface

- https://github.com/AIAnytime/SmolDocling-OCR-App /MIT/202503/python/inactive
  - SmolDocling OCR App built using SmolDocling 256M Model and Streamlit.
  - https://github.com/bibekpdl/SmolDocling-Document-Processor

- https://github.com/Abdoulaye-Sayouti/Secure-Offline-RAG-System /MIT/202412/python/inactive
  - A RAG system designed for efficient processing of diverse content types with minimal computational overhead.
  - This solution won 1st place in the Secure RAG Challenge by UnderstandTech

- https://github.com/kv1830/fast_pdf_trans
  - åŸºäº`MinerU`å®ç°pdfè½¬markdownçš„åŠŸèƒ½ï¼Œæ¥ç€å¯¹markdownè¿›è¡Œåˆ†å‰²ï¼Œ é€ç»™å¤§æ¨¡å‹ç¿»è¯‘ï¼Œæœ€åç»„è£…ç¿»è¯‘ç»“æœå¹¶ç”±`pypandoc`ç”Ÿæˆç»“æœpdfã€‚
# utils
- https://github.com/messkan/rag-chunk /MIT/202511/python
  - A Python CLI to test, benchmark, and find the best RAG chunking strategy for your Markdown documents.
  - Multiple chunking strategies: fixed-size, sliding-window, paragraph
  - Recall-based evaluation with test JSON files
  - [Roadmap Discussion: Is LangChain's "RecursiveCharacterSplitter" actually better? I'm building v0.3.0 to find out. : r/Rag](https://www.reddit.com/r/Rag/comments/1p2xhjq/roadmap_discussion_is_langchains/)

- https://github.com/2dogsandanerd/smart-ingest-kit /202511/python
  - [I extracted my production RAG ingestion logic into a small open-source kit (Docling + Smart Chunking) : r/Rag](https://www.reddit.com/r/Rag/comments/1p4ku3q/i_extracted_my_production_rag_ingestion_logic/)
  - Most tutorials tell you to use `RecursiveCharacterTextSplitter(chunk_size=1000)`. That's fine for demos, but in production, it breaks
    - PDF tables get shredded into nonsense.
    - Code blocks get cut in half.
    - Markdown headers lose their hierarchy.
  - I stripped out all the business logic from my app and left just the "Smart Loader".
  - It uses Docling (by IBM) for layout-aware parsing and applies heuristics to choose the optimal chunk size based on file type.
    - PDFs: Uses semantic splitting with larger chunks (800 chars) to preserve context.
    - Code: Uses small chunks (256 chars) to keep functions intact.
    - Markdown: Respects headers and structure.
    - Output: Clean Markdown that your LLM actually understands.

- https://github.com/ghodsizadeh/pdf2csv /MIT/202501/python
  - https://pdf2csv-py.streamlit.app/
  - This project provides a tool to convert tables from PDF files into CSV or XLSX format using the Docling library. 
  - It extracts tables from PDFs and saves them as CSV or XLSX files, optionally reversing text for right-to-left languages.
  - This project heavily depends on the Docling library for PDF table extraction, it will be installed automatically when you install this package.

- https://github.com/shoryasethia/markdrop /GPL/202507/python/inactive
  - A Python package for converting PDFs to markdown while extracting images and tables, generate descriptive text descriptions for extracted tables/images using several LLM clients.
  - PDF to Markdown conversion with formatting preservation using Docling
  - Automatic image extraction with quality preservation using XRef Id
  - Table detection using Microsoft's Table Transformer

- https://github.com/docling-project/docling-eval /MIT/python
  - Evaluation framework for document processing models and services.
  - Evaluate Docling on various datasets.

- https://github.com/docling-project/docling-core /MIT/python
  - A python library to define and validate data types in Docling.
  - a library that defines core data types and transformations in Docling.
  - Docling Core provides the foundational `DoclingDocument` data model and API, as well as additional APIs for tasks like serialization and chunking
  - Docling Core defines the `DoclingDocument` as a Pydantic model, allowing for advanced data model control, customizability, and interoperability.
# ocr/vlm
- https://github.com/emcf/thepipe /1.5kStar/MIT/202510/python/inactive
  - a package that can scrape clean markdown, multimodal media, and structured data from complex documents.
  - It can extract well-formatted data from a wide range of sources, including PDFs, URLs, Word docs, Powerpoints, Python notebooks, videos, audio, and more.
  - Uses VLM for OCR in text-only mode
  - Accepts a wide range of sources, including PDFs, URLs, Word docs, Powerpoints, Python notebooks, GitHub repos, videos, audio, and more
  - The default install only pulls in CPU-friendly dependencies so it is suitable for constrained environments and CI systems. 
    - GPU-enabled libraries such as PyTorch and Triton are left as optional extras.
  - ğŸ‘¾ By default, thepipe uses the OpenAI API, so VLM features will work out-of-the-box provided you pass in an OpenAI client.
    - If you wish to use a local vision-language model or a different cloud provider, you can provide a custom OpenAI client
  - structured extraction is being deprecated and will be removed in future releases. The current implementation is a simple wrapper around OpenAI's chat API, which is not ideal for structured data extraction. We recommend OpenAI's structured outputs for structured data extraction, or using Trellis AI for automated workflows with structured data.

- https://github.com/aitomatic/ai-vision-capture /apache2/202601/python
  - A Python library for extracting and analyzing content from PDF, Image, and Video files using VLM
  - Supports multiple providers including OpenAI, Anthropic Claude, Google Gemini, and Azure OpenAI.
  - Async Processing: Configurable concurrency for batch operations
  - Structured Output: Template-based data extraction to JSON
  - Pluggable Architecture: Provider abstraction with auto-detection
  - PDF-to-image conversion using PyMuPDF (fitz)
  - VisionCapture: Used for extracting specific fields from documents (e.g., forms, technical diagrams)
    - ä¼¼ä¹åªæå–å­—æ®µ, ä¸æå–æ’å›¾

- https://github.com/Aquos06/reader-vl /MIT/202503/python/inactive/æäº¤å°‘
  - a tool that transforms various document formats, like PDFs and DOCX files, into a unified structure.
  - Process PDFs and DOCX (for now).
  - Uses multimodal LLMs to extract meaning from diagrams, images, chart, and etc.
  - Structured Output: Converts documents into a well-defined schema, preserving relationships between text and visuals.
  - Fast & Efficient: Leverages YOLO for object detection and Tesseract OCR.
  - Extensibility: Designed for easy integration into AI pipelines for customization and scalability.
  - I am not solely using OCR to parse the pdf. I am using both OCR for normal text (we called it section) and VLM for the more abstract one like Table, Chart, Diagram, and etc
  - [Introducing Reader VL: A Tool for Unified Document Processing with VLM Integration : r/LocalLLaMA _202503](https://www.reddit.com/r/LocalLLaMA/comments/1j2m6m7/introducing_reader_vl_a_tool_for_unified_document/)
    - I'm developing a RAG system for enterprise (B2B) and noticed that many company documents contain charts, images, and other non-text elements. Our current parsers (Docling, Unstructured) struggle to process these formats. While searching for a solution, I couldn't find any parser leveraging modern VLMsâ€”so I decided to build one myself.
    - Currently, Reader VL processes entire documents in a single pass. I'm actively working on implementing chunking and batching functionalities to better support very large documents, such as those spanning hundreds or thousands of pages.

- https://github.com/PRITHIVSAKTHIUR/Doc-VLMs-exp /apache2/202507/python/inactive
  - document-focused Vision-Language Model application that provides advanced document analysis, text extraction, and multimodal understanding capabilities.
  - This application features a streamlined Gradio interface for processing both images and videos using state-of-the-art vision-language models specialized in document understanding.
  - Document-Specialized Models: Three cutting-edge VLM models optimized for document processing
  - Image Processing: Advanced document analysis and text extraction from images
  - Video Processing: Frame-by-frame video analysis with temporal understanding
  - Real-time Streaming: Live output generation with immediate feedback
  - Dual Output Format: Raw text stream and formatted markdown results
  - Canvas-Style Interface: Clean, professional output display
# code-rag
- https://github.com/starthackHQ/contextinator /apache2/202511/python
  - Turning messy repos into weapons of mass structured context.
  - It uses Abstract Syntax Tree (AST) parsing to extract semantic code chunks, generates embeddings, and stores them in a vector database-enabling AI agents to understand, navigate, and reason about codebases with unprecedented precision.
  - AST-Powered Chunking - Extract functions, classes, and methods from 23+ programming languages
  - Semantic Search - Find relevant code using natural language queries
  - Full Pipeline Automation - One command to chunk, embed, and store
  - ğŸ›¢ï¸ Docker (for ChromaDB)
# mineru
- https://github.com/zt6453928/ailat-translation /MIT/202601/python/js
  - https://mineru.net/apiManage/docs
  - AI-Powered Document Translation Tool
  - A web-based PDF document parsing and translation tool that supports intelligent document parsing, multi-language translation, and various export formats.
  - Supports PDF, DOCX and other document formats, extracts text, images, tables, and formulas
  - Export to PDF, Markdown, HTML, DOCX, JSON, LaTeX
  - Multiple Translation Engines: Supports DeepLX and OpenAI-compatible APIs
  - Real-time Preview: Side-by-side comparison of original and translated content
  - ğŸ›
    - é‡‡ç”¨ vlm æ–¹æ¡ˆ
    - pdfåŸæ–‡ä»¶ä¸Šä¼ åä¿å­˜åœ¨ `outputs/<task_id>/original.pdf` .
    - mineruè§£æåçš„æ–‡æœ¬å†…å®¹ä»…åœ¨memory, æœªæŒä¹…åŒ–
  - [å¼€æºä¸€ä¸ªæ”¯æŒå¤šæ ¼å¼æ–‡æ¡£ç¿»è¯‘åº”ç”¨ ](https://linux.do/t/topic/1511535)
    - å‡ºäºèº«è¾¹åŒå­¦å¯¹è®ºæ–‡ç¿»è¯‘çš„éœ€æ±‚ï¼Œæ‰€ä»¥å¼€å‘äº†è¿™ä¸ªé¡¹ç›®ï¼Œé™¤äº†è®ºæ–‡æˆ‘ä¹Ÿç”¨æ¥å¯¹ä¸€äº›AIæ•™ç¨‹æ–‡æ¡£è¿›è¡Œç¿»è¯‘
    - çµæ„Ÿæ¥è‡ªäºMinerU, PDFæ ¼å¼è¿˜æ”¯æŒå¯¹ç…§ç¿»è¯‘

- https://github.com/magicyuan876/mineru-tianshu /apache2/202601/python/ts/vue
  - å¤©æ¢ - ä¼ä¸šçº§ AI æ•°æ®é¢„å¤„ç†å¹³å°ï¼Œå°†éç»“æ„åŒ–æ•°æ®è½¬æ¢ä¸º AI å¯ç”¨çš„ç»“æ„åŒ–æ ¼å¼
  - æ”¯æŒæ–‡æ¡£ã€å›¾ç‰‡ã€éŸ³é¢‘ç­‰å¤šæ¨¡æ€æ•°æ®å¤„ç† | GPU åŠ é€Ÿ | MCP åè®®
  - åç«¯ï¼šFastAPIã€LitServeã€MinerUã€PaddleOCRã€SenseVoiceã€SQLiteã€Loguru
  - å‰ç«¯ï¼šVue 3ã€TypeScriptã€Viteã€TailwindCSSã€Piniaã€Vue Router
  - å¤šè§£æå¼•æ“: MinerUã€PaddleOCR-VLã€MarkItDownã€æ ¼å¼å¼•æ“
    - pipeline: MinerU æ ‡å‡†æµç¨‹ï¼Œé€šç”¨æ–‡æ¡£è§£æ
    - vlm-transformers/vlm-vllm-engine: MinerU VLM æ¨¡å¼
    - paddleocr-vl: 109+ è¯­è¨€ï¼Œè‡ªåŠ¨æ–¹å‘çŸ«æ­£, ä»…æ”¯æŒ GPU: PaddleOCR-VL ç›®å‰ä¸æ”¯æŒ CPU åŠ Arm æ¶æ„
  - æ–‡æ¡£: PDFã€Wordã€Excelã€PPT â†’ Markdown/JSONï¼ˆMinerUã€PaddleOCR-VL 109+ è¯­è¨€ã€æ°´å°å»é™¤ï¼‰
  - è§†é¢‘: MP4ã€AVIã€MKV â†’ è¯­éŸ³è½¬å†™ + å…³é”®å¸§ OCRğŸ§ªï¼ˆFFmpeg + SenseVoiceï¼‰
  - éŸ³é¢‘: MP3ã€WAVã€M4A â†’ æ–‡å­—è½¬å†™ + è¯´è¯äººè¯†åˆ«ï¼ˆSenseVoice å¤šè¯­è¨€ï¼‰
  - å¤§æ–‡ä»¶å¹¶è¡Œå¤„ç†: PDF è‡ªåŠ¨æ‹†åˆ†åŠŸèƒ½ï¼šè¶…è¿‡é˜ˆå€¼ï¼ˆé»˜è®¤ 500 é¡µï¼‰çš„ PDF è‡ªåŠ¨æ‹†åˆ†ä¸ºå¤šä¸ªå­ä»»åŠ¡å¹¶è¡Œå¤„ç†
    - å¹¶å‘å®‰å…¨: åŸå­æ“ä½œé˜²æ­¢ä»»åŠ¡é‡å¤ï¼Œæ”¯æŒå¤š Worker å¹¶å‘
    - Worker ä¸»åŠ¨æ‹‰å–: 0.5ç§’å“åº”ï¼Œæ— éœ€è°ƒåº¦å™¨è§¦å‘
  - RustFS å¯¹è±¡å­˜å‚¨ï¼šæ‰€æœ‰è§£æç»“æœçš„å›¾ç‰‡è‡ªåŠ¨ä¸Šä¼ åˆ°å¯¹è±¡å­˜å‚¨, S3 å…¼å®¹ APIï¼ŒåŸºäº minio-py å®ç°
  - GPU è´Ÿè½½å‡è¡¡: LitServe è‡ªåŠ¨è°ƒåº¦ï¼Œé¿å…æ˜¾å­˜å†²çªï¼Œå¤š GPU éš”ç¦»
  - ä¼ä¸šç‰¹æ€§: GPU è´Ÿè½½å‡è¡¡ã€ä»»åŠ¡é˜Ÿåˆ—ã€JWT è®¤è¯ã€MCP åè®®ã€ç°ä»£åŒ– Web ç•Œé¢
  - Tianshu æ”¯æŒå®Œå…¨ç¦»çº¿éƒ¨ç½²ï¼Œæä¾›ä¸¤ç§éƒ¨ç½²æ¨¡å¼ï¼š
    - æ–¹å¼ 1ï¼šLinux æœåŠ¡å™¨ï¼ˆæœ‰ GPU åˆ™åŠ é€Ÿï¼Œæ—  GPU è‡ªåŠ¨é™çº§ CPUï¼‰
    - æ–¹å¼ 2ï¼šCPU ä¸“ç”¨ç‰ˆï¼ˆMac/æ—  GPU ç¯å¢ƒï¼‰

- https://github.com/wzdavid/mineru-api /MIT/202601/python
  - [åªéœ€ 4 æ­¥æå®šï¼å¼€æºæ–‡æ¡£è§£ææœåŠ¡ MinerU-API æœ€æ–°å®‰è£…æŒ‡å— - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1999940036863492525)
  - åŸºäº Celery çš„å¼‚æ­¥æ–‡æ¡£è§£ææœåŠ¡
  - å¼‚æ­¥å¤„ç†ï¼šåŸºäºåˆ†å¸ƒå¼ä»»åŠ¡é˜Ÿåˆ—ï¼Œæ”¯æŒé«˜å¹¶å‘
  - æ”¯æŒä»»åŠ¡é‡è¯•å’Œæ•…éšœæ¢å¤
  - å®æ—¶ç›‘æ§ï¼šä»»åŠ¡çŠ¶æ€è·Ÿè¸ªå’Œé˜Ÿåˆ—ç»Ÿè®¡ã€‚ä½ å¯ä»¥éšæ—¶æŸ¥çœ‹ä»»åŠ¡çš„è¿›åº¦å’ŒçŠ¶æ€
  - æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ·»åŠ æ–°çš„è§£æå¼•æ“ã€‚å¦‚æœä½ éœ€è¦å¤„ç†æ–°çš„æ–‡æ¡£æ ¼å¼ï¼Œåªéœ€è¦æ·»åŠ ä¸€ä¸ªæ–°çš„è§£æå¼•æ“å³å¯ã€‚

- https://github.com/firecrawl/mineru-api /AGPL/202512/python
  - Fork of neka-nat/mineru-api - API server for MinerU.
  - designed for running MinerU on serverless platforms such as Runpod.
  - https://github.com/neka-nat/mineru-api /AGPL/202512/python
    - API server for MinerU.
# extraction
- https://github.com/datalab-to/pdftext /644Star/apache2/202506/python
  - Text extraction like `PyMuPDF`, but without the AGPL license. PDFText extracts plain text or structured blocks and lines. 
  - It's built on `pypdfium2`, so it's fast, accurate, and Apache licensed.
  - It first uses pypdfium2 to extract characters in order, along with font and other information. Then it uses a simple decision tree algorithm to group characters into lines and blocks. It does some simple postprocessing to clean up the text.
  - built on some amazing open source work, including:  pypdfium2, scikit-learn

- https://github.com/KatherLab/llmaixlib /MIT/202508/python/inactive
  - a Python toolkit for automated document preprocessing (including OCR) and information extraction using large language models
  - Preprocessing: Extracts human-readable Markdown or plain text from a wide range of document types, automatically falling back to OCR for scanned or image-based files.
  - Uses a large language model (LLM) to transform unstructured or semi-structured text into structured dataâ€”validated by Pydantic models or JSON Schemaâ€”via an OpenAI-compatible API.
  - OCR tools: Tesseract (for OCRmyPDF), a GPU for faster OCR (Marker and PaddleOCR)
  - OpenAI-compatible API endpoint: Required for information extraction

- https://github.com/nanonets/hands-on-vision-language-models /202409/python/inactive
  - [Best Vision Language Models for Document Data Extraction _202409](https://nanonets.com/blog/vision-language-model-vlm-for-data-extraction/)
  - [Benchmarking vision language models for document data extraction : r/computervision _202409](https://www.reddit.com/r/computervision/comments/1fv0ns5/benchmarking_vision_language_models_for_document/)
    - Models tested : Qwen2, MiniCPM, Bunny, GPT-4o mini, Gemini 1.5 flash,  Claude 3.5
    - I have chosen these models because they fit on a consumer GPU (less than 24GB VRAM)
  - Datasets used: SROIE and CORD receipt datasets - iâ€™ve explained why these in the blog, but tldr is that they are 
    - standardized (and hence simpler to benchmark)
    - complex enough to present a challenge to all the VLMs
    - no MLLM benchmarks focussing on extracting structured data from documents
    - Both fields and tables are present in the datasets
  - Takeaways
    - QWEN is the superior OS model, and Claude is the best among closed source
# more
