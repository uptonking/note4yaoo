---
title: lib-ai-app-community-generation
tags: [ai, community]
created: 2023-04-16T12:51:38.350Z
modified: 2023-04-16T12:52:03.130Z
---

# lib-ai-app-community-generation

# guide

- resources
  - [ã€è½»ç§‘æ™®ã€‘StableDiffusioné‚£äº›äº‹å„¿ï¼Œå…³äºLoRAã€DreamBoothã€æ¨¡å‹åˆ†å±‚èåˆç­‰](https://www.bilibili.com/video/BV1RT411D7h7/)
# discuss-video
- ## 

- ## 

- ## 

- ## æƒ³é—®ä¸‹è¿™ç§è§†é¢‘æ˜¯AIåšçš„ä¹ˆï¼Ÿ ç±»ä¼¼çŸ­å‰§
- https://x.com/ilovek8s/status/1911305338239983988
- 4oå‡ºå›¾ï¼Œç„¶å3DåŒ–ï¼Œåº•å›¾ä¸å¥½è¯´æ‹¿å•¥åšçš„ï¼Œå¯èƒ½å›¾ç”Ÿè§†é¢‘æ˜¯æ—¢æ¢¦
  - æˆ‘æœäº†ä¸€ä¸‹æ—¢æ¢¦çš„æ¨¡ç‰ˆé‡Œï¼Œç›®å‰è¿˜æ²¡çœ‹åˆ°æ¯”è¿™ä¸ªæ›´é€¼çœŸäº›çš„

- æˆ‘è§‰å¾— ä»¥åçš„ä¹¦ éƒ½ä¼šå˜æˆè¿™ä¸ªæ ·å­
# discuss-tts/audio
- ## 

- ## 

- ## In-browser TTS in 3 lines of code
- https://twitter.com/xenovacom/status/1716711760982319429
  - `import { pipeline } from '@â€‹xenova/transformers';`

# discuss-qwen
- qwen-image
  - https://huggingface.co/city96/Qwen-Image-gguf/tree/main
  - https://modelscope.cn/models/city96/Qwen-Image-gguf/files
  - [Qwen-Image ComfyUI Native Workflow Example - ComfyUI](https://docs.comfy.org/tutorials/image/qwen/qwen-image)

- https://github.com/ModelTC/Qwen-Image-Lightning /apache2/202508/python
  - Qwen-Image-Lightning: Speed up Qwen-Image model with distillation
  - Qwen-Image-Lightning-8steps-V1.1
  - Qwen-Image-Lightning-4steps-V1.0

- ## 

- ## 

- ## [The 4-Step lightening lora for Qwen Image is available now : r/StableDiffusion _202508](https://www.reddit.com/r/StableDiffusion/comments/1mngtnn/the_4step_lightening_lora_for_qwen_image_is/)
  - æä¾›äº†workflowç¤ºä¾‹

- Can anyone confirm the workflow is working? It's not for me, it's like the lora is ignored and i get a lot of `lora key not loaded:` errors.
  - I have tried unsucessfully with both the city96 qwen-image-gguf and the distilled version from DiffSynth-Studio.
- Did you update the comfyui to the nightly one?
  - I just updated to nightly and it is working fine now ! Thanks !

- The text encoder still takes a minute to load whenever i change the prompt
  - Try this https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/tree/main
  - I'm on Q3 RTX-3060 12gb, 4 steps, 1328x1328, total render time even changing prompts: Prompt executed in 49.27 seconds
  - If I've posted it is because I'm using it. It's the one recommended by city96, the fp8 clip is also very slow on my computer.

- Can anyone explain how these lightning loras work?
  - Use it like a regular lora (add a lora node) set cfg to one, steps to four, and don't bother with a negative prompt. In exchange for a small quality decrease, that might not be noticeable, you get a massive speedup. 
  - As of now, this LORA is absolutely amazing, I'm generating 1920 x 1074 images on my laptop with 8GB VRAM in under a minute, and it's getting more prompt adherence than I've ever seen even from ChatGPT.

- ## [Comfyorg upload Qwen-Image models bf16 and fp 8 : r/comfyui _202508](https://www.reddit.com/r/comfyui/comments/1mhzi3b/comfyorg_upload_qwenimage_models_bf16_and_fp_8/)
- VRAM usage reference - Tested with RTX 4090D 24GB
- Model Version: Qwen-Image_fp8
  - VRAM: 86%
  - Generation time: 94s for the first time, 71s for the second time
- Model Version: Qwen-Image_bf16
  - VRAM: 96%
  - Generation time: 295s for the first time, 131s for the second time

- ## [Qwen image 20B is coming : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mhf0kl/qwen_image_20b_is_coming/)
- ComfyUI is just a simple visual programming language with custom node support. 
  - If you want a simpler approach use some UI for it like https://github.com/mcmonkeyprojects/SwarmUI and if you need more control no need to pack everything into a workflow just use Krita AI or sth...

# discuss-flux
- flux-guff
  - [Lists of FLUX.1 Series Models : r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/1h3fcs8/lists_of_flux1_series_models/)
  - https://huggingface.co/city96/FLUX.1-schnell-gguf/tree/main
  - https://huggingface.co/city96/FLUX.1-dev-gguf/tree/main
  - https://hf-mirror.com/city96/FLUX.1-schnell-gguf/tree/main
  - https://hf-mirror.com/city96/FLUX.1-dev-gguf/tree/main
  - https://huggingface.co/TencentARC/flux-mini
  - ğŸŒ° [How to Run Flux Dev GGUF Models in ComfyUI (Low VRAM Guide) - Next Diffusion _202506](https://www.nextdiffusion.ai/tutorials/how-to-run-flux-dev-gguf-in-comfyui-low-vram-guide)
  - [å¦‚ä½•ç”¨ComfyUIè¿è¡ŒFLUX GGUFæ–‡ä»¶æ¨¡å‹_æ…•è¯¾æ‰‹è®°](https://www.imooc.com/article/371309)
  - ç‰ˆæœ¬æ¯”è¾ƒåŠç¤ºä¾‹ [Flux Examples | ComfyUI_examples](https://comfyanonymous.github.io/ComfyUI_examples/flux/)

- ## 

- ## 

- ## [What's the fastest flux model right now? : r/comfyui _202409](https://www.reddit.com/r/comfyui/comments/1fryi73/whats_the_fastest_flux_model_right_now/)
- On my 3090, fp8 e5m2 with the --fast command line switch is approx 50% faster than fp16, gguf, or fp8 e4m3.
  - I thought --fast was only useful for 40xx cards! TestigÃ±ng right away with my 3070

- Gguf models are the fastest after nf4

- ## [Fast Flux.1 Dev (in 8 steps!) - Turbo Alpha: First Impressions and Testing Results : r/StableDiffusion _202410](https://www.reddit.com/r/StableDiffusion/comments/1g26pim/fast_flux1_dev_in_8_steps_turbo_alpha_first/)
  - I spent my Saturday morning testing out the newly released Flux.1 Dev Turbo Alpha.
  - Quality vs. Speed Trade-off: It maintains most of the quality of Flux Dev, but there is an obvious decline in fine details (generations are done in 8 steps). You will notice details are off (e.g., a waterfall in an unexpected spot or a crooked pole).

- For those who don't know, these are model distillations.
  - A model is effectively retrained to do the same task in less steps and attempt to meet the same quality as the original. Distillations in general attempt to get 90%+ of the original quality in less steps.

- ## [Testing Flux. Dev vs HiDream. Fast â€“ Image Comparison : r/StableDiffusion _202506](https://www.reddit.com/r/StableDiffusion/comments/1l1dn1e/testing_fluxdev_vs_hidreamfast_image_comparison/)
- i use FLUX.1-Turbo-Alpha by alimama-creative. It gives great images in just 8 steps with euler_ancestral, Sgm_Uniform & cfg at 1.

- ## [Why is Flux "schnell" so much slower than SDXL? : r/StableDiffusion _202502](https://www.reddit.com/r/StableDiffusion/comments/1itw0j4/why_is_flux_schnell_so_much_slower_than_sdxl/)
- Schnell isn't a small model - it's the same as Dev model, and it just uses fewer steps. Why would you compare it to SDXL? 

- Use fp8 versions of Flux Schnell based models and 4 steps only, CFG1, flux guidance 3.5

- SDXL is an LDM architecture and Flux is a DiT architecture, which requires more computing resources.

- ## [Is Flux GGUF supposed to be faster or its just memory savings? : r/StableDiffusion _202408](https://www.reddit.com/r/StableDiffusion/comments/1evktoz/is_flux_gguf_supposed_to_be_faster_or_its_just/)
- It is actually slower at the moment, because it once dequants and calculate. to make faster, we need more optimized calculation.

- The current implementation of GGUF doesn't yet fully utilize the advantages of GGUF's structure. It's partially borrowing techniques used in LLMs, but it still needs to evolve further.

- ## [m4 mac miniæœ¬åœ°éƒ¨ç½²ComfyUI, æµ‹è¯•Flux-dev-GGUFçš„workflowæ¨¡å‹10æ­¥å‡ºå›¾, æµ‹è¯•AIç»˜å›¾æ€§èƒ½, åŸºäºMPS(fp16), ä¼˜ç‚¹æ˜¯èƒ½è€—å°å’Œé™éŸ³ - åˆ˜æ‚¦çš„æŠ€æœ¯åšå®¢ - åšå®¢å›­ _202505](https://www.cnblogs.com/v3ucn/p/18593990)
- [How to Use FLUX GGUF Files in ComfyUI _202505](https://promptingpixels.com/tutorial/flux-gguf)

- æœ€åæ˜¯10æ­¥è¿­ä»£çš„å‡ºå›¾æ•ˆæœï¼Œå¯ä»¥çœ‹åˆ°ï¼Œç²¾åº¦æ²¡æœ‰ä¸‹é™å¤ªå¤šï¼Œä¸»è¦é—®é¢˜è¿˜æ˜¯å‡ºå›¾é€Ÿåº¦å¤ªæ…¢

- ## [Flux.1 GGUFæ¨¡å‹è¯´æ˜åŠä½¿ç”¨](https://www.zhihu.com/tardis/zm/art/16474690938)

- GGUF ç‰ˆæœ¬ æ˜¯ Flux æ¨¡å‹çš„ä¸€ç§ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä¸“é—¨ä¸ºä½æ˜¾å­˜è®¾å¤‡è®¾è®¡
- GGUF æ˜¯ GPT-Generated Unified Format çš„ç¼©å†™ï¼Œæ˜¯ä¸€ç§é«˜æ•ˆçš„æ¨¡å‹å­˜å‚¨å’Œäº¤æ¢æ ¼å¼ã€‚å®ƒé€šè¿‡é‡åŒ–æŠ€æœ¯ï¼ˆå¦‚ 4 ä½ã€6 ä½ã€8 ä½ç­‰ï¼‰å‹ç¼©æ¨¡å‹æƒé‡ï¼Œä»è€Œå‡å°‘æ˜¾å­˜å ç”¨ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„ç”Ÿæˆè´¨é‡ã€‚
  - é‡åŒ–åŸç†ï¼šé‡åŒ–é€šè¿‡å‡å°‘æ¨¡å‹æƒé‡çš„ç²¾åº¦ï¼ˆå¦‚ä» 32 ä½æµ®ç‚¹æ•°å‹ç¼©åˆ° 4 ä½ï¼‰ï¼Œé™ä½æ˜¾å­˜éœ€æ±‚ï¼Œä½†å¯èƒ½ä¼šç•¥å¾®å½±å“ç”Ÿæˆè´¨é‡ã€‚
  - ä¼˜åŠ¿ï¼šGGUF ç‰ˆæœ¬å¯ä»¥åœ¨ä½æ˜¾å­˜è®¾å¤‡ï¼ˆå¦‚ 6GB æ˜¾å­˜ï¼‰ä¸Šè¿è¡Œï¼Œé€‚åˆæ²¡æœ‰é«˜ç«¯æ˜¾å¡çš„ç”¨æˆ·ã€‚
- åŸå§‹ç‰ˆæœ¬ï¼ˆå¦‚ Flux.1 Dev å’Œ Schnellï¼‰å¯¹æ˜¾å­˜è¦æ±‚è¾ƒé«˜ï¼Œé€šå¸¸éœ€è¦ 16GB æˆ–æ›´å¤šçš„æ˜¾å­˜æ‰èƒ½æµç•…è¿è¡Œã€‚
  - Q4 å’Œ Q5ï¼šé€‚åˆå¤§å¤šæ•°ç”¨æˆ·ï¼Œæ˜¾å­˜éœ€æ±‚é€‚ä¸­ï¼Œç”Ÿæˆè´¨é‡è¾ƒå¥½ã€‚

```table
é‡åŒ–çº§åˆ«	      æ˜¾å­˜éœ€æ±‚	 ç”Ÿæˆè´¨é‡
Q2/Q3ï¼ˆ2/3 ä½ï¼‰	6GB	     è¾ƒä½
Q4ï¼ˆ4 ä½ï¼‰	    8GB	     ä¸­ç­‰
Q5ï¼ˆ5 ä½ï¼‰	    10GB	   è¾ƒé«˜
Q8ï¼ˆ8 ä½ï¼‰	    16GB+	   æ¥è¿‘åŸå§‹ç‰ˆæœ¬
```

- GGUF ç‰ˆæœ¬çš„ä¼˜åŠ¿ä¸å±€é™â€‹
- ä¼˜åŠ¿â€‹
  - ä½æ˜¾å­˜éœ€æ±‚ï¼šæœ€ä½ä»…éœ€ 6GB æ˜¾å­˜ï¼Œé€‚åˆè€æ˜¾å¡æˆ–ä½ç«¯è®¾å¤‡ã€‚
  - å¿«é€Ÿç”Ÿæˆï¼šé‡åŒ–åçš„æ¨¡å‹æ¨ç†é€Ÿåº¦æ›´å¿«ï¼Œé€‚åˆéœ€è¦å¿«é€Ÿå‡ºå›¾çš„åœºæ™¯ã€‚
  - é«˜è´¨é‡ç”Ÿæˆï¼šå°½ç®¡æ˜¯é‡åŒ–ç‰ˆæœ¬ï¼Œä½† Flux GGUF çš„ç”Ÿæˆè´¨é‡ä»ç„¶éå¸¸å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨ 8 æ­¥æˆ–æ›´å¤šæ­¥æ•°çš„æƒ…å†µä¸‹ã€‚
- å±€é™â€‹
  - ç”Ÿæˆè´¨é‡ç•¥ä½ï¼šä¸åŸå§‹ç‰ˆæœ¬ç›¸æ¯”ï¼ŒGGUF ç‰ˆæœ¬çš„ç”Ÿæˆè´¨é‡ç•¥æœ‰ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨ä½é‡åŒ–çº§åˆ«ï¼ˆå¦‚ Q2 æˆ– Q4ï¼‰æ—¶ã€‚
  - ä¸æ”¯æŒè´Ÿæç¤ºï¼šFlux GGUF ç‰ˆæœ¬ä¸æ”¯æŒè´Ÿæç¤ºåŠŸèƒ½ã€‚

- ## ğŸ“Œ [FLUX.1å…¥é—¨æ•™ç¨‹ï¼šæ¨¡å‹èµ„æºæ±‡æ€»ä¸è¯¦ç»†è¯´æ˜ - çŸ¥ä¹ _202503](https://zhuanlan.zhihu.com/p/10106104364)
- ä¸€èˆ¬æƒ…å†µï¼šå®Œæ•´ç‰ˆï¼ˆfp16ï¼‰éœ€è¦ 24G æ˜¾å­˜æ‰èƒ½æ­£å¸¸é©¾é©­ï¼Œé˜‰å‰²ç‰ˆï¼ˆfp8ï¼‰16G å°±è¶³å¤Ÿï¼Œnf4 ç‰ˆæœ¬ 8-12G æ˜¾å­˜å¯æ­£å¸¸é©¾é©­ï¼Œ
  - è€Œ gguf æ ¼å¼é‡åŒ–çš„å¦‚æœ€å°çš„ Q2 ç‰ˆæœ¬ 6G æ˜¾å­˜ä¹Ÿèƒ½å¤Ÿæ­£å¸¸é©¾é©­ï¼Œè€Œä¸”ç”±äº gguf è¿‘æœŸå±•ç°å‡ºå¼ºåŠ²çš„æŠ€æœ¯å‘å±•ï¼Œå……åˆ†ä½“ç°é™ä½å†…å­˜éœ€æ±‚è€Œè´¨é‡æ›´å¥½çš„ç‰¹ç‚¹ï¼Œé»‘æš—æ£®æ—å®˜æ–¹å¼€å§‹å…¨é¢æ”¯æŒï¼Œæ‰€ä»¥ nf4 çš„ç‰ˆæœ¬å°†é€æ¸æ·˜æ±°ã€‚

# discuss-comfyui/sd
- image-gen-xp
  - æ¨¡å‹å‚æ•°: clip/text-encoder, vae, sampler, model
  - æµ‹è¯•æ¨¡å‹: sd-v1.5(+ hyper-lora), sdxl-lightning, flux

- resources
  - [ã€SD + ComfyUIã€‘åˆé›† - çŸ¥ä¹](https://zhuanlan.zhihu.com/c_1625633809227010048)
  - [Flux.1 ComfyUI å¯¹åº”æ¨¡å‹å®‰è£…åŠæ•™ç¨‹æŒ‡å— | ComfyUI Wiki](https://comfyui-wiki.com/zh/tutorial/advanced/image/flux/flux-1-dev-t2i)

- stable-diffusion
  - https://modelscope.cn/models/ByteDance/SDXL-Lightning/files
  - https://huggingface.co/ByteDance/SDXL-Lightning
  - [ComfyUIå¹³å°ä¸‹åº”ç”¨å­—èŠ‚SDXL-Lightning æ¨¡å‹ - è€Eçš„åšå®¢ _202405](https://appscross.com/blog/using-sdxl-lightning-model-under-comfyui-platform.html)
  - [Hyper-SD Â· æ¨¡å‹åº“](https://modelscope.cn/models/ByteDance/Hyper-SD/summary)

- ## 

- ## [I hope comfyui will support sdxs soon Â· Issue Â· comfyanonymous/ComfyUI _202403](https://github.com/comfyanonymous/ComfyUI/issues/3147)
  - This model can achieve a speed of 100FPS on a single GPU
  - https://github.com/IDKiro/sdxs

- I wrapped up all of this thread into something a bit easier to understand, with some other features just for fun:  https://openart.ai/workflows/-/-/fUxFDJrPkuSshjFyTl7F

- Just a quick word of feedback for others interested in this model:
  - It is fast, but it is also really baked-in. Even in a huge batch generation, different images are only minimally different. To be honest it feels more like a db of pre-generated images being retrieved than a full generative model.
  - Negative prompts as for most of these fast models do not work. This can very much limit the kind of generations
  - The direct generation of images which are not exactly 512x512 will just not work.

- ğŸš€ [SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions : r/StableDiffusion _202403](https://www.reddit.com/r/StableDiffusion/comments/1bo2aj6/sdxs_realtime_onestep_latent_diffusion_models/)
- There is a pre-release version (v0.9) available. At the moment it's only for Diffusers based Stable Diffusion installs.

- ## ğŸ“¡ [Current State of Text-To-Image models : r/StableDiffusion _202503](https://www.reddit.com/r/StableDiffusion/comments/1jnt4ch/current_state_of_texttoimage_models/)
- Flux is really good, pony is really good for *cough* nsfw stuff, SDXL shines in photrealism with the plethora of LORAs to give your stuff a unique feel.

- ## [Gguf speed question : r/comfyui _202409](https://www.reddit.com/r/comfyui/comments/1fguxif/gguf_speed_question/)
- Gguf is slower but smaller, use the Q4 gguf if you are trying to add a ton of overhead like stacked loras and controlnet.

- [Should GGUF will be faster than safetensors? : r/comfyui _202503](https://www.reddit.com/r/comfyui/comments/1j52vsy/should_gguf_will_be_faster_than_safetensors/)
- No, oftentimes GGUF is slower even. The main benefit of these quantized models (same as with LLMs) is it allows you to run a larger "better" model on lower end hardware at the cost of precision and time depending on which type of quantization you use
- GGUF is often slower unless the overhead of swapping stuff in and out of VRAM (for the non-GGUF model) outweighs the overhead of having to dequantize everything before actually performing calculations. If you can compile the GGUF model, that actually seems to make a huge difference. 
- Gguf save vram not speed, theyâ€™re technically slower as gguf is compression

- I've found GGUF is to save VRAM, not necessarily to speed up generations, though it can help to fit more operations on your cards at once. 

- It would actually use slightly more (but you're right about higher quality). GGUF quants are block-based, so they have a small header at the start of every chunk. FP8 is basically just casting to an 8-bit type and doesn't contain meta-information.

- 
- 
- 
- 

- ## [SD-Turbo is out : r/StableDiffusion _202312](https://www.reddit.com/r/StableDiffusion/comments/189zvcg/sdturbo_is_out/)
  - They actually seem to have released SD-turbo at the same time as SDXL-turbo.
  - It does not allow for commercial use. But DOES allow for non-commercial redistribution. 
  - Its like SDXL-turbo. But with 1/4 the memory, 1/4 the rendering time.... and 1/4 the variety of subject matter.

- ## [why is SD1.5 still so popular and so many new models come on civit? : r/StableDiffusion _202501](https://www.reddit.com/r/StableDiffusion/comments/1hzwkvl/why_is_sd15_still_so_popular_and_so_many_new/)
- Because of the lower cost of resources and training, works that pursue an artistic style rather than a "real photo" can and will be well represented by LORA.

- Fine-tuning with SD1.5 is efficient due to its lower parameter count and resolution, resulting in faster training and less burden for large-scale tasks. SD1.5 requires no compromise on accuracy. With advanced tools available today, fine-tuning is more efficient than ever. 

- ## [What is currently the fastest, low-resolution way to generate images on an M1 Mac? : r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/1heqec0/what_is_currently_the_fastest_lowresolution_way/)
- For 256x256, use miniSD , and I think you can use a sd1.5 LCM LoRA to make it very fast.
  - [justinpinkney/miniSD Â· Hugging Face](https://huggingface.co/justinpinkney/miniSD)

- ## [Current state of 1.5 vs SDXL? : r/StableDiffusion _202401](https://www.reddit.com/r/StableDiffusion/comments/19df545/current_state_of_15_vs_sdxl/)
- What's nice about SDXL:
  - prompt styling - you barely need to switch models or add loras, like you can, but usually the base models like juggernaut can just adapt to way more styles via prompts that they are already pretty solid.
  - larger images
- Whats nice about 1.5:
  - better NSFW support
  - better skin textures via custom models
  - easier to train
  - lower memory footprint
  - backlog of tools
  - contronet works better

- SD1.5 is better in the following ways:
  - Lower hardware requirement
  - Hardcore NSFW
  - "SD1.5 style" Anime (a kind of "hyperrealistic" look that is hard to describe). But some say AnimagineXL is very good. There is also Lykon's AAM XL (Anime Mix)
  - Asian Waifu
  - Simple portraiture of people (SD1.5 are overtrained for these type of images, hence better in terms of "realism")
  - Better ControlNet support?

- [SDXL vs. SD 1.5: A Deep Dive into Image Generation AI Performance _202308](https://sandner.art/sdxl-vs-sd-15-a-deep-dive-into-image-generation-ai-performance/)

- ## [comfyuiçŸ¥è¯†ç‚¹ç®€è®°](https://oq6szavaxui.feishu.cn/wiki/JoDRwnWz6iOi3SkYMxwcDycInkh)
- 
- 
- 

- ## [ComfyUI_æå‡å›¾ç‰‡ç”Ÿæˆé€Ÿåº¦ - çŸ¥ä¹ _202408](https://zhuanlan.zhihu.com/p/695820264)
- Turbo Lora åªèƒ½ç”¨äº SDXL æ¨¡å‹ï¼Œå®ƒçš„æ•ˆæœå…¶å®å¹¶ä¸å¥½ï¼Œæˆ‘ç”¨åœ¨å‡ºè§’è‰²å›¾æ—¶ï¼Œåå›¾ç‡æé«˜ï¼Œä¼šä¸¥é‡æ‹‰é•¿ä¸»ä½“æˆ–è‚¢ä½“æ··ä¹±ã€‚
  - Turbo Lora å¯¹é‡‡æ ·å™¨æ²¡æœ‰å›ºå®šè¦æ±‚ï¼ŒScheduler æ¨è sgm_uniformï¼Œä½†æˆ‘ç”¨å…¶å®ƒçš„ä¹Ÿæ²¡è§æ˜æ˜¾è´Ÿé¢å½±å“ã€‚

- Lightning çš„æ¨¡å‹è¶Šæ¥è¶Šå¤šï¼Œå…¶å‡ºå›¾å“è´¨æ•´ä½“ä¸Šè¦ä¼˜äº Turbo å’Œ LCMï¼Œå¯¹äºæ²¡æœ‰é‡‡ç”¨ Lightning æŠ€æœ¯çš„ SDXL æ¨¡å‹æ¥è¯´ï¼ŒåŠ ä¸ª Lightning Lora ä¹Ÿèƒ½é™ä½æ­¥æ•°äº§å‡ºä¸é”™çš„å›¾ã€‚
  - é‡‡æ ·å™¨å»ºè®® eulerï¼ŒScheduler éœ€ä½¿ç”¨ sgm_uniformã€‚ç¤ºä¾‹ä¸­æˆ‘ä½¿ç”¨çš„æ˜¯ 8 Steps Lora, å…¶æ•´ä½“æ•ˆæœå·²ç»è¶³å¤Ÿå¥½ï¼Œå¾ˆæ¥è¿‘åŸæ¨¡å‹ç›´å‡ºå›¾äº†ï¼Œå¹¶ä¸”åœ¨è§’è‰²å›¾å½¢ä¸­è¡¨ç°è¦è¿œä¼˜äº Turboã€‚
  - ä¸‹è½½é¡µä¸Šæœ‰ Lora å’Œ Unet æ¨¡å‹ï¼ŒUnet å–è‡ª StabilityAI çš„å®˜æ–¹ SDXL æ¨¡å‹ï¼Œå¹³æ—¶å¯èƒ½ç”¨çš„å¹¶ä¸å¤šï¼Œå»ºè®®ä¸‹è½½ä½¿ç”¨ sdxl_lightning_8step_lora å’Œ sdxl_lightning_4step_loraã€‚

- ç›®å‰ Hyper Lora æœ‰ SD1.5ã€SDXLã€SD3ã€Flux1 çš„ï¼Œå®ƒæœ‰å¤šç§æ­¥æ•°çš„ Lora å¯é€‰ï¼Œæ­¥æ•°è¶Šä½æ•ˆæœç›¸åº”çš„å·®ä¸€äº›ï¼Œ8 å’Œ 12 æ­¥çš„æ•ˆæœéå¸¸ä¸é”™ã€‚
  - é‡‡æ ·å™¨å»ºè®® ddimã€euler aã€eulerï¼ŒScheduler éœ€ä½¿ç”¨ sgm_uniformã€‚

- DMD2_SDXL_4step_lora ä»…ç”¨äº SDXL æ¨¡å‹ï¼Œé‡‡æ ·å™¨æ¨è LCM Karrasï¼Œæ­¥æ•° 4-8ï¼ŒCFG 1ã€‚
  - DMD2 æœ‰ fp32 å’Œ fp16 ä¸¤ç§æ¨¡å‹ï¼Œfp16 é€Ÿåº¦è¦æ¯” fp32 å¿«ä¸å°‘ï¼Œä¸¤è€…ç”Ÿæˆçš„å›¾ç‰‡è´¨é‡å¹¶æ— å¯è§å·®å¼‚ã€‚
  - Lightning åŠ Hyper å‡ºå›¾é€Ÿåº¦è¦æ¯” DMD2 æ…¢ä¸€äº›ï¼Œä½†è´¨é‡æ˜æ˜¾æ›´ä¼˜ï¼Œæ„å›¾è¾ƒåŸå›¾æ›´æ¥è¿‘ï¼Œç»†èŠ‚ä¼˜äºåŸå›¾ã€‚

- ä»¥ä¸Šæ˜¯ä½¿ç”¨ Lora çš„æ–¹æ³•æ¥æå‡å‡ºå›¾é€Ÿåº¦ï¼ŒLora ä½¿ç”¨æ–¹æ³•ä¸å¸¸è§„çš„ä¸€æ ·ï¼Œæƒé‡è®¾ç½®ä¸º 1 å°±å¥½ã€‚

- ## [å­—èŠ‚åˆæ•´æ´»ï¼Œæ–°å‹æ¡†æ¶ Hyper-SDï¼Œæ¯” SDXL-Lightning æ›´ä¼˜ç§€ï¼ - çŸ¥ä¹ _202404](https://zhuanlan.zhihu.com/p/694590649)
- Hyper-SD ä¸ä»…æ”¯æŒå¯¹ SDXL å¤§æ¨¡å‹çš„åŠ é€Ÿï¼Œè¿™æ¬¡è¿˜å¢åŠ äº†å¯¹ SD1.5 å¤§æ¨¡å‹çš„åŠ é€Ÿæ”¯æŒã€‚

- ## [What is the difference between lighting versions on checkpoints? : r/StableDiffusion _202412](https://www.reddit.com/r/StableDiffusion/comments/1hpevjb/what_is_the_difference_between_lighting_versions/)
- Lightning models take only about 5 steps to converge, so they are much faster. Make sure to set the CFG lower for Lightning models. It should be around 2, rather than the 7 or so for a regular model. From what I can tell, Lightning models don't seem to take much notice of negative prompts, which is a disadvantage.
  - UPDATE: Lightning models also work best with a different sampler schedule, such as SGM Uniform or Turbo. Some samplers work better than others.

- I recommend trying DMD2 checkpoints instead of lighting ones. Or just manually download the dmd2 lora and generate at 4-12 steps with CFG 1-1.5 using LCM scheduler.

- ## [Are there any fast, lightweight models? : r/StableDiffusion _202501](https://www.reddit.com/r/StableDiffusion/comments/1i6k86e/are_there_any_fast_lightweight_models/)
- Any SDXL model with DMD2 lora. It's newer and as fast as LCM, Turbo and Lighting but with better prompt adherence (according to DMD2 paper).

- [I work a lot with FLUX, but DMD2 keeps amazing me (12 examples) : r/StableDiffusion _202408](https://www.reddit.com/r/StableDiffusion/comments/1f0znay/i_work_a_lot_with_flux_but_dmd2_keeps_amazing_me/)
  - This is DMD2 for SDXL, something like 18 images/minute on a simple L4 Colab. I do 0.4 GS for 8 Steps (default is 0/4). It's NOT as coherent as FLUX of course, but it hits differently, right?
  - DMD2 is just an optimiser, like Lightning, Turbo or LCM. Your post does not specify what checkpoint you're using.
  - DMD2 is a LORA, but also a model based on PG2 or SDXL.

- ## [What are the currently known methods to speed up SDXL image generation in A1111/forge? (windows) : r/StableDiffusion _202502](https://www.reddit.com/r/StableDiffusion/comments/1ixuii6/what_are_the_currently_known_methods_to_speed_up/)
- For me the dmd2 lora is the best method right now
  - I personally use with 10 steps, 1 cfg and lcm karras
  - https://huggingface.co/tianweiy/DMD2
- The lora is to keep the image quality in lower steps. Low steps=faster generation There's dmd2, lcm and lightning loras for that purpose but I personally think dmd2 gives better results.

- Turbo loras etc

- A lcm LoRA will Bring SDXL down to 6 steps

- ## ğŸ†š [What's the Difference Between SDXL LCM, Hyper, Lightning, and Turbo? : r/StableDiffusion _202506](https://www.reddit.com/r/StableDiffusion/comments/1lk1anq/whats_the_difference_between_sdxl_lcm_hyper/)
- They are all distillation techniques that trade some quality and variability for increased generation speed.
- CFG: for some distill techniques, CFG needs to be very low, typically below 2, so negative prompting mostly doesn't work (unless you can raise CFG by other means, like APG). The main tradeoff is that CFG at exactly 1 typically doubles the generation speed, _and_ they usually work great at very low step counts. Some techniques like Hyper and PCM are available both as small CFG and normal CFG versions: the normal ones behave more like usual models, with CFG reduced a bit.
- Samplers: LCM, PCM "lcmlike" and DMD2 are "small CFG" distills that work specifically with the LCM sampler (although sometimes you can have successful gens with other samplers, like Euler). TCD is designed to work with the TCD distill, although it can work very well with other small CFG techniques; DDIM and Euler also tend to go well with those. Oh, and to make matters more confusing, note that "LCM" and "TCD" can mean the samplers, and/or the techniques/LoRAs.
- the original SDXL-Turbo generates at only 1-2 steps, but at a lower resolution (512x512), and AFAIK there are no official LoRAs available. There are some unofficial around, and many other models called "Turbo"
  - it'd fall into the "small CFG" variety.

- Small CFG: Lightning, Hyper, TCD, PCM, Turbo: low CFG with DDIM, TCD, Euler, or other samplers.
- Small CFG, LCM-like: LCM original, PCM-lcmlike, DMD2: low CFG with the LCM sampler.
- Normal CFG: Hyper, PCM; sampler likely depends on the model.

- ## [Lightning/DMD2/PCM equivalents for Flux? : r/StableDiffusion _202505](https://www.reddit.com/r/StableDiffusion/comments/1khkdvi/lightningdmd2pcm_equivalents_for_flux/)
- My favourite is Flux 1 Turbo Alpha, a LoRA that allows you to drop your steps to 8-12.
  - There's also the Hyper LoRA, which works similarly but it's double the size.

- That's what the Schnell version of the Flux model is.

- ğŸ†š [Compare 1 step real time generations between SDXL Turbo, Lightning and Hyper : r/StableDiffusion _202405](https://www.reddit.com/r/StableDiffusion/comments/1cbzwqh/compare_1_step_real_time_generations_between_sdxl/)
- Why is Lighting sd xl is really great. almost no quality loss and all superfast 1.5 model are garbage in comparison with full 1.5 or am I doing something wrong? is there a way to use Hyper in 1.5 with 1-6 steps and get good quality?
  - You need the 1.5 hyper lora models

- ## ğŸ†š [Lightning vs Turbo models?? : r/StableDiffusion _202404](https://www.reddit.com/r/StableDiffusion/comments/1bsg1cb/lightning_vs_turbo_models/)
- Turbo diffuses the image in one step, while Lightning diffuses the image in 2 - 8 steps usually (for comparison, standard SDXL models usually take 20 - 40 steps to diffuse the image completely).
  - I'm not fully sold on the idea of diffusing the image in just one step, since stable diffusion was originally intended to be a multi-step process.
- SDXL Turbo is optimized for 512Ã—512 image resolution, while Lightning supports 1024Ã—1024 full SDXL resolution (and also all the various resolutions SDXL was trained on). 
  - Also, SDXL Turbo doesn't support CFG scale or negative prompts

- Lightning works well with ControlNet.
  - Turbo, not so much since it generates in one step, and multiple steps are required for fine ControlNet influence control.
  - Turbo and all the derived models are limited by Stability AI's Non-Commercial license.
  - ğŸ’° Lightning models have a proper free and open-source license, and commercial use is allowed, no question asked, no registration required.

- turbo model works with 1 step and gives goodish results, there are 4 lightning models, for 1, 2, 4 and 8 steps, fewer steps are faster but more steps gives better quality, the 2 steps one is the fastest giving a passable quality, in line with turbo I would say, so the turbo model is the fastest as it only needs one step. 
  - The 1 step lightning exists but it's experimental and don't work too well. Sdxl turbo is optimized for 512x512, so speed is even faster, in can do 1024x1024, but quality drops. Lightning is optimized for 1024x1024.

- LCM increases the inference speed of older stable diffusion models (SD 1.5, SD 2.0) by reducing the number of steps needed to diffuse the image (basically the same as Lightning but for older models). However, Lightning is made to work with SDXL models and it's faster than LCM. While there's an LCM for SDXL, using SDXL Lightning is recommended because it's faster than LCM SDXL.

- ## [SDXL Turbo, SDXL Lightning, Cascade and SD3 : r/StableDiffusion _202402](https://www.reddit.com/r/StableDiffusion/comments/1b23p3l/sdxl_turbo_sdxl_lightning_cascade_and_sd3/)
- Neither Turbo nor Lightning are â€œimproved.â€ The whole point of using them is the speed, not quality. 

- Turbo needs as few as just 1 step, suitable for real-time applications. 
  - As for Lightning, it still retains a pretty good quality at 8 steps, as opposed to LCM which had noticeable quality loss. Lightning is good enough that I switch to using it for normal uses now.
- As for Cascade, I find SDXLs refined models to give better looking outputs but Stable Cascades ability to write all kind of random text pretty accurately is really impressive... 

- ## ğŸ†š [Hyper, Turbo, Lightning, Cascade, Pony, Pix Art and more - Whats the differences? : r/StableDiffusion _202405](https://www.reddit.com/r/StableDiffusion/comments/1cy9lcm/hyper_turbo_lightning_cascade_pony_pix_art_and/)
- Hyper, Lightning, Turbo = special checkpoints that only need 1 to 8 steps to make an image and work better with really low CFG of 1 to 2 (though Hyper can now work at 5-8 CFG). 

- Turbo: meant for 512x512 image size. Faster than lightning.
- Lightning: a fast model with good image quality. I prefer it over sdxl. Pretty popular in the community.
- Hyper: similar to lightning. Creator claims that it outperforms lightning in image aesthetics but so far I have been disappointed. 
- Cascade: almost as slow as sdxl on my poor old system. Usually creates pretty good images. A bit surprisingly not that popular in the community.
- Pony: I think it was trained with booru images and image tags so it is able to produce anime/hentai images with great prompt adherence.

- ## ğŸ§© [Difference between checkpoint , lora, model... : r/StableDiffusion _202308](https://www.reddit.com/r/StableDiffusion/comments/15jfsdt/difference_between_checkpoint_lora_model/)
- Technically they're all machine learning models, but checkpoints are usually referred to just as models. 
  - All models are static, meaning they only know what they were trained on. In order for them to learn something new, they have to be (re)trained again, also known as finetuning.
- Their differences in very rough terms:
  - Checkpoints are the big models that make images on their own.
  - Loras and all their variations like Lycoris are "mini models" that plug into a checkpoint and alter their outputs. They let checkpoints make styles, characters and concepts that the base checkpoint they're used on doesn't know or didn't know very well.
  - Hypernetworks are an older and not as good implementation of the same paper and same concept as Loras.
  - Textual Inversions are sort of bookmarks or compilations of what a model already knows, they don't necessarily teach something new but rearrange stuff that a model already knows in a way that it didn't know how to arrange by itself.
- SD 1.x, SD 2.x and SDXL are both different base checkpoints and also different model architectures
  -  SD 1.5 is say both the NES itself but also one game for it. All SD 1.x based models are compatible with SD 1.x Loras and models for extensions like ControlNet. 
  -  SD 2. X is the SNES, it's a different architecture so 1.x models won't be compatible with it, same for SDXL if you say that's like the N64.

- ControlNet models are also machine learning models that inject into the Stable Diffusion process and control the denoising process, they're used with an image made by preprocessor. That image is be used to guide and control that denoising process, also hence the name.

- ## [Super fast image generation with LCM Sampler : r/comfyui _202311](https://www.reddit.com/r/comfyui/comments/17s3s6u/super_fast_image_generation_with_lcm_sampler/)
- FYI LCM sampler is no longer required, ksampler supports lcm (coming from the original creator of that node)
- THis POST is now outdated ! LCM has been integrated in ComfyUI and can be used with normal samplers. Just update ComfyUI and you will be able to choose it.

- i do not understand how to use it. Any examples of workflow?
  - There you go: https://app.flowt.ai/c/ilKpVL
  - you just plug in a Lora that uses the LCM-weights thing, then set your LCM settings in K-Sampler and off you go.

- ## [How well does ComfyUI perform on macOS with the M4 Max and 64GB RAM? : r/comfyui _202503](https://www.reddit.com/r/comfyui/comments/1jhifyi/how_well_does_comfyui_perform_on_macos_with_the/)
- TLDR - if you want to work linear on one image, a Mac is a huge waste of time. Maybe 25% of the speed of a decent NVIDIA PC for AI generation. 
  - However, if you know how or want to multitask, itâ€™s easily the best system you can purchase.

- for generative AI stuff, compared with my Linux PC with a RTX 4090 - mac m4 pales in comparison. 
  - We are talking minutes vs seconds here for a 1024x1024 Flux generation. Most of it is tuned for Nivida CUDA and that is the key. 
  - Apple Silicon offers great performance for everyday computing, but its support for machine learning frameworks like PyTorch sucks butt compared to CUDA.
  - i can drive Comfui remotely from the mac with the server running on the linux pc.

- AI image and video generation require cuda cores to function properly so Apple or even AMD gpus are not recommended.

- Not well. Most local AI is still tied to having CUDA. Honestly the biggest missing link is getting Pytorch to run accelerated on Apple silicon. That will speed up most of the tasks that do CPU fallback which is where you get your abysmal performance.

- Apple has been so far behind in AI. Best just go for PC with Nvidia gpu.

- M4 Max MBP with 128 GB memory here. Letâ€™s just say I never, ever run image gen on that machine. Even for models/pipelines that ARE supported, itâ€™s not worth the agonizing wait.

- my Mac M4 16gb is much more slower than my old PC with the good old Geforce 3060. An Apple a day keeps the Comfyui away.

- ## æŠŠComfyUIå·¥ä½œæµæ— ç¼è½¬ä¸ºMCP Toolçš„ä¸€æ¬¾å·¥å…·ï¼šPixelle-MCP
- https://x.com/aigclink/status/1955163038035914972
  - ä¹Ÿå°±æ˜¯è¯´ç”¨è‡ªç„¶è¯­è¨€å³å¯è®©LLMæ§åˆ¶ComfyUIæ‰§è¡Œå¤æ‚çš„ç”Ÿæˆä»»åŠ¡
  - è‡ªåŠ¨åŒ–å’Œæ‰¹é‡åŒ–ï¼ŒLLMå¯ä»¥è‡ªåŠ¨åŒ–æ‰§è¡Œé‡å¤æ€§çš„ä»»åŠ¡ï¼Œæ¯”å¦‚æ‰¹é‡ç”Ÿæˆä¸åŒé£æ ¼çš„å›¾åƒï¼Œæˆ–è€…æ ¹æ®é¢„è®¾çš„è§„åˆ™è‡ªåŠ¨è°ƒæ•´å‚æ•°

- ## ComfyUI ç°åœ¨æ˜¯ä¸æ˜¯æ²¡äººå­¦äº†ã€‚æ—¶é—´çº¿ä¸Šçš„ ComfyUI å¤§ä½¬å¥½åƒéƒ½ä¸å‘æ–°æ•™ç¨‹äº† _202508
- https://x.com/hylarucoder/status/1954857846987968542
- ç°åœ¨å¾ˆå¤šé€šç”¨ç”Ÿå›¾æ¨¡å‹çš„èƒ½åŠ›å·²ç»è¶³å¤Ÿè¦†ç›–å¾ˆå¤šå·¥ä½œæµäº†ï¼Œè€Œä¸”comfyUIé“ºå¤©ç›–åœ°çš„é…ç½®ï¼Œçœ‹çš„è®©äººå¤´ç–¼ï¼Œæˆ‘ç°åœ¨åšçš„ YOOART IMAGE éƒ½æ˜¯æ¥å…¥çš„é€šç”¨æ¨¡å‹ï¼Œæ¯”å¦‚gpt-4o-imageã€å³æ¢¦ã€midjourneyã€qwen-imageã€imagen4 è¿™äº›ï¼Œæ™®é€šä»»åŠ¡éƒ½å¯ä»¥å®Œæˆï¼Œåªéœ€è¦è¾“å…¥æç¤ºè¯å°±å¥½
- æ˜¯ä¸æ˜¯å¯ä»¥è¿™ä¹ˆç†è§£ï¼Œåªè¦ä¸æ˜¯å¾ˆä¸“ç²¾çš„æˆ–è€…éœ€è¦æ‰¹é‡ç”Ÿæˆçš„å›¾ç‰‡ï¼ŒåŸºæœ¬å°±å‘Šåˆ« comfyui äº†ï¼Ÿ
  - comfyui åœ¨ä¸€äº›æ¯”è¾ƒå‚ç›´çš„é¢†åŸŸæ¯”å¦‚ç”µå•†çš„ç»†åˆ†åœºæ™¯ï¼Œéœ€è¦è‡ªå·±éƒ¨ç½²çš„è®­ç»ƒçš„æ¨¡å‹ï¼Œæ‰¹é‡åŒ–ä½œä¸šæ‰éœ€è¦å®ƒï¼Œå¯¹äºå¤§ä¼—æ™®é€šäººæ¥è¯´ï¼Œéšç€æ¨¡å‹èƒ½åŠ›è¶Šæ¥è¶Šå¼ºï¼Œé‚£ä¹ˆæç¤ºè¯å°±ä¼šæ›¿ä»£è¶Šæ¥è¶Šå¤šçš„å·¥ä½œæµï¼Œå…¸å‹çš„å°±æ˜¯ Chatgpt 4oã€flux-kontextã€å³æ¢¦ã€è¿™æ¥èƒ½ç”Ÿå›¾ï¼Œèƒ½æ”¹å›¾çš„èƒœä»»å¤§éƒ¨åˆ†ï¼Œæœªæ¥åº”è¯¥è¿˜æ˜¯ç”Ÿå›¾ Agent çš„å¤©ä¸‹å§ã€‚
  - åœ¨ä¸€äº›éœ€è¦å›ºå®šå·¥ä½œæµï¼Œå›ºå®šç»“æœçš„åœºæ™¯æˆ‘è§‰å¾—è¿˜æ˜¯éœ€è¦comfyuiï¼Œç±»ä¼¼ä¼ ç»Ÿçš„PSï¼ŒAE å§ï¼Œç°åœ¨æ™®é€šå¤§æ¨¡å‹ä¼šå æ®çš„åº”è¯¥æ˜¯ç±»ä¼¼ç¾å›¾ç§€ç§€ï¼Œå‰ªæ˜ è¿™ç§ç®€å•åŒ–çš„ç”Ÿæ€ä½ï¼Œä½†æ˜¯è¿™ç§ç”Ÿæ€ä½äººæ•°å¤šï¼Œä½†æ˜¯comfyuiçš„å•†ä¸šä»·å€¼ï¼Œå…¶å®è¿˜æ˜¯ä¸å°çš„ï¼Œä¸“ä¸šé‡æ´»å„¿è¿˜æ˜¯å¾—é å®ƒã€‚

- æ–°çš„å¼€æºæ¨¡å‹å¾ˆå¤§ï¼Œè¿è¡Œé—¨æ§›è¿‡é«˜ï¼Œè€Œä¸”æ•ˆæœæ²¡åŠæ³•æ‰“è¿‡å¥½çš„é—­æºæ¨¡å‹

- æˆ‘ä¸€ç›´è®¤ä¸ºå·¥ä½œæµæ˜¯è¿‡æ¸¡äº§ç‰©ï¼Œæ³¨å®šè¢«æ¨¡å‹å¹²æ‰

- æ„Ÿè§‰comfyUIå¾ˆéº»çƒ¦ï¼Œç§Ÿä¸ªäº‘ç”µè„‘é…ç½®åŠå¤©è¿˜ä¸å¦‚ç›´æ¥èŠ±é’±åœ¨å¹³å°ç”Ÿå›¾ ç”Ÿè§†é¢‘

- https://x.com/hylarucoder/status/1955106887407653234
  - æ±‡æ€»ä¸€ä¸‹å¤§å®¶çš„è§‚ç‚¹, æ—©æœŸ ComfyUI ç«æ˜¯å› ä¸ºç”Ÿå›¾å¿…é¡»ã€Œä¼šæ­å·¥ä½œæµã€
  - ç°åœ¨ é€šç”¨æ¨¡å‹å°±èƒ½è§£å†³ 80% çš„åœºæ™¯ã€‚æ­èŠ‚ç‚¹çš„è¾¹é™…ä»·å€¼ä¸‹é™ï¼Œè€Œ ComfyUI ç”Ÿæ€å•†ä¸šåŒ–åˆ†å‰è¿›ä¸€æ­¥æ‹‰å¤§äº†ç”¨æˆ·å’Œå¼€å‘è€…å­¦ä¹ å’Œç»´æŠ¤æˆæœ¬
  - è€Œå¼€æºä¸–ç•Œé‡Œé¢åˆæœ‰å¾ˆå¤šä¸€é”®åŒ–å¥—ä»¶ï¼ŒComfyUI è¿›ä¸€æ­¥æˆä¸ºäº†ç‰›å¤«äººã€‚ã€‚ã€‚ã€‚
# discuss-image
- ## 

- ## 

- ## 

- ## 

- ## æœ‰åˆ†æç§°ï¼ŒGPT-4o å›¾ç‰‡ç”Ÿæˆæ•ˆæœè¿™ä¹ˆå¥½æ˜¯å› ä¸ºé‡‡ç”¨äº†è‡ªå›å½’æ¨¡å‹ï¼ˆautoregressive modelï¼‰è€Œä¸å†æ˜¯æ‰©æ•£æ¨¡å‹ï¼ˆdiffusion modelï¼‰ã€‚
- https://x.com/jason2be/status/1905834259547361645
  - å¦‚æœçœŸæ˜¯è¿™æ ·çš„è¯ï¼Œé‚£æˆ‘å°†å†æ¬¡ç§° DeepSeek ä¸ºç¥ï¼Œå› ä»–ä»¬ä¸€æœˆä»½å¼€æºçš„ Janus-Pro å›¾åƒç”Ÿæˆæ¨¡å‹å°±æ˜¯è‡ªå›å½’æ¡†æ¶ï¼Œè™½ç„¶ç›®å‰æ•ˆæœè¿˜å¾…æå‡ï¼Œä½†æŠ€æœ¯è·¯çº¿é€‰æ‹©ä¸Šç›¸å½“äºåˆè¸©å¯¹äº†ä¸€æ¬¡ã€‚
- è¿™è·Ÿdeepseek æ²¡å•¥å…³ç³»ï¼ŒåŸºäºtransformerçš„å›¾åƒç”Ÿæˆ22å¹´23å¹´å°±å¼€å§‹æœ‰äº†ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªç ”ç©¶çƒ­ç‚¹ï¼Œå› ä¸ºå¤§å®¶éƒ½æƒ³è¦ä¸€ä¸ªç»Ÿä¸€çš„ç«¯åˆ°ç«¯æ¨¡å‹æ—¢èƒ½ç”Ÿæˆæ–‡å­—ä¹Ÿèƒ½ç”Ÿæˆå›¾åƒã€‚åˆ«è¯´å›¾åƒï¼Œopenai çš„sora ä¹Ÿæ˜¯åŸºäºtransformers
  - æ˜¯çš„ã€‚DeepSeek çš„ Janus-Pro åˆ›æ–°æ˜¯å°†è‡ªå›å½’ï¼ˆARï¼‰å’Œæ‰©æ•£æ¨¡å‹ç»“åˆï¼Œä½†è¿™ä¹Ÿæ˜¯æœ‰å…¶ä»–å›¢é˜Ÿåœ¨ç ”ç©¶çš„ã€‚DeepSeek é€‰ AR å’Œ GPT é€‰ Transformer ä¸€æ ·ï¼Œå…¶å®éƒ½æ˜¯è¡Œä¸šæŠ€æœ¯æ¼”è¿›çš„ä¸€éƒ¨åˆ†ã€‚è™½ç„¶æ˜¯ç‹¬ç«‹äº‹ä»¶ï¼Œä½†ä¹Ÿæš—ç¤ºä¸€äº›å¿…ç„¶ã€‚
- æ—©å°±æ˜¯ç ”ç©¶çƒ­ç‚¹äº†ï¼ŒåŒ…æ‹¬nips24çš„best paperå°±æ˜¯ç»™äº†è‡ªå›å½’çš„è§†é¢‘ç”Ÿæˆ

- æœºå™¨å­¦ä¹ å°±é‚£å‡ ç§æ¨¡å‹ï¼Œè®ºæ–‡éƒ½æ˜¯å…¬å¼€çš„ï¼Œä»€ä¹ˆå«è¸©å¯¹ä¸€æ¬¡ï¼Ÿ

- ## æ­ç§˜ä¸€ä¸‹ä¸€é”®å¤§èƒ¸æŠ€æœ¯åŸç†ï¼Œé¡ºä¾¿è¯·æ•™ä¸€ä¸‹æœ‰æ²¡æœ‰æ›´å¥½çš„è§£å†³æ–¹æ¡ˆ:
- https://twitter.com/moeimiku/status/1769285829586022574
  1. é€šè¿‡ SagmentAnyThing + Grounding-DINOè¯†åˆ«ä¹³æ²Ÿå’Œèƒ¸éƒ¨ï¼Œç„¶ååšå åŠ é®ç½©~  
  2. åˆ©ç”¨StableDiffutionå¯¹é®ç½©åŒºåŸŸåšåŸºäºåŸå›¾çš„é‡ç»˜
  - ç°åœ¨é‡åˆ°çš„é—®é¢˜æ˜¯ï¼Œçµæ•åº¦é»˜è®¤0.3çš„æƒ…å†µä¸‹ä¸èƒ½è¦†ç›–æ‰€æœ‰å›¾ç‰‡ï¼šæœ‰çš„å›¾ç‰‡è¯†åˆ«ä¸å‡ºæ¥ï¼Œéœ€è¦é™ä½åˆ°0.25. æœ‰çš„å›¾ç‰‡æŠŠå…¨èº«éƒ½è¯†åˆ«å‡ºæ¥äº†ï¼Œåˆ™éœ€è¦å¢åŠ çµæ•åº¦ã€‚
- å¯ä»¥åŠ ä¸ªé®ç½©å±‚åŒºåŸŸæŠ¹æ¶‚ç¼–è¾‘åŠŸèƒ½ï¼Œæ—¢èƒ½è§£å†³å‡†ç¡®æ€§çš„é—®é¢˜ï¼Œåˆä¼šå¤§å¤§å¢åŠ äº§å“çš„è¶£å‘³æ€§å’Œç²˜æ€§ã€‚
  - é è°±ï¼Œè¿˜æƒ³åšä¸€é”®ç©¿ä¸è¢œåŠŸèƒ½
- è¿™ä¸ªé€šè¿‡æ‹–æ‹½æ„å»ºç³»ç»Ÿçš„æ¡†æ¶æ˜¯ä»€ä¹ˆï¼Ÿ
  - comfyUI å…³æ³¨æˆ‘å›å¤´å‡ºæ•™ç¨‹

- ## [how to image generate locally? : r/ollama _202505](https://www.reddit.com/r/ollama/comments/1kjhul8/how_to_image_generate_locally/)
- r/StableDiffusion would be the place to ask. There are a couple different UIs like ComfyUI, Fooocus, Forge, and SwarmUI that can be used to do local generation.

- ## [How to generate text to image with ollama? Â· Issue Â· ollama/ollama-python _202409](https://github.com/ollama/ollama-python/issues/280)
- You need to know what's a LLM and how it work: llm stand for large language model, which basically mean it a model made to understand language, and can reply with similar language, understandable for human.
  - what you're trying to do is to have this llm generate an image, without being train on generating image but only generating text... So its impossible to have the model nativly provide you an image by its own.
  - If you're looking to have a chatbot capable of chatting and generating image, you must have two AI, one LLM and one made to generate image (like stable diffusion, etc), if you have a good computer, you can run both locally

# discuss-designer/builder
- ## 

- ## 

- ## âœ¨ we're bringing vision to Bolt.new _202502
- https://x.com/boltdotnew/status/1892620446106886396
  - Bolt now sees your app exactly like users do, enabling faster, more precise edits pixel by pixel (& fewer tokens!)
  - The Visual Inspector works across all web frameworks (React, Vue, Svelte, Next & others) and mobile apps (Expo).

- Does this send a screenshot with html2canvas so the app can see any layout issues and you donâ€™t need to take a screenshot? Or does it just send the raw html that you click on.
  - it's using https://github.com/qq15725/modern-screenshot to send images!

- I built this in 25 minutes using MCP tools
# discuss-ai-example ğŸŒ° 
- ## 

- ## 

- ## 

- ## Google Gemini çš„ StoryBook åŠŸèƒ½ç”¨äº† 20+ ä¸ª Agentsï¼Œå…¶ä¸­ 6 ä¸ªæ˜¯ä¸“é—¨ä¸ºæ•…äº‹ä¹¦åŠŸèƒ½è®¾è®¡çš„æ ¸å¿ƒ Agents
- https://x.com/iguangzhengli/status/1953770970680000637
  1. Writer - è´Ÿè´£åˆ›ä½œæ•…äº‹å†…å®¹
  2. Storyboarder - åˆ†é•œå’Œæ’å›¾è¯´æ˜
  3. NewStorybook - æ ¸å¿ƒ Agent
  4. IllustratorSingleCall - æ’ç”»å¯¼æ¼”
  5. Animator - åŠ¨ç”»å¯¼æ¼”
  6. Photos - Google Photos é›†æˆ
- StroyBook çš„ System Prompt å¾ˆå®¹æ˜“é€†å‘å‡ºæ¥ï¼Œä¼°è®¡ç°åœ¨æ ¹æœ¬æ²¡æœ‰è®¾é˜²(åé¢çœ‹åˆ°åä¼°è®¡é©¬ä¸Šè¦åŠ è¿‡æ»¤äº†)

- å„å›½æ–‡å­¦çš„å™äº‹é£æ ¼æœ‰å¾ˆå¤§å·®å¼‚ï¼Œä¸çŸ¥é“è¿™äº›Agentsæ˜¯å¦‚ä½•å®ç°çš„ã€‚
# discuss
- ## 

- ## 

- ## 

- ## 

- ## ç”¨å¯çµAI è®©ä¸“è¾‘å°é¢åŠ¨èµ·æ¥
- https://x.com/Nin19536/status/1804523356311765400
  - ç‰¹åˆ«å–œæ¬¢ Aimerï¼ï¼ å¥¹çš„ä¸“è¾‘å°é¢åŸºæœ¬éƒ½æ˜¯è¶…ç°å®é£æ ¼, ç‰¹åˆ«é€‚åˆ AI å›¾ç”Ÿè§†é¢‘
- æ„Ÿè§‰æ¡Œé¢ã€ç§»åŠ¨ç«¯æœ‰ä¸€ä¸ªåŠ¨æ€å£çº¸å°å·¥å…·çš„æœºä¼š

- ## è¿™å‡ å¤©åœ¨ç ”ç©¶Stable Diffusionã€‚éœ€æ±‚æ˜¯æŠŠå®¢æˆ·åšçš„è¡£æœç©¿åˆ°AIç”Ÿæˆçš„æ¨¡ç‰¹èº«ä¸Šã€‚
- https://twitter.com/felixding/status/1768511354208739692
  - å¦‚æœç”¨ä¸€å¼ è¡£æœçš„ç…§ç‰‡+ControlNetï¼Œç»“æœçš„ç»†èŠ‚æ€»æ˜¯ä¸å¤Ÿï¼Œå› ä¸ºæœ¬æ¥å°±åªç”¨äº†ä¸€å¼ ç…§ç‰‡ã€‚
  - å¦‚æœç”¨å¤šå¼ è¡£æœçš„ç…§ç‰‡è®­ç»ƒä¸ªLoraï¼Œç»“æœå°±å˜å¾—ä¸å¯æ§äº†ï¼Œæ¯”å¦‚è¡£æœçš„ç»†èŠ‚æ€»æ˜¯å’ŒçœŸå®æƒ…å†µä¸å®Œå…¨ä¸€æ ·ã€‚
- ç»†èŠ‚è¿˜åŸå¾ˆéš¾ï¼Œçœ‹çœ‹è¿™å‡ ä¸ªæ–¹æ¡ˆå‘¢
  - 1ã€OOTDiffusion
  - 2ã€ä¸€å¼ å‚è€ƒæœè£…ã€ä¸€å¼ å‚è€ƒåŠ¨ä½œ
  - 3ã€OutfitAnyoneï¼Œæœªå¼€æºï¼Œå¯ä»¥åœ¨çº¿ç”¨
- æˆ‘è¯•è¿‡äº†ã€‚å”¯ä¸€å¯è¡Œçš„æŠ€æœ¯æ–¹æ¡ˆæ˜¯ç”¨èº«æ¨¡å®æ‹ï¼Œç”¨sdç”Ÿæˆäººå¤´å’ŒèƒŒæ™¯ç„¶åç»„åˆèµ·æ¥ã€‚æœ€å¤§çš„BUGåœ¨äºåŒæ ·çš„è¾“å…¥æ¡ä»¶ï¼Œè¯­è¨€è¾“å…¥çš„æ‰©æ•£æ¨¡å‹ï¼Œç»“æœä¸€å®šä¼šæœ‰è‡³å°‘15%çš„åå·®ã€‚ç”¨å›¾ç”Ÿå›¾çš„è¯ï¼Œæµç¨‹å¤æ‚ï¼Œè¾¹ç¼˜ç»“åˆéƒ¨è¿˜å¤„ç†ä¸å¥½ã€‚æŠ€æœ¯ä¸Šæš‚æ—¶æ— è§£ã€‚
- ä½ ç”¨ControlNetäº†å—ï¼Ÿæˆ‘ç”¨Inpaint Anything+Segment Anythingï¼Œç„¶åä¸¢ç»™ContrlNetï¼Œè¿™æ ·ä¼¼ä¹è¿˜è¡Œï¼Œä½†æ˜¯å°±ä¼šé‡åˆ°æˆ‘è¯´çš„ç¬¬ä¸€ä¸ªé—®é¢˜ã€‚
  - ControlNetæˆ‘ç©å¾—é€é€çš„ï¼Œè¿™æ˜¯SDæœ€æœ‰ä»·å€¼çš„æ’ä»¶ï¼Œæœ¬æ„å°±æ˜¯ç”¨æ¥è§£å†³ç»“æœåå·®é—®é¢˜çš„ã€‚é€è¿‡éª¨éª¼ã€è¾¹ç•Œã€è¾¹ç¼˜ä¸‰å±‚çº¦æŸï¼Œå¯ä»¥æœ€å¤§åŒ–çš„è§£å†³ç»“æœåå·®é—®é¢˜ã€‚ä½†ä¹Ÿåªèƒ½ç¼©å°ç»“æœåå·®ã€‚åªè¦æ— æ³•åšåˆ°å¯å¤åˆ¶ã€å¯æ§åˆ¶ï¼Œå°±æ²¡æœ‰äº§é‡ï¼Œæ— æ³•æˆä¸ºå¯è§„æ¨¡ç”Ÿäº§çš„å·¥å…·ï¼Œè¿™ä¸ªæˆ‘æ—©å°±çœ‹ç©¿äº†ã€‚

- ## #DALLE3 ChatGPT è¾“å‡ºçš„é£æ ¼ä¼¼ä¹å¾ˆç¨³å®šï¼Œå¤ªç¾äº†ï¼
- https://twitter.com/lencx_/status/1721331238219506111

- ## ğŸ†šï¸ ç”¨ Midjourneyã€DALL-E 3ã€Adobe Firefly 2 è¿˜æ˜¯ Stable Diffusionï¼Ÿ
- https://twitter.com/FinanceYF5/status/1716288468186431811
  - åœ¨è¿‡å»çš„ 6 ä¸ªæœˆé‡Œï¼Œä½œè€…åœ¨æ‰€æœ‰ 4 ä¸ªå¹³å°ä¸Šç”Ÿæˆäº† 50, 000 å¤šå¼ å›¾åƒã€‚

- ## Generated Photosï¼šè¿™ä¸ªç½‘ç«™æä¾›äº†10ä¸‡ä¸ªä¸å­˜åœ¨çš„äººçš„ç…§ç‰‡ï¼Œè¿™äº›ç…§ç‰‡å…¨éƒ¨ç”±AIç”Ÿæˆã€‚
- https://twitter.com/xiaohuggg/status/1675480188023615489
  - ä½ å¯ä»¥åœ¨ä»»ä½•åœ°æ–¹å…è´¹ä½¿ç”¨å®ƒä»¬ï¼Œè€Œä¸å¿…æ‹…å¿ƒä»»ä½•æ³•å¾‹é—®é¢˜ã€‚è¿™äº›ç…§ç‰‡ç¬¦åˆGDPRå’ŒCCPAæ ‡å‡†ï¼Œæ²¡æœ‰ç‰ˆæƒã€æ²¡æœ‰è‚–åƒæƒã€‚
  - åŒæ—¶ä»–è¿˜æœ‰äººå·¥ç”Ÿæˆçš„é¢å­”åº“ï¼Œå…±æœ‰2675894å¼ äººè„¸ç…§ç‰‡ã€‚è¿˜æœ‰å¤šç§å·¥å…·å’Œæ•°åä¸‡å¼ å¤šæ ·åŒ–çš„æ•°æ®é›†ã€‚

- ## å¼€ä¸ª thread æ¥åšä¸ªäººäººéƒ½éš¾æ‡‚çš„ AI ä½œç”»ç§‘æ™®ã€‚
- https://twitter.com/haoel/status/1632211302356783104
- ç›®å‰ä¸»è¦æµè¡Œçš„AIä½œç”»æœ‰ï¼ŒOpenAIçš„Dall-E2, Google çš„ Imagenï¼ŒMidjourneyï¼Œæœ‰è¿˜Stability AI å…¬å¸å¼€çš„ Stable Diffusion ç­‰ç­‰ã€‚
  - å®ƒä»¬éƒ½æ˜¯2022å¹´æ‰å‡ºæ¥çš„ï¼Œæ— ä¸€ä¾‹å¤–ï¼Œå…¨éƒ¨éƒ½æ˜¯åŸºäº Diffusion ç®—æ³•æ¨¡å‹ï¼Œè¿™ä¸ªç®—æ³•çš„åŸç†å…¶å®å¹¶ä¸å¤æ‚

- ğŸ’¡ æœ€åè¯´ä¸€ä¸‹Stable Diffusionï¼Œä»–ä½¿ç”¨äº†ä¸€ç§å›¾ç‰‡å‹ç¼©ç®—æ³•å¯ä»¥å¤§è§„æ¨¡çš„å‡å°‘è®­ç»ƒçš„å†…å­˜å’Œæ—¶é—´ï¼Œåœ¨å·¥ç¨‹ä¸Šå¯ä»¥ç”¨æ›´å°‘çš„GPUå’Œæ—¶é—´æ¥è®¡ç®—æ›´å¤§é‡çš„æ•°æ®ï¼Œäºæ˜¯è®©â€œå¤§åŠ›å‡ºå¥‡è¿¹â€æˆä¸ºäº†å¯èƒ½ï¼Œ
  - å› ä¸ºå¼€æºï¼Œæ‰€ä»¥ï¼Œå®ƒçš„ç¤¾åŒºæ˜¯éå¸¸æ´»è·ƒçš„ã€‚å¼€æºå’Œå°é—­çš„ç«äº‰æ°¸è¿œéƒ½åœ¨ï¼ŒOpenAI å’ŒStability AIä¹‹é—´å°±åƒè‹¹æœå’Œå®‰å“

- Diffusionç®—æ³•éœ€è¦å…ˆæ‰¾ä¸€å †é«˜è´¨é‡çš„å›¾ç‰‡ï¼Œè®­ç»ƒå°±æ˜¯å¯¹æ¯å¼ å›¾ç‰‡ä¸€æ­¥ä¸€æ­¥çš„æŒ‰ä¸€ç§å…¬å¼ï¼ˆé«˜æ–¯å™ªå£°å…¬å¼ï¼‰æ¥åŠ å™ªç‚¹ï¼Œç›´åˆ°æ•´å¼ å›¾ç‰‡å˜æˆä¸€ä¸ªå®Œå…¨å™ªç‚¹çš„å›¾åƒï¼ˆå°±åƒç”µè§†çš„é›ªèŠ±ç‚¹ï¼‰ï¼ŒæŠŠæ‰€æœ‰çš„æ­¥éª¤éƒ½ä¿å­˜ä¸‹æ¥ï¼Œç„¶åç”¨ç¥ç»ç½‘ç»œçš„æ–¹å¼æ¥åå‘å­¦ä¹ å¦‚ä½•ä»ä¸€ä¸ªå®Œå…¨æ˜¯å™ªç‚¹çš„å›¾åƒå˜æˆä¸€å¼ é«˜æ¸…çš„å›¾
  - ä¸€æ—¦è¿™ä¸ªæ¨¡å‹äº§ç”Ÿï¼Œæœºå™¨å°±å¯ä»¥é€šè¿‡â€œå™ªç‚¹â€æ¥é¢„æµ‹å›¾å½¢ï¼Œæ‰€ä»¥ï¼Œæ•´ä¸ªç»˜ç”»çš„è¿‡ç¨‹å°±æ˜¯ç”¨ä¸€ç»„éšæœºæ•°ï¼ˆéšæœºçš„å™ªç‚¹ï¼‰æ¥é¢„æµ‹ä¼šæ˜¯ä¸€ä¸ªä»€ä¹ˆæ ·çš„ç”»ã€‚å¾ˆä»¤äººæƒŠè®¶å§ï¼ŒAIå°±æ˜¯ä»ä¸€å †ä¹±ä¸ƒå…«ç³Ÿçš„éšæœºæ•°ä¸­æ¥ç”»ç”»çš„ã€‚è¿™ç§ä¸ªç®—æ³•å¾ˆæœºå™¨ï¼Œå°±æ˜¯ä»¥å¤§åŠ›å‡ºå¥‡è¿¹ï¼Œä½†ç‰›é€¼çš„åœ°æ–¹æ˜¯ï¼Œå¯ä»¥äº§ç”Ÿæ¸…æ™°åº¦å’Œç»†èŠ‚åº¦å·¨é«˜æ— æ¯”çš„å›¾ç‰‡
- è¿™ä¸ªè¿‡ç¨‹éœ€è¦ä¾èµ–äºå‡ ä¸ªäº‹ï¼Œä¸€ä¸ªæ˜¯è®­ç»ƒçš„å›¾ç‰‡ï¼Œä¸€ä¸ªåˆå§‹åŒ–çš„éšæœºå™ªç‚¹ï¼Œè¿˜æœ‰å°±æ˜¯ Prompt çš„é¢„æµ‹è·¯å¾„ï¼Œæ•´ä¸ªè¿‡ç¨‹éå¸¸åœ°æœºæ¢°ï¼Œè€Œä¸”è¿™ä¸ªæ¨¡å‹ä¹Ÿä¸ä¿è¯èƒ½ç”Ÿæˆè®©äººè§‰å¾—èˆ’æœçš„å›¾ç‰‡ï¼Œæ‰€ä»¥éœ€è¦å„ç§äººä¸ºçš„è°ƒå‚ï¼Œå¹¶éœ€è¦äººé€šè¿‡åœ¨ç”Ÿæˆçš„å›¾ç‰‡ä¸­é€‰æ‹©è‡ªå·±å–œæ¬¢çš„å›¾ç‰‡åå†åº¦ç”Ÿæˆï¼Œç±»ä¼¼äºChatGPTç”¨ä¸Šä¸‹æ–‡æ¥è°ƒæ•´å†…å®¹
- ğŸ’¡ æ‰€ä»¥ï¼ŒåƒMidjourneyè¿™ç§é€šè¿‡èŠå¤©æœºå™¨äººæ¥è®©äººé€‰æ‹©æœ€å–œæ¬¢çš„å›¾ç‰‡ï¼Œå…¶å®å°±æ˜¯è®©äººæ¥å‘Šè¯‰æœºå™¨å“ªäº›éšæœºæ•°ï¼Œå“ªäº›é¢„æµ‹è·¯å¾„ï¼Œå“ªäº›Promptæ›´é è°±å¯ç”Ÿæˆæ›´å¥½çš„å›¾ç‰‡ï¼Œåœ¨ç”¨æˆ·ç”Ÿæˆå›¾ç‰‡çš„åŒæ—¶è®©ç”¨æˆ·æ¥åå“ºäº†AI æ¨¡å‹ï¼Œè€Œç”Ÿæˆå‡ºæ¥çš„å›¾ç‰‡åˆå¯ä»¥æˆä¸ºä¸‹ä¸€è½®çš„å›¾ç‰‡è®­ç»ƒé›†ï¼Œäºæ˜¯ï¼ŒAIä»¥åå°±å†ä¹Ÿä¸éœ€è¦ä½¿ç”¨äººç±»çš„å›¾ç‰‡
- å†è¯´ä¸€ä¸‹ **Prompt**ï¼Œè¿™æ˜¯ä¸€ç§Transformerè¯­è¨€æ¨¡å‹ï¼Œå®ƒæ¥å—æ–‡æœ¬æç¤ºå¹¶äº§ç”ŸTokenï¼Œ
  - Stable Diffusion ä»¥å‰ä½¿ç”¨çš„æ˜¯ OpenAIçš„CLIPï¼Œä½†å»å¹´11æœˆåˆ‡åˆ°äº†OpenCLIPï¼Œä½¿ç”¨äº†3.5äº¿çš„å‚æ•°ï¼Œè€ŒCLIPåªæœ‰6åƒä¸‡çš„å‚æ•°ï¼Œ
  - Promptå¯¹é«˜è´¨é‡çš„å›¾ç‰‡çš„ç”Ÿæˆæœ‰éå¸¸å¤§çš„å½±å“å› ç´ ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ç§æœªæ¥çš„æ›´æ¥è¿‘è‡ªç„¶è¯­è¨€çš„ç¼–ç¨‹è¯­è¨€
