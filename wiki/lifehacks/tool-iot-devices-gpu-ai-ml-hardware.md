---
title: tool-iot-devices-gpu-ai-ml-hardware
tags: [ai, gpu, hardware, iot, machine-learning]
created: 2026-01-15T15:43:36.953Z
modified: 2026-01-15T15:44:10.647Z
---

# tool-iot-devices-gpu-ai-ml-hardware

# guide

# discuss-stars
- ## 

- ## 

- ## 

- ## 
# discuss-news
- ## 

- ## 

- ## 
# discuss-vulkan
- ## 

- ## 

- ## üÜö [A few Strix Halo benchmarks (Minimax M2.5, Step 3.5 Flash, Qwen3 Coder Next) : r/LocalLLaMA _202602](https://www.reddit.com/r/LocalLLaMA/comments/1rabcyp/a_few_strix_halo_benchmarks_minimax_m25_step_35/)
  - My ROCm benchmarks are running against ROCm 7.2 as that is what my distro provides. My device has a Ryzen AI Max+ 395 @ 70W and 128GB of memory. All benchmarks are run at a context depth of 30, 000 tokens.
  - ÊµãËØïÁªìÊûúÔºå vulkanÁöÑpp/tgÈÄüÂ∫¶ÈÉΩÂø´‰∫érocm

- You really need to fix your setup. ROCm outperforms Vulkan at almost every model especially in higher depths.
  - Also your numbers are around 25% PP and 50%-60% TG for a standard 120W strix halo. 
  - https://kyuz0.github.io/amd-strix-halo-toolboxes/

- does that mean if I dump 30k tokens in context, I would need to sit and wait for at least 7 minutes before the first token appear with the STEP and minimax?
  - If the context is not in cache, yes, that is roughly accurate for MiniMax, for STEP it would be just under 4 minutes.
  - If the context grows incrementally or is only altered slightly, it only needs to process the changes, and prompt processing times are low. But yes, prompt processing is painfully slow for cache misses.
  - I'd be happy to be proven wrong and pointed to a setup / software stack that improves this on Strix Halo, but as far as I understand, this is just the state of the ROCm & Vulkan backends on the platform right now. I'm not sure if it's just all the hardware has got, or could be optimized over time.

- ## [Anybody using Vulkan on NVIDIA now in 2026 already? : r/LocalLLaMA _202602](https://www.reddit.com/r/LocalLLaMA/comments/1r6z3d4/anybody_using_vulkan_on_nvidia_now_in_2026_already/)
- Vulkan is faster than rocm for my rx7900xt for whatever that‚Äôs worth
  - Same on my RX7900XTX.

- In my exp w 3090s, P40s, M40s; custom lcpp.cuda builds are vastly superior to lcpp.vk.

- I use Vulkan with llama.cpp for my AMD GPUs (MI50, MI60, V340). It works great! I have no complaints. I have no experience using it with Nvidia GPUs though. Sorry.

- Vulkan is good and I use it alot since I have both Nvidia and AMD GPUs, but CUDA is nearly twice as fast on MXFP4 models that fit entirely on my Nvidia hardware.
# discuss-iot-unified-memory
- ## 

- ## 

- ## 

- ## 

- ## [Ryzen AI Max 395+ boards with PCIe x16 slot? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1ncdtei/ryzen_ai_max_395_boards_with_pcie_x16_slot/)
  - I'm looking to buy a Ryzen AI Max 395+ system with 128GB and a convenient and fast way to connect a dedicated GPU to it.
  - I've had very bad experiences with eGPUs and don't want to go down that route.

- It's never going to be possible. They basically stole PCIe lanes to make the memory bus wider and to add the GPU, which is why the AI max punches so much above it's weight

- The CPU does not have enough PCIe lanes for a x16 slot if you also want an NVMe SSD, which for obvious reasons all boards will have.
  - The Framework Mainboard has a PCIe x4 slot, that might be the most you can find.

- I hope nextgen Ryzen fixes this debacle.
  - It won't, at least not in the desktop or embedded variant. The pin count would grow too large.
  - I would, however, bet on an Epyc or Threadripper chip with a iGPU and a 512 bit memory bus.

- Epyc with iGPU exists but it got called MI300A APU
  - True, but it uses HBM so price is eye watering.
  - 512 or 768 bit memory is already possible on 9000 series, they just need to connect an iGPU to 64 of the PCI-E lanes.

- I made a post about this a few days ago. I already have the 128GB in order.
  - If your use case is mainly gaming, but also want to run inference on dGPU, do this:
  - Get the 128GB board
  - Get an ATX size case and decent power supply 
  - A PCIe 4x4 to 4x16 riser that is powered (because the board provides a max of 25W and you need 75W). These are compact so you can mount it with screws next to the MB inside the case
  - This combo will give you all 64Gbps bandwidth from the motherboard PCIe just like Occulink without dealing with external cables, power supplies or anything of that nature.

- Lookup Oculink vs Desktop performance in Gaming. Then in LLMs.
  - Gaming sees a drop of 2-5% from desktop IRL. That‚Äôs worth it for me every time because the FPS drop is insignificant. Going from 120fps to 112-115 is negligible.
  - LLMs will have almost no impact (except for slightly slower load time from NVME storage) when the model fits completely in VRAM. However, a worse impact will be seen for models offloading to CPU or other graphics cards in the same system.

- If you want dGPU don't get 395 and just put 128GB RAM with a desktop CPU.

- Minisforum MS-S1 has 16x Pcie slot, however the PSU is too weak to power a dGPU

- ## [Advice: Which MiniPC To Buy? (Framework, Bosgame, Minisforum) - AI MAX 395+/128GB : r/StrixHalo _202602](https://www.reddit.com/r/StrixHalo/comments/1r18v84/advice_which_minipc_to_buy_framework_bosgame/)
  - Framework - $3700
  - Minisforum MS-S1 - $3900
  - Bosgame M5 - $2850
  - The BOSGAME is significantly cheaper than the other options, but doesn't have a lot of the connectivity options that Minisforum and Framework have.

- I faced a similar question a few weeks ago. I went with a fourth option and ended up purchasing a Corsair AI 300 workstation. It is a little bit more expensive than Bosgame, but cheaper than the alternatives, and it comes with good support and a two-year warranty.

- Framework. They support LVFS so firmware updates are simple under Linux. Standard power supply so easy to replace in future. mITX so easy to move to different case. Basically good flexibility for the future.

- Having proper mini-itx board with normal PC power supply played a role in my choice. I can resell it later as a beefy server if push comes to shove or as a small gaming PC with the original case.

- ## üÜö [Tested: AMD's Strix Halo vs Nvidia's DGX Spark : r/Amd _202512](https://www.reddit.com/r/Amd/comments/1pxs0dd/tested_amds_strix_halo_vs_nvidias_dgx_spark/)
- Strix halo doesn‚Äôt support FP8 since it‚Äôs on rdna 3.5. That‚Äôs a tough sell. If these machines used rdna4 they‚Äôd be absolute monsters. It‚Äôs unfortunate that only the 9070 xt and 9060 xt use rdna4. All of these awesome APUs being released would do so much better with rdna4 imo. Hopefully rdna5/udna1 is implemented in all products when it‚Äôs released.

- The Samsung Xclipse GPUs are based on RDNA, and they were used in the past in phones like Galaxy S24 (non-US/non-Qualcomm versions).

- Don't worry, AMD will continue releasing old RDNA iGPUs until Intel miraculously survive Intel's mismanagement crisis.

- ## üÜö [DGX Spark vs AI Max 395+ : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o6izz2/dgx_spark_vs_ai_max_395/)
- I just ran some benchmarks to compare the M2 ultra.
- OSS-120B
  - DGX PP=817, TG=41
  - 395 PP512=350, TG=34 (Vulkan)
  - 395 PP512=645, TG=46 (Rocm) *per Sillylilbear‚Äôs tests
  - M2U PP=590, TG=70
- OSS-20B
  - DGX PP512=2053, TG=48
  - 395 PP512=1000, TG=47
  - M2U PP512=1000, TG=80
- Llama 3
  - DGX PP512=7991, TG=21
  - 395 PP512=1000, TG=47
  - M2U PP512=2500, TG=70

- NVIDIA didn't build this for our community. It's a dev platform for GB200 clusters, meant to be purchased by institutions. For an MLE prototyping a training loop, it's much more important that they can complete 1 training step to prove that it's working than that they can run inference on it or even train at a different pace. 
  - If you want to replicate GB200 environment as closely as possible, you need three things: NVIDIA Grace ARM CPU, Infiniband, and CUDA support. RTX 6000 Pro Blackwell only provides one of those three. Buy two DGX Sparks and you've nailed all three requirements for under $10k.

- If you want LLMs with working speeds and diffusion models with slow speeds, both devices are fine. Vulkan support for AI Max 395+ is really good, so you can get better performance with most llms than DGX Sparks (or at least the same) for LLM uses.
  - However, the main problem arrives when you try to use the latest non-LLM models such as TTS, openmcp, and omni models with video support, where you are dependent on ROCm for HIP. Most of these latest models are optimized and tested for CUDA, and they usually fail on Halo (even with ROCm 7.0).
  - I own a 395+, and since I am a developer, I am really happy with my purchase. I can keep multiple 30B MOE models in memory and can get a very fast response. Every day, I try to run new AI models on my system, but the success rate for non-LLMs is 40% compared to my 4090, where it is 90%.
# discuss-multi-gpu
- ## 

- ## 

- ## 

- ## 

- ## [128GB VRAM quad R9700 server : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/)
  - I originally planned to pick up another pair of MI100s and an Infinity Fabric Bridge
  - But then I saw benchmarks for the R9700, particularly in the llama.cpp ROCm thread, and saw the much better prompt processing performance for a small token generation loss. The MI100 also went up in price to about $1000, so factoring in the cost of a bridge, it'd come to about the same price. So I sold the MI100s, picked up 4 R9700s and called it a day.
  - llama 7B Q4_0, 90.89
  - qwen3moe 30B. A3B Q8_0, 72.51
  - qwen3vl 32B Q8_0, 14.75

- It starts with you using the hardware you've got, then buying cheap ex-datacenter cards on ebay, then next thing you know you're buying every card in town. I cleared out my local Micro Center's stock of these GPUs lmao.

- Interesting, so you're getting PCIe 4.0 x8/x4/x4 (from the CPU) for the first 3 and then one more Pcie 4.0/3.0 x4 (probably from the chipset).. the 9700 is PCIe 5.0, so I'm guessing your memory interactions are slow, and probably worth bumping up to 96GB?
  - To get the necessary PCIe lanes, you can either bump up to Threadripper or Siena (I've got an 8224P), which breaks your AM5 desire...
# discuss-ai/ml-hardware
- ## 

- ## 

- ## 

- ## üÜö [Mac Studi M3 Ultra vs Nvidia 6000 Blackwell : r/LocalLLM _202602](https://www.reddit.com/r/LocalLLM/comments/1qsv2p0/mac_studi_m3_ultra_vs_nvidia_6000_blackwell/)
- Nvidia RTX 6000 Pro if you need it to be fast. Mac if you need to run the best possible model and can accept that it might be slow. Simple as that.

- I have an RTX PRO 6000 and a M3 Ultra with 256GB RAM. The RTX PRO 6000 is quite a bit faster at both prompt processing (10x?) and token generation (3x?). Speed matters to me so I only use the RTX PRO 6000. I would only use the M3 Ultra if I wanted to run a model that was too big for the RTX PRO 6000. So far I have not needed to run a model that didn't fit on the RTX PRO 6000 but it is nice to know that I can with the M3 Ultra when/if I might need to some day.
  - The other thing about Mac is that you can now build clusters of them over TB5 for even faster AI - checkout exo:

- If you‚Äôre just asking it questions and you want the smartest possible answers, mac is probably the way to go. M3 ultra 512gb is by far the most convenient way to run big sota models at home.
  - If you are going to try agentic coding, image or video generation or anything that uses long context, be warned mac will be very very slow. For giving it a page or three of text and asking for an answer, you‚Äôll be fine, maybe wait 10 to 30 seconds before it starts answering. More than that and it will slow to a crawl. Expect to wait half an hour or more for a response to finish if you‚Äôre using coding agents.
  - Rtx pro on the other hand, will be way faster but can‚Äôt fit the big sota models like glm 4.7, deepseek 3.2, kimi k2.5 etc
  - If you get an rtx pro plus a ton of ram you can run the big sota models maybe at the same speed as mac (possible with lower wait time for the answer to start) but then you have an engineering problem on your hands as to how to keep all that ram cool, not to mention it‚Äôs probably about as expensive as the m3 ultra now

- ## ["Introducing AMD Ryzen AI Halo, a mini-PC powered by Ryzen AI Max+ that delivers desktop-class AI compute and integrated graphics for running LLMs locally." - AMD is on the move! Would you get one? : r/LovingAI _202601](https://www.reddit.com/r/LovingAI/comments/1qiybwp/introducing_amd_ryzen_ai_halo_a_minipc_powered_by/)
- unified 128 Gb, lpddr5 8000.

- It's just a minisforum ms max

- ## [Gigabyte Announces Support for 256GB of DDR5-7200 CQDIMMs at CES 2026 : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1q9xn78/gigabyte_announces_support_for_256gb_of_ddr57200/)
- Dual channel is useless, with model that make use of 256gb the speed'd be unuseablely slow
- This makes zero sense because Threadripper Pro exists.
  - 256GB (2x128) CQDIMM 7200 will probably cost about $4000 (based on upcharge for rarity and speed from 4x64 6400 currently at ~$3200)
  - But you can get 256GB (8x32) 6400 RDIMM for about $2100. Threadripper Pro has 8 channel memory so 8 channel @ 6400 is faster than dual channel @ 7200 for any task that you would actually need 256GB RAM for.
  - Threadripper Pro 9955WX 16c: $1799 + Asus Pro WS WRX90E-SAGE SE motherboard $1249 + 8x32 6400 RDIMM $2100 = $5148
  - Ryzen 9950X 16c $549 + Gigabyte Aorus Tachyon $600 + 2x128 7200 CQDIMM $4000 = $5149
  - Not to mention that on the Threadripper Pro system you get 7 real x16 slots, dual 10GbE, etc

- Two comments saying that its useless, but I don't think so.
  - 256GB of 7200MT/s dual channel is very good. In fact it's faster than older Threadripper (quad-channel DDR4-3200) and on top of that this is initial support with initial modules, it will scale to higher transfers.
  - In 2025 we saw that MoEs are becoming more sparse (Qwen Next 80B A3B, GPT-OSS 120B A5B, MiniMax 230B A10B). If this trend continues (It's very likely) and we get models like 200B A5B then this combined with just one GPU would be a killer combo for local LLMs.
- Strix halo exists now with quad channel at 8000 Mt/s, and people call it slow for dense large models.
  - I have a Strix Point LPdDR5X with dual channel 8000 Mt/s and it gets the job done but not really fast. 

- ## [Does it make sense to have a lot of RAM (96 or even 128GB) if VRAM is limited to only 8GB? : r/LocalLLM](https://www.reddit.com/r/LocalLLM/comments/1q7e34g/does_it_make_sense_to_have_a_lot_of_ram_96_or/)

- Since you can't upgrade VRAM, that bulk RAM is decent alternative.

- The limitation is speed.
  - Typical DDR5 RAM bandwidth is around 80 GB/s. With 128 GB of system RAM, you could run an 80B model (quantized to Q8) at roughly 1 token/s. (you can very roughly calculate t/s by dividing memory bandwidth/model size, in this case 80GB/s / 80B@Q8 = 1)
  - Typical VRAM bandwidth varies widely, from about 200 GB/s to 2000 GB/s on high-end gaming GPUs. If you somehow had 128 GB of VRAM, you could run an 80B Q8 model at roughly 2.5 tokens/s on the low GPU end and up to 25 tokens/s on the high end GPU.
  - While MOE may be 100B-parameter models overall, they do not use the entire model for every token, only a subset, for example ~5B parameters. Because of this, even with a 100B MoE model running from system RAM, you could still achieve something like 20 tokens/s.

- I have M3 Ultra with 256GB and the entire model of 120b gpt-oss is in UMA RAM. It generates around 70 tokens per second.

- The model will overflow into RAM, so it will run, but very slowly, dependent on your RAM bandwidth. Most people have only dual channel RAM setups so it'll be very slow, even on DDR5.

- ## [[TweakTown] Minisforum BD395i MAX motherboard at CES 2026: built-in AMD Strix Halo APU, use your own GPU : r/hardware](https://www.reddit.com/r/hardware/comments/1q8jo44/tweaktown_minisforum_bd395i_max_motherboard_at/)
- The dGPU can offload the compute intensive attention part of a large MoE LLM model while retaining most of the memory usage on the iGPU.

- You could already do that with the Framework Desktop's ITX board

- I already have the Strix Halo chip (Evo-X2) running a 5080 via OcuLink on the 2nd M.2 adapter. This gaming setup is 4K Heaven, and chews through MOE LLMs on the side.
  - Honestly the onboard graphics 8060s is going to be more than fine for most people, I was playing 4K at 100+ fps (DLSS, FG, No RT) and it felt great. Then I tried Path-Tracing in Cyberpunk with the 5080 and couldn‚Äôt go back.

- I‚Äôve seen 395+ clusters but they were always limited by I/O.

- ## [Minisforum BD395i MAX motherboard at CES 2026: built-in AMD Strix Halo APU, use your own GPU : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1q8x9yp/minisforum_bd395i_max_motherboard_at_ces_2026/)
- The new board's PCI slot is not PCIe 5.0 x16, it is PCIe 4.0 x4. The Minisforum product brochure for the new board is very clear. The reporting was sloppy.
  - yeah afaik the strix halo chip itself is using all its big pcie lanes for its integrated gpu, leaving very few lanes available for external stuff.
- Panther lake with 20 pcie lanes and lpddr5x will make obsolete

- [Minisforum shows desktop Mini-ITX board with Ryzen AI Max+ 395 ‚ÄúStrix Halo‚Äù and 128GB LPDDR5X memory - VideoCardz.com : r/sffpc](https://www.reddit.com/r/sffpc/comments/1q8k3ng/minisforum_shows_desktop_miniitx_board_with_ryzen/)
- Look at framework desktop benchmarks. Your games will never take advantage of those 128GB of RAM. You could maybe expect to use something like 20GB on big games...

- This is good to have another alternative to Framework. And with a full x16 slot this time.
  - physical slot but it will only be x8 capable as the chip only has 16 total PCIe 4 lanes.
- Oh, then that's not made clear by the article saying "full PCIe 5.0x16 interface". Still that's better than the x4 slot on the Framework.
  - Won‚Äôt be PCIe 5.0 either, Strix Halo is PCIe 4.0.
  - Unless we see a definitive statement from Minisforum that the slot will be x8 or x16, I‚Äôd expect the slot to be x4 electrically like all the other boards. The issue is there just aren‚Äôt a lot of spare PCI lanes, and to get to x8 requires some really tough cuts elsewhere such as an M.2 slot or the Wi-Fi/BT/some of the USB.

- ## [What do we think about Gorgon Point (Ryzen AI 9 HX 470)? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1q4jc99/what_do_we_think_about_gorgon_point_ryzen_ai_9_hx/)
  - The new APU is promised to support DDR5-6400 (102.4 GB/s) and LPDDR5X-8533 (136.5 GB/s) which should move some models that were barely usable on Strix Point to the usable territory.
  - However, it really seems that to utilise these capabilities, manufacturers would have to get chips that are basically inaccessible right now.

- 470 is the successor of 370. Big HELL NO. Still worse than 395.

- Gorgon Point is a mid-cycle refresh, its not a replacement for strix halo. That will only come around 2027 as medusa halo (as announced before). 
- Looks better than HX 370, but still less interesting than Ryzen AI Max 395 with his 256 bits bus.

- It takes a long time for software to catch up to the hardware. For example, Qualcomm released the Snapdragon X chips back in May 2024 with Llama being shown to run on it, but NPU support for LLMs only dropped a year after release. CPU and GPU support on llama.cpp also took a couple of months to show up and this was with Qualcomm engineers pitching in.
  - AMD needs to do a heck of a lot more on the software side if it wants these APUs to be useful for LLMs and local AI.

- I think that the real revolution will come once this kind of APUs use DDR6. But apple will be the first. And will do it with more memory lanes too

- It's going to be worse than Strix Halo which offers not only 256 GB/s but also has over 75 TOPS from igpu alone (same NPU). Heck, Intel's Nova Lake is gonna offer 75+ TOPS from NPU alone.

- ## Á≠âËøôÊ≥¢AIÊ≥°Ê≤´Á†¥Ë£ÇÔºåÊàëÈÉΩ‰∏çÊï¢ÊÉ≥‰ºöÊúâÂ§öÂ∞ë‰∏ãÊû∂ÁöÑÊúçÂä°Âô®Á°¨‰ª∂‰ºöÂÜ≤ÂáªÂûÉÂúæ‰Ω¨Â∏ÇÂú∫„ÄÇÊêìÊâãÁ≠âÂæÖ
- https://x.com/riaqn_zh/status/2004907309336875298
- ‚ÄúÊàëÂÖÑÂºüÂú®Á≠âÊàø‰ª∑Â¥©Áõò‚Äù
  - Ê≤°ÈÇ£‰πàÂø´Ôºå‰Ω†Áúã22Âπ¥ËøòÊúâ‰∏ÄÊ≥¢Êö¥Ê∂®Âë¢„ÄÇ

- ‰∏çÁî®Â§™‰πÖÔºåÊúçÂä°Âô®‰ºöÊääÊóßÁ°¨‰ª∂Ê∑òÊ±∞ÁöÑÔºåÂ§ßÂéÇ‰∏çÂ∑ÆÈí±ÔºåËÄå‰∏î‰ºö‰∏ªÂä®ËøΩÊ±ÇÊúÄÊñ∞ÊòæÂç°ÔºåÂç≥‰Ωø‰∏çÁ†¥Ë£Ç‰πüËÉΩÊç°Âà∞Á°¨‰ª∂
- ‰∏çÁî®Á†¥Ë£ÇÔºåÂæàÂ§öÊï∞ÊçÆ‰∏≠ÂøÉÁöÑÊúçÂä°Âô®2-3Âπ¥Â∞±Ê∑òÊ±∞‰∫Ü

- ËøôÊ¨°ÁöÑÊ≥°Ê≤´ÂèØ‰∏çÂÉèÊàøÂú∞‰∫ßÂíåÂÖÉÂÆáÂÆôÂëÄ„ÄÇÂ¶ÇÊûúÁúüÁöÑÁÆóÂäõË∂≥Â§üÔºåÊòØÂèØ‰ª•‰ªéÂ∫ïÂ±ÇÊïôËÇ≤Âà∞ÁîüÊ¥ªÁöÑÂêÑÁßçÊñπÈù¢ÈÉΩÂèØ‰ª•ÊîπÈù©„ÄÇÂ∞±Ë¶ÅÁúãË∞ÅÊääËõãÁ≥ïÁîªÂ§ßËøòÂÆûÁé∞ÂÆÉ„ÄÇ

- ## [Would a Ryzen AI Max+ 395 benefit from dedicated GPU? : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1ps2cjk/would_a_ryzen_ai_max_395_benefit_from_dedicated/)
  - I just ordered a Framework desktop motherboard, first time I will have some hardware that let me play with some local AI.
  - The motherboard has a 4x pci express port, so with an adapter I could put a gpu on it.
  - I was wondering if it would benefit from a dedicated GPU like a 5060 or 5070 ti (or should it be an AMD GPU?)?

- The 395 combined with the 128 GB of memory is a somewhat affortable System for use of "large" LLMs. It isn't the fasted, but it is a compromise and gets more t/s than LLMs larger than the VRAM of a regular GPU.
  - A GPU combined with a 395 makes no real sense to me, except if you have a mix of models were some fit into the GPU VRAM. A real upgrade would be a GPU with similar sized VRAM than the 395 like a RTX 6000.

- I guess an 395 together with an 5090 could be currently the best AI machine when it comes to cost and power consumption. It should do well with LLM (395 brings the RAM and the 5090 is a very capable extension) as well as with image generation tasks (5090 in lead, 395 is just the host). But at least the Framework case doesn't support it.
  - The 5090 is limited to the 32 GB VRAM onboard. The performance advantage of the 5090 is lost by communication overhead.
- Only when you have something communication heavy. When it's fitting in the 32 GB, like most text2image models, it'll work very well.

- ## üÜö [Studio + Air combo vs MacBook Pro : r/MacStudio _202512](https://www.reddit.com/r/MacStudio/comments/1pfl6hk/studio_air_combo_vs_macbook_pro/)
- I used to have a MBP for my work and travel machine. After three years I realized I had it docked 90% of the time as I didn‚Äôt like taking it when I traveled so I got a cheap M2 Air for travel. 
  - In August of this year I moved to an M4 Max Studio plus M4 Air (24/512 version) combo. I don‚Äôt need heavy lifting when I travel so the Air meets all my needs. Most of my computing is done on my Studio. 
  - For me, it‚Äôs worked out great. The Studio is awesome and I have everything synced to my Air with my NAS and iCloud. I have a nice and small Crucial 4TB external SSD hooked up to my Studio for large files and projects, and when I travel I just take it with me. 

- Im going back and forth with the same question for years now and after quite a few MacBooks running as a main machine, I‚Äôd gladly stay with faster desktop and laptop you don‚Äôt need to care about.
  - Price to performance with desktop
  - Better cooling
  - Better longevity
  - and better workflow for your mind - now you have one work machine and other one for private use (and sometimes work related). Less time spend in front of main, work computer means less distractions when doing work. If you work and have fun with the same machine it‚Äôs pretty hard to differentiate work from having fun, which leads to burnout and other issues.

- I‚Äôve used Mac laptops exclusively since 1993, and MacBook Pros exclusively since 2012. I bought a Mac Studio M3 Ultra base model this summer and, when my MacBook Pro died, I replaced it with the base model M4 MacBook Air.
  - One thing I haven‚Äôt really worked out yet is syncing the two machines. I‚Äôve been meaning to try ChronoSync and its Agent application and have the trials downloaded. For now I just make sure I have the files I need on the Air. I think it‚Äôs a great combination.

- Performance wise, the M4 Max Mac Studio and MacBook Pros can both be configured identically, though the M4 Max in the MacBook (especially the 14‚Äù) can thermal throttle, whereas the Studio has more robust cooling. Ultimately the right choice for you will depend on how you prefer to work.

- I have this exact setup. 16GB M1 Air and a 64GB M4 Max. But the air is more of personal computer, I don't try to sync them. I use Rust Desk to phone into the studio with varying degrees of success.

- ## [AMD 395+ and NVIDIA GPU : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1p9dier/amd_395_and_nvidia_gpu/)
  - Is there any reason I can‚Äôt put an NVIDIA GPU in an AMD 395+ machine?
- It works fine simultaneously, several of us have done it
- Nvidia dGPUs work better in the Strix Halo than do either Intel or AMD, apparently.

- Is there a 395+ machine that you can fit an Nvidia GPU in?
  - I'm looking into doing it with a Framework Desktop motherboard, which is mITX sized.

- Physical space, cooling, power would be the first 3 problems...
  - External GPU docks exist

- NVIDIA GPUs work fine on 395 platform as long as you can put them together physically (with M.2 to PCIe adapters or 395 boards with standard PCIe slots). AMD dGPUs may be more problematic due to VBIOS compatibility issues.

- ## [Need some honest opinions on GPU Ai in a box : r/ollama](https://www.reddit.com/r/ollama/comments/1p4z140/need_some_honest_opinions_on_gpu_ai_in_a_box/)
  - For my use case my models and software burn about $2000 per month if I rent a pod using runpod and I have to be extra careful due to rate limits. I want to consider running my models using llamacpp or ollama or offering direct inference for customer using their own on prem machine shipped by me.

- The DGX Spark and similar are not meant to be servers. Could you use them? I guess. But for 8b models you don‚Äôt need $3000 of anything. At full 16bit you‚Äôd need 16GB of memory plus some context. 24GB tops. Less with quants.
  - And don‚Äôt deploy with llama.cpp based anything. Those are personal use tools. Use vLLM, SGLang or TensorRT.
- Why can‚Äôt you use it as a server? Is it only for inference via vLLM?
  - tldr the ones I mentioned are built for production and high parallel work loads.

- Have you tried your models on various Mac minis or studios? Very stable and speedy enough.
  - Yes i run my models on Mac mini m4 but running multiple models at the same time is a pain

- ## üè† [Most Economical Way to Run GPT-OSS-120B for ~10 Users : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p4evyr/most_economical_way_to_run_gptoss120b_for_10_users/)
- A quad of 3090s will get you there cheapest, assuming you have a way of mounting four 3-slot GPUs in a reasonably secure manner! It‚Äôs probably going to need a bunch of PCIe riser cables and a mining frame to accommodate the space, noise and cooling requirements‚Ä¶ you‚Äôd have 1500W of GPU alone!

- 4x 3090 with vLLM

- What's your use case? Are you running documents through it (RAG)? or just Q&A to the base model?
  - If its the latter, you mainly need something with sufficient VARM. As once the model its loaded in VRAM it can server multiple users. I'm running 20B on RTX 4090. For 120B you can do it on 4x 3090

- 3 x Radeon R 9700 AI PRO 32 GB = 96 GB total mit Vulkan. Cards will be around 4K USD. Then u need some reasonble CPU and some 128 GB RAM.

- ## [Macbook pro 128gb RAM owners, whats the best AI you're running for reasoning & knowledge? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p0yu23/macbook_pro_128gb_ram_owners_whats_the_best_ai/)
- GLM-4.5-Air
  - minimax is next to it but it‚Äôs hard to fit in the good quant. 
- Agree! I use MiniMax-M2 @ Q3 and it works mostly great, with a relatively long context. But I am not sure it‚Äôs better than GLM-4.5-Air @Q6. Both on par for my coding and agentic tasks. MMM2 is faster though.
- what do you serve GLM with? I really like using LM Studio for tools parsing on MLX quants, but LM Studio doesn‚Äôt offer native tools parsing for GLM which causes issues for opencode and zed.
  - I serve GLM-4.5-Air with LM Studio. Use this fixed Jinja template to replace the original one in LM Studio for GLM
  - I really don't get why they never fixed it officially for such popular and useful local model. Maybe with the GLM-4.6-Air release

- I actually use GPT-OSS 120b and GLM 4.5 Air a lot. Qwen Next is pretty good too. Was running an unsloth IQ2 of Minimax M2 today- fun model to chat with but makes a lot of small mistakes at that quantization. Can do similar with Qwen3 235b a22b. Those are cool big models, and nice for low stakes, lower context stuff, but that may not be right for your uses.

- ## üß© [Apple is considering putting miniHBM on iPhones in 2027 : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1oz9vs3/apple_is_considering_putting_minihbm_on_iphones/)
- Mobile HBM will be very expensive though, i think it is possible they will do a hybrid for macs
  - At 20 watts tdp don't expect gpu type performance, probably like nvidia orin order of magnitude
- It will probably be slower due to the battery constraints but they will improve the battery too. However when hbm reaches the mac studio, there will be way less power constraints , it will run real fast like 2.8-8TB/s.. even on the m7 or m8 max macnooks, it will be fast

- What's mini-HBM?
  - Mini-HBM isn‚Äôt ‚ÄúVRAM stacked on a wafer‚Äù and it isn‚Äôt literally on top of the GPU. It‚Äôs a small HBM-style DRAM stack placed next to the SoC on a silicon interposer.

- Looking at mobile devices, the memory bandwidth for the most part is less than 100GB/s so they really need to up those. And companies are too focused on getting their proprietary models to run well rather than let people just run the open weights models.
  - I do agree, but there is a world that models are introduced in the background before users are able to directly chat with them. Small models can unlock categorization and data collection workflows that are not time sensitive, and will give users improved recommendations and insights without sending data off site. Realistically they could even have a nightly processing step that runs through data over night so it doesn't affect UX.

- I fear on-device inference will greatly reduce battery life which is quite important on a mobile device.

- HBM for bandwidth, Neural block in each GPU core for prefill, 3nm and arm64 for energy efficiency. Apple is shaping up to be a good consumer AI company.
  - Yeah, it is gonna be good, the 2nm process ( 20 nm metal pitch) will come in 2026 and fp 8 native compute will likely come too.. they make good hardware but real expensive on the ram and ssd configs But still so much cheaper than nvidia.

- Ironically Huawei uses LPDDR phone RAM on GPU.

- I don‚Äôt understand why it‚Äôs Iphone first and then Mac not the other way around.
  - Apple is a mobile first company, they usually test on iphones first then implement it for ipads and macs ‚Ä¶ if it works on iphones, it will be easier and better on macs. M series chips are essentially scaled up iphone chipa

- ## [AMD Ryzen AI Max 395+ 256/512 GB Ram? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1oyy0fy/amd_ryzen_ai_max_395_256512_gb_ram/)
  - I‚Äôm looking at the new AI boxes using the Ryzen AI Max+ 395 (GMKtec EVO-X2, Minisforum‚Äôs upcoming units, etc.) and I‚Äôm wondering if we‚Äôll actually see higher-end RAM configs ‚Äî specifically 256GB or even 512GB LPDDR5X.

- There‚Äôs little point in producing a box with more RAM if the bandwidth isn‚Äôt sufficient. 
- Will not see higher RAM configs for Strix Halo. The 256 bit bus width and available LPDDR5X density prohibit this. Even if somehow someone would make a more dense module, the limited bandwidth would not make it useful for LLMs.

- AMD to my knowledge has no interest in supporting this SoC with a 128GB+ config, and the more likely result is a more bespoke SoC in a successor that goes up to 256GB/512GB, possibly with higher bandwidth or a better programming model.
- AMD will never do any powerful AI consumer products, that they wont eat their workststion/server shit

- AMD's next iteration of this chip, Medusa Halo, and that is expected to use either 384-bit LPDDR6 or 256-bit LPDDR5X meaning possible configurations of: 273, 342, and all the way up to 691GB/s in the extreme case.

- Samsung is releasing lpddr6x modules with more 10k mhz modules, which if utilized would increase the memory bandwidth for more than 330 Gb/s. Another way is if AMD uses octa channel method as used in threadripper pro, it can be a game changer. It can increase the memory bandwidth to more than 650 gb/s. With octa channel they can easily go upto 256gb using 32gb modules or 192gb with 24gb modules. It would increase cost, but AMD can capture numerous clients from mac lineup.

- ## [Is it possible we ever get CPU native LLMs? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1oyj16n/is_it_possible_we_ever_get_cpu_native_llms/)
- CPUs aren't really suitable for the type of calculations AI uses. "CPU native LLMs" will be just regular LLMs that are running in NPU unit inside of a regular cpu. One day, when NPUs will get decent, it'll be normal.

- MoE models run pretty well on CPU, I have a 21B running on an SBC at 15 tps.

- it's common knowledge on machine learning 101 . Machine learning /AI is mostly about matrix calculations , and which can be done by CPU‚Äôs as well however it depends on your CPU core size. Meanwhile GPU is meant todo this operation for each pixel 100times per second. CPU is costly consumes a lot more power per core and its core isn‚Äôt scalable . You can‚Äôt have cpu with 1million core. If you do it will be size of a watermelon and cost you 10million dolar to buy and 10K electric bill.

- ## [Mac studio ultra M3 512GB : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1oy9kkv/mac_studio_ultra_m3_512gb/)
  - Is there anyone use it in production environment? I didn't find serious benchmark data about it, is it possible to run such as kimi-k2 thinking with two Mac studio 512GB ? 
- Yes, you can run a quantized version of Kimi K2 on two clustered Mac Studio M3 Ultras, but it's not ideal for a production environment. 
  - The main issue is that processing a large 256k context window will be extremely slow, as the M3 Ultra's GPU performance creates a bottleneck despite the large amount of unified memory. 
  - While tools like MLX or Exo can link the machines, this setup is better suited for a single developer's workstation than for serving multiple requests with low latency.

- I recently saw a post showing someone getting 15 tps on Kimi-K2 on two linked M3 Ultra Mac Studios. So it can be done, and 15 tps is certainly usable.
  - I just finished building a server using an EPYC 9455P and RTX Pro 6000 (just one) which can hit the same 15 tps on Kimi-K2 for about the same price and without any of the other drawbacks of the Mac (like trying to link two machines, being stuck on a Mac, etc.).
  - The EPYC 9005 series has 12 independent DDR5 channels at 6400 MT/s, giving it 614 GB/s of memory bandwidth. Not quite as high as the M3 Ultra's 800 GB, but not far off, especially considering you aren't limited to 512 GB and you have 96 lanes of PCIe 5.0 to use for whatever you feel like.

- Problems is with lack of continuous batching. you have to queue incoming requests, vLLM does them in async way but do it support Apple GPU. I do a lot of tests with aider and prefill was never a problem.. but I test only small models

- ## [Model recommendations for 128GB Strix Halo and other big unified RAM machines? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1oy1v7q/model_recommendations_for_128gb_strix_halo_and/)

- GPT-OSS-120B is the best model you can run on the Strix Halo. 
  - GLM Air fits, but it runs less than half the speed, and GPT-OSS-120B is already super slow in practice. 
  - You can get 50 tokens/sec, but in real use the prompt processing is so slow it feels like 1 token/second for anything but light chatting.

- ## [Ryzen AI MAX+ 395 - LLM metrics : r/ollama _202511](https://www.reddit.com/r/ollama/comments/1oxw4ir/ryzen_ai_max_395_llm_metrics/)
  - MACHINE: AMD Ryzen AI MAX+ 395 "Strix Halo" (Radeon 8060s) 128GB Ram
  - OS: Windows 11 pro 25H2 build 26200.7171 (15/11/25)
  - INFERENCE ENGINES: Lemonade V9.0.2, LMstudio 0.3.31 (build7)
  - I would reccomend Lemonade (supported by AMD) but the python API is the generic OpenAI style while LMstudio Python API is more friendly. Up to you.
  - LMstudio (No NPU) is faster with Vulkan llama.cpp engine rather than Rocm llama.cpp engine (bad bad AMD).
  - Lemonade offers also NPU only mode (very power efficient but at 20% of GPU speed) perfect for overnight activities, and Hybrid mode (NPU+GPU) useful for large context/complex prompts.

- Framework has an ITX sized board which you can put in a server or desktop case. It even has 1 PCIE4x4 slot. The CPU doesn't have enough PCIe lanes for more slots
  - Easy, buy a Xeon or a threadripper server. Dual CPU with lots of lanes.

- ## [Why aren't there cheap NVLink adapters for RTX 3090s? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1owxob9/why_arent_there_cheap_nvlink_adapters_for_rtx/)
  - Is the NVLink only a wire jumper linking both cards together? Can I make my own homemade connections?
- 4 slot bridges go for $600-700 on eBay

- I believe they are passive but contain 128x high speed lanes with two 90 degree connectors, that's a signal integrity nightmare all those traces need to be matched in length. Does this justify them being $600 tho? Not at all
  - This is the correct answer. You can't just jumper a high-speed interconnect. They went from flexible ribbon to PCB for a reason.
  - That being said, it IS 100% supply and demand (similar to current DDR4/DDR5 prices). The 4-slot adapters were $59-$79 when I got mine a few years back. The 3-slot were like $120-150. BECAUSE gamers didn't care about them. The current demand is from people re-using 3090's for compute work.

- ## [Nvidia Tesla H100 80GB PCIe vs mac Studio 512GB unified memory : r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1owy5sm/nvidia_tesla_h100_80gb_pcie_vs_mac_studio_512gb/)
  - A Nvidia Tesla H100 80GB PCIe costs about ~30, 000
  - A max out mac studio with M4 ultra with 512 gb unified memory costs $13, 749.00 CAD

- H100 is not 10, 000 times faster, more like 10X for training and 20X for inference, so one order of magnitude.

- H100 costs $30, 000 because they allow clustering with HBM 3tb/s allowing you to combine multiple H100s into a single giant GPU super computer. Cant do that with the Mac. Can't do that with the Pro 6000.
  - These LLMs are trained with clusters as large at 500 - 15, 000 H100s...

- ## [Kimi K2 Thinking Q4_K_XL Running on Strix Halo : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ouuko3/kimi_k2_thinking_q4_k_xl_running_on_strix_halo/)
- When file size is larger than your mem+vram, there's little you can do since bottleneck is the disk read speed(besides bad system managed caching). You can buy 3 more machine and use RPC to speed up

- ## [Half-trillion parameter model on a machine with 128 GB RAM + 24 GB VRAM : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1oueiuj/halftrillion_parameter_model_on_a_machine_with/)
  - CPU: Intel i9-13900KS
  - RAM: 128 GB (DDR5 4800 MT/s)
  - GPU: RTX 4090 (24 GB VRAM)
  - I‚Äôve successfully run Qwen3-Coder-480B on llama.cpp
  - UD-Q3_K_XL: ~2.0 tokens/sec (generation)
  - UD-Q4_K_XL: ~1.0 token/sec (generation)

- Be careful with any method of running a model that heavily leverages swapping in and out of your SSD, it can kill it prematurely. Enterprise grade SSD can take more of a beating but even then it's not a great practice.
  - This is only half correct. Repeatedly writing to an ssd shortens its lifespan. But repeatedly reading from an ssd is not harmful.
  - When you use mmap() for models exceeding RAM capacity, 100% of the activity on the ssd will be read activity. No writing is involved other than initially storing the model on the ssd.
- writing and erasing data on ssd‚Äôs are intensive, and ssd‚Äôs generally have a limit on how many times you can do that before they become read only or inoperable.
- Memory mapping is reading to memory and discarded as needed. It isn't writing to disk so no concern on excessive writing like swap space / Windows virtual memory.
- It is not swapping! It is using mmap (memory-map model). So it is only reading from SSD (there are no writes, context is kept in RAM).

- ## [What is the best hardware under 10k to run local big models with over 200b parameters? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1otdr19/what_is_the_best_hardware_under_10k_to_run_local/)
  - Short term I will use this to refactor codebases, coding features, etc. I don't mind if it runs slow, but I need to be able to run thinking/high quality models that can follow long processes (like splitting big tasks into smaller ones, and following procedures). 
  - But long term I just want to learn and experiment, so anything that can actually run big models would be good enough, even if slow.

- Caution: buying hardware to run specific models is not sustainable unless you are 110% satisfied with the model at hand and will not find the urge to upgrade to something even better. Its a race against big datacenters and you might as well just rent the hardware.
  - Actual answer: Stack 3090s, 4090s, however cheap you can get them. thats how you run big models fast and "cheap" (relatively speaking). RAM Prices are skyrocketing and it is entirely unfeasable imo to buy (64gb 6000mhz is $360+++). 3090s for 700-800 are way higher performance. 
  - Rule of thumb i use: DDR5 6000Mhz is 10x slower than a 4070's VRAM. If u can get 10x more RAM than VRAM for the same price, speeds *can* be compared, but then u start looking at 6-channel motherboards etc.
  - tl; dr: due to ram situation, just stack 3090s while you can. nothing else beats its price/performance. good luck

- A good alternative to the 3090 for running large MoE models is the AMD Mi50 32gb, much cheaper and has 1tb memory bandwidth due to the HBM2 memory
- 8xMI50: qwen3 235B Q4_1 runs at ~21t/s with 350t/s prompt processing (llama.cpp)

- an M3 ultra will have very bad prompt processing, so if you use any prompts longer than 8k or so, get ready for a world of pain. MoE only helps so much too.
  - Partial offloading, especially with MoE, is still mainly RAM Bandwidth bound, im not 100% sure what 4 or 6 channel motherboards are available, and if you can get enough high speed RAM at any price below 10k to make that worthwhile.
  - If you want an out of the box option that isnt speed optimized, m3 ultra works. If you at all have speed requirements (and by that i mean, not waiting half an hour for a big response), i dont think 10k will cut it without some heavy DIY (e.g. crazy motherboard with tons of PCIe, think older threadrippers, to use all those gpus)
- Remember there are models such as Qwen 3 Next with closer to linear prompt processing time, and more such models will continue to appear over the coming years :) also even with large system prompts (ie for agentic uses), you can cache those and get decent performance. At the moment we're just throwing compute at everything, but the algorithms and implementations will become more efficient over time.

- ## üÜö [Benchmark Results: GLM-4.5-Air (Q4) at Full Context on Strix Halo vs. Dual RTX 3090 : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1osuat7/benchmark_results_glm45air_q4_at_full_context_on/)
  - I benchmarked the GLM-4.5-Air (Q4) model running at a near-maximum context on two very different systems: a Strix Halo APU and a dual RTX 3090 server. Both tests were conducted under Debian GNU/Linux with the latest llama.cpp 
- Why is the prompt processing for RTX is so low? It should be hundreds if not thousands. The model is too big and you have to spill some part to regular system RAM? These prompt processing figures make it unusable in such setup then
  - This is essentially a pp119702 test. I'm sure their numbers are MUCH higher at 0 context.

- Numbers from my system: RTX 5090 + pcie5 x16 + DDR5-6000 and 46 MoE layers offloaded to the CPU:
  - PP 31.2x faster than your Strix Halo machine
  - PP 10.7x faster than your Dual 3090 machine (you are prob limited by slow pcie)
  - TG 2.1x faster than your Strix Halo machine
  - TG 1.7x faster than your Dual 3090 machine

- vllm will be much faster for dual GPUs.

- ## [I tested Strix Halo clustering w/ ~50Gig IB to see if networking is really the bottleneck : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1ot3lxv/i_tested_strix_halo_clustering_w_50gig_ib_to_see/)
  - TLDR: While InfiniBand is cool, 10 Gbps Thunderbolt is sufficient for llama.cpp.
  - During inference, I observed that the network was never used at more than maybe ~100 Mbps or 10 MB/s most of the time, suggesting the gain might not come from bandwidth‚Äîmaybe latency? But I don't have a way to prove what exactly is affecting the performance gain from 2.5 Gbps to 10 Gbps IP over Thunderbolt.

- Llama cpp doesn‚Äôt use tensor parallel so everything is done sequentially. This test was meaningless. You need to test it with TP on VLLM or Sglang
  - As I state in the post, there is no RCCL support. Without RCCL support, frameworks like vLLM and PyTorch can't perform collective operations (all-reduce, all-gather, etc.) across multiple nodes. This is the fundamental blocker for tensor-parallel inference on Strix Halo‚Äîyou literally can't split a model across nodes without these primitives. It's always the software support that's lacking on the AMD side
- It's meaningless because:
  - Pipeline parallelism only help you run models that you can't fit in a single node. It can't be faster than the single slowest node. So there is no sense testing it for performance, unless you want to test for performance bugs in implementation.
  - Using pipeline parallelism, the network transfer between nodes are minimal. Each token only has 2880 elements of embedding. Even you use 100Mbps network it's only like 1ms time for a token. So what are you trying to test?

- I believe you can use GLOO instead if NCCL is not available (I assume RCCL is the rocm version).

- It is not meaningless at all. It's quite meaningful since network speed is a topic that often comes up. You don't have just be doing TP for it to be of interest.

- As expected. I don't find the difference to be substantial between 2.5 to 10 to 50. Sure, it gets a little faster but not nearly as much as the increase in network speed would suggest. Not enough for me to pay several times more for a 10GBE network versus 2.5GBE.

- ## [Mac vs. Nvidia Part 2 : r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1opo89e/mac_vs_nvidia_part_2/)
  - I recently purchased a Mac Studio M4 Max w/ 64GB (128 was out of my budget). I also was able to get my hands on a laptop at work with a 24GB Nvidia GPU (I think it‚Äôs a 5090?).
  - I was shocked how less capable the Nvidia GPU is! I loaded gpt-oss-20B with 4096 token context window and was only getting 13tok/sec max. Loaded the same model on my Mac and it‚Äôs 110tok/sec. I‚Äôm running LM Studio on both machines with the same model parameters. 
- The laptop 5090 has 24gb
  - Yep and it is essentially a 5080 desktop crammed into a laptop (Nvidia always does this).

- 13 tokens/second sounds right if you load gpt-oss-20b into some dual channel DDR5 system memory. I don't use LM Studio personally but by any chance did you not tell the 5090 rig to load any layers into the GPU?
  - So I had that problem at first where LM studio was not loading all the layers into the GPU and the utilization stayed low. But I changed a setting that forces the model to be loaded exclusively to the GPU and utilization when up, but the gain was only like 3-4tok/sec speed up
- You must be running that model with layers on the CPU or you're mistaken about which GPU your laptop has

- Remember the 5090 mobile is the 5080 desktop chip (GB203) but upgraded to use 3GB memory modukes instead of 2GB like the 5080. Like most laptop gpus it is comparably power and heat limited compared to the desktop equivalent (5080).

- Make sure you have all the layers of the model loaded onto the GPU. 

- ## [If I want to train, fine tune, and do image gen then... DGX Spark? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1om7ccz/if_i_want_to_train_fine_tune_and_do_image_gen/)
- Apple is no good for finetuning/training, only for inference afaik.
  - AMD works, I guess. Story of their hardware in AI.
  - Both Apple and AMD probably don't work for image stuff (especially for anything niche, less supported). It's usually based on CUDA.
  - If you just want to "do everything AI", DGX will work and it will be slow-ish and if you can live with that (or optimize using MXFP4 etc.), it's perfect for you. If you got dem cash of course.

- ## ü§î [Best setup for running local LLMs? Budget up to $4, 000 : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1olsh0j/best_setup_for_running_local_llms_budget_up_to/)
- Depends on what kind of route you want to go. Do you want an open mining rack style build with the potential to have 8 GPUs? Do you want a system that fits into a standard ATX case? What size models do you want to run? Do you want to do image or video generation?
- I've done a few builds recently and the two best value routes I found are for a DDR4 based system:
  - Huananzhi H12D-8D  ÂçéÂçóÈáëÁâå
  - AMD EPYC 7532 (The cheapest EPYC Rome that gives you full memory bandwidth.)
  - AMD EPYC 7D12 (Best value EPYC, downside is missing half of the memory bandwidth.)
  - DDR4 2133 to 3200 RDIMM 8x16GB (128GB) or 8x32GB (256GB).
- For a DDR5 system:
  - GIGABYTE MS03-CE0
  - Intel 8480+ QYFS 
  - DDR5 4800 RDIMM 8x16GB (128GB) or 8x32GB (256GB)
  - I built/purchased the above parts with 256GB of memory for around $2, 000, the price of the memory was half of the build. Currently the memory price has risen by +50% and 256GB of memory is around $1, 500.
- For the GPUs the best value for text only inference is still the MI50 16GB (~$150) and the MI50 32GB (~$250-300, price has risen could have been had for around ~$150 1-2 months ago). 
  - If you want a more plug and play experience or want to do image/video generation, then you're probably still looking at getting 3090s (~$700). 
  - There are other GPUs to look for, but I'm sure they'll be recommended by others in this thread.
- With the above parts you have two routes. 
  - You can either go for GPUs and run smaller models fast, or you can go for a hybrid approach with a single 3090 for prompt processing and put the rest of the money into memory to run a larger model at a slower speed (Deepseek at home). 
  - If you want an example build of the top end you can have with CPU+GPU hybrid, then this video is a good comparison point. The video showcases 12-channels of DDR5 5600 with a 3090 getting 15t/s with DeepSeek_V3_0324 Q4_K_M. Using that as a comparison point, you'd expect the DDR5 4800 system above to be around ~7-9t/s and the DDR4 3200 system to get ~4-6t/s.

- 4x 3090s and whatever else supports this
- My 4x3090 rig is loud, uses a lot of power, puts off a lot of heat, and seems to require constant care and feeding. The DGX Spark sits on my desk, is quiet, and "just works". Unless you need to eek out the absolute most inference performance per $ spent, I would go with the turn-key solution and call it a day.

- If you go with multiple GPUs, you need a server mobo in order to get a server CPU that has enough pci-e lanes. GPUs want 16x each even though you can get by on less
  - Server motherboards are meant to run in very loud 2U chasis' with high airflow. I stuffed mine into a large PC case but have extra fans and an AIO CPU cooler for SP5.
  - Also, server CPUs a generally woser at gaming and single threaded performance than consumer CPUs.
  - If money is no object a ThreadRipper is the way to go.

- ## [Building PC in 2026 for local LLMs. : r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1ol3lcy/building_pc_in_2026_for_local_llms/)
- you can run a quantized deepseek-v3.1-terminus with 671b params at roughly 20 t/s, with full 128k context, using a single 5090 if your CPU + RAM is beefy enough, and if you're using `ik_llama.cpp` .
  - 2x AMD Epyc 9355 and a shit ton of RAM ought do it. My server build has 768 gb RAM and I use it to power Roo Code and SillyTavern

- You will get much closer to your goal with an nVidia RTX pro 5000 blackwell, with 72 gb of vram, >$6K. Another option is something like strix halo computers, ~$2k. Will be slow around 10 tps, but is faster than most can read.
  - Rumor has it that in more than a year, AMD will release medusa halo, that has twice the memory and twice the speed of strix halo

- No. No local model running on a single GPU (or even several) can match the huge professional cloud models in quality. We do local models for privacy and for the fun of hobbying.
  - local models can be fine tuned locally into very specialized models that can do specific things much better than the SoTA cloud models. This gets pretty niche

- ## [DGX Spark Benchmarks (Stable Diffusion edition) : r/StableDiffusion _202510](https://www.reddit.com/r/StableDiffusion/comments/1ogjjlj/dgx_spark_benchmarks_stable_diffusion_edition/)
  - tl; dr: DGX Spark is slower than a RTX5090 by around 3.1 times for diffusion tasks.
  - I happened to procure a DGX Spark (Asus Ascent GX10 variant). This is a cheaper variant of the DGX Spark costing ~US$3k, and this price reduction was achieved by switching out the PCIe 5.0 4TB NVMe disk for a PCIe 4.0 1TB one.
  - Based on profiling this variant using llama.cpp, it can be determined that in spite of the cost reduction the GPU and memory bandwidth performance appears to be comparable to the regular DGX Spark baseline.
  - Now on to the benchmarks focusing on diffusion models. Because the DGX Spark is more compute oriented, this is one of the few cases where the DGX Spark can have an advantage compared to its other competitors such as the AMD's Strix Halo and Apple Sillicon.
  - While the DGX Spark is not as fast as the Blackwell desktop GPU, its performance puts it close in performance to a RTX3090 for diffusion tasks, but having access to a much larger amount of memory.

- 3 times slower than a 5090 so‚Ä¶. Roughly the equivalent of a 5060ti?
  - Their benchmark for SDXL 3.13it/s makes it roughly equal to 3090, that also has about 3it/s.
- If we did the math purely based on TFLOPs/ in FP16/BF16, it would indicate closer to 5070-like performance.

- You can find many benchmarks for AMD AI MAX and compare. Nothing spectacular here, because inference performance limited by memory bandwidth, and DGX has 270Gb/s while AMD thingy has 250Gb/s.

- ## [Running DeepSeek-R1 671B (Q4) Locally on a MINISFORUM MS-S1 MAX 4-Node AI Cluster : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1oevvyu/running_deepseekr1_671b_q4_locally_on_a/)

- So you spend $20, 000 to get 5 TPS. You could have spent $1000 and run it on ram/cpu and got the same speed.

- ## [Fast loading and initialization of LLMs : r/LocalLLaMA _202407](https://www.reddit.com/r/LocalLLaMA/comments/1e3hknt/fast_loading_and_initialization_of_llms/)
  - Now normally I use vLLM to serve LLMs, but it seems to be very slow at loading up models. For Llama 8 FP16 it takes 8s to be ready from a warm cache. Qwen 14 takes 11s with a warm cache. Way too slow.
  - The above is for a single 3090 on PCIe 4.0 x16, 4 sticks of DDR4 RAM rated at 3200 on a X570s motherboard with a Ryzen 5600X CPU.

- A co-worker and I are building a local LLM server for testing (see my profile) and we have looked at various ways to maximize loading of LLM files across different CPU brands/architecture for a customer.
  - We did consult with one of our biggest customers who has a few dozen local LLM workstations and they all have AMD Threadripper 7980X boxes, 192GB of RAM (4x48GB) with a pair of RTX A6000 48GB cards w/ NVLink bridge and dual 15TB U.2 NVMe drives. They have moved to ZFS w/o L2ARC and storing all their LLM files on 30TB worth of U.2 NVMe drives and a 168GB ARC.
  - We are building a Splunk app for them to monitor their usage of LLM apps and models.
  - TL; DR: Use ZFS and set ARC to cache as many LLMs as you can for faster transfer to VRAM. Or if you only need to test a coupe of LLM constantly, then create a RAM disk and copy your LLM files there.

- ## üÜö [What laptop would you choose? Ryzen AI MAX+ 395 with 128GB of unified RAM or Intel 275HX + Nvidia RTX 5090 (128GB of RAM + 24GB of VRAM)? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o3evon/what_laptop_would_you_choose_ryzen_ai_max_395/)
- more net RAM = more net model + context size, at whatever speed.

- The Ryzen and run the larger models like gpt oss 120b or glm 4.5 air. The 5090 will run models that fit much faster like Qwen3 30b variants.
  - The Ryzen may be very slow on promo processing. A big deal if you are dropping 50k or more tokens. You may be waiting a few minutes before the output starts.

- Depends, do you want to run larger models somewhat slowly - 395 Or do you want to run smaller models and larger MoE models very quickly - 5090

- depends what you want to do, if you are gaming then the 5090 makes more sense, for ai the 395 makes more sense since you will be able to run better stuff.

- For LLMs the Ryzen can run large MOE models much faster, but the one with the 5090 can run smaller models that fit into the 24 GB vram really really fast. For image or video generation, Nvidia unfortunately is still the top choice, it usually takes some time until new models are supported on Amd 

- I have GMK x2 and I'm killing this machine with everything you can think about, run everything even Q2 GLM4.6 with 10 tokens (not bad for 115G) model all in ram.

- 
- 
- 
- 
- 
- 
- 

- ## [4x64 DDR5 - 256GB consumer grade build for LLMs? : r/LocalLLaMA _202504](https://www.reddit.com/r/LocalLLaMA/comments/1k6p20z/4x64_ddr5_256gb_consumer_grade_build_for_llms/)
  - I have recently discovered that there are 64GB single sticks of DDR5 available - unregistered, unbuffered, no ECC, so the should in theory be compatible with our consumer grade gaming PCs.
  - Both AMD 7950x specs and most motherboards (with 4 DDR slots) only list 128GB as their max supported memory - I know for a fact that its possible to go above this, as there are some Ryzen 7950X dedicated servers with 192GB (4x48GB) available.

- I have 64GB of DDR5-6000 and it is great at inference - of models that don't take more than around 16GB (preferably 10GB) - anything bigger becomes too slow to use.

- If you're going for a CPU based build, you want to go for epyc, not a consumer CPU.
  - It won't be super fast; expect memory speed of around 200GB/s, so about 1/5th the performance of a 3090 or 4090 in token generation, and maybe 1/10th in processing speed.

- Yeah, I've got 128Gb of DDR4 3200, now I am running 110Gb models with 0.3t/s

- On desktop Zen 4/Zen 5, I wouldn't recommend doing that.
  - You're quite limited by the Infinity fabric bandwidth, limiting you to a max of 62-68GB/s on DDR5-6000 to 6400, while theoritical DDR5 6000 128-bit is 100GB/s.

- I'm running a Ryzen 9 7900X on MSI PRO B650M-A WIFI AM5 Micro-ATX with 256GB using 4 of those 64GB DDR5 sticks. So it is possible. Your memory bandwidth drops, as you need to slow the memory down to stay stable. If you are building from scratch you may want to use a CPU with more memory channels.

- ## [Did someone ever benchmark how cpu inference performs with quad and eight channel memory ? : r/LocalLLaMA _202401](https://www.reddit.com/r/LocalLLaMA/comments/1920l93/did_someone_ever_benchmark_how_cpu_inference/)
  - Since people always say that bandwidth is the problem. And a full 8 channel memory board with ddr4 3200 would be about 200gb/s per second i was wondering if anyone ever benchmarks that stuff and how it scales with cores ?

- I have a 8ch epyc build, after some experiments i have found that the effective bandwidth is about 135 gb/s, so a 40gb model is ~ 3, 3 t/s, a 20gb is twice the speed.

- Intel Xeon E5-2680 v4, 128GB DDR4 2400 RAM 4 chanels. llama.cpp, model: mixtral8x7b Q5_K_M 6 tokens/sec

- R720 2x xeon 2670, 192gb ddr3-1333 dram, llama.cpp running mixtral q3_k_m quant w/ 10k context, pure cpu inference: 3.6 t/s. If use installed P40: 9.1 t/s

- ## [AI setup for cheap? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ovbzi3/ai_setup_for_cheap/)
  - My current setup is: i7-9700f, RTX 4080, 128GB RAM, 3745MHz. In GPT, I get ~10.5 tokens per second with 120b OSS, and only 3.0-3.5 tokens per second with QWEN3 VL 235b A22b Thinking. 
- If you're only getting 10 tok/s, you're probably not using GPU at all. I have i7 13700k with a 4090. I get 38 tok/s with GPU, and 11 tok/s with only CPU.
  - If you're running llama.cpp, did you compile with CUDA support. Did you remember to set your -ngl 99 flag? using --n-cpu-moe instead of -ot exps=CPU?
  - Try llama-server -ngl 99 --n-cpu-moe 32 -ngl 99 -c 50000 -fa on -m file/to/model.gguf --no-mmap -t 8 -ub 2048 -b 2048 --jinja

- The CPU doesn't matter, the CPU's memory speed (i.e. your ram) determines TPS

- ## [AI LLM Workstation setup - Run up to 100B models : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ov7idh/ai_llm_workstation_setup_run_up_to_100b_models/)
  - Planning to buy 320GB DDR5 RAM (5 * 64GB) first. Also with high MT/s(6000-6800 minimum) as much as possible to get better CPU-only performance. In future, I'll buy some more DDR5 RAM to make that 320GB to 512GB or 1TB.

- 50+t/s with 30-50B Dense models is not possible for CPUs. As you need 20GB(32B Q4)x 50 ~= 1000GB/s of bandwidth, which is impossible before Epyc Venice or Diamond Rapids Xeon.

- Planning to buy 320GB DDR5 RAM (5 * 64GB) first
  - Don't do 5. In order to get maximum bandwidth you need to have an even number of DIMMs all the same size (and there might be some restrictions beyond that depending on CPU). If you have 5 you'll have 4*64GB of 'fast' memory and 64GB of slow memory. Keep in mind also that if you only have 5 out of 8 DIMMs installed you only get 5/8 the maximum memory bandwidth for the platform which directly impact your performance.

- "50+t/s with 30-50B Dense models"? A 6000 Blackwell can barely do that: I get 58t/s with Qwen3-32B-Q4. My 400W Epyc with 12x 5200MHz RAM only gets 14-18t/s.
  - The only reason CPU is usable with MoE is because the amount of RAM needed and the fact that bandwidth is often the bottleneck before compute and even then it's medeocre unless you offload the attention calculations which are more compute than memory bound.

- Optimized Power saving Setup
  - You seem to be confusing power draw with efficiency. Running a 200W CPU for 5min is not better than a 600W GPU for 1min. Get a RTX 6000 Max-Q, which runs the models you want and is one of the most efficient inference engines that are available. My Epyc system idles at ~90W while a Max-Q idles at ~15W and can be put in some <40W desktop.

- ## ü§î [Budget LLM pc builds, new CPU only approaches : r/LocalLLaMA _202410](https://www.reddit.com/r/LocalLLaMA/comments/1fycnc1/budget_llm_pc_builds_new_cpu_only_approaches/)
  - iGPU and lot's of memory: Like using 192gb of DDR5 RAM on the AMD Ryzen 9 7950x iGPU, or the budget Ryzen 5 8500G?
  - AVX-512: Llamafile now has AVX-512 Support, meaning 10x Faster Prompt Eval Times For AMD Zen 4, like AMD Ryzen 9 5900X?

- Unfortunately the answer is no. Even if you can build a consumer PC with a lot of DDR5 Ram, they cannot take advantage of it for LLMs, for the following reasons:
  - they have limited memory bandwidth for large models.
  - they have limited compute throughput for long context.
- Only 64-96 core Genoa EPYCs with 12-channel DDR5 RAM (and the new Intel Granite Rapids) can approach practical levels of performance, and this is only for models up to 70GB . But these are certainly not budget options.

- 2 channels of ddr5 9000/10000 should be something like 150 GB/s. Thats more than enough to have a real time conversation with the current gen 10B class models. Even the 30B+.

- Faster RAM, more channels (e.g. Strix Halo and HEDT with 4, EPYC with 8-12), and faster/more cores+AVX512 can enable the practical use of slightly bigger models but still far smaller than the 96-192GB people are discussing about.

- let's define model performance first. There are 2 stages: Prompt Processing (the model reading your input) which is compute bound, and Token Generation (the model writing its response) which is memory bound.
- The rule of thumb for token generation is:
  - `tokens/sec = memory_bandwidth_in_gb_per_sec / model_size_in_gb`
- The actual bandwidth which can be achieved is ~70-75% of the theoretical.
  - A dual-channel 3600MT/s DDR4 system has 51.2 GB/s theoretical memory bandwidth. Therefore, the actual achievable bandwidth is between 35.8 - 38.4 GB/sec.
  - An example model: Qwen2.5 7b when quantized to 8 bits is 8.1GB (by default models are shipped to FP16 which is double that).
  - So, you may expect a token generation speed between 4.4 and 4.7 tokens/sec (38.4 / 8.1) for this model. Half of that for the FP16 version of the model, and double of that for the Q4 version of the model.
- That's why large (dense) models don't make sense in CPUs. A 70GB model (e.g. Llama-3.1 70b q8_0) would be slower than 1 t/s. You need 8+ channel memory to reach practical speeds.
- ü§î But if you need to process long context (summarize documents, explain/debug/enhance code, continue a story, have long chat sessions) then the Prompt processing is compute bound, therefore compute throughput is very important and this is where modern GPUs seriously outperform CPUs by 25-100x due to their tensor cores.
  - For the above model, you may achieve 60 tokens/sec for prompt processing with a CPU and about 2500 tokens/sec with a RTX 3090. Therefore, if you want to summarize a 1500 words document, it will take 1 second in the GPU and 40 seconds in the CPU (just for the model to ingest your input).
- P. S. CPUs become practical with Mixture of Experts (MOE) models because the active parameters are usually 15-25% of the total, therefore the speed is 4-6x of that of a dense model.

- My understanding is that due to the memory bandwidth being a bottleneck, using a NPU or iGPU isn't any faster than simply using the normal CPU cores themselves when you're limited to two channel memory. CPU only can certainly be useful, and I think people are too quick to discount it as an option for small context sizes and smaller models 

- ## üÜö [Thread for CPU-only LLM performance comparison : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nj4axf/thread_for_cpuonly_llm_performance_comparison/)
  - I could not find any recent posts about CPU only performance comparison of different CPUs. 
  - With recent advancements in CPUs, we are seeing incredible memory bandwidth speeds with DDR5 6400 12 channel EPYC 9005 (614.4 GB/s theoretical bw). 
  - AMD also announced that Zen 6 CPUs will have 1.6TB/s memory bw. The future of CPUs looks exciting. 
  - For this CPU only comparison, I want to use ik_llama 
  - use CPU only inference (No APUs, NPUs, or build-in GPUs allowed)
- llama-bench benchmark (make sure GPUs are disabled with CUDA_VISIBLE_DEVICES="" just in case if you compiled for GPUs):
  - qwen3moe 30B Q4_1: 38.9
  - GPT-OSS 120B Q8_0: 24.7

- those dusty EPYCs/Xeons with fat memory channels you see on eBay suddenly look like budget LLM toys..it; s crazy that decommissioned gear can outpace shiny new desktop CPUs for this niche.

- That's my server, I think there are some config issue here as using thread 64 would be much slower, maybe I should enable HT.
  - CPU: 1S Epyc 7B13(64c, HT disabled manually)
  - RAM: 8 x 64GB DDR4 2666
  - Motherboard: Tyan S8030GM2NE
  -  qwen3moe 30B Q4_K: 31.0 
  -  gpt-oss 120B F16: 14.9
-  Yes, there is definitely something wrong with the server in your case. You should get better results than my server.

- [CPU Only OSS 120 : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o6d4a6/cpu_only_oss_120/)
  - just try it on CPU only (gulp) on my home lab server and actually it's more than usable at a fraction of the power cost too. This is also running in a VM with only half cores given.
  - prompt eval time = 260.39 ms / 13 tokens ( 20.03 ms per token, 49.92 tokens per second)
  - eval time = 51470.09 ms / 911 tokens ( 56.50 ms per token, 17.70 tokens per second)total time = 51730.48 ms / 924 tokens

- ## [Looks like Intel Arrow Lake can support 4 DIMMs @ up to 6400 speeds : r/LocalLLaMA _202411](https://www.reddit.com/r/LocalLLaMA/comments/1gindy1/looks_like_intel_arrow_lake_can_support_4_dimms/)
  - After searching through a few boards, it looks like Arrow Lake can do 4 Dimms @ 6400. For an ASrock example, see below - select vendor "Corsair", and there is a 24GB per DIMM options @ 6400. Crucial and ADATA have 48GB "4 channel" options @ 5600.
  - Anyway just wanted to pass along that we may see "certified" 6400+ speed 4 DIMM setups become common with Arrow Lake (Core Ultra 200 series). An x86 way to have 192GB-256GB (when DIMMs are available) on a standard desktop at reasonable speed.

- But they are only dual channel, so you can expect at most around 100 GB/s of memory bandwidth.
  - Missed opportunity...

- I‚Äôm running 6200 stable on 128gb 4 dimms with Ryzen 7950x3D since 1+ year lol

- 4 dimms does not mean 4 channels. MBs have had 4 slots forever. Arrow Lake is dual channel.
- Arrow lake is dual channel. 4 dimms does not mean 4 channels. MBs have had 4 slots forever.

- it actually has 4 channels. It has the ability to address each half of the ram seperately. And it's a 4 channel controller, for 4x 32Bit. It's just that that's the same bandwidth as 2x 64 bit.

- [Is 96GB of DDR5 6800Hz RAM enough for training? : r/LocalLLaMA _202402](https://www.reddit.com/r/LocalLLaMA/comments/1b1e4z8/is_96gb_of_ddr5_6800hz_ram_enough_for_training/)
  - I went with 96GB of fast RAM thinking that it would be more than enough, but I've been seeing that people recommend at least 128GB for training and interfacing.
- What is your ram capacity and ram speed, I'm running 4x32gb at 6200mhz which speeds up inference. You can run AIDA to benchmark your ddr memory speed bandwidth
  - I want to get 128 gb of ram 6000MT/s. Cl30. But most people say that there is no way the ryzen 9 7950x3d could reach that at all without it melting or being unstable. Even 5200 would be a miracle. Apparently for am5, 4 DIMMS should not be used. You shouldn‚Äôt quadrank. The most you can go for is 96 gb ram 6400MT/s (2x48). So how did you do it? Please teach me

- ## [CPU RAM only speeds on 65B? : r/LocalLLaMA _202307](https://www.reddit.com/r/LocalLLaMA/comments/14q4d0a/cpu_ram_only_speeds_on_65b/)
- I have both Dual RTX 3090s+NVLink and 128GB RAM (@3200) and for 65B models, using the CPU (i have a 3rd gen 8 core Ryzen) is just too slow. It's around 1 token per second, far from 7/s. From what i've seen getting a better CPU (16 cores) doesn't help much.

- The 7950x with DDR5 6000 on a 65b_4_ks model is 1.75t/s.
  - When it comes to token generation speed, the core count doesn't really matter. What does matter is the RAM bandwidth.
  - However, if you don't have a GPU, then the core count becomes important for prompt evaluation speed. But it's not worth buying a high-end CPU just for this.
  - In my opinion, if you can tolerate a speed of 2t/s on 65b models, the most cost-effective option would be to go for the 13400f($170)processor (disable the E-cores), paired with 64GB of high-frequency DDR5 RAM, and a second hand RTX 2060 for just $100. This GPU can be used for prompt processing and offloading as many layers as possible.

- I have CPU Ryzen 9 3950X and 64Gb RAM at 3600MHZ. The airoboros-65b-gpt4-1.4.ggmlv3.q5_K_M.bin generates at the average of 1077ms per token, with very small variance actually.

- ## [Is RAM latency very relevant for LLMs (Ollama)? : r/LocalLLaMA _202411](https://www.reddit.com/r/LocalLLaMA/comments/1gws9yp/is_ram_latency_very_relevant_for_llms_ollama/)
- No overclocking will not produce notable gains. When you offload to CPU your major bottleneck is the processing not memory bandwidth. You'll do a lot of work and adding potential instability for something that you won't really notice, maybe an extra token per hundred generated.
  - You are confidently wrong. I went from 3.45 tokens/s to 5.29 tokens/s just by enabling XMP profile (2666 MHz to 3600 MHz).

- ## [Will DDR6 be the answer to LLM? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o0i4fz/will_ddr6_be_the_answer_to_llm/)
  - Bandwidth doubles every generation of system memory. And we need that for LLMs.

- I think the combination of smart quantization, smarter small models and rapidly improving RAM will make local LLM's inevitable in 5 years.

- Prompt processing will be even more critical with faster RAM - you need lots of compute for larger models, DDR6 will be used for, and CPUs do not have enough compute. You still absolutely would need GPU.

- Isn't Apple unified memory just multi channel RAM? It does deepseek fairly well.
  - Unified memory without upgradable ram is such a double-edge sword. I want it but I don't want it to be "The future"

- ## [How fast big LLMs can work on consumer CPU and RAM instead of GPU? : r/LocalLLaMA _202407](https://www.reddit.com/r/LocalLLaMA/comments/1edryd2/how_fast_big_llms_can_work_on_consumer_cpu_and/)
  - Would not it be cheaper to build a PC with 256-512 GB of RAM and run very big models on it than buying two Rtx 3090 and having only 48gb of VRAM?

- I'll get some example numbers with Llama 3.1 8B Instruct Q6_K with a context size of 8192 tokens.
  - Running on my RTX 4060 Ti: 25.46 tokens/s
  - Running on my Ryzen 5 7600: 6.66 tokens/s
  - As you can see, CPUs are the devil.
  - The RTX 4060 Ti's memory bandwidth is 288 GB/s, and my RAM is 81.25 GB/s (dual-channel DDR5 5200), and dividing those numbers comes out to close to the same ratio--while the GPU memory is 3.54x as fast, using the GPU for inference is 3.82x as fast.
  - Using shared memory with the GPU is far worse because PCI-e 4.0 x8 is only about 16 GB/s one way (PCI-e 5.0 is only twice that fast).

- I built a pc with 128G RAM , I9 14900K and one 4090 GPU. I have loaded Llama3.1 70B Q2 with Ollama, and test it, the token per second is about 9. Then I loaded Mistral Large 2 123B Q2, the token per second is about 2

- 14900K has only two memory channels which gets bandwidth bottleneck at 6 threads alone! So you get almost same CPU performance as me with 12700H in a laptop. I wish you researched more before building such a system there was no point of buying 14900K at all neither for LLMs or gaming expect you use it for something else ofc..

- For running LLMs on CPU you must buy something with at least 8 memory channels. Dual channel CPUs get bandwidth bottleneck at 6 threads alone and begin loosing performance severely as you increase thread count. Not cores rather only threads! So with 8 memory channels you can use 24 threads and gain around 4 times more performance. (Exact performance depends on clock, memory speed etc ofc but it is roughly like this.)

- ## [Running LLMs partially on cpu. DDR5 R1(single rank) vs R2(dual rank) : r/LocalLLaMA _202403](https://www.reddit.com/r/LocalLLaMA/comments/1b3vhc7/running_llms_partially_on_cpu_ddr5_r1single_rank/)
  - 2x32gb, dual rank(8x32bit/256bit), 6800mhz, 13600 MT/s, total bandwidth 217.6 GBps
  - 2x24gb, single rank(4x32bit/128bit), 7800mhz, 15600 MT/s , total bandwidth 124.8 GBps

- I would go for the first one with much more bandwith. As far as I know this usually is the bottleneck. But I haven't benchmarked anything like it myself.
  - 217 GBps sounds almost too good to be true. Only 3.5x slower than 4080's bandwidth.

- ## [Anyone actully try to run gpt-oss-120b (or 20b) on a Ryzen AI Max+ 395? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nabcek/anyone_actully_try_to_run_gptoss120b_or_20b_on_a/)
- people post actual good benchmarks, 49T/s on TG and 700T/s on PP. 
  - Better than my 14900k (96GB 6800) + RTX3090: (32T/s on TG and 220-280T/s on PP).

- Ryzen 7950X + 3080 + 96GB DDR5-6000 gets me around 330T/s on PP and 15 on TG for the 120b

- ## [Most economical way to run GPT-OSS-120B? : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1n13rsq/most_economical_way_to_run_gptoss120b/)
- I get 100t/s on short (2-3k) context and ~85t/s on longer context (12-14k) using three 3090s.
  - Two Mi50s and a Cascade Lake ES Xeon get me ~25t/s on the same 12-14k context. 

- Framework PC - 128GB LPDDR5x-8000 ! I have 5090 + DDR4-2933 = 18 t/s for GPT-OSS-120B

- Also 5090 + DDR5-6000 (offloading 22 layers) = ~35 t/s for GPT-OSS-120B (full precision ‚âà 60GB)

- Given that you already have a PC, I think, the most economical way to run gpt-oss 120b at good speeds is to buy 3 used rtx 3090. It will give you 72gb of vram and it will cost you around 1800 + PSU. It will be roughly the x4 speed of the Framework. 

- I have a Framework Desktop with 64GB. It almost explodes with GPT-OSS-120b loaded but I can actually run the 4bit version for a question or two until it fails and it runs at 50 tokens/s, even with the system having been exiled to SWAP. (openSUSE Tumbleweed, llama.cpp)

- ## [gpt-oss-120b on CPU and 5200Mt/s dual channel memory : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mj6xif/gptoss120b_on_cpu_and_5200mts_dual_channel_memory/)
  - I have run gpt-oss-120b on CPU, I am using 96GB dual channel DDR5 5200Mt/s memory, Ryzen 9 7945HX CPU. I am getting 8-11 tok/s. I am using CPU llama cpp Linux runtime.

- 5800x with 96gb of system ram DDR4 3200 in dual channel. Getting just over 5t/s with the 120, nothing offloaded to GPU

- ## [How to run gpt-oss-120b faster? 4090 and 64GB of RAM. : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mnsg6d/how_to_run_gptoss120b_faster_4090_and_64gb_of_ram/)
  - llama-server --hf-repo unsloth/gpt-oss-120b-GGUF --hf-file gpt-oss-120b-F16.gguf ^ -c 16384 -ngl 99 -ot ".ffn_.*_exps.=CPU" -fa ^
  - with 16k context here, I am getting around 14tps 

- I'm getting 35T/s and 120T/s prefill on a 3090 and 14900K but that is with 96GB of fast DDR5 (6800)

- ## [gpt-oss-120b performance with only 16 GB VRAM- surprisingly decent : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/)
  - GPU: RTX 4070 TI Super (16 GB VRAM)
  - CPU: i7 14700K
  - System RAM: 96 GB DDR5 @ 6200 MT/s (total usage, including all Windows processes, is 61 GB, so only having 64GB RAM is probably sufficient)
  - Model runner: LM Studio
  - 13 t/s is a speed that I'd consider "usable"

- Just posting my numbers too! 5090 + 60GB of DDR5, 22 cpu moe layers offloaded:
  - 36.61 tokens per second

- ## [10.48 tok/sec - GPT-OSS-120B on RTX 5090 32 VRAM + 96 RAM in LM Studio (default settings + FlashAttention + Guardrails: OFF) : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mk9c1u/1048_toksec_gptoss120b_on_rtx_5090_32_vram_96_ram/)
  - Just tested GPT-OSS-120B (MXFP4) locally using LM Studio v0.3.22 (Beta build 2) on my machine with an RTX 5090 (32‚ÄØGB VRAM) + Ryzen 9 9950X3D + 96‚ÄØGB RAM.
  - Everything is mostly default. I only enabled Flash Attention manually and adjusted GPU offload to 30/36 layers + Guardrails OFF + Limit Model Offload to dedicated GPU Memory OFF.

- Try llama.cpp with-ot ".ffn_(up|down)_exps.=CPU" This offloads up and down projection MoE layers instead of full MoE layers. You should get 30 t/s
  - I have a budget workstation that costs less than your GPU alone and I get 20 t/s with Unsloth their 120b. That is 20 t/s for the first 1K tokens, it slows down to 13 t/s at 30K context.
  - My specs: 16 GB RTX 5060 Ti + 16 GB P5000 + 64 GB DDR5 6000

- I can second that: With the same (short) prompt I get 17.9 t/s
  - Specs: 16 GB RTX 5060 Ti + 128 GB DDR5 5600 / Ryzen 9 9900X
  - .\llama-server.exe -c 60000 --chat-template-kwargs "{\"reasoning_effort\": \"low\"}" -fa -ctk f16 -ctv f16 -m "c:/....../gpt-oss-120b-GGUF/gpt-oss-120b-BF16.gguf" -ub 512 --temp 1.0 --top-p 1.0 --top-k 0 --min-p 0 --repeat-penalty 1.0 --no-mmap -sm none -ngl 99 --n-cpu-moe 44

- Thats really bad. I get 30T/s for 3090 + 14900K 96GB. 25T/s for the 14900K with just 8GB VRAM usage.
  - This is the trick: 
  - --n-cpu-moe 36 \    #this model has 36 MOE blocks. So cpu-moe 36 means all moe are running on the CPU. You can adjust this to move some MOE to the GPU, but it doesn't even make things that much faster.
  - --n-gpu-layers 999 \   #everything else on the GPU, about 8GB

- I get 9 t/s on integrated gpu in my thinkpad, you are doing something wrong

- I'll trade my 4070 Ti Super that gets 50 tokens/second for your ridiculously slow 5090.

- GPU: NVIDIA RTX 4000 SFF Ada Generation GPU: AMD ATI 04:00.0 Raphael Memory: 54.6GiB / 94.2GiB
  - eval rate: 8.03 tokens/s 

- ## [16‚Üí31 Tok/Sec on GPT OSS 120B : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1ndit0a/1631_toksec_on_gpt_oss_120b/)
  - CPU: Intel 13600k
  - GPU: NVIDIA RTX 5090
  - Old RAM: DDR4-3600MHz - 64gb
  - New RAM: DDR5-6000MHz - 96gb
  - Model: unsloth gpt-oss-120b-F16.gguf
  - 16 tok/sec with LM Studio ‚Üí ~24 tok/sec by switching to llama.cpp ‚Üí ~31 tok/sec upgrading RAM to DDR5
  - `llama-server --n-gpu-layers 999 --n-cpu-moe 22 --flash-attn on --ctx-size 48768 --jinja --reasoning-format auto -m C:\Users\Path\To\models\unsloth\gpt-oss-120b-F16\gpt-oss-120b-F16.gguf  --host 0.0.0.0 --port 6969 --api-key "redacted" --temp 1.0 --top-p 1.0 --min-p 0.005 --top-k 100  --threads 8 -ub 2048 -b 2048`

- You can get more speed on computers with hybrid cores (a mix of p and e cores) by pinning llama.cpp to p-cores only. 

- Maybe even some more speed to win by offloading only up and down projection MoE layers: https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune#improving-generation-speed
  - In my testing, the suggestion in that link is outdated.

- You should spend money on unified memory systems for models like this instead of on a strong GPU like 5090. For example, M4 Max has GPU equivalent to 4070 mobile, which is not super fast, but it can run this model at 75 t/s on llama.cpp and 95 t/s on mlx (though mlx implementation currently has slow PP speed).

- You want to only use 2 lanes if possible so 48x2. If you use all 4 ram slots your speed will be limited.

- For more "long running" sessions, servers, permanent agent running etc, llama.cpp, vLLM and especially ktransformers are FAAAAR better options.

- I am getting 10-11 tokens per second with GPT-OSS-120b on DDR5 4800, 7940HS CPU, 96GB RAM in LM Studio. Guessing I could get at least another 50% performance based on what you're saying
  - GLM 4.5 Air is running at 4-5 TPS with this setup. Qwen3 30b-A3B runs at about the same speed as GPT-OSS-120b.

- ## [gpt-oss-120b in 7840HS with 96GB DDR5 : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nf3fof/gptoss120b_in_7840hs_with_96gb_ddr5/)
  - With this setting in LM Studio Windows, I am able to get high context length and 7 t/s speed (noy good, but still acceptable for slow reading).
  - Is there a better configuration to make it run faster with iGPU (vulkan) & CPU only? I tried to decrease/increase GPU offload but got similar speed.
  - I read that using llama.cpp will guarantee a better result. Is it significantly faster?

- Don't force the experts onto CPU, just load them all in gpu, that's why you have the iGPU in the first place! You should be able to load ALL the layers on GPU as well.

- Loading all layer to iGPU will result unable to load vulkan0 buffer, I think because only 48GB can be allocated to my iGPU
  - No, . Put them all there, it will work. If dont, put 23 or so, do a tryout load. VRAM is also your shared ram, all equal. I got ryzen 7940hs, runing unsloth Q4-K-XL, with 20K context, its about 63Gb of space, i just put all on the GPU on LMstudio, ans just one processor on inference. I get 11 tokens per second, linux mint.

- Thoughts from someone who has the same iGPU and used to have 96GB memory:
  - Your offload config looks about right for your memory size (I wrote a comment about it on a lower message thread)

- ## [Managed to get GPT-OSS 120B running locally on my mini PC! : r/selfhosted _202508](https://www.reddit.com/r/selfhosted/comments/1mk6jlt/managed_to_get_gptoss_120b_running_locally_on_my/)
  - I was able to get the GPT-OSS 120B model running locally on my mini PC with an Intel U5 125H CPU and 96GB of RAM to run this massive model without a dedicated GPU, and it was a surprisingly straightforward process. The performance is really impressive for a CPU-only setup. 
  - MINIPC: Minisforum UH125 Pro
  - CPU: Intel u5 125H
  - RAM: 96GB
  - Model: GPT-OSS 120B (Ollama)
  - prompt eval rate: 31.83 tokens/s
  - eval rate: 2.77 tokens/s
  - This is running on a mini pc with a total cost of $460 ($300 uh125p + $160 96gb ddr5)

- ## [You can now run OpenAI's gpt-oss model on your local device! (14GB RAM) : r/selfhosted _202508](https://www.reddit.com/r/selfhosted/comments/1mjbwgn/you_can_now_run_openais_gptoss_model_on_your/)
  - The 20B model runs at >10 tokens/s in full precision, with 14GB RAM/unified memory. Smaller versions use 12GB RAM.
  - The 120B model runs in full precision at >40 token/s with ~64GB RAM/unified mem.
  - There is no minimum requirement to run the models as they run even if you only have a 6GB CPU, but it will be slower inference.
  - Thus, no is GPU required, especially for the 20B model, but having one significantly boosts inference speeds (~80 tokens/s). With something like an H100 you can get 140 tokens/s throughput which is way faster than the ChatGPT app

- ## [What hardware to run gpt-oss-120b? : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1miggb2/what_hardware_to_run_gptoss120b/)
- 64GB RAM with ~16GB - 24GB VRAM offloading, or just 128GB RAM.

- Is it really possible to run it with just 128 GB RAM and no VRAM?
  - I'm running it with 96GB RAM and 12GB VRAM and it's usable
  - my setup: 2x RTX 4070, 96GB DDR5 6400MHz RAM, Ryzen 9 7900X
  - It's not a memory issue with WSL (I know that an I've allocated 80 GB of memory to WSL), it just doesn't run very fast. I hit around 10-15 tokens/sec with CPU on Windows using LM Studio, but running it in the same LM Studio (or Ollama) on Linux does not go well. I'm getting around 0.8 tokens/sec, not sure why.
  - I'm running the 20B version at around 200-250 tokens/sec though, which is great
  - I can run it straight from CPU on 96GB and it seems to run about the same. I'm not sure. to stop LM Studio from using my GPUs, I ran it on a WSL instance without GPU access, it got the same-issue Tok/s, maybe slightly lower by the token per second, but really not that big of a difference when it's already that slow

- I was able to run gpt-oss-120b in LM-studio on a 5060ti 16Gb video card + 64Gb DDR4 RAM. I placed 8 layers in video memory, the rest in RAM. The performance was 10 tokens per second, for comparison, the younger model worked at a speed of 85 tokens per second.

- ## üí°üöß [gpt-oss 120B is running at 20t/s with $500 AMD M780 iGPU mini PC and 96GB DDR5 RAM : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1nxztlx/gptoss_120b_is_running_at_20ts_with_500_amd_m780/)
  - Everyone here is talking about how great AMD Ryzen AI MAX+ 395 128GB is. But mini PCs with those specs cost almost $2k. 
  - I searched for mini PCs that supported removable DDR5 sticks and had PCIE4.0 slots for future external GPU upgrades. 
  - I focused on AMD CPU/iGPU based setups since Intel specs were not as performant as AMD ones. The iGPU that came before AI MAX 395 (8060S iGPU) was AMD Radeon 890M (still RDNA3.5). Mini PCs with 890M iGPU were still expensive.
  - The cheapest I could find was Minisforum EliteMini AI370 (32GB RAM with 1TB SSD) for $600. Otherwise, these AI 370 based mini PCs are still going for around $1000.
  - Next, I looked at previous generation of AMD iGPUs which are based on RDNA3. I found out AMD Radeon 780M iGPU based mini PC start from $300 for barebone setup (no RAM and no SSD). 780M iGPU based mini PCs are 2x times cheaper and is only 20% behind 890M performance metrics. 
  - I checked many online forums if there was ROCm support for 780M. Even though there is no official support for 780M, I found out there were multiple repositories that added ROCm support for 780M (gfx1103)
  - üñ•Ô∏è I bought MINISFORUM UM870 Slim Mini PC barebone for $300 and 2x48GB Crucial DDR5 5600Mhz for $200. I already had 2TB SSD, so I paid $500 in total for this setup.
  - There was no guidelines on how to install ROCm or allocate most of the RAM for iGPU for 780M. So, I did the research and this is how I did it.
  - I know ROCm support is not great but vulkan is better at text generation for most models (even though it is 2x slower for prompt processing than ROCm).
  - Mini PCs with 780M are great value and enables us to run large MoE models at acceptable speeds. Overall, this mini PC is more than enough for my daily LLM usage (mostly asking math/CS related questions, coding and brainstorming).
  - I was getting great numbers that aligned with dual DDR5 5600Mhz speeds (~80GB/s).
  - I know ROCm support is not great but vulkan is better at text generation for most models (even though it is 2x slower for prompt processing than ROCm).

- ROCM with gpt-oss 120B mxfp4
  - tg128: 18.7 
- VULKAN (RADV only) all with Flash attention enabled
  - qwen3moe 30B. A3B Q4_1:  32.6, 22.3
  - gpt-oss 20B MXFP4 MoE: 28.1, 24.8
  - gpt-oss 120B MXFP4 MoE:  20.4, 18.1
  - qwen3moe 235B. A22B Q3_K:4.3
  - glm4moe 106B. A12B Q4_1:9.1

- just in case someone wants to compare with strix halo:
- STRIX-HALO @ Debian 13 6.16.3+deb13-amd64 (kernel >= 6.16.x for optimal memory sharing)
- ROCm
  - gpt-oss 120B MXFP4 MoE: 47.8
- Vulkan
  - gpt-oss 120B MXFP4 MoE: 51.5

- For me also same but the problem is when context become big speed decrease
  - I get 18t/s at 8k context 

- DDR5 is almost 2x faster than my DDR4 tower PC with AMD Ryzen 5950x CPU. DDR6 should come soon (2026 or 2027?). Also, It is high time that consumer PC industry embraced quad channel memory setup (e.g. DDR5 with 4 channels in mini PC would be amazing).

- Pretty incredible is 96gb the max or can it go 128?
  - it can potentially go up to 256GB but I could not find SO-DIMM DDR5 with that size. But yes, 2x64GB = 128GB is possible but those sticks are expensive! From $200 for 96GB to $400 for 128GB. So, 96GB is cost effective.

- with 90GB RAM allocated to iGPU, gpt-oss-120b-GGUF should comfortably fit 64k context. Also, running with that context will be slow for the initial cache loading (it may take hours).
  - Update: just laoded gpt-oss 120b with 130k context. With flash attention, that context took extra 5GB only. So, I would say it is possible to load the full context.

- 2x64GB dual channel near or above 6000 mt/s are not seen yet. 2x48GB dual channle can go up to 6800mst/s and some may overclock(Ë∂ÖÈ¢ë) it to even higher speed depending your luck, may not be stable.
  - The key is to use 2 slots only. 4 slots will drop the speed significantly even from the exact same brand model spec.

- Is this a one-off for only running gpt-oss 120B or is this platform expected to be somewhat future proof and newer models a likely to work on it?
  - Yes. This is future proof as long as llama.cpp and vulkan exist. Yes, this will run Qwen3 235B. Q3 should run at 6t/s.

- did you also compare the performance against running it on the CPU only, without iGPU? If I remember correctly, using the iGPU mostly improves pp performance while tg is still limited by the (shared) memory bandwidth speed? Is that (still) true?
  - Also, since you seem into getting the most out of (relatively) limited hardware, I think it could be an interesting experiment to run a bigger MoE using mmap and a PCIe Gen 4 NVMe SSD (max. ~8 GB/s). I think this might be surprisingly usable for use cases without limited context, etc.
- Yes, I tested with ik-llama for CPU. The best I got for gpt-oss 120b with CPU was 13t/s. So, iGPU improves TG by ~65-70%. I also tried glm 4.5 air in vulkan. I got 9t/s TG. I haven't tried SSD offloading. But yes, I could try qwen3 235B Q4 for that.

- Excellent results! My M4 Max 128GB was more like $6k and is only about 2.5X faster (55tok/s) with flash attention. Without flash attention, it‚Äôs down around <10tok/sec.
  - What a cool budget option you found! gpt-oss-120b is a great tool-using, private, safe LLM.

- I squeeze 11 tokens/ s with mini pc ryzen 7940hs, 780M and 64 GB 5600 mhz ddr5. Vulkan cpp. I fit 21 Layers. The rest goes to cpu. Inference 6 cpu cores. Context 18000. Maybe 20000. Linux mint mate latest version. Do not use last vulcan cpp 1.51. Use 1.50.2
  - I get 13 t/s with CPU only in ik-llama cpp

- Can you give AMDVLK a try in addition to RADV for your Vulkan perf? On my (completely different but still AMD so it may transfer to yours) hardware AMDVLK basically matches ROCm in PP while still being slightly faster than ROCm at TG (not as fast as RADV though).
  - you're right, RADV starts off slower, but then doesn't slow down as much when context increases.

- Can someone do the same with a Ryzen AI HX 370? They are on Alibaba for around 400$ now (Mini PCs incl an Oculink port) and can be equipped with 128GB DDR5.

- Whats the max context it can run?
  - I have not tested it yet. But with 90GB RAM allocated to iGPU, gpt-oss-120b-GGUF should comfortably fit 64k context. Also, running with that context will be slow for the initial cache loading (it may take hours).
  - Update: just laoded gpt-oss 120b with 130k context. With flash attention, that context took extra 5GB only. So, I would say it is possible to load the full context.
- The problem with context is not at the ininitial run, it tends to deminish the infererence speed as the context filled up.

- I wonder what speedup you would get if you slapped a 3060 12gb EGPU onto it
  - much slower. I have a mini PC with an even older iGPU (680m), and I run almost all MoE models > 20B sometimes faster than folks with 12GB vRAM

- Would a similar approach work with say an Intel iGPU on a desktop motherboard using vulkan? I've got an older 12th Gen i7 but 4x64gb DDR4 (will be slow I know) and wondering how it would compare to just CPU-only.
  - It should be possible but ddr4 will be 2 times slower
- No, desktop iGPUs are very weak, usually. They just exist to provide video out. AMD has some large desktop iGPUs (the G series processors), but I don't think Intel does. Intel iGPUs are also generally not as good as AMD's, at least for llama.cpp.

- ## [More RAM or faster RAM? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1nzf0zf/more_ram_or_faster_ram/)
  - If I were to run LLMs off the CPU and had to choose between 48GB 7200MHz RAM (around S$250 to S$280) or 64GB 6400MHz (around S$380 to S$400), which one would give me the better bang for the buck? This will be with an Intel Core Ultra.
- Little to no difference in speed. You need to optimize for the number of memory channels you have to ensure the highest bandwidth possible.
  - This is why folks opt for older Xeon or Epyc machines, because even with slower ram, they have oodles(Â§ßÈáèÊàñÂæàÂ§ö) more ram bandwidth.

- I bought 2x64 sticks at 6400 on paper but only 5600 stable for my system. I can run GLM 4.6 at 5 t/s and q2, but it beats anything else I could run easily. Cost me 380 euros, totally worth it.

- More Ram.. I use all 32 GB Vram from RTX5090 and 50+ GB Ram just to run Wan2.2.
  - I would say at the minimum you should get 128 GB ram if you want to run LLM (so you can offload and run 70B model). 
  - Personally my spec is 5090 + 256 GB ram so I can offloading most mid size LLM.

- VRAM is up to 20 times faster than 6000MHz RAM, there lies your answer
  - Those models are MoE models and will run at usable speed. Even a 355b GLM 4.6 runs at 4 to 5 tokens per second on 128gb ram on my system. 
  - With upcoming implementations of MTP this might get uplifted into the 10 tokens per second range. MoE models are also getting sparser and sparser. 128 GB ram even if it's just dual channel is absolutely worth it in my opinion.

- Neither of those rams will make a significant difference in inference speeds for llms, both are quite slow bandwidth wise. The useful bit of those ram kits is the capacity, could you get a cheaper 64gb kit instead?

- You even sure your CPU and mobo can utilize these speeds?

- 
- 
- 
- 
- 
- 
- 

- ## üñ•Ô∏è ÊàëÊÉ≥‰π∞‰∏™ Â∑≤Ë£ÖÂ•Ω‰ΩÜÂèØËá™Â∑±ÈÖçÁΩÆÁöÑÂè∞ÂºèÊú∫Â∑•‰ΩúÁ´ô Êàñ Ëá™Â∑±‰π∞Êú∫ÁÆ±/cpu/2Âº†3090ÊòæÂç°Ëá™Â∑±Ë£ÖÂè∞ÂºèÊú∫, Ë¶ÅÊ±ÇÂè∞ÂºèÊú∫ÁöÑÊú∫ÁÆ±Ë¶ÅÂ∞ΩÈáèÂ∞èÔºåÂêåÊó∂ËÉΩÂèëÊå•Â§öÂº†ÊòæÂç°ÁöÑËÆ°ÁÆóËÉΩÂäõÔºåÊï£ÁÉ≠Ë¶ÅÊ≠£Â∏∏
- Èì≠ÁëÑ ARL-HX Ëø∑‰Ω†ÂèåÂç°Â∑•‰ΩúÁ´ô
  - ÂÜÖÈÉ®Êê≠ËΩΩ‰∫Ü‰∏§Âº†Intel Arc Pro B60ÊòæÂç°ÔºåÂêàËÆ°Êèê‰æõ48GB GDDR6ÊòæÂ≠ò

- [Ê±ÇÊé®Ëçê‰∏ÄÊ¨æÂ∞èÂûãÊú∫ÁÆ±Ôºü - Áü•‰πé](https://www.zhihu.com/question/1945481402184364122)
  - ÂùöÂÆöÂ∞èÂûãÂåñÔºåÂ∞±ÂæóÊääÈ¢ÑÁÆóÊèêÂà∞600‰ª•‰∏ä„ÄÇ‰∏™‰∫∫Êé®ËçêÊñπÁ≥ñÊú∫Ê¢∞Â§ßÂ∏àc28Ëøô‰∏™ÂûãÂè∑ÔºåÂª∫ËÆÆÁõ¥Êé•‰∏äÈó≤È±ºÊêú‰∫åÊâãÁöÑ
  - ÂùöÊåÅË¶ÅÂ∞èÊú∫ÁÆ±‰∏ç‰ªÖÊòØÊèêÈ´òÈ¢ÑÁÆóÁöÑÈóÆÈ¢òÔºåÂêéÈù¢Ë£ÖÊú∫‰ºöÊúâ‰∏ÄÂ†ÜÈ∫ªÁÉ¶

- ### [2025Âπ¥5ÊúàÊõ¥Êñ∞ÔºåÁîµËÑëÊú∫ÁÆ±Êé®Ëçê„ÄÇÊé®Ëçê‰∏ÄÊ≥¢È´òÈ¢úÂÄºÁöÑÊú∫ÁÆ±„ÄÇÂåÖÂê´ITX, M-ATX, ATX, E-ATXÊú∫ÁÆ±](https://www.zhihu.com/tardis/zm/art/210537601?source_id=1003)
- ITXÔºö‰∏ìÈó®‰∏∫Â∞èÂûãÁîµËÑëÊú∫ÁÆ±ËÆæËÆ°ÁöÑ‰∏ªÊùøËßÑÊ†ºÔºåÂ∞∫ÂØ∏ 170mm√ó170mm„ÄÇ
  - mini-itxÊú∫ÁÆ±ÁöÑÁâπÁÇπÊòØÊØîËæÉÂ∞èÂ∑ßÔºåÊê∫Â∏¶Áõ∏ÊØîÂ§ßÊú∫ÁÆ±Ë¶ÅÊñπ‰æø‰∫õÔºå‰ΩÜÊòØÊï£ÁÉ≠Áõ∏ÂØπÂ§ßÊú∫ÁÆ±Ë¶ÅÂ∑Æ‰∏ÄÁÇπÁÇπÔºå‰πüÊúâÊï£ÁÉ≠ÂæàÂ•ΩÁöÑITXÊú∫ÁÆ±„ÄÇITXÊú∫ÁÆ±ÁöÑÂè¶‰∏Ä‰∏™Áº∫ÁÇπÂ∞±ÊòØÈÖçÂ•óÁöÑ‰∏ªÊùøÂèäÁîµÊ∫êË¶ÅË¥µ‰∏çÂ∞ë„ÄÇ
- DTXÔºöÂ∞∫ÂØ∏‰∏∫170mm√ó203mm„ÄÇËøô‰∏™Â∞∫ÂØ∏ÁöÑ‰∏ªÊùøÂæàÂ∞ëËßÅÔºåROGÁöÑÈÉ®ÂàÜÈ´òÁ´Ø‰∏ªÊùøÊòØËøô‰∏™ËßÑÊ†ºÔºàÂ¶ÇROG C8I‰∏ªÊùøÔºâÔºåËøô‰∏™ËßÑÊ†º‰πüÊòØ‰∏∫Â∞èÈí¢ÁÇÆËÆæËÆ°ÁöÑÔºåÈÉ®ÂàÜITXÈí¢ÁÇÆÊú∫ÁÆ±ÊîØÊåÅËøô‰∏™Â∞∫ÂØ∏ÔºåË¥≠‰π∞Êú∫ÁÆ±ÁöÑÊó∂ÂÄôÈúÄË¶Å‰ªîÁªÜÁúãÁúã
- ATXÔºöATX ÂèØ‰ª•ÁêÜËß£‰∏∫ÂÖ®Â∞∫ÂØ∏‰∏ªÊùøÔºåÂ∞∫ÂØ∏305mm√ó244mm„ÄÇ
  - ATXÔºà‰∏≠Â°îÔºâÊú∫ÁÆ±ÊòØÁõÆÂâçÂ§ßÈÉ®ÂàÜÁî®Êà∑ÁöÑÈÄâÊã©ÔºåËøôÁ±ªÁöÑÊú∫ÁÆ±Â∞∫ÂØ∏ÂÅèÂ§ßÔºåÂü∫Êú¨ÈÉΩÊîØÊåÅATXÔºåM-ATXÔºåITXÁâàÂûãÔºåÈÉ®ÂàÜÊîØÊåÅÂåÖÊã¨E-ATXÂú®ÂÜÖÁöÑÊâÄÊúâÁâàÂûã„ÄÇ
  - ‰ºòÁÇπÂú®‰∫éÊï£ÁÉ≠Â•ΩÔºåATXÊú∫ÁÆ±Âü∫Êú¨ÈÉΩÊîØÊåÅ240Âèä360Ê∞¥ÂÜ∑ÔºåÂØπÂ§ßÂûãÂèåÂ°îÈ£éÂÜ∑Êï£ÁÉ≠ÁöÑÊîØÊåÅ‰πüÊõ¥Â•Ω„ÄÇ
  - Êâ©Â±ï‰ΩçÂ§öÔºåÊñπ‰æøÂÆâË£ÖÊõ¥Â§öÁöÑÁ°¨ÁõòÁ≠âËÆæÂ§á„ÄÇ
- M-ATXÔºöÂ∞±ÊòØÁº©Â∞èÁâàÁöÑ ATX„ÄÇÈïø244mmÔºåÂÆΩÂ∫¶ÊúâÂ§öÁßçËßÑÊ†º„ÄÇ‰∏ªÊµÅ‰∏∫244mm√ó244mm„ÄÇ
  - M-ATXÊú∫ÁÆ±ÁöÑ‰ºòÁÇπÂú®‰∫éÂ§ßÂ∞èÈÄÇ‰∏≠ÔºåÈÖçÂ•óÁöÑM-ATX‰∏ªÊùø‰ª∑Ê†ºÂêàÈÄÇÔºåÊÄß‰ª∑ÊØîÈ´òÔºåÊâ©Â±ïÊÄß‰πüÂ§üÁî®ÔºåÊòØÂ§ßÈÉ®ÂàÜÁî®ÁöÑÈÄâÊã©„ÄÇ
  - M-ATXÊú∫ÁÆ±ÁöÑÊï£ÁÉ≠‰πü‰∏çÈîôÔºåÂæàÂ§öÈÉΩÊîØÊåÅ240Ê∞¥ÂÜ∑Âèä‰∏≠Á≠âÂ∞∫ÂØ∏ÁöÑÈ£éÂÜ∑„ÄÇ
  - ‰ΩÜÊòØÁõÆÂâç‰ºòÁßÄÁöÑM-ATXÊú∫ÁÆ±ÊØîËæÉÂ∞ëÔºåÂ∞§ÂÖ∂ÊòØÈ´òÁ´ØÁöÑM-ATXÊú∫ÁÆ±Âá†‰πéÊòØÁ©∫ÁôΩ„ÄÇ
- E-ATXÔºöÂä†Â§ßÂûã‰∏ªÊùøÔºåÂ∞∫ÂØ∏305mm √ó 330 mm„ÄÇ‰∏ªË¶ÅÁî®‰∫éÊúçÂä°Âô®‰∏ªÊùø„ÄÇ‰æãÂ¶ÇÊê≠ÈÖçAMD 3990XÁöÑ‰∏ªÊùø„ÄÇ
  - E-ATXÔºà‰∏≠Â°îÔºåÂÖ®Â°îÔºâÊú∫ÁÆ±Êé®Ëçê, ËøôÁ±ªÂûãÁöÑÊú∫ÁÆ±ÊØîËæÉÂ§ßÔºåÈÄöÂ∏∏ÈÉΩÊîØÊåÅ360Ê∞¥ÂÜ∑ÔºåÂ§ßÂûãÈ£éÂÜ∑ÔºåÊï£ÁÉ≠Âü∫Êú¨ÈÉΩÈùûÂ∏∏‰ºòÁßÄ„ÄÇÁ°¨Áõò‰Ωç‰πüÂæàÂ§ö„ÄÇ
  - ËøôÁ±ªÂûãÁöÑÊú∫ÁÆ±ÊØîËæÉÈÄÇÂêàÊ∏∏ÊàèÂèëÁÉßÂèãÔºåÊàñÊúçÂä°Âô®Â∑•‰ΩúÁ´ô„ÄÇ
  - ÂÅöÊúçÂä°Âô®ÊàñÂ∑•‰ΩúÁ´ôÁî®ÁöÑÊúãÂèãÂª∫ËÆÆËÄÉËôë‰ΩøÁî®ÂàÜÂΩ¢Â∑•Ëâ∫ÁöÑMeshify 2ÂíåD7Ôºå‰ª•ÂèäËøô‰∏§Ê¨æÊú∫ÁÆ±ÁöÑXLÂûãÂè∑ÔºàÂèåÊòæÂç°Áî®Ôºâ„ÄÇÊàë‰∏™‰∫∫Âú®‰ΩøÁî®Ëøô‰∏§Ê¨æÊú∫ÁÆ±ÔºåÊï£ÁÉ≠‰ºòÁßÄÔºåËÆæËÆ°ÔºåÁî®ÊñôÔºåÂÅöÂ∑•ÈÉΩÈùûÂ∏∏‰∏çÈîô„ÄÇMeshify 2ÊòØÁ™ÅÂá∫Êï£ÁÉ≠ÔºåD7ÊòØÁ™ÅÂá∫ÈùôÈü≥„ÄÇ
  - ÂàÜÂΩ¢Â∑•Ëâ∫ Torrent: ÁÆ±‰ΩìÈïøÂÆΩÈ´òÔºàmmÔºâ544ÔºàÈïøÔºâ*242ÔºàÂÆΩÔºâ*530ÔºàÈ´òÔºâmm

- ### [25L Portable NV-linked Dual 3090 LLM Rig : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1l0zsv7/25l_portable_nvlinked_dual_3090_llm_rig/)
  - cpu: AMD Ryzen 7 5800X 3.8 GHz 8-Core Processor
  - Motherboard: Asus ROG Strix X570-E Gaming ATX AM4 Motherboard
  - NVlink SLI bridge
  - Mechanic Master C34Plus Portable Desktop ATX Case with Aluminum Handle: 5.2 Kilograms, 39.1 x 18.5 x 34.3 cm.
  - CORSAIR RMe Series Fully Modular Low-Noise Power Supplies: 1200 watts
  - ‚ö†Ô∏è WARNING - these components don't fit if you try to copy this build. The bottom GPU is resting on the Arctic p12 slim fans at the bottom of the case and pushing up on the GPU. Also the top arctic p14 Max fans don't have mounting points for half of their screw holes
  - All that being said, with a 300w power limit applied to both gpus in a silent fan profile, this rig has surprisingly good temperatures and noise levels considering how compact it is.
  - During Cinebench 24 with both gpus being 100% utilized, the CPU runs at 63 C and both gpus at 67 Celsius somehow with almost zero gap between them and the glass closed. All the while running at about 37 to 40 decibels from 1 meter away.
- I did a bunch of testing back when I had a 4x 3090 rig. The sweet spot was always between 250-300W for inference. Above that I saw no improvement in inference speed (this was a DDR4 system, YMMV with DDR5). Below 250 speed would start dropping off quite quickly.
  - If memory serves me, I settled on 275W and enjoyed the power savings while not sweating the .05 tokens/sec it cost me for not running over 300W
- Neat build. Even power limiting to 200W doesn't have that big of a hit on inference. (Exl2)
  - I power limit all mine to 200w. It's perfectly fine.

- ### [Smallest case possible for dual gpu? : r/sffpc _202312](https://www.reddit.com/r/sffpc/comments/187og11/smallest_case_possible_for_dual_gpu/)
- Someone else just posted details of their NR200 build with dual 4090s, might be worth a look?
  - 18.49 x 37.49 x 29.18 cm
  - Êñ∞Ê¨æmini-itxÊòØ NR200P V2
- Didn't realise the NR200 had the possibility of going dual GPU. I'mm super new to SFF.
  - pretty sure you would have to do PCIe birfurcation for that
- Yeah, max mobo you're going to fit in there is an ITX, and building round it already a squeeze with one GPU to think about (with any kind of air CPU cooler)

- check out Cerberus (mATX) or Cerberus X (full ATX)

- Generally sff cases are designed around ITX motherboards with only one pcie slot, finding a sff case that will fit an m-ATX motherboard is going to be near I possible. I have a nr200p same as the guy with dual 4090s, it only officially supports ITX boards.. it's also not really small form anymore (I have a tophat with a radiator it)

- [Smallest case for mATX + air cooled cpu + 2 gpu : r/buildapc _202308](https://www.reddit.com/r/buildapc/comments/15x7h9h/smallest_case_for_matx_air_cooled_cpu_2_gpu/)
  - Asus Prime AP201 MicroATX Mini Tower Case: 20 x 35 x 46
  - Sliger Cerberus. mATX case much smaller than the Asus AP201 (huge at 33L, when the Sliger Cerberus is 19.6L).

- [Good pc case for dual gpu setup? : r/VFIO _202402](https://www.reddit.com/r/VFIO/comments/1ao7gsj/good_pc_case_for_dual_gpu_setup/)
  - Take a look at Fractal Design Meshify 2, or even the XL version. It should have plenty room for what you want.

- ### [Small-form-factor all-air-cooled dual RTX 3090 SLI 1000W build : r/sffpc _202107](https://www.reddit.com/r/sffpc/comments/orug4s/smallformfactor_allaircooled_dual_rtx_3090_sli/)
  - Case: Sliger Cerberus X
  - Motherboard: MSI MEG X570 Godlike
  - CPU: AMD Ryzen 5600X
  - RAM: HyperX Predator 32GB 3333Mhz
  - GPU: 2x RTX 3090 Founder's Edition SLI
  - PSU: Silverstone SX1000 Platinum SFX-L
  - CPU Cooler: Cryorig H7
  - Case fans: 2x Arctic P14 140mm, 3x BeQuiet Pure Wings 2 92mm
  - Temps under full load: GPU core 66/50, GPU memory junction 102/102, CPU 85
  - Using software to estimate power draw at full load I'm guessing around 800W. I am confident the PSU can handle a 5900X even without undervolting but temps would be an issue so I'm leaving it with the 5600X for now. The 5600X is slightly undervolted using PPT 70W, negligible performance hit compared to stock but a few degrees lower temps.

- ### [Looking for a 2 x 3090 case : r/LocalLLaMA _202503](https://www.reddit.com/r/LocalLLaMA/comments/1j3h23g/looking_for_a_2_x_3090_case/)
  - I am looking for either a case or an open air rig where I could fit two 3090 FE connected with nvlink.
  - I need to be able to use pcie extender otherwise there is not enough spacing on the Mobo for putting the nvlink because of the 3 slots GPUs.

- I feel like this shouldn't require too much special consideration. I have a standard mid-tower case and it fits 2 3090s in it just fine.

- If you can afford it, phanteks enthoo pro 2 server edition is amazing. Internal fan bracket lets you direct air onto the gpus. My 2x3090s stay below 30 C at idle, and never go above 70 on load.
  - 24 x 56 x 58, 77L

- I can just barely fit 3 MSI 3090s in my Corsair 7000D Airflow, 2 horizontal and one vertical with riser. Tons of fans.
  - 24 x 55 x 60

- Almost any full ATX case with 8 pcie slots would work fine. Antec P101 for example.

- [Smallest case for dual air-cooled 3090's? : r/sffpc _202104](https://www.reddit.com/r/sffpc/comments/mzfg3p/smallest_case_for_dual_aircooled_3090s/)
- Wondering if getting a smaller case with Quadro A6000 would be better option than bigger case with Dual 3090?
  - An A6000 would be amazing but it‚Äôs just so damn expensive. It doesn‚Äôt make sense to me from a value perspective.
  - And since the 3090 and A6000 have the same amount of cuda cores, having dual 3090‚Äôs would actually give me up to twice the performance as a single A6000, but with the same amount of total memory.
- Are you sure you can combine the VRAM in your workload? You'd most likely need an NVLink bridge aswell.
  - The 3090 has NVLink, although certain features are disabled in software. For ML training though I‚Äôm pretty sure I would be getting the full NVLink bandwidth.

- Fractal R6 has a lot of room. So does the 7

- If you need the NVLink, you'll need one with two 16x slots spaced 4 slots apart (NVidia only sells 4-slot spacing NVLink-bridges). 
  - Also: Do you need the PCIE bandwith for your workload? Most consumer CPUs don't have enough PCIE lanes, so the two GPUs will be running in only 8x each.
- Yeah, Cerberus X won't be an option if you wanna aircool the CPU. The O11D is a pretty giant case (57L).
  - If you want something smaller, I'd recommend the Meshify C. It's just under 40L, has a standard mid-tower layout, can fit big ATX PSUs, a big aircooler like the NH-D15 and has good ventilation (if you add 3x 120mm fans in the front) so your GPUs will get enough airflow.

- ## [‰∏ÄÊ¨°ÊÄß‰ª∑ÊØîÁàÜÊ£öÁöÑÊòæÂç°ÂçáÁ∫ß‰πãÊóÖÔºöÊäòËÖæ V100 ÊòæÂç°Áª≠ÈõÜÔºå‰∫åÊâãÁ•ûÂç°Ë∑ëComfyUIÔºÅ - Áü•‰πé _202507](https://zhuanlan.zhihu.com/p/1929587520502498303)
- Tesla V100 SXM2	
- V100 ÊòØ Volta Êû∂ÊûÑÔºàsm_70ÔºâÔºåËÄåÁé∞Âú®‰∏ÄÂ†Ü AI Ê°ÜÊû∂ÈÉΩË¶ÅÊ±Ç Ampere Ëµ∑Ê≠•Ôºàsm_75 ‰ª•‰∏äÔºâ„ÄÇÂØºËá¥Ëøô‰∫õÊ®°ÂùóÁõ¥Êé•Êä•Èîô
- LTX-Video-Q8-Kernels ÁöÑ setup.py Êú™ÈÄÇÈÖç Tesla V100 ÁöÑ sm_70 Êû∂ÊûÑÔºå‰ªÖÊîØÊåÅ AmpereÔºàsm_80+ÔºâÁ≠âËæÉÊñ∞Êû∂ÊûÑ„ÄÇ
- Triton 3.3.0 ‰∏é sm_70 ÂÖºÂÆπÊÄßÂ∑ÆÔºåsageattention ÁöÑ INT8 ‰ºòÂåñÂèØËÉΩ‰∏çÊîØÊåÅ V100„ÄÇPyTorch 2.7.1+ Ë¶ÅÊ±Ç sm_75ÔºåËÄå PyTorch 2.4.1 ÊòØÊúÄÂêé‰∏Ä‰∏™ÊîØÊåÅ sm_70 ÁöÑÁâàÊú¨„ÄÇ Ëß£ÂÜ≥Ôºö ÂõûÈÄÄÂà∞ PyTorch 2.4.1 Âíå Triton 3.0.0
- is_flash_attention_available ÊòØ PyTorch 2.5.0+ ÁöÑ APIÔºåPyTorch 2.4.1 ‰∏çÊîØÊåÅ„ÄÇ
- FlashAttention-2 ÁöÑ flash_attn_func ‰∏çÂ≠òÂú®‰∫é FlashAttention-1Ôºà1.0.9ÔºâÔºåËÄå V100Ôºàsm_70Ôºâ‰∏çÊîØÊåÅ FlashAttention-2„ÄÇ Ëß£ÂÜ≥Ôºö ÂÆâË£Ö FlashAttention-1
- PyTorch ÁöÑ Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåFlashAttention-1ÊàñËÄÖxformersÂÜÖÂ≠òÊïàÁéá‰ΩéÔºåËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºàÂ¶Ç Wan2.1„ÄÅHunyuanÔºâÈúÄ Â§ßÈáèÊòæÂ≠òÔºåV100 ÁöÑ 16GB ‰∏çË∂≥„ÄÇËÄåsageattentionÂèà‰∏çÊîØÊåÅV100ÔºåÊâÄ‰ª•ÊöÇÊó∂Êó†Ëß£

- ## [Laptop with 32 GB VRAM for Stable Diffusion : r/StableDiffusion _202305](https://www.reddit.com/r/StableDiffusion/comments/1353g26/laptop_with_32_gb_vram_for_stable_diffusion/)
- I don't think so, unless you do an "egpu" with a card in a separate case connected via thunderbolt. My card has 16gb VRAM and I can run a 2048x2048 without tiling. Tiling lets that go much higher, so that's the way to go if you want to do large images. Generally speaking the vram stuff is mostly an issue if you want to train LoRAs/embeddings/hypernetworks/etc.

- Is 16 GB of VRAM on your machine enough to train LoRAs and checkpoints? I can do very limited training with my current 8 GB VRAM machine, and I'd be very interested in training LoRAs and checkpoints to personalize my work. 
  - Yes, 16gb is enough. Until recently I was able to do batch sizes of 8 for Dreambooth fine-tuning, generally in other AI training it's good to do that or accumulate gradients

- ## [AMD ËÆ°Âàí‰∫é 2026 Âπ¥Êé®Âá∫ Zen 6 Êû∂ÊûÑ Ryzen Â§ÑÁêÜÂô®ÔºåÊúâ‰Ωï‰∫ÆÁÇπÂÄºÂæóÊúüÂæÖÔºü - Áü•‰πé _202506](https://www.zhihu.com/question/1944114025425269198)
- 2006Âπ¥ÔºåATiË¢´AMD‰ª•54‰∫øÁæéÂÖÉÊî∂Ë¥≠Ôºå‚ÄúFusionËûçÂêà‚ÄùÊàê‰∏∫Êñ∞AMDÊúÄÈ´òÊàòÁï•„ÄÇCPU„ÄÅGPU„ÄÅ‰∏ªÊùøËäØÁâáÁªÑ‰∏âËÄÖÂêà‰∏ÄÔºåÈõÜÊàê‰∏∫ÂâçÊâÄÊú™ÊúâÁöÑSOCÂçïËäØÁâáÔºåÂç≥‚ÄúAPU‚ÄùÔºåÂºÇÊûÑËøêÁÆó‰ª•ÊèêÂçáËøêÁÆóÊïàËÉΩ
- 2017Âπ¥Ôºå‚ÄúZenÁ¶Ö‚ÄùÊû∂ÊûÑÁöÑÈîêÈæôÂ§ÑÁêÜÂô®‰∏äÂ∏Ç„ÄÇÂçïËäØÁâáËÆæËÆ°ÁöÑAPUÂäõÊåΩÁãÇÊæúÔºåÊ°åÈù¢Â§ÑÁêÜÂô®Ëß¶Â∫ïÂèçÂºπ„ÄÅÁßªÂä®Â§ÑÁêÜÂô®ËøÖÈÄüÂ¥õËµ∑„ÄÅÊãø‰∏ãÂèåÊ∏∏Êàè‰∏ªÊú∫Â§ßÂçï„ÄÇ‰ΩÜÁß∞ÈõÑÊ°åÈù¢„ÄÅÂà∂Èú∏ÊúçÂä°Âô®Á´ØÔºåAMD‰æùÈù†ÁöÑÊòØZen2ÂºÄÂßãÁöÑChipletÂ§öÊô∂Á≤íÊ®°ÂùóÂåñËÆæËÆ°ÔºåÂ§öÊ†∏Â†ÜÊ≠ªËÄÅÂ∏àÂÇÖ„ÄÇ
- Êë©Â∞îÂÆöÂæãÂ§±ÊïàÔºåÂçïËäØÁâáÈù¢ÁßØÊúâÈôêÔºåÂÖàËøõÂà∂Á®ã‰ª£‰ª∑ÊòÇË¥µÔºåÂâ•Á¶ª‰∫ÜGFÊ†ºÁΩóÊñπÂæ∑Â∑•ÂéÇ„ÄÅÂá†ËøëÁ†¥‰∫ßËæπÁºòÊä¢ÊïëÂõûÊù•ÁöÑAMDÔºåÂøÖÈ°ªÂæóÁ≤æÊâìÁªÜÁÆó
- Êñ∞‰∏Ä‰ª£Zen6Êû∂ÊûÑÔºåCCDËÆ°ÁÆóÊ†∏È¶ñÂèë2Á∫≥Á±≥Âà∂Á®ãÔºåËøûË¥¢Â§ßÊ∞îÁ≤óÁöÑËãπÊûú„ÄÅÈ´òÈÄöÈÉΩÊ≤°Êï¢‰∏ãËøô‰πàÈáçÁöÑÊú¨„ÄÇ
- AMDÈÄâÊã©Â∞ÜChipletËøõË°åÂà∞Â∫ïÔºö 2nmÁöÑZen6 CCDËÆ°ÁÆóÊ†∏Ôºå2-3Áßç3nmÁöÑRDNA5GPUÔºåÁî®4Áßç‰ª•‰∏ä3nmÁöÑAPUÁªÑÂêàÂú®‰∏ÄËµ∑
  - Strix HaloÊû∂ÊûÑÈÇ£È¢óAI MAX+ 395‰∏äÂèñÂæóÁöÑÁªèÈ™åÔºåÊàê‰∏∫Êó•ÂêéÁöÑÂ∞ÅË£ÖÊ†áÂáÜ

- Strix HaloÊû∂ÊûÑÁöÑAI MAX+ 395ÔºåÊÄßËÉΩË∂≥‰ª•Â™≤ÁæéRTX4060ÔºõÂè™ÊòØÂèåCCDËÆ°ÁÆóÊ†∏ÊãâÈ´ò‰∫ÜÊàêÊú¨ÔºåËØïÊ∞¥Êñ∞Â∞ÅË£ÖËâØÁéá‰∏çÈ´òÔºåÂèàÈÅá‰∏äAIÊé®ÁêÜÊú¨Âú∞ÂåñÁÉ≠ÊΩÆÔºåÈÄ†ÊàêÂá∫ÂéÇ‰ª∑Â•áÈ´òÔºåÂçïU‰ª∑Ê†ºË∂ÖËøá4060Ê∏∏ÊàèÊú¨Êï¥Êú∫ÔºåÂè´Â•Ω‰∏çÂè´Â∫ß„ÄÇ

- 2027Âπ¥Êú´‰∏äÂ∏ÇÁöÑAI MAX 500Á≥ªÂàóÊñ∞ÂìÅÔºåÊòØAPUÔºå‰πüÊòØ‰∏ÄÈ¢óÂèØ‰ª•ÂÅöÊàêÁã¨ÊòæÁöÑGPUÔºö
  - „ÄêÂ∞èÊùØ„ÄëMedusa Halo miniÔºå‰πüÊòØ AT4 GPUÔºå12Â§ßÂ∞èÊ†∏2LPÔºå24CU RDNA5ÔºåPTX 1050ÂêåËäØÔºåÂØπÊ†á4060
  - „ÄêÂ§ßÊùØ„ÄëMedusa HaloÔºå‰πüÊòØ AT3 GPUÔºåËá≥Â∞ëÂÖ∑Â§á12Ê†∏ÔºåÂèØÈÄâCCDËÆ°ÁÆóÊ†∏Ê°•Êé•Ë°•ÈΩêÔºå48CU RDNA5ÔºåPTX 1060ÂêåËäØÔºåÂØπÊ†á4070
  - Â∞èÊùØ‰ΩøÁî®128bit LPDDR5XÂÜÖÂ≠òÊéßÂà∂Âô®ÔºåÊòæÂ≠òÂèØËææ128GÔºåÊõ¥ÈÄÇÂêàÁªèÊµéÊ∏∏ÊàèÊú¨
  - Â§ßÊùØÈááÁî®384bit LPDDR6ÂÜÖÂ≠òÊéßÂà∂Âô®ÔºåÊòæÂ≠òÂèØËææ512GÔºåÈíàÂØπ‰ª∑Ê†º‰∏çÊïèÊÑüÁöÑAIË°å‰∏öÁî®Êà∑

- ## [How well does ComfyUI perform on macOS with the M4 Max and 64GB RAM? : r/comfyui _202503](https://www.reddit.com/r/comfyui/comments/1jhifyi/how_well_does_comfyui_perform_on_macos_with_the/)
- TLDR - if you want to work linear on one image, a Mac is a huge waste of time. Maybe 25% of the speed of a decent NVIDIA PC for AI generation. 
  - However, if you know how or want to multitask, it‚Äôs easily the best system you can purchase.

- I don't know if you'll find this information but based on estimates comparing to the M4 Pro, it should reach close to half the performance of an $800 PC with an RTX 3060

- I have a Mac Mini M4 Pro with 48GB of unified RAM. As my daily driver for everyday things, it's great, even great at media encoding, but for generative AI stuff, compared with my Linux PC with a RTX 4090 - it pales in comparison. We are talking minutes vs seconds here for a 1024x1024 Flux generation. Most of it is tuned for Nivida CUDA and that is the key. Apple Silicon offers great performance for everyday computing, but its support for machine learning frameworks like PyTorch sucks butt compared to CUDA.

- I transitioned from dabbling with generative AI images on M1 Max Macbook to using a PC that I owned with Nvidia RTX 3060 graphics card. 
  - The PC with ComfyUI was 3 times faster than my Mac which was using DrawThings (I installed ComfyUI on Mac but abandoned it because DrawThings was more convenient and faster). 
  - After getting more involved I ended up buying a PC with Nvidia RTX 4090 graphics card. Very happy with that decision. I love the Mac for most things but most likely even the M4 Max might prove to be frustrating to use to keep up with the rapid advances in the ComfyUI - Stable Diffusion world.

- AI image and video generation require cuda cores to function properly so Apple or even AMD gpus are not recommended.

- I ran ComyUI Flux Schnell, Pro and several LORAs on Mac studio Max4 base model and most models will run fine. I only ran into memory issues with video generation therefore moved to M3 Ultra 96GB. I dont have a reference point with PC but i am getting 15-30sec for 1024x1024 img generation for most workflows.
  - That's pretty slow. A NVIDIA 3090ti 24gb PC can do that in under half that

- my Mac M4 16gb is much more slower than my old PC with the good old Geforce 3060. An Apple a day keeps the Comfyui away.

- ## [Setting up ComfyUI with AI MAX+ 395 in Bazzite : r/StableDiffusion _202510](https://www.reddit.com/r/StableDiffusion/comments/1nux1f0/setting_up_comfyui_with_ai_max_395_in_bazzite/)
  - Qwen took 3 min 20s with fp8 image and clip at 20 steps
  - 1 min 22s for Qwen 4-step lightning lora with 8 steps - 8-step lora doesn't work since it's bf16
  - Flux was super slow first time, running flux-dev at fp8, but after that it was about 1 min 51s
  - WAN 2.2 fp8 high-lo 20 steps took 4 min 14s for an image (no speed up lora)
  - WAN 2.2 fp8 with Lightx2v took 18 seconds for an image

- 3 times more expensive than 4070 but 3 times slower than 4070?
  - The purpose is the 128gb of unified ram, it's meant for LLM use, not image generation. 
  - Obviously if you only care about small models that fit in 16gb vram there are way cheaper and faster methods.

- [System Question: AMD Ryzen AI Max + 395 with 128GB LPDDR5x 8000mhz Memory -- Will this work to run ComfyUI? : r/comfyui](https://www.reddit.com/r/comfyui/comments/1nr9ttv/system_question_amd_ryzen_ai_max_395_with_128gb/)
  - It would be really sloooooow. Assuming you‚Äôre talking about ai max. There is a YouTube video of a review in Chinese. He shows running some T2i and t2v using some Chinese software. Regardless it was super slow.
  - TLDR CUDA is still king.
- You can load large LLMs and run them decently on that machine but it is not meant for heavy image and video work. 
  - A dedicated GPU will run rings around that machine for rendering time. With a 5090 I can generate 8 seconds of 720p video with FP16 high and low noise models and Loras using sage attention 2 in about 3 to 5 minutes, you don‚Äôt need to be running them as high as I am if you want good results with 16 a 24gb vram. 
  - The main difference is that VRAM is faster (much faster) than ram and the GPU chip turns out many more TFLOPS of 16 floating point precision than the tiny 8060S can, not to mention the LPPDR 8000 ddr ram is much slower than GDDR7. 
  - If you just want to run language models get that machine. Otherwise, you‚Äôll be badly equipped and your render times will be forever

- ## [DIY vs Nvidia dgx spark? : r/StableDiffusion _202509](https://www.reddit.com/r/StableDiffusion/comments/1nfoi9d/diy_vs_nvidia_dgx_spark/)
- As already suggested, go for 5090. Or if you need more VRAM and have the budget, go for RTX PRO 6000 (if you are planning some video work, that may be a better option).

- The spark can be a good option for LLMs but not for image generation. Here the currently best options that you can run in an office are the 5090 and the RTX Pro 6000.
  - When it's coming to training you could (should) even consider multiple 5090 like 2 or 4.

- just buy 5090, DGX spark is 40/5060 Ti ish performance

- Spark can do nvlink? did you mean network based NCCL?
  - You want to train Stable Diffusion, i assume UNET based model like XL and 1.5 variant. The less pain in the ass way to train in multi gpu setup is DDP. splitting unet is pain you need special libs like https://github.com/mit-han-lab/distrifuser
  - Incur communication cost, no free meal, DDP even though less demanding than FSDP in communication, it stilll need allreduce the gradient across rank.
  - Is 128G VRAM really important for your use case? DGX Spark is mainly for LLM workflow, large weight model but low active compute sequence (tokens), since LLM sequence isn't as crazy as diffusion models.
  - Engineering and salary cost. Your engineer need to learn to parallelizing gpu, manage networking, setting up torch distribution
  - Most SD trainer is mainly for single gpu
  - DGX Spark is not GDDR memory but LPDDR, and boy moving tensor from GDDR to share memory (gpu internal memory) is already slow, relative to SM speed, and now LPDDR is much slower

- ## [Will this thing work for Video Generation? NVIDIA DGX Spark with 128GB : r/StableDiffusion _202504](https://www.reddit.com/r/StableDiffusion/comments/1ju2mfk/will_this_thing_work_for_video_generation_nvidia/)
- For a machine with 128 GB of LPDDR5 with 273 GB/s memory bandwidth and a paltry 200 GbE ConnectX-7 I find $4k a bit much.

- 4090 still superior to this despite the 128GB of memory. That memory speed is much slower and the TOPS is lower. A 4090 is still best dollar value if your focus is SD and video.

- Check out the HP Z2 G1a that's dropping on Monday. Allocate up to 96GB of RAM to the GPU (I've heard that on Linux you can allocate even more) and the price has surprised me. I'll be getting one as soon as I know Ollama and SD support its APU.
  - Yes, it's 110GB on Linux. Are you surprised that the price is so high? Other Strix Halo machines also with the same APU and the same 128GB of RAM are much cheaper. The Framework Desktop starts at $2000 or just $1700 to buy the motherboard. The GTK is well spec'ed out for around $1800 ready to run. Those are like half the cost of the HP.

- ## [DGX Spark? : r/comfyui _202506](https://www.reddit.com/r/comfyui/comments/1ljbgxn/dgx_spark/)
- 4090 or 5090 for images. RTX 6000 Pro if you're doing video. DGX Spark is for running medium to large-ish LLMs slowly and does not have the right mix of performance for image/video work.
  - The 6000 is about 3x the price of a 5090 so I'll have to think about that one.
  - 96GB of RAM on one GPU. Video models were mostly engineered to run on 80GB H100s. You can run them on less VRAM but with hokey compromises and limitations. Not saying people don‚Äôt do it but I don‚Äôt have the appetite, would rather just use models as intended.
- Yes, any RTX *90 series card will be faster. DGX Spark is targeted at large text based models. I would not buy this if you plan on using it for image/video gen.

- Just get a nuc and a pro 6000 Blackwell and you'll be set for a long while.
  - Wow it's at least 10k‚Ç¨ just for the gpu!

- [NVIDIA says DGX Spark releasing in July : r/comfyui](https://www.reddit.com/r/comfyui/comments/1labkat/nvidia_says_dgx_spark_releasing_in_july/)
  - Lmao. It has 1/5 the memory bandwidth and cuda cores of a rtx6000 pro. 
  - comfyUI would need to be able to run on arm CPU. All Mac users have ARM CPU 

- ## üßÆü§î [NVIDIA says DGX Spark releasing in July : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1kq4ey4/nvidia_says_dgx_spark_releasing_in_july/)
- Let's do some quick napkin math on the expected tokens per second:
  - If you're lucky you might get 80% out of 273 GB/s in practice, so 218 GB/s.
  - Qwen 3 32B Q6_K is 27 GB.
  - A low-context "tell me a joke" will thus give you about 8 t/s.
  - When running with 32K context there's 8 GB KV cache + 4 GB compute buffer on top: 39 GB, so still 5.5 t/s. If you have a larger.
  - If you run a larger (72B) model with long context to fill all the RAM then it drops to 1.8 t/s.
- Yes, these architectures aren't the best for dense models, but they can be quite useful for MoE. Qwen 3 30B A3B should probably yield 40+ t/s. Now we just need a bit more RAM to fit DeepSeek R1.

- Is that how you can calculate the maximum speed? Just bandwidth / model size => tokens / second? I guess it makes sense, I've just never thought about it that way. I didn't realize you would need to transfer the entire model size constantly.
  - You don't transfer the model, but for every token generated it needs to go through the whole model, which is why it is bandwidth limited for single user local inference.
  - As for bandwidth, it's a MT/s multiplied by the bus width. Normally in desktop systems one channel = 64bit so dual channel is 128bit etc. Spark uses 8 of DDR5X chips of which each is connected with 32bits, so 256bit total. The speed is 8533MT/s and that give you the 273GB/s bandwidth. So (256/8)*8533=273056MB/s or 273GB/s.
- > "You don't transfer the model, but for every token generated it needs to go through the whole model"
  - Except when you use models with "sparse" support, apparently. Which is why its a big deal the things have hardware accel for sparse models.

- My MacBook Pro M1 Pro is close to 5yo and it runs qwen3 30B-a3B q4 at 45-47t/s on commands with context. It might drop to 37t/s with long context.
  - when you run a smaller quant like Q4 of the 30B A3B model you might get close to 60 t/s in your not-long-context case.

- Running the same Qwen model with a 32k context size, I can get 13+ tokens a second on my M4 Max.
  - With just 32k context size set, or also mostly filled with text? Anyway, 13 tps * 39 GB gives us about 500 GB/s. The M4 Max has 546GB/s memory bandwidth, so this sounds about right, even though it's a bit higher than expected.

- thank you. those numbers look terrible. I have a 3090, I can easily get 29 t/s for the models you mentioned.
  - I don't think you can fit a 27 GB model file fully into 24 GB VRAM. I think you could fit about Q4_K_M version of Qwen 3 32B (20 GB file) with maybe 8K context into 3090, but it would be really close. So comparison would be more like Q4 quant and 8K context at 30 t/s with risk of slowdown/out of memory vs. Q6 quant and 32K context at 5 t/s and not being near capacity.

- But 128GB of memory will be amazing for ComfyUI. Operating on 12GB is impossible, you can generate a random image, but you can't then take the character created and iterate on it in any way or use it again in another scene without getting an OOM error. At least not within the same workflow. For those of us who don't want an Apple for our desktops this is going to bring a whole new range of desktops we can use alternatively. They are starting at $3k from partnered manufactures and might down to the same price as a good desktop at $1-2k in just another year.
  - You're probably better off with an RTX 4090 (and a full desktop PC to support it, so it is going to be more expensive) for image generation, as the Spark is going to be slower than a gpu. It can run far bigger models, yes. But 128GB is too much for just image generation while the speed will suffer due the limited bandwith. A sweetspot would be half the memory at twice the speed, but that doesn't quite exist, at least in that price range. A modded RTX 4090 with 48GB of ram (and the accompanying desktop) is going to perform better - although the entire thing would probably cost more than twice as much. BUT, if you already have a desktop, upgrading your gpu will give you better bang per buck.
- It likely depends on how big your workflows are. Your right in that if I don't run out of memory on my gaming graphics card, image generation is super fast, but if I do run out of memory all the speed in the world is not going to help me finish my workflow. Also the speed is not as important for developing, since your the only user. I can let this little guy do the work while I game on my gaming card and the power draw is so low it can share the same circuit.
  - This is similar to my thoughts. You have CUDA-capable running in the background and reasonably low wattage. Just throw some stuff at it and come back later to see the results. It won't bog down your main system and hopefully it won't waste too much electricity (maxing at 170 watts? Each, since you could have multiple linked). Having an always available local LLM is also just nice. MSTY or similar can make it available to your whole home network rather easily.

- So, basically like a 128 GB strix halo but almost triple the price. Yawn.
  - But it has CUDA man. CUDA!!!!!

- Just a note that DGX Spark is listed as $3999 on their page currently. There are some licensed competitors that have cheaper machines available but they all sacrifice something to get there.

- You can get an Apple Studio M4 128 GB for a little less than DGX Spark. The Apple device will have slower prompt processing but more memory bandwidth and thus faster token generation. So there is a choice to make there.

- ## [Comparing AI Performance of DGX Spark to Jetson Thor - DGX Spark / GB10 User Forum / DGX Spark / GB10 - NVIDIA Developer Forums _202508](https://forums.developer.nvidia.com/t/comparing-ai-performance-of-dgx-spark-to-jetson-thor/343159)
  - Jetson Thor: ‚ÄúNVIDIA¬Æ Jetson Thor‚Ñ¢ series modules give you the ultimate platform for physical AI and robotics, delivering up to 2070 FP4 TFLOPS of AI compute and 128 GB of memory with power configurable between 40 W and 130 W.‚Äù
  - DGX Spark: ‚ÄúPowered by the NVIDIA GB10 Grace Blackwell Superchip, NVIDIA DGX‚Ñ¢Spark delivers 1 petaFLOP of AI performance in a power-efficient, compact form factor.‚Äù
  - Is this a case of marketing terminology conflation, or might the Jetson AGX Thor provide better local inference performance compared to DGX Spark?

- The NVIDIA Jetson Thor Developer Kit is a purpose-built developer platform targeted at developers creating robotics and physical AI solutions that deploy with embedded Jetson modules. 
  - DGX Spark is a purpose-build compute to build and run AI, targeted at AI developers and data scientists who need to augment current laptop, desktop, cloud, or data center resources to provide large local memory and access to the NVIDIA AI software stack for their AI prototyping, fine-tuning, inference, data science, and general edge workloads.

- The RAM being similar but performance different could be down to scaling with power draw. The DGX Spark ships with a 240w USB-C brick, and I think it‚Äôs specced to draw significantly higher than the Thor at 170W.

- The DGX Spark has 6144 CUDA cores, or just as many as RTX 5070. I believe I saw numbers claiming 1000 TOPS in FP4 sparse mode. I‚Äôm not sure how many tensor cores, or what type of tensor cores even, if any?
  - The AGX Thor has 2560 CUDA cores, with 96 fifth-generation Tensor cores; benchmarks I‚Äôve seen so far indicate it performs on LLMs about as fast as an RTX 5070. I‚Äôm unsure if these were tests used in FP4 Sparse mode, as NVIDIA rates it for 2070 TOPS in FP4 sparse mode
  - I suppose I can see the Spark using more power due to using CUDA cores instead of Tensor Cores as the main source of processing, offering FP32 performance needed for precision during training and perhaps greatly versatility

- Some other differences that are noteworthy
  - DGX Spark has one NVENC/ NVDEC chip. Thor has Two.
  - DGX Spark has a connect-x nic. Thor is not connect-x but a 4x25g nic. It doesn‚Äôt appear to support RDMA among other features you get with connect-x. which also means you probably can‚Äôt combine the thor modules very easily.

- ## [AGX Thor LLM Inference Performance & Implications for DGX Spark? : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1n0rheb/agx_thor_llm_inference_performance_implications/)
  - Excited to see the initial benchmarks rolling in for the AGX Thor following yesterday's release. A recent YouTube video showed around 30 tokens/sec generation speed with gpt-oss-120b using llama.cpp
  - This got me thinking about the DGX Spark. NVIDIA advertises the AGX Thor as having 2 PFLOPS of FP4 performance, while the DGX Spark is listed at 1 PFLOP

- Why would LLM inference scale with available FLOPs? 
  - Most of the operations used are dot produces (particularly matrix-matrix products) which are memory bound operations fundamentally; the number of FLOPs don't matter (within reason). Even a 4 core CPU with sufficient bandwidth can still run an LLM like GPT OSS
  - Where the FLOPs might matter is prefill (long context operation) or for concurrent inference serving, like maybe if you were doing async agents or something in parallel. These operations are compute bound, and FLOPs do matter (though in the case of prefill, with KV caching it doesn't matter for iterative workflows like chatting or iterative coding, and you're memory bandwidth bound again).

- I think one of the biggest drawbacks of the Spark (and something that is holding back many current offerings for AI and Handheld gaming) is the memory bandwidth
  - The one benefit that Spark might bring is NVIDIA MIG, which would allow us to partition GB10 into several instances and maybe run several models in parallel. Might be interesting for exploring LM Agents, especially if you got several of them working at the same time.

- I think there are two versions of the future: MXFP4 - will become the standard. MXFP4 - will not become the standard. 
  - In the first case, DGX Spark and Blackwell will make other solutions garbage. In the second case, DGX Spark will be garbage.

- ## [Nvidia DGX Spark | Hacker News _202508](https://news.ycombinator.com/item?id=45008434)

```markdown
<!-- FP4-sparse -->
| GPU Model | FP4-sparse (TFLOPS) | Price ($) | $/TF4s |
|-----------|---------------------|-----------|--------|
| 5090      | 3352                | 1999      | 0.60   |
| Thor      | 2070                | 3499      | 1.69   |
| Spark     | 1000                | 3999      | 4.00   |

<!-- FP8-dense -->
| Model        | FP8-dense (TFLOPS) | Price | $/TF8d (4090s have no FP4) |
|--------------|---------------------|-------|----------------------------|
| 4090         | 661                 | 1599  | 2.42                      |
| 4090 Laptop  | 343                 | vary  | -                         |

<!-- Geekbench -->
| Model      | Geekbench 6 (compute score) | Price | $/100k |
|------------|-----------------------------|-------|--------|
| 4090       | 317800                      | 1599  | 503    |
| 5090       | 387800                      | 1999  | 516    |
| M4 Max     | 180700                      | 1999  | 1106   |
| M3 Ultra   | 259700                      | 3999  | 1540   |

```

- Memory is the bottleneck. It limits the size of the models you can run and what you pay for.
  - Spark: 200B 
  - 5090 : 12B (raw)

- spark is $3, 999 and current M3 Max 28-Core CPU 60-Core GPU is the same price.

- I run 4 Mac Studio ultras at work (they‚Äôre pricy when maxed out), for local-first AI dev services. But there‚Äôs a few things that make me want to switch to the Spark. 
  - Networking is the biggest one, the Macs have Thunderbolt and Ethernet, but if I run distributed inference with EXO over Thunderbolt; the drop in tokens/second is massive. These Sparks get RDMA and can stack nicely. 
  - The other big one is access to CUDA, MLX has come a long way but being able to have CUDA and GPU access in containers would simplify the stack so nicely. 
  - If I had a USB-C/Thunderbolt backplane it might compare, but scaling with the Spark is likely a lot more straightforward.

- Biggest problem with Macs is that they don't have dedicated tensor cores in the GPU which makes prompt processing very slow compared to Nvidia and AMD.
  - there's been a little speculation that Apple adding TensorOps to Metal 4 suggests M5/M6 may get tensor cores.

- If that would be true why aren't Mac sales banned in China instead of Nvidia GPUs?
  - Because it's only a superior solution if you just want one box, and that mostly for inference. Once you start scaling to larger loads, it's much trickier to get a clusters of Macs to efficiently process them in parallel, whereas datacenter GPUs are designed for clusters.

- ## [What is the estimated token/sec for Nvidia DGX Spark : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1krsast/what_is_the_estimated_tokensec_for_nvidia_dgx/)
- generation rate (tokens / s) are almost always bound by memory bandwidth not compute. 
  - It will be bound by the LPDDR5x memory to 273 GB/s.
  - Of course the compute will help with prompt processing and batching multiple queries, and the huge RAM will allow you to (slowly) run big models

- Slightly more than Strix Halo, due to better GPU/drivers, but nothing major.

- Ohhh.. now i see why they are willing to sell this high memory product to the general public. This is straight up trash tier performance. Fast enough that it will be bought and used by AI developers and enthusiasts... but slow enough as to not be hoarded and abused by cloud providers.

- Strix Halo devices are all at $2000 and are now widely shipping from many manufacturers. These are RDNA3.5 devices and while WIP, have full PyTorch support.

- ## [Nvidia digits specs released and renamed to DGX Spark : r/LocalLLaMA _202503](https://www.reddit.com/r/LocalLLaMA/comments/1jedy17/nvidia_digits_specs_released_and_renamed_to_dgx/)
- Framework Desktop is 256GB/s for $2000‚Ä¶ much cheaper for running 70gb - 200 gb models than a Spark.
  - Yup, and being X86 is much more usable. These small AMD APUs are quite nice for a console/multi-media box purposes when not using LLMs. 
  - Nvidia offering is ARM so Linux only and not even X86 Linux so pretty much no gaming will be possible.

- ## [Local inference with Snapdragon X Elite : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1l5k290/local_inference_with_snapdragon_x_elite/)
- I've been using mine (Surface Laptop 7) since it came out. It's good, but not in the exact way marketed.
  - I use it with LM Studio and AnythingLLM running models up to about 21B, the model size is limited by my 32GB integrated RAM. The token rate on an 8B is like 17-20 per second. 
  - But the NPU doesn't seem to have to do with anything. All the inference is on CPU, but not in that bad way people complain about if they have Intel products, more in the good way people talk about if they have Macs.

- I've been using local inference on multiple Snapdragon X Elite and X Plus laptops.
  - In a nutshell, llama.cpp or Ollama or LM Studio for general LLM inference, using ARM accelerated CPU instructions or OpenCL on the Adreno GPU. CPU is faster but uses a ton of power and puts out plenty of heat; the GPU is about 25% slower but uses less than half the power, so that's my usual choice.
  - I can run everything from small 4B and 8B Gemma and Qwen models to 49B Nemotron, as long as it fits completely into unified RAM. 64 GB RAM is the max for this platform.

- NPU support for LLMs is here, at least by Microsoft. 
  - You can download AI Toolkit under Visual Studio Code or Foundry Local. 
  - Both of them allow running of ONNX-format models on the NPU. 
  - Phi-4-mini-reasoning, deepseek-r1-distill-qwen-7b-qnn-npu and deepseek-r1-distill-qwen-14b-qnn-npu are available for now.
- Microsoft has AI Foundry which uses the Hexagon NPU. You're limited to Phi models and DeepSeek Distill Qwen models. Performance is fine but the models are old.
- Microsoft just added Qwen 7B and 14B DeepSeek Distill models that run on NPUs. I think for the moment, only the Snapdragon X Hexagon NPU is supported using the QNN framework. These are ONNX models that require Microsoft's AI Toolkit to run. 

- the X elite, would maybe use the gpu for the transformers i dont know if it would be possible for the llm since im using llama cpp
  - I don't think it's possible. A lot of Python ML-related frameworks don't have ARM64 Windows wheels or they can't be compiled without getting into a dependency nightmare. They won't work under WSL Linux because there's no Vulkan or OpenCL GPU passthrough.
  - If you want to use the Adreno GPU for inference, you're stuck with llama.cpp, LM Studio or Ollama, which all use the same ggml backend code.

- [Snapdragon X CPU inference is fast! (Q\_4\_0\_4\_8 quantization) : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1emd3bg/snapdragon_x_cpu_inference_is_fast_q_4_0_4_8/)
  - On a Surface Pro 11 with a Snapdragon X Plus 10-core chip, running CPU inference for Llama 3.1 8B, I'm getting the following:
  - llama_print_timings: prompt eval time = 895.37 ms / 126 tokens ( 7.11 ms per token, 140.72 tokens per second)
  - llama_print_timings: eval time = 90360.33 ms / 1391 runs ( 64.96 ms per token, 15.39 tokens per second)
- I get ~5.5 tokens/s with Llama 3.1 8B Q8_0 and DDR4-3600 memory, so ~15.5 tokens/s for Q4 and dual-channel LPDDR5X-8400 memory doesn't seem that impressive.
- Well it's not close to m2 levels at all. M2 max gets 66 t/s and 671 pp/s at q4
- Wow, I have ~5.8 tokens/s on 8+gen1 in llama 3.1 8b

- ## [Snapdragon X Elite - local llm? : r/LocalLLaMA _202406](https://www.reddit.com/r/LocalLLaMA/comments/1ddyc51/snapdragon_x_elite_local_llm/)
- I have purchased the new Surface Pro CoPilot+PC and am struggling to get the LLMs to run using the embedded NPU. I guess I was spoiled by using Ollama and Llama.cpp, now I am having to learn the basics of quantizing a model and using HuggingFace APIs to host the models.

- Should be good for 13B. There is a video out there showing it‚Äôs real world performance already on a model called ‚ÄúPhi Silica‚Äù

- [Snapdragon X Elite llama.cpp ? : r/LocalLLaMA _202406](https://www.reddit.com/r/LocalLLaMA/comments/1dj6h6x/snapdragon_x_elite_llamacpp/)

- [$899 mini PC puts Snapdragon X Elite into a mini desktop for developers (with 32GB RAM) : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1cxhh1q/899_mini_pc_puts_snapdragon_x_elite_into_a_mini/)
  - every modern CPU since AMD phenom is a SoC with unified memory architecture. The key thing is the memory controller and how many steps it needs. CPUs benefit from being able to quickly go back and forth between ram compared to GPUs, which is why RAM performance isnt a direct competitor to VRAM in GB/s because the MT/s is more important for RAM and how efficiently your data and instructions are packed into every transfer.
  - Thanks for correcting that AMD and other competitors have had UMA for years. My mistake. But still stands that apple has 800GB/s memory bandwidth vs things like Snapdragon X Elite‚Äôs 136GB/s

- ## [È™ÅÈæôX EliteÂÜçÈÅ≠ÁóõÊÆ¥, Á¨¨‰∫å‰ª£ÈÖ∑ÁùøUltra Ëã±ÁâπÂ∞îMeteor LakeÊúâÂ§öÂº∫Ôºü - Áü•‰πé _202406](https://zhuanlan.zhihu.com/p/701851374)
- Á¨¨‰∏Ä‰ª£ÈÖ∑ÁùøUltraÂπ≥Âè∞ÔºàÊµÅÊòüÊπñÔºåMeteor LakeÔºâÁöÑÂõõÂ§ßÊ®°Âùó‰∏≠ÔºåÈô§‰∫ÜËÆ°ÁÆóÊ®°ÂùóÈááÁî®‰∫ÜËá™ÂÆ∂ÁöÑIntel 4Â∑•Ëâ∫ÔºåÂÖ∂‰ªñ‰∏â‰∏™Ê®°ÂùóÈÉΩÊòØÂè∞ÁßØÁîµ‰ª£Â∑•„ÄÇ
  - Âà∞‰∫Ü„ÄêÊúà‰∫ÆÊπñ„ÄëÔºåÂ∞±ËøûÊúÄÊ†∏ÂøÉÁöÑËÆ°ÁÆóÊ®°Âùó‰πüÊîπÁî®‰∫ÜÂè∞ÁßØÁîµÁöÑN3BÂ∑•Ëâ∫ÔºåËã±ÁâπÂ∞î‰ªÖ‰øùÁïô‰∫ÜËá™Â∑±ÁöÑÂÖàËøõÂ∞ÅË£ÖÂ∑•Ëâ∫ÔºàFoverosÔºâ„ÄÇ
  - Âú®„ÄêÊúà‰∫ÆÊπñ„Äë‰∏äÔºåËã±ÁâπÂ∞îËøòÂ∏¶Êù•‰∫ÜÁ±ª‰ººËãπÊûúÁªü‰∏ÄÂÜÖÂ≠òÊû∂ÊûÑÁöÑËÆæËÆ°ÔºåÁõ¥Êé•Â∞ÜLPDDR5X-8533ÂÜÖÂ≠òÂ∞ÅË£ÖÂú®‰∫ÜËäØÁâá‰πã‰∏äÔºåÂèØÈÄâ16GBÊàñ32GBÂÆπÈáè„ÄÇ
  - ËøôÁßçËÆæËÆ°ÁöÑÂ•ΩÂ§ÑÊòØÔºåËÉΩ‰ΩøÊï∞ÊçÆ‰º†ËæìË¥üËΩΩÈôç‰ΩéÂ§ßÁ∫¶40%ÔºåÂª∂ËøüÊõ¥‰ΩéÔºåÁõ∏ËæÉ‰º†ÁªüÁöÑÊùøËΩΩÂÜÖÂ≠òËøòËÉΩËäÇÁúÅÂ§ßÁ∫¶250Âπ≥ÊñπÊØ´Á±≥ÁöÑ‰∏ªÊùøÁ©∫Èó¥
- „ÄêÊúà‰∫ÆÊπñ„ÄëÁöÑÊ†∏ÂøÉÁ´û‰∫âÂäõÔºåÂ∞±ÊòØËÆ°ÁÆóÊ®°Âùó‰∏≠ÁöÑCPU„ÄÅGPUÂíåNPU‰∏âÂ§ßÂçïÂÖÉÈÉΩËøéÊù•‰∫ÜÂÖ®Èù¢ÊèêÂçá„ÄÇ
  - È¶ñÂÖàÂ∞±ÊòØNPUÔºå‰ªéÁ¨¨‰∏Ä‰ª£ÈÖ∑ÁùøUltraÁöÑ11TOPSÔºåÂ§ßÊ∂®Âà∞48TOPS
  - „ÄêÊúà‰∫ÆÊπñ„ÄëCPUÈÉ®ÂàÜÁöÑAIÁÆóÂäõ‰∏∫5TOPSÔºåGPU‰∏∫67TOPSÔºåÂú®ÂºÇÊûÑËÆ°ÁÆóÁöÑÂä†ÊåÅ‰∏ãÔºåÊï¥‰ΩìAIÁÆóÂäõÈ´òËææ120TOPSÔºå‰∏Ä‰∏æË∂ÖË∂ä‰∫ÜÈ™ÅÈæôX EliteÁöÑÊï¥‰Ωì75TOPSÔºàNPU‰∏∫45TOPSÔºâÔºåÂíåÈîêÈæôAI 300ÁöÑÊï¥‰Ωì80TOPSÔºàNPU‰∏∫50TOPSÔºâ„ÄÇ
  - AI PCÊó∂‰ª£‰πãÊâÄ‰ª•Ê†ºÂ§ñÂº∫Ë∞ÉAIÁÆóÂäõÔºåÂ∞±ÊòØÂõ†‰∏∫ÂæÆËΩØÂç≥Â∞ÜÂú®WindowsÊìç‰ΩúÁ≥ªÁªüÂ±ÇÈù¢ÔºåÂ∞±ÊääÁîüÊàêÂºèAIÊäÄÊúØÂ∫îÁî®Âà∞Âü∫Á°Ä‰ΩìÈ™å‰πã‰∏≠„ÄÇÂ¶ÇÊûú‰ΩøÁî®CPUÂíåGPUËøõË°åÂ§ÑÁêÜÔºåÁ¨îËÆ∞Êú¨ÁöÑÁª≠Ëà™‰ºöÂ∞øÂ¥©„ÄÇÊ≠§Êó∂ÔºåÂîØÊúâ‰ΩéÂäüËÄóÈ´òAIÊÄßËÉΩÁöÑNPUÔºåÊâçËÉΩÂú®ÂÖºÈ°æÁª≠Ëà™ÁöÑÂêåÊó∂ÔºåÈöèÊó∂ÈöèÂú∞‰∫´ÂèóAIÂ∏¶Êù•ÁöÑ‰æøÂà©
  - Âú®GPUÊñπÈù¢Ôºå„ÄêÊúà‰∫ÆÊπñ„ÄëÂçáÁ∫ßÂà∞‰∫ÜÊñ∞‰∏Ä‰ª£ÁöÑXe2Êû∂ÊûÑÔºåÊÄßËÉΩÊúâ‰∫ÜÂπ≥Âùá50%ÁöÑÊèêÂçáÔºåÊúâÊú∫‰ºö‰∏éAMDÈîêÈæôAI 9 HX 370ÈõÜÊàêÁöÑRadeon 890MÊé∞Êé∞ÊâãËÖï

- ## üÜöüå∞ [AMD AI Max+ 395 CPU Êú¨Âú∞Â§ßÊ®°ÂûãÊé®ÁêÜÊÄßËÉΩËØÑÊµãÊä•Âëä - Áü•‰πé _202509](https://zhuanlan.zhihu.com/p/1952045270763283746)
- ÈíàÂØπÊê≠ËΩΩAMD AI Max+ 395 CPUÁöÑÈõ∂ÂàªGTR9Ëø∑‰Ω†‰∏ªÊú∫ËøõË°å‰∫Ü‰∏ÄÁ≥ªÂàó‰∏•Ê†ºÁöÑÂ§ßÊ®°ÂûãÊé®ÁêÜÈÄüÂ∫¶ÊµãËØï„ÄÇ
  - Á°¨‰ª∂Âπ≥Âè∞: Èõ∂Âàª (MINISFORUM) GTR9 Ëø∑‰Ω†‰∏ªÊú∫
  - Ê†∏ÂøÉÁªÑ‰ª∂: AMD AI Max+ 395 CPU
  - ‰ªªÂä°Á±ªÂûã: Êú¨Âú∞Â§ßËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜ
  - ÊÄßËÉΩÊåáÊ†á: Tokens/s (ÊØèÁßíÁîüÊàêTokenÊï∞) ‚Äî ËØ•Êï∞ÂÄºË∂äÈ´òÔºå‰ª£Ë°®Êé®ÁêÜÈÄüÂ∫¶Ë∂äÂø´
- ËÆæËÆ°‰∫ÜÊ∂µÁõñÂ§öÁßç‰ªªÂä°Á±ªÂûãÁöÑÊ†áÂáÜÂåñÈóÆÈ¢òÔºö
  - ÁªºÂêàËÉΩÂäõ: "‰Ω†ÊòØË∞ÅÔºüËØ∑ËØ¶ÁªÜ‰ªãÁªç‰∏Ä‰∏ã‰Ω†ËÉΩÂπ≤‰ªÄ‰πà„ÄÇ"
  - Áü•ËØÜÈóÆÁ≠î: "‰Ωú‰∏∫‰∏ì‰∏ö‰∫∫Â∑•Êô∫ËÉΩ‰∏ìÂÆ∂ÔºåËØ∑ÂëäËØâÊàëÂ¶Ç‰ΩïÂ≠¶‰π†Ê∑±Â∫¶Â≠¶‰π†Ôºü"
  - Êï∞Â≠¶ËÆ°ÁÆó: "Â¶ÇÊûúA+B=12, A-B=10ÔºåÂàôAÁöÑÂÄºÊòØÔºü"
  - Ëá™ÁÑ∂ËØ≠Ë®ÄÁêÜËß£: "ËØÜÂà´Âè•Â≠ê‚ÄòÊàëÂ∞Ü‰ºöÂú®ÊòéÂ§©Êó©‰∏äÁöÑ8ÁÇπÂà∞ÊπñÂåóÈªÑÈôÇÁöÑÊ£ÆÊûóÂÖ¨Âõ≠‚Äô‰∏≠ÁöÑÊâÄÊúâÂú∞Âêç„ÄÇ"
  - ‰ª£Á†ÅÁîüÊàê: "ËØ∑‰ΩøÁî®PythonÁºñÂÜô‰∏Ä‰∏™Ë¥™ÂêÉËõáÊ∏∏Êàè„ÄÇ"

- ÂèÇËØÑÂ§ßÊ®°Âûã:
  - deepseek-r1:70b, 30
  - qwen3 Á≥ªÂàóÔºà32b / 30b / 14b / 8bÔºâ
  - gpt-ossÔºà120b / 20bÔºâ

```markdown
| Model          | Ollama | LM Studio |
|----------------|--------|-----------|
| deepseek-r1:70b | 4.43  | 4.97      |
| qwen3:32b      | 8.97   | 10.12     |
| qwen3:14b      | 19.47  | 21.70     |
| qwen3:8b       | 29.93  | 35.96     |
| gpt-oss:120b   | 30.84  | 42.07     |
| gpt-oss:20b    | 42.57  | 60.54     |
| qwen3:30b      | 48.93  | 68.70     |
```

- ÂØπÊØî‰∏§ÁªÑÊï∞ÊçÆÂèØËßÅÔºåÂêå‰∏ÄÊ®°ÂûãÂú®LM-Studio‰∏≠ÁöÑÊé®ÁêÜÈÄüÂ∫¶ÊôÆÈÅç‰ºò‰∫éOllama
- AMD AI Max+ 395 CPUÈááÁî®CPU/GPUÂÖ±‰∫´ÂÜÖÂ≠òÁöÑÁªü‰∏ÄÂÜÖÂ≠òÊû∂ÊûÑÔºàUMAÔºâÔºåËøôÁßçËÆæËÆ°Â§©ÁÑ∂ÈÄÇÂêàËøêË°åÊ∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâÊ®°ÂûãÔºàÂ¶Çgpt-ossÁ≥ªÂàó„ÄÅqwen3:30bÔºâ„ÄÇ
  - MoEÊ®°ÂûãËôΩÁÑ∂ÊÄªÂèÇÊï∞ÈáèÂ∫ûÂ§ßÔºå‰ΩÜÊØèÊ¨°Êé®ÁêÜ‰ªÖÊøÄÊ¥ªÈÉ®ÂàÜ"‰∏ìÂÆ∂"ÂèÇÊï∞ÔºåÈùûÂ∏∏Â•ëÂêàËøôÁßçÂ§ßÂÆπÈáèÂÜÖÂ≠ò‰ΩÜÁªùÂØπÁÆóÂäõÁõ∏ÂØπÊúâÈôêÁöÑÁ°¨‰ª∂„ÄÇ
  - Áõ∏ÊØî‰πã‰∏ãÔºåÂØπ‰∫éÂèÇÊï∞ÂØÜÈõÜÁöÑ‰º†ÁªüÁ®†ÂØÜÊ®°ÂûãÔºàÂ¶Çdeepseek-r1:70b„ÄÅqwen3:32bÔºâÔºåÁî±‰∫éÈúÄË¶ÅÊõ¥È´òÁöÑÁªùÂØπÁÆóÂäõÔºåËØ•Â§ÑÁêÜÂô®ÁöÑÈõÜÊàêÊòæÂç°ÂàôÁ®çÊòæÂêÉÂäõ„ÄÇ

- DFRobot‰Ωú‰∏∫Âú®ÂçïÊùøËÆ°ÁÆóÊú∫ÔºàSBCÔºâ„ÄÅAIËæπÁºòËÆ°ÁÆóÂíåÂºÄÊ∫êÁ°¨‰ª∂È¢ÜÂüüÁöÑÂàõÊñ∞ËÄÖÔºåÊ≠§Ê¨°ÊµãËØïÁªìÊûúÊÑè‰πâÈùûÂá°„ÄÇËã•Êú™Êù•DFRobotÊé®Âá∫Âü∫‰∫éAMD AI Max+ 395 CPUÁöÑÂçïÊùøËÆ°ÁÆóÊú∫ÔºåÂ∞ÜÂÖ∂Âº∫Â§ßÁöÑÊú¨Âú∞AIÊé®ÁêÜËÉΩÂäõ‰∏éDFRobotÊàêÁÜüÁöÑÊ®°ÂùóÂåñ‰º†ÊÑüÂô®ÁîüÊÄÅÔºàÂ¶ÇGravityÁ≥ªÂàóÔºâÁõ∏ÁªìÂêàÔºåÂ∞ÜÂÇ¨ÁîüÂá∫Êõ¥Â§öÂÆûÊó∂„ÄÅÊô∫ËÉΩÁöÑÁâ©ËÅîÁΩë‰∏éÊú∫Âô®‰∫∫Â∫îÁî®

- ## üÜö [AI max+ 395 128gb vs 5090 for beginner with ~$2k budget? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1nunlls/ai_max_395_128gb_vs_5090_for_beginner_with_2k/)
- ComfyUI? 5090.  LLMs? AI Max
  - FWIW, you can also use a thunderbolt eGPU with the AI Max.

- I've been able to run ComfyUI on max+ 395. It's a pain, but it's possible.

- As for ComfyUI, I run that just fine on my Max+ 395 as well. But saying ComfyUI doesn't mean much. It's just a framework to run things. What exactly do you want to do? If it's just image gen, then the Max+ is as good as anything else. If it's video gen, it does work but uses way more memory than Nvidia. If it's voice cloning, that just works too. So other than using way so much memory, which you have plenty of on a Max+ 128GB, what is this bugginess you are talking about?
  - Max+ 395 owner, and I agree, the level of experimenting i've reached has been great with the device.
- I love my 395+ 128 GB, and I've got LLM, image generation, voice, etc. working, but I cannot for the life of me get video working. Mainly trying to experiment with I2V with WAN 2.1 and 2.2, but have never got a successful run. Either get OOM or seemingly infinite time per iteration. Not sure if you can provide any tips. Using the pytorch 2.9.0+rocm7 RC since that was the most stable for everything else.

- I‚Äôd personally go 395, because almost all AI labs are going with MoE architecture at this point. So big RAM can accomplish a lot even if it‚Äôs not the fastest.

- The AI Max+ 395 handles all my AI needs for now and it‚Äôs pretty snappy once you get things working in Linux.
  - The downside is if you need/want to run anything super fast - you will need a different setup. Eventually I might get a setup like that, and use the Max+ 395 for my mobile AI needs.
  - Still, I can get 20+ TPS on GLM Air 4.5, and even faster with the GPT-OSS models.

- How much memory ddr5 with 5090? Just 32gb is limited 128gb can run huge Moe's.
  - If your running 30b models and under 5090 all day. 
  - If your running large 120b moe models or smaller dense models 395plus all day.

- if you want to do some training stuff, CUDA, and ... maybe gaming, go for the 5090
  - if you just want to run some local model to try them or serve them locally 24/7, strix halo is probably a better option, almost all the best latest open model are MoE and they need a lot of ram/vram (glm 4.5 air, gpt oss 120b, some qwant of qwen 235, qwen 3 next, ... will all run better on strix halo vs 5090+128gb ram). Also strix halo is usually a full machine, while the 5090 you still need the rest of the PC

- AMD AI 395 miniPC preferably with Oculink (there are couple) where you can hook later external dGPU like AMD AI PRO R9700.

- If you're interested in image/video gen, you need the 5090. That is going to be miserably slow on CPU.
  - If you just want to run MoE models interactively, the AI 395 will be enough.
  - Personally? I'd go with both. You can in fact attach a 5090 to an AI 395 system. Then you can run LLMs for your prompt generation/improvement workflows while generating video and imagery to your heart's content.
  - I don't think people who have not experienced a Blackwell GPU understand how much faster they are for image/video work. Even against a 4090 my workflow times cut in half.
- I agree, I think I start with 395 and add the 5090 if it becomes necessary.

- if it was only to play around and learn llm stuff i'd lean towards the framework 128.
  - but the moment you mention comfy, amd is off the table. get the 5090 instead.

- If you're looking mostly to be able to inference bigger MoE models with llama.cpp then I think the 395 can be a good choice.
  - However, if you have an interest in playing round w/ image/video generation, doing any further poking around (vLLM/SGLang, voice, PyTorch, model training/fine tuning, etc) then I think the 5090 is the clear way to go.
  - If you're working w/ smaller models btw it's not just about stability either, the 5090 is a whole different animal in performance - as long as you're OK w/ running ~30B/smaller models.

- ## [Advice: 2√ó RTX 5090 vs RTX Pro 5000 (48GB) for RAG + local LLM + AI development : r/LocalLLM _202509](https://www.reddit.com/r/LocalLLM/comments/1nsyiag/advice_2_rtx_5090_vs_rtx_pro_5000_48gb_for_rag/)
- As someone with 2x 5090s, started from 1.. don't do it. Go straight to the pro 6000. Thank me later. It'll save you time, money, and effort. :) I now have a pro 6000. You can pick one up from Exxact for $7200.
  - Dual 5090s are great‚Ä¶ you can run many models. 
  - However, even with quantized KV cache it‚Äôll crash often. You can‚Äôt max out the 1million context Qwen coder. You can‚Äôt really fine tune models that don‚Äôt fit in a single GPU. You‚Äôll have issues with accelerate. 
  - Heat is very hot. At least 10 degrees warmer in a whole room when fine tuning. Power usage is very high. You‚Äôll need to run new electrical or have your machine shut off when someone plugs in a vacuum. You‚Äôll need to run dual psus or buy a 2000w+ psu. 
  - Case options are very limited. The cards are massive in size. You‚Äôll need 8 slot pc cases minimum for 2 cards. 
  - You‚Äôll want to run 70b+ models guaranteed. Even Qwen next 80b. 2x 5090s isn‚Äôt enough. 
  - This is all solved with a single pro 6000. So for an extra $1000 you can save yourself a lot of headaches and get access to nvidia enterprise

- Rent GPUs online unless you have a reason other than inference

- Remember the most important thing: whatever you choose initially, you will have to switch to RTX 6000 PRO 96Gb on Epic or Threadripper platrofm eventually if you want to continue running large local llms efficiently. The cost of those things will dwarf your current current setup. So, do not sweat much while deciding on the current config. Just make sure that your case and motherboard allow to connect at least 2 GPUs via riser cards with PCI 5.0 on each, even if at x8 speed. 

- Be sure you get a motherboard with dual x8 pcie slots if you go with two GPUs. Most AM5 boards don‚Äôt support that but a handful do. Just wanted to give you a heads up
  - Yeps, Asus Pro Art 2x pice 5.0

- ## [The NVIDIA DGX Spark at $4, 299 can run 200B parameter models locally - This is our PC/Internet/Mobile moment all over again : r/LlamaFarm _202509](https://www.reddit.com/r/LlamaFarm/comments/1nee9fq/the_nvidia_dgx_spark_at_4299_can_run_200b/)
- DGX has 273gb/s versus 1.7Tb/s for a rtx 5090...
  - The bandwidth is terrible. You'll run your models at 5 tps
  - I literally don't see who's gonna buy this aside from people who just got blinded by the unified RAM...
- At 273 GB/s memory bandwidth this unified RAM there will make inference VERY SLOW... For comparison: RTX 4090 has 1008 GB/s and RTX 5090 has 1792 GB/s memory bandwidth

- u can easily get an m1 max with 128 gb of unified memory for 1k cheaper than that and double the memory bandwidth. Don‚Äôt see how this is a good deal.
  - It's not a good deal. It is more of a sign. MLX is growing, but the ecosystem is still tiny compared to CUDA. With Blackwells coming to PCs and AMD shipping powerful GPUs, there will be real competition in this area for the first time.

- M3 Ultra with 256GB RAM does cost roughly $1000 more, but has 256GB unified memory
  - And 819 gb/s vs Spark's 273.

- Mac Studio, Ultra series, either M1 or M2 with 96GB or 128GB RAM. And more recently, the M4 Max. I bet by next year the Mini will likely have a 128GB configuration too.
  - What I am NOT saying is that Apple‚Äôs offerings are the best local inference for 128GB workloads. I am saying they are relatively cheap and capable, and the inflection point for fairly large local models (>60GB) was two years ago.
  - I bought my M4 Max 128GB last year for local models. It does not disappoint.

- No thanks, I'm not taking my wallet out until I see the AMD Medusa Halo 128GB AI mini-PCs in two years. I CAN WAIT.

- Jetson AGX Thor claims 2000 TOPS of AI at FP4, but it is the same memory bandwidth as DGX spark so I have my doubts. Guess we have to wait for some benchmarks.

- [DGX spark vs MAC studio vs Server (Advice Needed: First Server for a 3D Vision AI Startup (\~$15k-$22k Budget) : r/deeplearning](https://www.reddit.com/r/deeplearning/comments/1lylfuw/dgx_spark_vs_mac_studio_vs_server_advice_needed/)
- Macs are decent for inference, but nobody "real" is training models on Mac. Even Apple was using TPUs earlier (when that team was still run by the ex-Google guy) and grapevine says they're on Nvidia now

- DGX Spark is like a RTX 5060(70?) class GPU with 128GB of slowish (for GPU) memory.
  - The only thing Spark really has going for it is it might be SM100 (real Blackwell with Tensor Memory) instead of SM120 (basically Ada++) which may be useful for developing SM100 CUDA kernels without needing a B200.
  - Much better off I think with a NVIDIA RTX PRO 6000 Blackwell Series (96GB) for most people, or 512GB Mac Studio if you need very large LLMs but less GPU perf.

- ## üçé [MacBook M4 Max isn't great for LLMs : r/LocalLLaMA _202503](https://www.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/)
- M4 Max is about 50% faster than an Nvidia P40 (both in compute throughput and memory bandwidth). 
  - It is about 2.5x slower than a 3060 in compute¬π throughput (FP16) and 50% faster in memory bandwidth. 
  - Compared to 3090, it is about 7x slower in compute¬π throughput (FP16) and almost 2x slower in memory bandwidth.
- P40s (and generally Pascal) were the last ones without tensor cores (which increase FP16 throughout by 4x).
  - The lack of tensor cores is also the reason Apple M3 Ultra/M4 Max and AMD 395 Max, lag in Prompt Processing throughput compared to Nvidia, even if the M3 Ultra almost matches a 3080/4070 in raster throughput (FP32).
  - Compared to CPU-only inference, P40s are still great value, since they cost $150-300 and are only matched by dual 96-core Epycs with 8-12 channel DDR5 which start from $5000 used.
- Pascal doesn't even have FP16 support, all the operations are done through fp32 units afaik so throughput is effectively halved. It wasn't until Ampere that NVidia had FP16 support.

- Try swapping to serving with LMStudio - then use MLX, and speculative decoding with 0.5b as draft for 14b! Tripled my speed on my M1 Max
  - Speculative decoding really is great. It at least doubled my speeds. In token generation. Prompt processing didn't get and bump though. I'd love to have a 128gb+ RAM machine to also activate KV Cache

- ## [Êú¨Âú∞ÈÉ®ÁΩ≤Â§ßÊ®°ÂûãÊÄß‰ª∑ÊØî‰πãÁéãÁúüÁöÑÊòØApple Mac Studio M3 Ultra 192Êàñ512ÂêóÔºü - Áü•‰πé _202503](https://www.zhihu.com/question/14548406514)
- deepseek r1 Áî®mac studioÊµãËØï
  - ollama: 16 tops
  - lmstudio: 18 tops
  - 512GbÂÜÖÂ≠òËøòÊòØÂè™ËÉΩÈÉ®ÁΩ≤4bitÈáèÂåñÁâàÊú¨Ôºå8bit‰∏çË°å
- Â¶ÇÊûúÊåâÁÖß18.11token/sÁöÑËæìÂá∫ÈÄüÂ∫¶Ôºå‰∏çËÄÉËôëÂÖ∂‰ªñÔºåÂÖ®Â§©24Â∞èÊó∂ËøêË°åÔºà‰∏çËÄÉËôëprefillÁ≠âÊó∂Èó¥Ôºâ18.11√ó86400s=156.5‰∏átokensÔºåËÄåÂÆòÁΩëÁôæ‰∏átokensÂîÆ‰ª∑Êâç16ÂùóÔºå156.5‰∏áÂîÆ‰ª∑25ÂùóÈí±
  - ËÄåMac studio 512GÂÜÖÂ≠òÔºå1tÁ°¨ÁõòÁâàÊú¨ÂîÆ‰ª∑7.3wÊïôËÇ≤‰ºòÊÉ†‰πüË¶Å6.7wÔºåÈúÄË¶Å365√ó24h‰∏çÂÅúËøêË°å8Âπ¥/7.3Âπ¥ÊâçËÉΩÂõûÊú¨

- ÈÇ£Ë¶ÅÁúã‰Ω†ÊÄé‰πàÂÆö‰πâÊÄß‰ª∑ÊØî‰∫ÜÔºå[ktransformers] + [epyc] Áé∞Âú®‰πüÂ∑≤ÁªèÁé©ÂæóÂæàÂº∫‰∫ÜÔºåËãπÊûúËÉúÂú®‰∏äÊâãÂ∞±ËÉΩÁî®‰∏çÁî®ÊäòËÖæË£ÖÔºåÂú®ÈÇ£‰∏™‰ª∑‰ΩçÁÆó‰∏Ä‰∏™ÂèØË°åÊñπÊ°àÔºå‰ΩÜÂπ∂ÈùûÁÆóÂîØ‰∏ÄÊÄß‰ª∑ÊØîÊñπÊ°à

- ## [MAC mini M4ËäØÁâá32G+256ËÉΩË∑ëÂ§ßÊ®°ÂûãÂêóÔºü - Áü•‰πé _202503](https://www.zhihu.com/question/14795834393)
- ÁúüË∑ëaiÂè™Êé®ËçêPro‰ª•‰∏äÁöÑËäØÁâá„ÄÇ
  - m4ÁöÑÂÜÖÂ≠òÂ∏¶ÂÆΩÂÖ∂ÂÆûÂíåÊôÆÈÄöÊ†∏ÊòæwinÊú∫Â≠êÂ∑Æ‰∏çÂ§ö„ÄÇËÄåÂà∞‰∫ÜProÔºåÂÜÖÂ≠òÂ∏¶ÂÆΩÂ∞±ËææÂà∞256‰∫ÜÔºåËææÂà∞4060ÁöÑÊ∞¥ÂáÜ„ÄÇ
  - maxÂíåultraÂ∏¶ÂÆΩÂàÜÂà´ÊòØ400ÔºãÂíå800ÔºãÔºåÂü∫Êú¨ÊòØÊòæÂç°Á∫ßÂà´ÁöÑÂ∏¶ÂÆΩÔºåÁî®Êù•Ë∑ëaiÊé®ÁêÜÂæàÂêàÈÄÇ„ÄÇ

- ‰∏ªË¶ÅÊòØÂÜÖÂ≠òÂ§ßÂïäÔºåÊàëÁî®‰∏Ä‰∏áÂá∫Â§¥ÁöÑmacËÉΩË∑ë27BÁöÑgemma3 8bitÈáèÂåñÔºåÂ¶ÇÊûúÁî®ÊòæÂç°ÁöÑËØùËá≥Â∞ëÂæóÊòØ‰∏™3090Á∫ßÂà´ÁöÑ

- Mac Mini ÂΩìÂâØÊú∫ÂèØ‰ª•ÔºåÁªü‰∏ÄÂÜÖÂ≠òÁúãÁùÄÈ¶ôÔºå‰ΩÜÂæàÂ§öÊ®°Âûã‰∏çÊîØÊåÅ MPSÔºåÊàñËÄÖÁî® MPS Ë∑ëÂæóÊØî CPU ËøòÊÖ¢„ÄÇ

- ## [Apple M5 could ditch unified memory architecture for split CPU and GPU designs | Hacker News _202412](https://news.ycombinator.com/item?id=42552494)
- UMA hurts the GPU too much. Widely parallel processing wants to access memory in bigger chunks than a CPU. If you try to mix access and modification, you lose the benefit of widely parallel processing. 
  - Other GPU designers have considered and eschewed unified memory models, to the tune of hundreds of millions in research dollars.
- I agree that single cache-line fetches are pretty poor for parallel vector units, but supporting the former in an environment designed for the latter doesn't seem to off-putting (the CM-5 did this).

- You can split the CPU and GPU and still have UMA. Splitting CPU/GPU is a packaging and interconnect concern and is not mutually exclusive with UMA.

- ## [Ê±ÇÊé®ËçêÔºÅÊÉ≥ÁªÑ‰∏ÄÂè∞256G‚ûïÁ¨îËÆ∞Êú¨Ë∑ëÂ§ßÊ®°Âûãü•π - Â∞èÁ∫¢‰π¶](https://www.xiaohongshu.com/explore/6892017700000000030274de?xsec_token=ABHUpmj6nmLewRxGBOYsEYt2FVRxDHjqOHSnhQfgUJJnc=&xsec_source=pc_search&source=web_explore_feed)
- ÊôÆÈÄöÁ¨îËÆ∞Êú¨‰∏çÊîØÊåÅ256ÔºåÂà´ÂºÇÊÉ≥Â§©ÂºÄ
  - Á¨îËÆ∞Êú¨ÂÜÖÂ≠òÊúâÊòØÊúâÔºå‰ΩÜÊòØÁ¨îËÆ∞Êú¨‰∏çÊîØÊåÅÔºå‰π∞‰∫Ü‰πüÊ≤°Áî®ÔºåÂà∞Êó∂ÂÄôÊèí‰∏äÂéª‰∏çËØÜÂà´Â∞±ËÄÅÂÆû‰∫Ü
- Âà´ÊÉ≥‰∫ÜÔºåÁ¨îËÆ∞Êú¨‰∏çÊîØÊåÅeccÂÜÖÂ≠òÔºåË∑ëÊó∂Èó¥ÈïøÂÆπÊòìÂá∫Èîô
- Âì™‰∏™ËãπÊûúÁ¨îËÆ∞Êú¨Êúâ512ÂÜÖÂ≠ò ‰Ω†ÊâæÂá∫Êù•ÊàëÁúãÁúãÔºåÊàëÂ∞±Áü•ÈÅìÊé®Âá∫Ëøá2tÂÜÖÂ≠òÁöÑmac proËøòÊúâ512ÂÜÖÂ≠òÁöÑmac studioÔºåmbpÊúÄÂ§ßÁöÑÂ∞±128gÂÜÖÂ≠òÂêß m4max

- Êï£ÁÉ≠Âéã‰∏ç‰ΩèÁöÑ

- Á¨îËÆ∞Êú¨ÁõÆÂâçËÉΩÊú¨Âú∞ÈÉ®ÁΩ≤Â§ßÊ®°ÂûãÁöÑÂè™ÊúâmÁ≥ªËäØÁâáÁöÑmbpÂÜÖÂ≠òÊãâÊª°Ôºà‰ªçÁÑ∂ÊòØÂãâÂº∫Â§üÁî®Ê∞¥Âπ≥ÔºâÂíåryzen ai max+395ÊùøËΩΩÂÜÖÂ≠òÊãâÊª°128ÔºàËøô‰∏™ÁöÑÂÜÖÂ≠òÂíåÊïàÁéá‰º∞ËÆ°‰∏çÂ§™Â§üÔºåÂîØ‰∏Ä‰ºòÂäøx86Êó•Â∏∏Êñπ‰æøÔºâÔºåÂõ†Ê≠§‰∏ªË¶ÅÂæÄitx‰∏äÈù¢ÂéªÂáëÔºåÁé∞Êúâ‰∫ßÂìÅ‰πüÂ∞±ÊòØÊàëËØ¥ÁöÑËøô‰∏§Â•óÂç≥ËãπÊûúmÁ≥ªÂíåamd aiÔºåÂõ†‰∏∫ÊúâÁªü‰∏ÄÂÜÖÂ≠òÂèØ‰ª•Áî®Ê†∏ÊòæÊù•Ë∑ëaiÂÖçÂéªÂ§öË∑ØgpuÁöÑÈ∫ªÁÉ¶„ÄÇ

- Â¶ÇÊûú‰Ω†ÈúÄË¶ÅcpuÈ´òÊÄßËÉΩÔºåÈÇ£‰πàÂèØ‰ª•ËÄÉËôëÁî®ÊØîËæÉÊñ∞ÁöÑÊúçÂä°Âô®Âπ≥Âè∞ÔºàÂõ†‰∏∫Êàë‰º∞ËÆ°ËøûhedtÈÉΩÊó†Ê≥ïÊª°Ë∂≥‰Ω†ÁöÑÂÜÖÂ≠òÈúÄÊ±ÇÔºâÂíåitx‰∏ªÊùøÔºàÂΩìÁÑ∂ÔºåÂçéÊìé‰∏ç‰∏ÄÂÆöËøòÂú®ÂÅöËøôÁ±ªÂ•áÁâπÁöÑ‰∫ßÂìÅÔºâ„ÄÇ‰Ω†Âè™ÈúÄÊ±ÇÂçïÂç°ÁöÑËØùÂèØ‰ª•ÂãâÂº∫Â°û‰∏ÄÂ•óitxÔºå‰ΩÜÊòØÂêÑÊñπÈù¢ÈÉΩÂæàÊûÅÈôê‰∫ÜÔºåÂåÖÊã¨‰æõÁîµÂíåÊï£ÁÉ≠ÔºåÂ§ßÊ¶ÇÊòØË∑ë‰∏çÊª°ÁöÑ„ÄÇÂ¶ÇÊûú‰Ω†ÈúÄÊ±ÇÂ§öÂç°ÁöÑËØùÔºåÂèØËÉΩÂæóÂá†Âº†Âç°Áî®Êâ©Â±ïÂç°ËøûÊé•‰ª•ÂàÜÁùÄÁî®itx‰∏ªÊùø‰ªÖÊúâÁöÑÈÇ£‰∏Ä‰∏§‰∏™pcie x16Êé•Âè£ÔºåËøôÊ†∑ÂÅöÊÄßËÉΩËÇØÂÆö‰ºöÊúâÊäòÊçüÔºåÂπ∂‰∏î100%ÈúÄË¶ÅÂÅöÂàÜ‰ΩìÁöÑ‰∏§‰∏™itxÊú∫ÁÆ±ÔºåËøôÁßçÊÉÖÂÜµ‰Ω†ÈúÄË¶ÅËá™Â∑±ÁîªËÆæËÆ°ÂõæÂéªtb‰πãÁ±ªÁöÑÂπ≥Âè∞ÊâæÂ∑•ÂéÇÂÆöÂÅöÊùøÊùêÔºåÂ•ΩÂ§ÑÊòØ‰æõÁîµÂíåÊï£ÁÉ≠Ê≤°‰πãÂâçÈÇ£‰πàÊûÅÈôêÔºåÂùèÂ§ÑÊòØËøô‰∏ÄÂ•ó‰∏ãÊù•‰º∞ËÆ°‰∏çÊØîeatxÊú∫ÁÆ±ËΩª‰æøÂ§öÂ∞ë„ÄÇ

- ## [ÈÄâ‰∏™ËÉΩÊú¨Âú∞Ë∑ë70BÂ§ßÊ®°ÂûãÁöÑÁ¨îËÆ∞Êú¨ÂΩì‰∏ªÂäõÊú∫ËØï - Â∞èÁ∫¢‰π¶](https://www.xiaohongshu.com/explore/6811b115000000002301de1a?xsec_token=ABZEBEajWMF79jFUPp-RsmTEccFriosXMc79IFdgiGHWo=&xsec_source=pc_search&source=web_explore_feed)
  - ‰πãÂâç‰∏ÄÁõ¥Áî®ÁÅµÂàÉ16 2024ÁâàÁöÑÂΩì‰∏ªÂäõÊú∫ÔºåË∑ëËµ∑Êù•È£éÊâáÂÆûÂú®ÊòØÂìçÔºåÂºÄ‰∏™Á∫ø‰∏ä‰ºöÂà´‰∫∫ÈÉΩÂê¨‰∏çÊ∏ÖÔºåËÄå‰∏îÈáçÈáèÂÆûÂú®Êó†Ê≥ïÁßªÂä®„ÄÇ
  - ÊúÄËøëÁúãÂà∞HP Êàò99 UltraÂà∞Ë¥ßÂ∞±Áõ¥Êé•Êãø‰∏ã‰∫Ü 128G„ÄÇÁî®‰∫Ü‰∏ÄÂ§©ÔºåÊÑüËßâAMD AI395+Ë∑ü‰πãÂâçi9-14900hxÊÄßËÉΩÂ∑ÆÂà´‰∏çÂ§ß ‰∏çËøáÂô™Èü≥Â•Ω‰∫ÜÂæàÂ§öÔºåÈõÜÊòæ8060sÁöÑÊÄßËÉΩËôΩÁÑ∂‰∏çÂ¶Ç‰πãÂâç4070Ôºå‰ΩÜÊòØË∑ë‰∏™Êú¨Âú∞Â∞èÊ®°Âûã‰πüÊ≤°‰ªÄ‰πàÂéãÂäõ„ÄÇ
  - ÈáçÈáè‰∏ä1.6KGÁ®çÂæÆÈáç‰∫Ü‰∏ÄÁÇπÁÇπÔºåË∑üX1CÊ≤°Ê≥ïÊØîÔºåË∑ümacbook pro 14Â∑Æ‰∏çÂ§öÔºåÊØîÁÅµÂàÉÂº∫Â§™Â§ö‰∫Ü„ÄÇ
  - Ëß¶Â±èÁöÑ

- pdÂÖÖÁîµËÉΩÊúâÂéüÂéÇÂÖÖÁîµÂô®Âá†ÊàêÁöÑÊÄßËÉΩÔºü
  - ÂéüÂéÇ130wÁöÑÔºå65wÁöÑPD‰ºöÊèêÁ§∫ÊÖ¢ÈÄüÂÖÖÁîµÂô®ÔºåÂè™Êúâ‰∏ÄÂçä‰∏çÂà∞ÁöÑÈÄüÂ∫¶

- Êú¨Âú∞70BÈÄüÂ∫¶ÊÄé‰πàÊ†∑Ôºü
  - Êúâ‰∏™20-30 token/s ÂáëÂêàÁî®
- Êàëqwrn32ÈáèÂåñ8‰πüÊâç5‰∏™tokenÔºå‰Ω†Ëøô‰∏™70b 20-30tokenÊòØÊÄé‰πàÊù•ÁöÑ
  - 70ÈáèÂåñ4ÂèØ‰ª•ÁöÑÔºåÈáèÂåñ8ËÇØÂÆö‰∏çË°å

- ## üÜö [ÂÖ≥‰∫éÂá†‰∏™Ê°åÈù¢Á∫ßÁöÑAIÁªü‰∏ÄÂÜÖÂ≠òÈõÜÊàêÊñπÊ°àÁöÑÂØπÊØî - Áü•‰πé _202503](https://zhuanlan.zhihu.com/p/31599083340)
- ÁõÆÂâçÊ°åÈù¢Á∫ßÂà´ÁöÑAIÊñπÊ°àÔºåÈô§‰∫ÜnvÁöÑÁã¨ÊòæÂ§ñÔºåËøòÊúâÂá†‰∏™Áªü‰∏ÄÂÜÖÂ≠òÁöÑÈõÜÊàêÊñπÊ°à
  - ËãπÊûúMac miniÂíåMac studio
  - AMD AI 395 MAX
  - Ëã±‰ºüËææ DGX Spark

- Â∏¶ÂÆΩ Ôºö(‚ùìÊé®ÁêÜ/ÈÉ®ÁΩ≤Êó∂ÈáçË¶Å)
  - m4pro 64G Âíånv dgx spark 128GÈÉΩÊòØ 273 GB/s
  - amd AI 395 max+128G ÊòØ            256 GB/s
  - m3ultra 96GÊòØ                     400 GB/s (m3uÊó†128GÔºå256G‰ª•‰∏äÊâçÊúâ800GB/s)
  - m4max 128G ÊòØ                     546 GB/s

- AIÁÆóÂäõÔºö(‚ùìËÆ≠ÁªÉÊó∂ÈáçË¶Å)
  - nv dgx sparkÊòØ  1000 TOPS (FP4)
  - amd AI395maxÊòØ  126 TOPSÔºàint4Ôºâ
  - m3ultraÊòØ       72 TOPS(int4)
  - m4maxÂíåm4proÈÉΩÊòØ 38 TOPS(int4)

- ‰ª∑Ê†ºÔºö
  - nv dgx spark  3000 ÁæéÂÖÉÔºà‰º∞ËÆ°23000‰∫∫Ê∞ëÂ∏ÅÔºüÔºâ
  - amd ai395max  25999 ‰∫∫Ê∞ëÂ∏Å
  - m3ultra 96G   32999 ‰∫∫Ê∞ëÂ∏Å
  - m4max 128G    29249 ‰∫∫Ê∞ëÂ∏Å
  - m4pro 64G+1T  16999 ‰∫∫Ê∞ëÂ∏Å

- ÁªºÂêàÁúãÊù•Â¶ÇÊûúÊÄß‰ª∑ÊØîÂíåÈÄöÁî®ÊÄßÊØîËæÉÂ•ΩÁöÑÈÄâÊã©Â∫îËØ•ÊòØnv DGX SparkÔºàÁîüÂõæÔºåÁîüËßÜÈ¢ë‰πãÁ±ªÁöÑÁÆóÂäõÊØîÂ∏¶ÂÆΩÊõ¥ÈáçË¶ÅÔºâÔºå
  - Â¶ÇÊûúÂçïÁ∫Ø‰∏∫‰∫ÜLLMÊÄßËÉΩÔºàÁªü‰∏ÄÂÜÖÂ≠òÂ∏¶ÂÆΩÊØîÁÆóÂäõÊõ¥ÈáçË¶ÅÔºâm3ultra 96G ÂèØËÉΩÊòØÊØîËæÉÂ•ΩÁöÑÈÄâÊã©„ÄÇ
- Ëá≥‰∫é‰πãÂâçÊúâÁúãÂà∞ÁöÑ‰∏Ä‰∫õÁî®mac miniÈÄöËøáÈõ∑ÁîµÂè£Â†ÜÂè†ÁöÑËôΩÁÑ∂ÂèØ‰ª•‰ΩéÊàêÊú¨ÂÅöÂà∞Â§ßÊòæÂ≠òÔºå‰ΩÜÊòØÂá†‰πéÊ≤°Âï•ÂÆûÁî®‰ª∑ÂÄºÔºåÂõ†‰∏∫Èõ∑ÁîµÂè£Â∏¶ÂÆΩÂè™Êúâ15GB/s„ÄÇ„ÄÇ„ÄÇ
  - Èõ∑ÁîµÂ†ÜÂè†ÊòØ‰∏∫‰∫Ü‰ΩéÂª∂ËøüË∑ëtensor parallelismÂêßÔºåÂèà‰∏çÊòØremoteËÆøÈóÆÂÜÖÂ≠ò„ÄÇPCIEÂ∏¶ÂÆΩ‰πü‰∏çÂ¶ÇÂÜÖÂ≠òÔºå‰ΩÜÂ§öÂç°Âπ∂Ë°åËøòÊòØÊúâÊïàÁöÑ

- Áé∞Âú®ÁΩë‰∏äai max 395ÁöÑÂ∞è‰∏ªÊú∫Â∑≤ÁªèÂçñÂà∞14000Â∑¶Âè≥‰∫ÜÔºåËøôÊ†∑ÊØî‰∏ãÊù•ÔºåÊÑüËßâai max 395ÊÄß‰ª∑ÊØîËøò‰∏çÈîô„ÄÇ

- ## [Â¶Ç‰ΩïËØÑ‰ª∑ÂîÆ‰ª∑ 18999 ÂÖÉÁöÑÊÉ†ÊôÆÊöóÂΩ±Á≤æÁÅµ MAX Ê∏∏ÊàèÊú¨? Âì™‰∫õ‰∫ÆÁÇπÂÄºÂæóÂÖ≥Ê≥®? - Áü•‰πé _202503](https://www.zhihu.com/question/15023061538/answers/updated)
- ÊöóÂΩ±Á≤æÁÅµMAXËøô‰∏™Êñ∞Ê®°ÂÖ∑Â∞±Áî®Êù•Âèñ‰ª£ÊöóÂΩ±Á≤æÁÅµPlusÁöÑÔºå‰æùÁÑ∂ÊòØ‰∏ªÊâì‰∏Ä‰∏™‚Äú‰∏ÄÁ∫øÂìÅÁâå‰∏≠ÁöÑÊÄß‰ª∑ÊØî‚ÄùÂÆö‰Ωç„ÄÇ
- ‰∏ÄÁ∫øÂìÅÁâåÈ´òÁ´ØÊ∏∏ÊàèÊú¨ÁöÑÂÆàÈó®ÂëòÔºåÂéüÊù•ÊöóÂΩ±Á≤æÁÅµPLUSÁöÑÊõø‰ª£ËÄÖ„ÄÇ

- Ëøô‰ª£MaxÊï¥‰ΩìÁöÑÂçáÁ∫ßÁÇπÔºö
  - Êï£ÁÉ≠ËßÑÊ†ºÂçáÁ∫ßÔºöÈááÁî®‰∫ÜVCÂùáÁÉ≠ÊùøËÆæËÆ°ÔºåÂä†‰∏äÊ∂≤ÈáëÊï£ÁÉ≠‰∏é4Âá∫È£éÂè£ËÆæËÆ°ÔºåÊï¥‰ΩìÂèØ‰ª•ÂÅöÂà∞250W+ÁöÑÊÄßËÉΩÈáäÊîæÔºåÂèåÁÉ§75+175WÔºõ
  - Â±èÂπïÊîπ‰∏∫Êõ¥‰∏ªÊµÅÁöÑ16ÂØ∏Ôºö2.5KÂàÜËæ®Áéá16:10ÊØî‰æãÈ´òÂàÜÂ±èÔºå500nit‰∫ÆÂ∫¶240HzÂà∑Êñ∞ÁéáÔºõ
  - Êñ∞Â¢ûÂ§ßÂ∏àÊ®°ÂºèÔºåÂÖÅËÆ∏Áî®Êà∑ÊâãÂä®Ë∂ÖÈ¢ë„ÄÇ

- ÂÖ∂‰ªñÈÖçÁΩÆÂè™ËÉΩËØ¥‰∏≠ËßÑ‰∏≠Áü©‰∫ÜÔºö
  - 32GB DDR5-5600ÂÜÖÂ≠ò+1TBÁ°¨ÁõòÔºåÂèåÁ°¨Áõò‰ΩçÂèåÂÜÖÂ≠òÊèíÊßΩÔºõ
  - Âè™Áªô‰∫Ü2A2CÔºà2‰∏™Èõ∑Áîµ4Ôºâ+HDMI+RJ45ÁöÑÊé•Âè£ÔºåÂú®Ê∏∏ÊàèÊú¨ÈáåÈù¢ÁÆóÊØîËæÉÂ∞ëÁöÑ‰∫ÜÔºõ
  - Áªô‰∫ÜRGBËÉåÂÖâÈîÆÁõòÔºå‰ΩÜÁõÆÂâçÊù•ÁúãËøòÊòØÂõõÂå∫RGBËÄåÈùûÂçïÈîÆRGBÔºåÂπ∂‰∏îÊñπÂêëÈîÆ‰æùÊóß‰∏∫ÂçäÈ´òÔºõ
  - ÁîµÊ±†ÂÆπÈáè83WhÔºå‰∏ªÊµÅÊóóËà∞ÂÆö‰ΩçÁöÑÊ∏∏ÊàèÊú¨ÈÉΩ90Wh+‰∫ÜÔºåÊúâÁöÑÁîöËá≥99Wh

- ÁõÆÂâçÊÉ†ÊôÆÁ≤æÁÅµÈÅáÂà∞‰∏Ä‰∏™ÊØîËæÉÂ∞¥Â∞¨ÁöÑÈóÆÈ¢òÔºåÊØî‰∏äÊ≤°ÊúâÂìÅÁâåÂäõÔºåÊØî‰∏ãÊ≤°ÊúâÊÄß‰ª∑ÊØîÔºö
  - Âä†3000ÂèØ‰ª•‰∏äÈÄºÊ†ºÊõ¥È´ò„ÄÅÊõ¥ÊúâÊ∞õÂõ¥ÊÑü„ÄÅÂèØÁé©ÊÄßÊõ¥È´òÁöÑ‰∏îÂìÅÁâåÊõ¥Âº∫ÁöÑROGÊû™Á•û9Á≥ªÂàóÔºõ
  - È¢ÑÁÆóÊõ¥‰ΩéÔºåÁ≠â‰∫åÁ∫øÂìÅÁâå‰∏ä‰∫Ü‰πãÂêéÔºåÂ§ßÊ¶ÇÁéá1W4‰∏çÂà∞Â∞±ÂèØ‰ª•Êãø‰∏ã‰ΩéUÈ´òÊòæÁöÑRTX5080Ê∏∏ÊàèÊú¨‰∫ÜÔºåÊÄß‰ª∑ÊØîÊõ¥È´ò„ÄÇ
- ÂõΩÂÜÖÊ∏∏ÊàèÊú¨Â∏ÇÂú∫ÁõÆÂâçËøòÊòØ‰ª•ÊÄß‰ª∑ÊØî‰∏∫‰∏ªÂØºÔºåÂ∞±ËøûROGËøô‰∏§Âπ¥ÈÉΩÂèòÂæóÊúâÊÄß‰ª∑ÊØîËµ∑Êù•ÔºåÊâÄ‰ª•ÂØπ‰∫éÊöóÂΩ±Á≤æÁÅµMAXËøôÁ±ªÁöÑ‰∏ç‰∏ä‰∏ç‰∏ãÁöÑ‰∏ªÊµÅÁ≥ªÂàóÊóóËà∞Êú¨ÔºåÊàëÊòØË∞®ÊÖéÁúãÂ•ΩÁöÑÔºåÊØî‰∏äÊó†ÂìÅÁâåÂäõÔºåÊØî‰∏ãÊó†ÊÄß‰ª∑ÊØî„ÄÇ

- Â∞èÁ±≥GÁöÑÁõÆÊ†áÂÆ¢Êà∑ÊòØËøΩÊ±ÇÊÄß‰ª∑ÊØîÁöÑÁî®Êà∑Ôºå‰ΩÜËøôÂ∏ÆÁî®Êà∑‰ºöÂõ†‰∏∫Â∞èÁ±≥GÊ≤°ÊúâÊÄß‰ª∑ÊØî‰∏çÈÄâÊã©Â∞èÁ±≥G‚Ä¶‚Ä¶

- ## üÜöüìà [2025‰π∞‰∏™RTX 5090Á¨îËÆ∞Êú¨ÔºåÊúâ‰ªÄ‰πàÊé®ËçêÂêóÔºü - Áü•‰πé](https://www.zhihu.com/question/1890528801219405195)

> ‰∏çÂ§™Âú®ÊÑè‰æøÊê∫Ôºå‰∏ªË¶ÅÊòØÁúãÊÄßËÉΩ+ÊÄß‰ª∑ÊØî„ÄÇÁ•ûËàüÊÄß‰ª∑ÊØîÂ§™È´ò‰∫ÜÔºå‰ΩÜÊòØÂèàÊúâÁÇπÂÆ≥ÊÄïÁøªËàπ„ÄÇ

- Âú®ÂúàÂÆöË¶ÅRTX5090Ê∏∏ÊàèÊú¨ÁöÑÂâçÊèê‰∏ãÔºåÈ¢ò‰∏ªÁöÑÊúÄ‰Ω≥ÈÄâÊã©ËÇØÂÆöÊòØ‰∏ÄÁ∫øÂìÅÁâåÈ´òÁ´ØÊ∏∏ÊàèÊú¨‰∏ÄÁõ¥‰ª•Êù•ÁöÑÊÄß‰ª∑ÊØîÊâõÊääÂ≠êÔºåÊöóÂΩ±Á≤æÁÅµMAXÂïä
  - U9-275HX+RTX5090+2.5k 240hz 500nit IPS+32/1T PCIe 5.0ÁöÑÊ†∏ÂøÉÈÖçÁΩÆÔºå250w+ÁöÑÊï¥Êú∫ÊÄßËÉΩÈáäÊîæË°®Áé∞‰∏≠Á≠âÂÅè‰∏äÔºåÊãìÂ±ïÊÄß‰∏Ä‰∏™5.0 M.2‰∏Ä‰∏™4.0 M.2ÔºåÈáçÈáè2.75kgÔºåÊé•Âè£ÊòØ‰ø©Èõ∑Áîµ4Ôºå‰ø©USB-AÔºåHDMIÂíårj45„ÄÇ
  - ‰∏äÈù¢Ëøô‰∏™ÈÖçÁΩÆ‰πüÂ∞±22999ÔºåÂõΩË°•ÂêéÊâç20000Âá∫Â§¥
  - ROG‰πüÂÆåÂÖ®Ê≤°ÂøÖË¶ÅÁúãÊû™Á•û9 PlusË∂ÖÁ´ûÁâàÂêßÔºåÊçÜÁªë‰∫Ü64GÂÜÖÂ≠òÂêé‰ª∑Ê†ºÁõ¥Êé•33999‰∫ÜÔºåÂÆûÂú®ÊòØÂ§™Â§∏Âº†„ÄÇÊû™Á•û9 Ë∂ÖÁ´ûÁâàÈô§‰∫ÜÂÜÖÂ≠òÁ°¨ÁõòÈÉΩÁº©Â∞è‰∏ÄÂçä‰ª•ÂèäÂ∞∫ÂØ∏‰∏∫16Âêã‰πãÂ§ñÔºåÂá†‰πéÊ≤°Âï•Âå∫Âà´Ôºå‰ª∑Ê†ºÂàôÁõ¥Êé•‰∏ãÈôçÂà∞‰∫Ü29999ÔºåÁ´ãÁúÅ4000.

- ## [ÊúâÊ≤°Êúâ24gÊòæÂ≠òÁöÑÁ¨îËÆ∞Êú¨ÁîµËÑëÊé®ËçêÔºü - Áü•‰πé _202409](https://www.zhihu.com/question/666131987)

> ÊÉ≥Ë¶Å‰π∞‰∏™Êñ∞ÁîµËÑëÊù•Áé©AI„ÄÇ‰∏ªË¶ÅËøòÊòØÊÉ≥ËÄÉËôëÁ¨îËÆ∞Êú¨Ôºå‰ΩÜÊòØÁúã‰∫Ü‰∏ÄÂúàÔºåÂ•ΩÂÉèÈÉΩÊòØ8gÊòæÂ≠ò„ÄÇÊúÄÂ§ßÁöÑ‰πüÂ∞±16gÊòæÂ≠ò„ÄÇÊ≤°ÊúâÊâæÂà∞Êúâ24gÊòæÂ≠ò„ÄÇ

- ÁõÆÂâçÂÅöAIÊé®ËçêNÂç°ÔºåNÂç°24GÊòæÂ≠òÂè™Êúâ4090Ôºå4090ÂÅöÊàê‰æøÊê∫ÂºèÂè™ÊúâËøô‰∏Ä‰∏™ÊñπÊ≥ï

- Êúâ‰∏â‰∏™ÊñπÊ°àÔºö
- Á¨¨‰∏Ä‰∏™ÊñπÊ°àÊòØÂÜÖÁΩÆÊòæÂç°ÁöÑÁ¨îËÆ∞Êú¨ÁîµËÑëÔºå‰ºòÁÇπÊòØ‰æøÊê∫ÊÄßÂ•ΩÔºåÁº∫ÁÇπÊòØÊÄßËÉΩÂ∑ÆÁÇπÔºå‰πüÂæóÂÅöÂ•ΩÊï£ÁÉ≠„ÄÇ
  - ÊÉ≥Ë¶Å16GÊòæÂ≠òÂè™ËÉΩRTX4090ÁöÑÂûãÂè∑ÔºåÁªùÂ§ßÈÉ®ÂàÜRTX4080ÁßªÂä®Á´ØÊòØ12GÊòæÂ≠òÔºåÁõÆÂâç‰ΩøÁî®RTX4090ÁöÑÁ¨îËÆ∞Êú¨ÁîµËÑë‰∏çÁÆóÂ§öÔºåÊÉ≥Ë¶ÅÊÄß‰ª∑ÊØîÂ∞±Êú∫Ê¢∞Èù©ÂëΩËÄÄ‰∏ñ16SuperÔºåÊÉ≥Ë¶ÅÊõ¥Âº∫ÁöÑÊï¥Êú∫ÊÄßËÉΩÈáäÊîæÂèØ‰ª•ËÄÉËôëROGÊû™Á•û„ÄÇ
- Á¨¨‰∫å‰∏™ÊñπÊ°àÊòØÊãìÂ±ïÂùûÂ§ñÊé•Ê°åÈù¢ÊòæÂç°Ôºå‰ºòÁÇπÊòØÊÄßËÉΩÂº∫‰∏çÁî®ËÄÉËôëÊï£ÁÉ≠ÔºåÂπ∂‰∏îÂêéÊúüËøòËÉΩÈöèÊó∂Êó†ÁóõÂçáÁ∫ßÊòæÂç°ÔºåÁº∫ÁÇπÊòØ‰∏ßÂ§±‰∫Ü‰æøÊê∫ÊÄßÔºåÂπ∂‰∏îË¶ÅÊ≥®ÊÑèÂà´Ê≤°Êñ≠ÁîµÂ∞±ÊãîÊòæÂç°„ÄÇ
  - Á¨îËÆ∞Êú¨ÊúâÈõ∑Áîµ3/4ÊàñËÄÖUSB4Êé•Âè£Â∞±Ë°åÔºåRTX4080‰ºöÊúâÊÄßËÉΩÊçüÂ§±Ôºå‰ΩÜÊòØËÉΩÊé•ÂèóÔºåÊØïÁ´üÊÄé‰πàÈÉΩÊØîÁßªÂä®Á´ØÂº∫
- Á¨¨‰∏â‰∏™ÊñπÊ°àÂ∞±ÊòØÊîæÂºÉÁπÅÊñáÁºõËäÇÔºåÁõ¥Êé•‰∏äÂè∞ÂºèÊú∫Âêß„ÄÇ

- ## [Á∫†ÁªìÈÄâÂì™Ê¨æÁ¨îËÆ∞Êú¨ÁîµËÑëÔºü‰∏ªË¶ÅÁî®‰∫éstable diffusion? - Áü•‰πé](https://www.zhihu.com/question/620893866)
- ÁúãÂà∞Ê•º‰∏ãÊúâ‰∏™Â•ΩÁÇπÁöÑÂª∫ËÆÆÔºåÈõ∑ÁîµÊé•Âè£ÁöÑÁ¨îËÆ∞Êú¨+ÊòæÂç°ÊãìÂ±ïÂùû+Áã¨Á´ãÊòæÂç°
  - Ëøô‰∏™ÊñπÊ°àÁöÑËØùÔºåÊãìÂ±ïÂùû‰∏çÂÅöÊé®ËçêÁ°ÆÂÆû‰∏çÁü•ÈÅìÊÄé‰πàÈÄâÔºå‰ΩÜÊòæÂç°4090ÊôÆÈÅç10000‰ª•‰∏ä‰∫Ü

- ## [AIÁªòÁîªÔºàStable DiffusionÔºâÁî®‰ªÄ‰πàÊòæÂç°ÊØîËæÉÂ•ΩÔºü - Áü•‰πé](https://www.zhihu.com/question/638915747)
- Âú®‰Ω†ËÉΩÊâøÂèóÁöÑËåÉÂõ¥ÂÜÖÔºåÈÄâÊòæÂ≠òÊúÄÂ§ßÁöÑ

- ÁõÆÂâçÁöÑÊù°‰ª∂‰∏ãÂè™ÊúâNÂç°ËÉΩÊ≠£Â∏∏Áé©AI„ÄÇ
- ÊÉ≥Ë¶Å‰Ωé‰ª∑ÊãâÊª°AI„ÄÇÂ∞±Áî®RTX TITAN 24GBÔºà‰∏çÂà∞5000Ôºå‰ΩÜÈúÄË¶ÅÂä†Ê∞¥ÂÜ∑ÂéãÔºåÊàñËÄÖÈ≠îÊîπÁöÑRTX 2080Ti 22GÔºà‰∏çÂà∞3000ÔºâÈùûÊ∂°ËΩÆÂç°ÁöÑÁâàÊú¨„ÄÇ‰ΩÜÁé∞Âú®ÁõÆÂâçÁúã24GÁöÑ3090ÊÄß‰ª∑ÊØîÊúÄÈ´ò„ÄÇ

- Êé®Ëçê‰∏ÄÂùóÊÄß‰ª∑ÊØîÈ´òÁöÑÊòæÂç°ÔºåRTX 2080ti 22g„ÄÇaiÁîªÂõæÊÄßËÉΩÊé•ËøëRTX 4090ÁöÑ‰∏ÄÂçäÔºå‰ª∑Ê†ºÂè™Ë¶Å‰∫îÂàÜ‰πã‰∏Ä„ÄÇËÄå‰∏î22gÂ§ßÂÜÖÂ≠òÔºåÂØπÊ®°ÂûãËÆ≠ÁªÉ‰πüÂæàÂèãÂ•Ω„ÄÇ‰∏çËøáRTX 2080ti Â∑≤Áªè‰∏äÂ∏ÇÂæàÂ§öÂπ¥‰∫ÜÔºåË¶ÅÊ∑òÂà∞‰∏ÄÂùóÂ•ΩÂç°‰∏çÂÆπÊòìÔºåÊúÄÂ•ΩÊúâÂá†Âπ¥‰øù‰øÆÁöÑÊõ¥Â•Ω

- Ë∑ë4K‰ª•‰∏äÁ®≥Á®≥ÁöÑÔºåÊòæÂ≠ò‰∏∫32GÁöÑÊòæÂç°‰∏ÄÂÆöÊòØÈ¶ñÈÄâÔºå‰∏çÊòØÈ≠îÊîπÊ¨æÔºåÈÇ£Â∞±Âè™ÊúâÂç≥Â∞Ü‰∏äÊû∂ÁöÑ5090‰∫Ü„ÄÇ
  - Ë∑ë2K‰ª•‰∏äÁöÑËØùÔºå12G-16GÁöÑÂç°ÈÉΩÂèØ‰ª•Ôºå‰∏çËøá12GË∑ëÊüê‰∫õÂ§ßÊ®°ÂûãÂèØËÉΩ‰∏çÂ§üÁî®
  - 2KÂ∞ÜÂ∞±Áî®ÔºåÁõÆÂâçÈÄâ4060TI 16GÁöÑÁî®Êà∑Â§öÁÇπÔºå‰∏çËøáËøôÊ¨æÊòæÂç°Âè™ÊòØÈ¢ÑÁÆóÂ∞ëÁöÑÈÄâÊã©ÔºåÊØïÁ´üÊòæÂ≠ò‰ΩçÂÆΩË¢´ÈòâÂâ≤ÔºåGPUÊÄßËÉΩ‰πü‰∏ÄËà¨„ÄÇ
  - Ê≤°Áü≠ÊùøÔºåÂá∫ÂõæÂø´ÔºåÈ¶ñÊé®4070TIS 16GÔºåËøôÊ¨æÂç°ÔºåÊàëÁî®‰∏ãÊù•ÂæàÊª°ÊÑèÔºåÁº∫ÁÇπÂ∞±ÊòØÊ∫¢‰ª∑È´ò„ÄÇ
  - Ë∑ë1K‰ª•‰∏äÁöÑËØùÔºå3060 12GÔºå2060 12GÔºåÈÉΩÂ∑≤ÁªèÂèØ‰ª•Ë∑ë‰∫Ü

- ÁªèËøá‰∏çÊñ≠ÁöÑ‰ºòÂåñÔºåÁé∞Âú®ÁöÑStable DiffusionÂØπÊòæÂç°Ë¶ÅÊ±Ç‰∏çÁÆóÂ§™È´òÔºå
  - Â¶ÇÊûúÂè™ÊòØË∑ëÂõæÔºå4060Ti 16GÂ∞±Â§üÁî®ÔºåÁÇº‰∏πÁöÑËØùÔºå4090‰πüË∂≥Â§ü‰∫Ü„ÄÇ

- Ëá≥‰∫é‰πüÊòØ3000Â§öÁöÑ2080Ti-22GÔºåÈ≠îÊîπÁöÑÊúâÈ£éÈô©ÔºåÂ§öÂá†GÊòæÂ≠òÂØπËøô‰∏™ËΩØ‰ª∂Êù•ËØ¥Áî®Â§Ñ‰∏çÂ§ßÔºåÊúâÊñ∞ËøòÊòØ‰π∞Êñ∞„ÄÇ
  - ‰πãÂêéÂ∞±ÊòØ7000Ëøô‰∏ÄÊ°£ÁöÑÔºå5070TiÔºå‰∏∫‰ªÄ‰πà‰∏çÊòØ4070TisÂë¢ÔºåÂõ†‰∏∫50Á≥ªÊòæÂç°ÂèØ‰ª•Áî®4‰ΩçÊ®°ÂûãÔºå40Á≥ªÂè™ËÉΩ8‰Ωç„ÄÇ

- 10Á≥ªÊòæÂç°‰∏çÊîØÊåÅ4bitÔºàÂÖ∂ÂÆû‰πü‰∏çÊîØÊåÅ8bit„ÄÅ16bitÔºâÔºå‰ΩÜÊòØQ4ËÉΩË∑ëÂïäÔºåÂ∞±ÊòØÊÖ¢Âëó„ÄÇÂ∑•‰ΩúÂéüÁêÜÊòØÊåâÂéü‰ΩçÊï∞Â§ßÂ∞èË£ÖÊòæÂ≠ò‰∏≠ÔºåË∑ëÁöÑÊó∂ÂÄôÂàÜÊÆµËΩ¨Êç¢Êàêfp32Êù•Ë∑ë„ÄÇ

- ## [Â¶Ç‰ΩïËØÑ‰ª∑HPÊúÄÊñ∞ÂèëÂ∏ÉÁöÑÊê≠ËΩΩAI MAX 300Á≥ªÂàóÂ§ÑÁêÜÂô®ÁöÑÊàò99 UltraÔºü ‚ÄúÊàò 99 Ultra‚ÄùÁßªÂä®Â∑•‰ΩúÁ´ôÂ∑≤‰∫é3Êúà17Âè∑‰∏äÊû∂‰∫¨‰∏ú  - Áü•‰πé](https://www.zhihu.com/question/15255805038)
- ‰ªäÂπ¥ÂîØ‰∫åÁöÑStrix HaloÁ¨îËÆ∞Êú¨ÔºàÂè¶‰∏Ä‰∏™ÊòØÂπªXÔºâÔºåÂêåÊó∂ÂèØËÉΩÊòØÂîØ‰∏ÄÁöÑÂ∏∏ËßÑÂΩ¢ÊÄÅ‰∫ßÂìÅ„ÄÇ
  - ËøôÊú∫Âô®Âú®Êµ∑Â§ñÁöÑÂêçÁß∞ÊòØZbook Ultra G1aÔºåÂÆûË¥®‰∏äÊòØEliteBook X G1aÁöÑÂ§çÁî®Ê®°ÂÖ∑‰ΩÜÂä†Âéö+Â¢ûÂº∫Êï£ÁÉ≠ÁöÑÁâàÊú¨„ÄÇÂÆö‰ΩçÂ∞±ÊòØÊóóËà∞ËΩªËñÑÁßªÂä®Â∑•‰ΩúÁ´ôÁöÑÂ∞èÂ∞∫ÂØ∏ÁâàÊú¨ÔºåÁ±ª‰ººThinkPad P1Ôºå‰ΩÜÊòØÂÅö‰∫Ü14ÂêãÁöÑÁâàÊú¨„ÄÇ
  - CPUÊñπÈù¢ÔºåÈô§‰∫ÜÂíå‰πãÂâçÂ∑≤Áªè‰∏äÂ∏ÇÁöÑÂπªX‰∏ÄÊ†∑ÁöÑAI MAX 390/395ÔºàÂàÜÂà´ÂØπÂ∫î16C+40CU/12C+32CUÔºâ‰πãÂ§ñÔºåËøòÂ§ö‰∫Ü‰∏ÄÈ´ò‰∏Ä‰Ωé‰∏§‰∏™Êñ∞ÁöÑÈÖçÁΩÆÔºåÊõ¥‰ΩéÁöÑAI MAX 385ÊòØ8C+32CUÁöÑËßÑÊ†ºÔºåÂêåÊó∂ËøòÊúâ‰∏™È°∂ÈÖçÁöÑÂïÜÁî®ÁâàAI MAX+ Pro 395ÔºåËøôÈ¢óCPUÊòØÊÉ†ÊôÆÁã¨Âç†ÁöÑ„ÄÇ
- ÂÖ∂‰ªñÂú∞ÊñπÂ∞±Âü∫Êú¨ÂíåÂéüÁâàÁöÑEliteBook‰∏ÄËá¥‰∫ÜÔºö
  - 2.8k 120hzÁöÑOLEDËß¶Êë∏Â±è
  - 74.5whÁîµÊ±†ÔºåÊîØÊåÅPD 3.1Âø´ÂÖÖ
  - ÂçïÁ°¨Áõò‰Ωç

- Ëøô‰ª∑Ê†ºÂè™ËÉΩËØ¥ÊòØÂ•ΩÂÆ∂‰ºô‰∫ÜÔºå55wÊÄßËÉΩÈáäÊîæÁöÑÂ∏∏ËßÑÂΩ¢ÊÄÅÊú∫Âô®ÔºåÂçñÁöÑÊØîÈöîÂ£ÅÂπªXÁöÑÂπ≥ÊùøÂΩ¢ÊÄÅ+80wÊÄßËÉΩÁöÑËøòË¥µ... Â¶ÇÊûú‰∏ÄÂÆö‰π∞ËÄÉËôëÔºåÂè™Êé®ËçêÈ°∂ÈÖçÁâàÊú¨ÔºåËµ∑Á†ÅËøòÊòØÊúâÁã¨Âç†CPUÁöÑ

- Â¶ÇÊûú‰∏ªË¶ÅÊòØÂÜ≤ÁùÄStrix HaloËøôÈ¢óCPUÊù•ÁöÑÔºåÈÇ£ËøòÊòØÊõ¥Êé®ËçêÂπªXÔºåÂõ†‰∏∫ÊÄß‰ª∑ÊØîÊõ¥È´òÔºåAI MAX+ 395+128/1TÁöÑÁâàÊú¨‰πüÂ∞±20999ÔºåÂ¶ÇÊûúÊòØ32GÁöÑÁâàÊú¨ÔºåÂ∞±Âè™Ë¶Å14999‰∫Ü„ÄÇ

- ‰∏∫‰ªÄ‰πàSTX HaloËøôÈ¢óCPUÁúãËµ∑Êù•ÂæàÂ•ΩÔºå‰ΩÜÂÆûÈôÖ‰∏äÊó†OEMÊÑøÊÑèÁî®Ôºü‰∏™‰∫∫ËÆ§‰∏∫‰∏ªË¶ÅÂéüÂõ†ÊòØ‰∏§ÁÇπÔºö
  - ‰∏Ä‰∏™ÊòØËøôÈ¢óÂ§ÑÁêÜÂô®ÂéüÊú¨Â∫îËØ•Âú®2024Âπ¥ÂíåSTX Point‰∏ÄËµ∑‰∏äÔºåÂç¥Âõ†‰∏∫ÁßçÁßçÂéüÂõ†Âª∂ËØØÂà∞‰∫Ü‰ªäÂπ¥ÔºåÂõ†Ê≠§ÂØπÊ†áÁöÑÂØπË±°‰πü‰ªéÂéüÊù•ÁöÑRTX 4060ÂèòÊàê‰∫ÜRTX 5050ÔºåËøôÊó∂ÂÄôSTX HaloÁöÑÊ†∏ÊòæÊÄßËÉΩÂÆûÈôÖ‰∏äÂ∞±Ê≤°ÊúâÂ§™Â§ßÁöÑÂê∏ÂºïÂäõ‰∫Ü
  - Âè¶‰∏ÄÁÇπÂú®‰∫éSTX Halo‰∫ßÁîüÁöÑÊúÄÂ§ßÊÑè‰πâÂú®‰∫éÂú®Á©∫Èó¥Â∞∫ÂØ∏ÂèóÈôêÁöÑÂπ≥Âè∞‰∏äÂÅöÂà∞Â∞ΩÂèØËÉΩÈ´òÁöÑÊÄßËÉΩÔºå‰ΩÜÁé∞Âú®ROGÂ∑≤ÁªèÂú®14.0ÂêãÁöÑËΩªËñÑÂπ≥Âè∞‰∏äÂÅöÂá∫Êù•BD1ÁöÑÂç°‰∫ÜÔºàRTX 5080 on Âπª14 AirÔºâÔºåÂπªXÁöÑ‰∏ä‰∏Ä‰ª£‰πüÊó©Â∑≤ÁªèÊåëÊàòËøáÂú®13ÂêãÁöÑÂπ≥Êùø‰∏äÂÅöH45+BD2ÊòæÂç°
- STX HaloËøô‰πà‰∏™ÊÄ™Áâ©CPUÊú¨Ë∫´Â≠òÂú®ÁöÑÊÑè‰πâÂ∞±Â∑≤ÁªèÂèóÂà∞‰∫Ü‰∏•ÈáçÁöÑÂä®Êëá‰∫Ü„ÄÇÁªßÁª≠ÂæÄÂ∞èÂ∞∫ÂØ∏ÂÅöÔºüÈÇ£Êï£ÁÉ≠Êõ¥ÂÆåËõãÔºåÊÄßËÉΩ‰πüË∑üÁùÄÂØÑ„ÄÇÁªßÁª≠ÂæÄÂ§ßÂ∞∫ÂØ∏ÂÅöÔºüÂÅöÂà∞15ÂêãÂèä‰ª•‰∏äÁöÑÂπ≥Âè∞‰∏äÔºå‰º†ÁªüÁöÑCPU+GPUÊñπÊ°àÊàêÊú¨‰ΩéÔºåÊï£ÁÉ≠Êõ¥Â•ΩÂÅöÔºåÊï¥‰ΩìÊïàÊûúËøòÊõ¥Â•Ω
  - ‰∏ç‰ºöÊúâ‰∫∫ÁúüÁöÑËßâÂæóAÂç°Ê∏∏Êàè‰ΩìÈ™åÂ•ΩÂêßÔºåÂ∞§ÂÖ∂ÊòØÁßªÂä®Á´ØAÂç°

- ÊúÄÂ§ßÁöÑÁº∫ÁÇπËøòÊòØ‰ª∑Ê†º„ÄÇ2.5‰∏áÁöÑ‰ª∑Ê†ºÔºåÂ¶ÇÊûú‰ª•‰ªñÂØπÊ†áÁöÑ7945HX+RTX 4060ÁöÑÁ¨îËÆ∞Êú¨ÁîµËÑëÔºåÈÉΩÂèØ‰ª•‰π∞3Âè∞‰∫ÜÔºåÈÄâÊã©R9000P‰πüÂ∞±8000Â§öÂá∫Â§¥„ÄÇÈÇ£Â∞±Áúã‰Ω†ÊÑø‰∏çÊÑøÊÑè‰∏∫‰∫ÜËΩªËñÑ‰ªòÂá∫Ê∫¢‰ª∑‰∫Ü„ÄÇ

- ÊÉ†ÊôÆÊàò99 UltraÔºåÊµ∑Â§ñÂØπÂ∫îÂûãÂè∑Zbook Ultra G1aÔºå‰æøÊê∫ÁßªÂä®Â∑•‰ΩúÁ´ôÂÆö‰Ωç„ÄÇÂ∏∏ËßÑÁöÑÈÖçÁΩÆ‰æãÂ¶ÇÂÖ®ÈáëÂ±ûÊú∫Ë∫´„ÄÅ2.8K OLEDÂ±èÂπï„ÄÅÊÉ†ÊôÆÂ∑•‰ΩúÁ´ôÁöÑÁ•ñ‰º†Êé•Âè£3C+HDMI+1A„ÄÅ74.5WhÁîµÊ±†Â∞±‰∏çÂÜçËµòËø∞‰∫Ü„ÄÇ
  - ÊúÄÂ§ßÁöÑ‰∫ÆÁÇπÂ∞±ÊòØAMDÈîêÈæôAI MAX 300Á≥ªÂàóÁöÑÂ§ÑÁêÜÂô®„ÄÇAI MAX+ 395Â§ÑÁêÜÂô®ÊòØÊÉ†ÊôÆÁã¨Âç†ÔºåCPUËßÑÊ†ºÂíåÊ°åÈù¢Á´Ø9950XÁõ∏ÂΩìÔºåÂè™‰∏çËøáÂèóÂà∂‰∫éÁ¨îËÆ∞Êú¨ÁîµËÑëÊï£ÁÉ≠‰∏çËÉΩÂÆåÂÖ®ÂèëÊå•ÔºõËØ¥ÊòØÈõÜÊòæÔºå‰ΩÜÊòØÊÄßËÉΩÂ†™ÊØîRTX 4060ÁöÑÁã¨Êòæ„ÄÇ
  - ÂêåÊó∂Âè¶‰∏ÄÂ§ßÂ•ΩÂ§ÑÂú®‰∫éÂÆûÁé∞Á±ª‰ºº‰∫éËãπÊûú‰∏ÄÊ†∑ÁöÑ128GBÁªü‰∏ÄÂÜÖÂ≠òÔºåÊúÄÂ§öÂèØ‰ª•ÂàÜÈÖç96GBÁªôÊòæÂ≠ò„ÄÇ128GBÂÜÖÂ≠òÁâàÊú¨‰∏∫4ÈÄöÈÅìÔºåÂÜÖÂ≠òÂêûÂêêÈÄüÂ∫¶ËÉΩÂ§üËææÂà∞200GB/sÂá∫Â§¥ÔºåÊØîÂ∏∏ËßÑÁöÑÂèåÈÄöÈÅì100GB/sÁõ¥Êé•ÁøªÂÄç‰∫Ü
- 14ÂØ∏ÁöÑÁ¨îËÆ∞Êú¨ÁîµËÑëÔºåÈáçÈáèÂà∞‰∫Ü1.6kgÔºåÂ±û‰∫éÊòØÂÅèÈáçÁöÑ„ÄÇÊÄßËÉΩÈáäÊîæ‰πüÂ∞±Âú®55WÔºå‰∏çËÉΩÂèëÊå•Âá∫ÂÆåÊï¥ÊÄßËÉΩ„ÄÇ‰Ωú‰∏∫ÁßªÂä®Â∑•‰ΩúÁ´ôÂè™ÊúâÂçïÁ°¨Áõò‰Ωç„ÄÇ

- ÈóÆÈ¢òÊòØÊÄßËÉΩÈáäÊîæËøò‰∏çÂ¶ÇÂπªxÔºåËøô‰∏™Âè™Êúâ55w„ÄÇ

- ## [ROGÂπªX 2025ÂØπ‰∫éËøêË°åAIÂ∫îÁî®ÔºàÂ¶ÇÊú¨Âú∞Â§ßÊ®°ÂûãÔºâÊúâÊ≤°ÊúâÊòæËëó‰ºòÂäøÔºüËÉΩÂê¶Âä©ÂäõÁî®Êà∑ÊèêÂçáAI‰ΩìÈ™åÔºü - Áü•‰πé _202503](https://www.zhihu.com/question/14670046303/answers/updated)
- Â±û‰∫é‰ªÄ‰πàÈÉΩËÉΩÂπ≤Ôºå‰ªÄ‰πàÈÉΩÂπ≤‰∏çÂ•ΩÁöÑÂØåÂì•Áé©ÂÖ∑„ÄÇ
  - ‰æøÊê∫ÂíåÁ¶ªÁîµÊÄßËÉΩÊØî‰∏çËøáÂá†ÂçÉÂùóÁöÑmac air„ÄÇ
  - Ê∏∏ÊàèÊÄßËÉΩÊØî‰∏çËøáÂêå‰ª∑‰ΩçÁöÑ4080mÔºàÈ©¨‰∏äÂ∞±5080m‰∫ÜÔºâ
  - Â§ßÊ®°ÂûãË¢´Âêå‰ª∑‰Ωçmac studio m4ÊâìÁàÜ„ÄÇ

- ËøôÁé©ÊÑèË∑ëÂ§ßÊ®°ÂûãÁöÑËÉΩÂäõÂ∞±ÊòØ‰∏™3060 12gÊàñËÄÖÊúÄÂ§ö4060ti 16gÁöÑÊ∞¥Âπ≥ÔºåÂèçÊ≠£12gÔΩû16gÊòæÂ≠òË∑ë‰∏çËµ∑Êù•ÁöÑ‰∏úË•ø395‰πüÊ≤°ÈÄüÂ∫¶‰∫ÜÔºåËÄåÂè™Ë¶ÅÁî®ÊòæÂ≠òËÉΩË∑ëËµ∑Êù•ÔºåÈÄüÂ∫¶ÁªùÂØπÂêäÊâì395„ÄÇ
- ÊåâÊàëÁêÜËß£ËøôÁé©ÊÑèÁöÑÊÑè‰πâÊòØÂÖ∑Â§áËΩªËñÑÊú¨Â∞ëÊúâÁöÑÂêåÊó∂ÂÖ∑Â§á64g‰ª•‰∏äÂÜÖÂ≠ò„ÄÅ‰∏çÂ∞øÂ¥©ÁöÑÁª≠Ëà™„ÄÅË∂≥Â§üÂº∫ÁöÑÊòæÂç°‰ª•Â∫î‰ªòÂõæÁâáÂíåËßÜÈ¢ëÁºñËæë„ÄÇÁÑ∂ËÄåÂú®Ëøô‰∏™Âú∫Âêà‰∏ãÔºåÂç¥Ë¢´MacBook proÂÖ®Êñπ‰ΩçÁàÜÊùÄ„ÄÇ

- [Áã¨ÂÆ∂È¶ñÂèëAI Max+395Â§ÑÁêÜÂô® ROG ÂπªX 2025Ë∑ëÂàÜËß£Á¶ÅÔºåÊòØÂê¶ÂÄºÂæóË¥≠‰π∞Ôºü - Áü•‰πé _202502](https://www.zhihu.com/question/12616419565)
- ÊúÄËøëÊàëÁúãÂà∞‰∏öÂÜÖÊñ∞Âá∫Áé∞‰∫Ü2ÁßçÊ°åÈù¢Á∫ßAIËÆ°ÁÆó/PCÁ±ª‰∫ßÂìÅÔºå‰∏Ä‰∏™ÊòØÂú®NVIDIA GTCÂ§ß‰ºö‰∏äÊ≠£ÂºèÂèëÂ∏ÉÁöÑDGX SparkÔºàËäØÁâá‰ª£Âè∑GB10ÔºâÔºåËøòÊúâÂü∫‰∫éAMD Ryzen AI MAX PROÂ§ÑÁêÜÂô®ÁöÑÁ¨îËÆ∞Êú¨/ÁßªÂä®Â∑•‰ΩúÁ´ô/Âè∞ÂºèÊú∫ÔºåÈÉΩÂÆ£Áß∞ËÉΩÊîØÊåÅ70B‰πÉËá≥Êõ¥È´òÂèÇÊï∞ÁöÑÊ®°Âûã„ÄÇ
  - NVIDIA DGX SparkÂè∑Áß∞‚ÄúÊúÄÂ∞èÁöÑ AI Ë∂ÖÁ∫ßËÆ°ÁÆóÊú∫‚ÄùÔºåÂÆÉÁöÑÂ§ÑÁêÜÂô®ÊúâÁÇπÂÉèÂæÆÁº©ÁâàÁöÑDGXËÆ°ÁÆóÁ≥ªÁªüÔºàÂèÇËÄÉ‰∏ãÂõæÔºâÔºåÂú®GB10ÂçïËäØÁâá‰∏äÈõÜÊàê‰∫ÜGrace CPU‚Äî‚Äî20‰∏™Arm CoreÔºå‰ª•ÂèäBlackwellÊû∂ÊûÑÁöÑGPU„ÄÇ
  - AMD Ryzen AI MAX PROÁ≥ªÂàóÔºà‰ª£Âè∑Stirx HaloÔºâÔºåÊõ¥Êé•Ëøë‰º†ÁªüÈõÜÊàêÊòæÂç°ÁöÑx86 CPUÔºå‰ΩÜÊï¥ÂêàGPUÁöÑÊÄßËÉΩÂç¥ÊØîËæÉÂº∫„ÄÇÂÖ∂ÈªòËÆ§TDPÂäüËÄó55WÔºåÊ†πÊçÆ‰∏çÂêåÁ≥ªÁªüËÆæËÆ°ÔºåcTDPÂèØË∞ÉÂäüËÄóÂú®45-120WËåÉÂõ¥„ÄÇ
  - Êó†ËÆ∫‰ΩøÁî®CPUËøòÊòØGPUÂÅöAIËÆ°ÁÆóÔºåÂú®LLMÊé®ÁêÜÁöÑPrefillÔºàÂÜÖÂÆπËæìÂÖ•ÁêÜËß£ÔºâÈò∂ÊÆµÁöÑÁì∂È¢àÊòØÁÆóÂäõÔºõËÄåÂú®DecodeËæìÂá∫Êó∂ÁöÑÊÄßËÉΩÔºàToken/sÔºâÂàô‰∏ªË¶ÅÂèóÂà∂‰∫éÂÜÖÂ≠òÂ∏¶ÂÆΩ„ÄÇ
  - Êàë‰ª¨ÁúãÂà∞‰∏äÈù¢2Ê¨æ‰∫ßÂìÅÈÉΩ‰ΩøÁî®‰∫Ü256‰ΩçLPDDR5x-8533ÂÜÖÂ≠òÔºàAMDÁöÑÂÆûÈôÖËøêË°åÈÄüÁéá‰∏∫8000ÔºâÔºåÊØî‰º†ÁªüAI PCÁöÑ64‰ΩçÂèåÈÄöÈÅìÂÜÖÂ≠òÊèêÈ´ò‰∫Ü‰∏ÄÂÄçÔºåÁõ∏ÂΩì‰∫é4ÈÄöÈÅì„ÄÇ
- Áî±‰∫éGrace ARM CPUÂè™ËÆ§ËØÅ‰∫ÜDGX‚Ñ¢ OSÊìç‰ΩúÁ≥ªÁªüÔºåÂ∫îËØ•Âè™ËÉΩË∑ëLinuxÔºà‰∏çÂÖºÂÆπWindowsÔºâÔºåÊâÄ‰ª•DGX Spark‰∏ªË¶ÅÂ∞±ÊòØÁî®‰∫éËÆ°ÁÆóÔºåÂõæÂΩ¢ÊÄßËÉΩÊñπÈù¢‰∏çÁü•ÊòØÂê¶ÂÅö‰∫Ü‰ºòÂåñÔºü
  - DGX SparkÁöÑAIÊÄßËÉΩÔºå‰∏éGeForce RTX 5070Ê°åÈù¢ÊòæÂç°ËæÉ‰∏∫Êé•Ëøë„ÄÇ‰∏çËøáÊúâ‰∏ÄÁÇπÔºå5070ÁöÑÊòæÂ≠òÂ∏¶ÂÆΩÈ´òËææ672GB/sÔºåËøô‰∏ÄÁÇπÂç≥‰ΩøÊòØ256bit LPDDR5xÂÜÖÂ≠òÁöÑÈõÜÊòæ‰πüÂøòÂ∞òËé´Âèä„ÄÇÊØïÁ´ü‰∏ÄÂùó5070Áã¨ÊòæÂ∞±ÊòØ250W TGPÂäüËÄóÔºåÂÖ∂Á©∫Èó¥Âç†Áî®‰πüÂæàÈöæÂÅöÂà∞MiniÊú∫ÁÆ±/ËΩªËñÑÁ¨îËÆ∞Êú¨ÈáåÈù¢„ÄÇ

- ## [AIÁªòÁîªÔºàStable DiffusionÔºâÁî®‰ªÄ‰πàÊòæÂç°ÊØîËæÉÂ•ΩÔºü - Áü•‰πé](https://www.zhihu.com/question/638915747)

- ## [‰∏çËÆ°È¢ÑÁÆóÔºåÂ∏ÆÊàë‰π∞‰∏ÄÂè∞ÊÄßËÉΩË∂ÖÂº∫ÔºåÂ§ñËßÇÁÆÄÁ∫¶ÁöÑÁ¨îËÆ∞Êú¨ÁîµËÑëÔºüÁî®‰∫éÊú¨Âú∞ÈÉ®ÁΩ≤ÂêÑÁßçÂ§ßÊ®°ÂûãÔºåÊöÇÂÆöÂæÆËΩØÔºåËøòÊúâÂà´ÁöÑÈÄâÂêóÔºü - Áü•‰πé](https://www.zhihu.com/question/624402976)
- WindowsÁ¨îËÆ∞Êú¨ËøôËæπÁöÑÊÄßËÉΩÂ§©Ëä±ÊùøÊòØ i9-13980HX/R9-7945HX +RTX4090Ôºà175WÔºâ
  - Ëøô‰ø©CPUÊÄßËÉΩÂ∑Æ‰∏çÂ§öÔºå7945HS‰æøÂÆúÁÇπ
  - Êé®Ëçê‰Ω†ÈÄâi9-13980HXÔºåÂõ†‰∏∫IntelÂØπÂêÑÁßçÁîü‰∫ßÂäõÂ∑•ÂÖ∑ÁöÑÂÖºÂÆπÊÄßÊõ¥Â•Ω„ÄÇ
  - ÁßªÂä®Á´ØRTX4090ÁöÑÊÄßËÉΩÁõ∏ÂΩì‰∫éÊ°åÈù¢Á´ØÁöÑ4070TiÔºå‰ΩÜÊòØÂÆÉÊúâ16GÊòæÂ≠òÔºåÊØî12GÊòæÂ≠òÁöÑ4070TiÊõ¥ÈÄÇÂêàË∑ëÊ®°Âûã„ÄÇ
  - ÁõÆÂâçÔºåÊê≠ËΩΩ13980HSÂíåRTX 4090Á¨îËÆ∞Êú¨ÁöÑ‰ª∑Ê†ºÊôÆÈÅçÂú®22000ÂÖÉ‰ª•‰∏ä„ÄÇÊØîÂ¶ÇROGÊû™Á•û7ÂíåÂæÆÊòüGP78HS
  - ÂêåÊ†∑ÁöÑ‰ª∑Ê†º‰Ω†ÂèØ‰ª•‰π∞Âà∞i7 13700KÂíåRTX4090 24GÁöÑÂè∞ÂºèÊú∫ÔºàÊÉ†ÊôÆÊöóÂΩ±Á≤æÁÅµ9plusÔºâ

- ÂΩìÂâçÁ¨îËÆ∞Êú¨Á´ØÊúÄÂº∫ÁöÑ4090Á¨îËÆ∞Êú¨ÊòæÂç°ÔºåËßÑÊ†º‰∏äÊåÅÂπ≥Ê°åÈù¢4080ÔºåË∑ëÂàÜÊÄßËÉΩ‰∏ä‰∏éÊ°åÈù¢4070tiÁõ∏ÂΩì
  - Á¨îËÆ∞Êú¨Á´ØÊ¨°Âº∫ÁöÑ4080Á¨îËÆ∞Êú¨ÊòæÂç°ÔºåËßÑÊ†º‰∏äÊé•Ëøë‰∫éÊ°åÈù¢4070tiÔºåË∑ëÂàÜÊÄßËÉΩ‰∏éÊ°åÈù¢3090Áõ∏ÂΩì

- Âª∫ËÆÆËøòÊòØÊà¥Â∞îÔºåÊÉ†ÊôÆÔºåËÅîÊÉ≥Ëøô‰∏âÂÆ∂ÁöÑÂ∑•‰ΩúÁ´ôÁ¨îËÆ∞Êú¨Âêß„ÄÇÂ∑•‰ΩúÁ´ôÈÖçÁΩÆÁöÑÁã¨Á´ãÂõæÂΩ¢ÊòæÂç°ÈÄÇÂêàÁöÑË¶ÅÊ±ÇÁöÑÂ§ßÈáèÊ®°ÂûãÔºåÂÜçÂ∞±ÊòØÂ∑•‰ΩúÁ´ôÁöÑÂÜÖÈÉ®ÁªìÊûÑ„ÄÅÈÖçÁΩÆÈÉΩÊòØÈíàÂØπÂ§ÑÁêÜÂ§ßÂûã3DÊ®°ÂûãÂÅö‰∫ÜÂä†Âº∫„ÄÅ‰ºòÂåñÁöÑÔºåÂÖ∂Â∑•‰ΩúÁ®≥ÂÆöÊÄßÊòØÊôÆÈÄöÁ¨îËÆ∞Êú¨Êó†Ê≥ïÊØîÊãüÁöÑ„ÄÇ

- ## [amdÂèëÂ∏ÉÊñ∞ÁöÑËäØÁâáÔºåËøôÊ¨°ËäØÁâáË∑üËãπÊûúÁöÑm4‰∏ÄÊ†∑ÔºåÂ∞ÜgpuÂíånpuÈõÜÊàêÂà∞Âêå‰∏ÄÂùóËäØÁâá‰∏äÂéªÔºåËØ¥Êòé‰∫Ü‰ªÄ‰πàÔºü - Áü•‰πé _202501](https://www.zhihu.com/question/9253691862)
  - AMD Âú® CES 2025 ‰∏äÂèëÂ∏É‰∫ÜÈîêÈæô AI Max 300‚ÄúStrix Halo‚ÄùÁ≥ªÂàó APUÔºåÊê≠ËΩΩ‰∫ÜÊúÄÈ´ò 40CU ÁöÑË∂ÖÂº∫Ê†∏ÊòæÔºåÊ≠§Â§ñÈõÜÊàê‰∫Ü 50 TOPS‚ÄúXDNA 2‚Äù NPU„ÄÇ
  - ËøôÁßçÂ∞ÜÂº∫Âäõ CPU„ÄÅGPU„ÄÅNPU Á≠âÈõÜÊàêÂà∞‰∏Ä‰∏™ËäØÁâáÁöÑÂÅöÊ≥ïÔºåÁúãËµ∑Êù•ÊúâÁÇπÂÉèËãπÊûúÂú® M Á≥ªÂàóËäØÁâá‰∏≠ÁöÑËÆæËÆ°„ÄÇ

- ËãèÂ¶àÁúãÂà∞apple MÁ≥ªÂàóËäØÁâáË¢´ÊãøÊù•Ë∑ëAIÂ§ßÊ®°ÂûãÊé®ÁêÜÁöÑÊó∂ÂÄôÔºå‰πüÈºìÊç£‰∫Ü‰∏Ä‰∏™Ryzen AI Max 300 Á≥ªÂàóÂ§ÑÁêÜÂô®ËßÑÊ†ºÊõùÂÖâÔºåÊúÄÈ´ò16Ê†∏ÂøÉ„ÄÅ40CUÊ†∏Êòæ „ÄÇÊúÄÈ´òÂèØ‰ª•‰ªéÂÜÖÂ≠ò‰∏≠ÂàÜÈÖç96GBË∂ÖÂ§ßÊòæÂ≠òÔºåÁªìÂêàROCmÔºàÂºÄÊîæËÆ°ÁÆóÂπ≥Âè∞ÔºâÁ≥ªÁªüÊîØÊåÅÔºåÊàñËÉΩÂèòÊàêÊñ∞‰∏Ä‰ª£Â∞èÂûãÂ∑•‰ΩúÁ´ôÁ•ûU

- ## [Â¶Ç‰ΩïÁúãÂæÖËã±‰ºüËææÂÖ¨Âè∏ÂèëÂ∏ÉÁöÑÊ°åÈù¢Á∫ßAIË∂ÖÁÆó? - Áü•‰πé](https://www.zhihu.com/question/8970511370)
- Project Digits Áé∞Âú®ÊîπÂêçÂè´ÔºöNVIDIA DGX‚Ñ¢ Spark „ÄÇ
  - ÂÖ∂ÂÆö‰ΩçÊòØÔºö‰∏™‰∫∫Ê°åÈù¢Á´ØÁöÑ AI Ë∂ÖÁ∫ßËÆ°ÁÆóÊú∫„ÄÇ
  - Áî± GB10 Ë∂ÖÁ∫ßËäØÁâáÈ©±Âä®
  - ‰ΩøÁî® FP4ÔºåAI ÊÄßËÉΩËææ 1000 TOPS
  - ÈÖçÂ§á NVIDIA Blackwell GPUÔºå‚ΩÄÊåÅÁ¨¨‰∫î‰ª£ Tensor Core ÊäÄÊúØ
  - NVIDIA Grace CPU ÂÆûÁé∞ÔºåÈááÁî® 20-core È´òÊÄßËÉΩ Arm Êû∂ÊûÑ
  - 128 GB Áªü‰∏ÄÂØªÂùÄÁ≥ªÁªüÂÜÖÂ≠ò
  - ÊîØÊåÅÈ´òËææ 4 TB ÁöÑ NVMe Â≠òÂÇ®
  - ÊîØÊåÅÈ´òËææ 200B ÂèÇÊï∞ÁöÑÂ§ßËØ≠Ë®ÄÊ®°Âûã
  - ÈÄöËøá NVIDIA Connect-X ÁΩëÁªúËøõË°åËøûÊé•ÔºåÂèØËøûÊé•‰∏§‰∏™ DGX SparkÔºåÊîØÊåÅÈ´òËææ 405B ÂèÇÊï∞ÁöÑÊ®°Âûã

- ## üöÄ [Â¶Ç‰ΩïËØÑ‰ª∑Ëã±‰ºüËææÊñ∞ÂèëÂ∏ÉÁöÑÊ°åÈù¢ AI Ë∂ÖÁ∫ßÁîµËÑë Project DigitsÔºü - Áü•‰πé _202501](https://www.zhihu.com/question/8953765123)
- ‰∏äÂçàËøòÂú®ÁúãAMD strix haloÔºå‰∏ãÂçàNvidiaÁ™ÅÁÑ∂Â∞±Êîæ‰∫Ü‰∏Ä‰∏™Áõ∏ÂêåÂÆö‰ΩçÁöÑ...

- ÊÉ≥‰π∞ÁöÑÂêåÂ≠¶Ê≥®ÊÑè‰∏ãËøô‰∏™ËÆæÂ§áÁöÑÂÜÖÂ≠òÔºåÂÆÉÊòØÁªü‰∏ÄÂÜÖÂ≠òÔºåÂç≥CPUÂíåGPUÂÖ±‰∫´LPDDR5X. ÂÆÉ‰∏çÊòØGDDR6Ôºå‰πü‰∏çÊòØHBM2ÁöÑ„ÄÇ
  - ËôΩÁÑ∂Êúâ 128GBÔºå‰ΩÜÊòØÊ†πÊçÆ Grace Êû∂ÊûÑ CPU ÁöÑ Product BriefÔºåÂçï CPU ÁöÑÂÜÖÂ≠òÂ∏¶ÂÆΩÊúÄÂ§ßÂè™Êúâ512GB/s
  - ÊâÄ‰ª•Â¶ÇÊûúÁî®Ëøô‰∏™ËÆæÂ§áÊù•ËøêË°åÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåÁì∂È¢àÂ∞±‰ºöÂèòÊàêËøô‰∏™ÂÜÖÂ≠òÂ∏¶ÂÆΩ„ÄÇ
  - ÁÆÄÂçïÊù•ËÆ≤ÔºåÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊØèÁîüÊàê‰∏Ä‰∏™tokenÔºåÂ∞±ÈúÄË¶ÅÂ∞ÜÊï¥‰∏™Ê®°ÂûãÊâ´‰∏ÄÈÅçËøõË°åËÆ°ÁÆóÔºàÂÆûÈôÖ‰∏äÊØîËøô‰∏™ÊèèËø∞Â§çÊùÇÂæàÂ§öÔºâ„ÄÇËøôÊÑèÂë≥ÁùÄÔºåÂΩìÊµÆÁÇπÁÆóÂäõÂÖÖË£ïÁöÑÊó∂ÂÄôÔºåÊâ´ÊèèÁöÑÈÄüÂ∫¶Â∞±ÂÜ≥ÂÆö‰∫ÜÁîüÊàêÊñáÊú¨ÁöÑÈÄüÂ∫¶‰∏äÈôê„ÄÇ
  - ÁõÆÂâçËøô‰∏™ËÆæÂ§áÁöÑÂÜÖÂ≠òÂ∏¶ÂÆΩÊ∞¥Âπ≥Ë∑ü M4 Max ÁöÑ MacBook Ê≤°‰ªÄ‰πàÂå∫Âà´ÔºàApple MacBook Pro M4 Max 128GB ÂÜÖÂ≠òÂ∏¶ÂÆΩÊòØ546GB/sÔºâ
- Êãø [Llama-3.3-70b-instruct-4bit] ‰∏æ‰æãÔºåËøô‰∏™4bitÈáèÂåñÊ®°ÂûãÂ§ßÂ∞èÁ∫¶‰∏∫40GBÔºåÈÇ£‰πàÊâ´‰∏ÄÈÅçÂ∞±ÊÑèÂë≥ÁùÄGPUË¶ÅÂ§ÑÁêÜ40GBÁöÑÊï∞ÊçÆÔºåÂ¶ÇÊûúÊÉ≥Ë¶ÅÊØèÁßíÈíüÁîüÊàê10 tokenÔºåÁÆÄÂçïËÆ°ÁÆóÂèØÂæóÔºå40GB\*10 = 400GB, ËøôÊÑèÂë≥ÁùÄÂÜÖÂ≠òÂ∏¶ÂÆΩËá≥Â∞ëÊúâ 400GB/s ÊâçËÉΩ‰øùËØÅÊØèÁßíÈíüËÉΩÁîüÊàê 10 token.
  - ÂõûÂà∞ digits Ëøô‰∏™ËÆæÂ§áÔºåÂú®512GB/s ÁöÑÊÉÖÂÜµ‰∏ãÔºå**ËøêË°å 70b-4bit ËßÑÊ®°ÁöÑÊ®°ÂûãÔºåÁîüÊàêÈÄüÂ∫¶ÁêÜËÆ∫ÊúÄÂ§ßÂÄºÊòØ 512/40 = 12.8 token/s**

- nvÁâàÁöÑmac mini„ÄÇÂΩìÁÑ∂mac miniÊúâmacosÔºåproject digitsÂè™ÊúâlinuxÔºåÊìç‰ΩúÁ≥ªÁªüÁîüÊÄÅ‰∏ä‰ª•ÂèäÊ°åÈù¢Á∫ßÂÆö‰Ωç‰∏ä‰º∞ËÆ°‰ºöÂØºËá¥ÊãâËÉØÊéâÔºånvËôΩÁÑ∂ÊúâcudaÁîüÊÄÅÔºå‰ΩÜÊòØÂú®Ê°åÈù¢Á∫ßÁõ∏ÊØîosÁöÑÁîüÊÄÅÔºåËøòÊòØÊúâÁÇπÂõ∞ÈöæÁöÑ„ÄÇ
- Â§ßÊ®°ÂûãÊé®ÁêÜÂÖ∂ÂÆûÊòØ‰∏™ÂæàÂæÆÂ¶ôÁöÑ‰∫ßÂìÅÈúÄÊ±ÇÔºåÂ§ßÊòæÂ≠òÂÆπÈáèÂæàÈáçË¶ÅÔºåmemory bound‰πüÊòØ‰∫ãÂÆû„ÄÇ
  - ÊÄª‰ΩìÊù•ËÆ≤ÔºåÂÆπÈáèÊØîÂ∏¶ÂÆΩÈáçË¶ÅÔºåÊØïÁ´üÂÆπÈáèÂÜ≥ÂÆö‰∫Üyes/noÔºåÂ∏¶ÂÆΩÂÜ≥ÂÆö‰∫Ütoken/sÁöÑ‰ΩìÈ™å„ÄÇ‰ΩÜÊòØ‰ΩìÈ™åÂ∑ÆÂà∞‰∏ÄÂÆöÁ®ãÂ∫¶‰πüÊòØ‰∏™yes/noÁöÑÈóÆÈ¢ò„ÄÇ
- ‰ª•ai pc‰ªäÂ§©‰∫ãÂÆû‰∏äÁöÑË∂ÖÁ∫ßÂ∫îÁî®‰∏∫‰æãaiÁºñÁ®ãËÄåË®ÄÔºå10 token/s‰ª•‰∏ãÂü∫Êú¨Â∞±ÊòØÁé©Á•®ÔºåÂä†‰∏äaiÂï∞Âì©Âï∞Âó¶Ë¶ÅÂàÜÊûê‰∏ÄÂ§ßÂ†ÜÔºåÂü∫Êú¨Ë¶ÅÂÅöÂà∞ÂèØÁî®ËøòÊòØÂæó30ÔΩû40 token/s
  - ÊåâÁÖßÊøÄÊ¥ªÈáèÁÆóÔºå30ÔΩû40token/sÔºåÂ¶ÇÊûú10GBÁöÑÊøÄÊ¥ªÈáèÂ∞±300ÔΩû400GB/sÁöÑÂÜÖÂ≠òÂ∏¶ÂÆΩ
  - denseÊ®°Âûã10B‰∏ä‰∏ãÂÅöaiÁºñÁ®ãÂá†‰πéÊ≤°Ê≥ïÁî®ÔºåmoeÂèØ‰ª•Êêè‰∏ÄÊêè„ÄÇ
  - ÁõÆÂâçÁöÑÊ®°Âûã‰∏ªÊµÅÁöÑÂü∫Êú¨ÈÉΩÊòØ70b‰ª•‰∏ãÁöÑdenseÔºå‰ª•Âèä200b‰ª•‰∏äÁöÑmoeÔºå70b‰ª•‰∏ãÁöÑdenseÂæàÂ∞¥Â∞¨ÔºåÊïàÊûú‰∏äÊØîËæÉÈöæÊé•Ëøë200b‰ª•‰∏äÁöÑmoeÔºåÂÆπÈáèÈúÄÊ±ÇÂ∞èÔºå‰ΩÜÂ∏¶ÂÆΩÈúÄÊ±ÇÂèØÊòØÂÆûÊâìÂÆûÁöÑË∂ÖÁ∫ßÈ´ò„ÄÇÊØîËæÉÈÄÇÈÖç[gddrÊòæÂ≠ò] ÁöÑÊ≠£ÁªèÊ∏∏ÊàèÂç°„ÄÇ

- GB10ÁöÑËäØÁâáÔºåÂ∫îËØ•ÊòØ‰ªéÊúçÂä°Âô®ÁöÑGB100ÂΩì‰∏≠Á†ç‰∏ÄÂàÄ‰∏ãÊù•Áî®ÁöÑ„ÄÇCPUÊòØËÅîÂèëÁßëÂÆöÂà∂ÁöÑ‰∏ÄÈ¢óARMÔºåGPUÈÉ®ÂàÜÁÆóÂäõËææÂà∞1‰∏™PÔºåfpÁÆóÂäõ„ÄÇÊúÄÂ§ßÁöÑ‰∫ÆÁÇπÁõ¥Êé•Êãø‰∫Ü128GBÁöÑ LPDDR 5xÂÜÖÂ≠òÂÅöÊòæÂ≠ò„ÄÇ
  - ÂæàÂ§ö‰∫∫ÂÖ∂ÂÆû‰∏çÁü•ÈÅìÔºåÁé∞Âú®Ë∑ëÂ§ßÊ®°ÂûãÊó†ËÆ∫ÊòØËÆ≠ÁªÉËøòÊòØÊé®ÁêÜÔºåÊúÄÂ§ßÁöÑÁì∂È¢àÊòØÊòæÂ≠òÂÆπÈáè‰∏çË∂≥ÔºàMemory boundÔºâÔºåËÄå‰∏çÊòØÁÆóÂäõ‰∏çË∂≥(Compute bound)„ÄÇ
  - ÊØîÂ¶Ç‰∏Ä‰∏™200BÁöÑÊ®°ÂûãÔºåÂú®fp4ÊàñËÄÖint 4ÁöÑÂâçÊèê‰∏ãÔºåÂÖâÊòØÊòæÂ≠òÂç†Áî®Â∞±Ë¶ÅÊúâ100GBÂ§ßÂ∞è„ÄÇËøêË°åËµ∑Êù•‰πãÂêéËøòË¶ÅÊúâkv cacheÈöèÁùÄ‰∏ä‰∏ãÊñáÈïøÂ∫¶Âç†Áî®ËÄåÂ¢ûÂ§ß„ÄÇ‰∏ÄÂº†ÊòæÂç°Ë£Ö‰∏ç‰∏ãÔºåÂ∞±Ë¶ÅÂàÜÂ∏ÉÂú®Â§öÂº†Âç°‰∏äÔºåÈÇ£‰πàÂ∞±‰ºö‰∫ßÁîüÈÄö‰ø°ÂºÄÈîÄ‰ªéËÄåÂØºËá¥ÁÆóÂäõÊó†Ê≥ïË¢´ÂÖÖÂàÜÂà©Áî®Ôºå‰∏çÂæó‰∏çÁ≠âÂæÖÈÄö‰ø°ÂÆåÊàê‰πãÂêéÂÜçËøõË°åËÆ°ÁÆó„ÄÇ
  - ‰πãÂâçÊ∂àË¥πÁ∫ßÁöÑRTX 4090ÔºåÊúÄÂ§ßÁöÑÊòæÂ≠òÂè™Êúâ24GB„ÄÇRTX 5090Ôºå‰πüÂè™Êúâ32GBÊòæÂ≠òËÄåÂ∑≤„ÄÇÊï∞ÊçÆ‰∏≠ÂøÉÂç°‰æãÂ¶ÇA100Êúâ40GÂíå80GÔºå‰ΩÜ‰ª∑Ê†ºÂèà‰ºöÊòæËëóÊØîÊ∂àË¥πÁ∫ßÊòæÂç°Ë¥µ„ÄÇ
  - ÊâÄ‰ª•Áé∞Âú®Ëøô‰∏™128GBÁöÑÂÜÖÂ≠ò‰Ωú‰∏∫Áªü‰∏ÄÊòæÂ≠ò‰ΩøÁî®ÔºåËá≥Â∞ëËß£ÂÜ≥‰∫ÜÊòæÂ≠ò‰∏çÂ§üÁî®ÁöÑÈóÆÈ¢ò„ÄÇ3000ÁæéÂÖÉÁöÑÂîÆ‰ª∑ÔºåÁîöËá≥ÂèØ‰ª•ËØ¥ËâØÂøÉ‰∫Ü„ÄÇ

- CPUÊòØËÅîÂèëÁßëÂÆöÂà∂ÁöÑ‰∏ÄÈ¢óARMÔºåË∑ëLinuxÁé©ÊØõÊ∏∏Êàè

- ÂÖ∂ÂÆûÂâç‰∏§Âπ¥AIÂàöÂºÄÂßãËµ∑Â§¥ÁöÑÊó∂ÂÄôÔºålocal LLMÂíåÊñáÁîüÂõæÁà±Â•ΩËÄÖÂ∞±ÂèëÁé∞Â§öÊï∞‰ªªÂä°ÈÉΩÊòØmemory boundÁöÑ‰∫ÜÔºåÂ∏ÇÈù¢‰∏äÈô§‰∫ÜÊØîÊ±ΩËΩ¶ËøòË¥µNÂÆ∂Êé®ÁêÜÂç°ÔºåÂè™ÊúâMacÁöÑÁªü‰∏ÄÂÜÖÂ≠òËÉΩË£ÖÁöÑ‰∏ã„ÄÇ
- Ëøô‰∏™ÂÖ∑ÊúâÂèØÁî®ÊÄßÔºåcudaÁîüÊÄÅ‰∏éÊÄßËÉΩÊòØ‰∏çÁî®ÊãÖÂøÉÁöÑÔºåmacË∑ëaiÂ∞±ÊòØË°å‰∏∫Ëâ∫ÊúØÔºåÈÄüÂ∫¶ÊÖ¢Âà∞ÁàÜÁÇ∏ÔºåÁîüÊÄÅ‰πüÊòØÊÆãÁº∫ÁöÑ
- Âè™ÊòØ‰∏∫‰∫Ü 128G Áªü‰∏ÄÂÜÖÂ≠òÁé∞Âú®‰πü‰∏çÈúÄË¶Å‰π∞ËøôÁé©ÊÑèÂÑøÂïä.. $4799 ÁöÑ Mac Studio ÊàñËÄÖ $4999 ÁöÑ MacBook Pro Â∞±Ë°å. ÂΩìÁÑ∂Âè™ÊòØ‰∏∫‰∫Ü AI ÁõÆÁöÑÁöÑËØù, ËÄÅÈªÑËøô‰∏™ÁõíÂ≠êÁ°ÆÂÆûÊõ¥ÊúâÊÄß‰ª∑ÊØî.

- ‰∏âÂçÉÂàÄÔºå128GBÔºåËøô‰∏™‰ª∑Ê†ºÂÖ∂ÂÆû‰∏çÊØîËãπÊûúÂ•ΩÂ§öÂ∞ë„ÄÇËÄå‰∏î‰∏âÂçÉÂàÄÊòØstarting price

- ÁªèÂÖ∏ÁöÑ72BÁöÑLlamaÊ®°ÂûãÔºå8ÊØîÁâπÁ≤æÂ∫¶ÈúÄË¶Å84GBÁöÑÊòæÂ≠òÔºåÈÇ£Â∞±ÈúÄË¶Å2ÂùóA100ÊàñËÄÖ4Âùó24GBÁöÑ4090/3090ÔºåËøô‰∏§ÁßçÊñπÊ°àÈÉΩË¶ÅÊØî3000ÁæéÂÖÉÂ§ö‰∏îÂ§çÊùÇ„ÄÇ
  - Ë¶ÅÁü•ÈÅìProject digitsÊòØ‰∏ÄÊï¥‰∏™Êú∫Âô®Ôºå72BÁöÑÁªèÂÖ∏Ê®°ÂûãÂèØ‰ª•Áõ¥Êé•Ë∑ëÔºåËøôÊ†∑Â∞±Âü∫Êú¨‰∏äÂèØ‰ª•ÂÅöÁªùÂ§ßÂ§öÊï∞ÁöÑÂæÆË∞ÉÂ∑•‰Ωú„ÄÇ
  - ÁîöËá≥‰∏§Âè∞Êú∫Âô®ÔºåÂ∞±ÂèØ‰ª•Ë∑ë4ÊØîÁâπÁ≤æÂ∫¶ÁöÑ200BÊ®°ÂûãÔºåËøô‰πàÂ§ßÁöÑÊ®°ÂûãÊîæÂà∞‰πãÂâçÂü∫Êú¨‰∏äÂè™ÊúâÂ§ßÁöÑÂÖ¨Âè∏ÊàñËÄÖÂÆûÈ™åÂÆ§ÊâçÊúâÂèØËÉΩË∑ëÁöÑËµ∑Êù•ÔºåËÄåÁé∞Âú®6000ÁæéÂÖÉÂ∞±ËÉΩÂÆåÁæéËß£ÂÜ≥ÔºåËøôÂØπ‰∫éÁªùÂ§ßÂ§öÊï∞ÁöÑÁ©∑LabÊù•ËØ¥ÈÉΩÊòØÂ§©Â§ßÁöÑÁ¶èÈü≥„ÄÇ

- Strix HaloÂíåM4 MaxÊòØÈ´òËßÑÊ†ºÁöÑÊ∂àË¥πÁ∫ß‰∫ßÂìÅÔºåÊê≠ËΩΩStrix HaloÁöÑÊòØÂêÑÁßçÂìÅÁâåÂºÄÂèëÁöÑÁ¨îËÆ∞Êú¨ÁîµËÑë„ÄÅSFF‰∏ªÊú∫ÔºåÁîöËá≥ÊòØÂáÜÁ≥ªÁªü„ÄÅMoDT‰∏ªÊùøÔºåÂÆÉ‰ª¨Êê≠ËΩΩWindows 11Êìç‰ΩúÁ≥ªÁªüÔºå‰πüÂèØ‰ª•ÂÆâË£Ö‰ªªÊÑèGNU/LinuxÊìç‰ΩúÁ≥ªÁªüÔºåÂ∞±ÂÉè‰∏Ä‰∏™ÊôÆÈÄöÁöÑx86ÁîµËÑë‰∏ÄÊ†∑
  - Project DigitsÁöÑCPUÔºåÊòØËÅîÂèëÁßëÂÆöÂà∂ÁöÑ20Ê†∏ARM CPUÔºå10Â§ßÊ†∏‰∏∫Cortex-X925Ôºå10‰∏≠Ê†∏‰∏∫Cortex-A725„ÄÇA725ÂíåM4Â∞èÊ†∏Â§ßÊÄßËÉΩÊé•ËøëÔºåËÄåX925Âàô‰∏çÂ¶ÇM4Â§ßÊ†∏ÂíåZen5„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåARM LinuxÁöÑÁîüÊÄÅÂπ∂‰∏çÂ¶Çx86ÔºåËøôÂíåARM macOSÂÆåÂÖ®‰∏çÂêå„ÄÇStrix HaloÁöÑ16Ê†∏32Á∫øÁ®ãÁöÑzen5 CPUÔºåÂü∫Êú¨‰∏äÊòØÈ°∂Á∫ßÁöÑCPUÈÖçÁΩÆÔºåÂÖ∂Â§öÁ∫øÁ®ãÊÄßËÉΩÊòØÊãâÊª°ÁöÑ„ÄÇ
  - Project DigitsÁöÑÂÜÖÂ≠òÔºåËôΩÁÑ∂ÂÆòÊñπÊ≤°ÊúâËØ¥ÊòéÔºå‰ΩÜÂæàÂ§ßÂèØËÉΩ‰πüÊòØ256‰ΩçÁöÑÔºåÊ≠§Êó∂Project DigitsÂú®‰ΩøÁî®LPDDR5XÁöÑ8533MT/sÁöÑÂÜÖÂ≠òÊó∂ÔºåÂÖ∂ÈÄüÁéáÂêåÊ†∑‰∏∫272GB/sÔºåËÄåËøô‰∏ÄÂ∏¶ÂÆΩÔºå‰ªÖ‰∏éStrix Halo/M4 ProÁõ∏ÂêåÔºåÊòØM4 MaxÁöÑ‰∏ÄÂçäÔºõÂ¶ÇÊûúÊòØ512‰ΩçÔºåÂàôÂº∫‰∫éStrix Halo‰∏ÄÂÄçÔºåÂπ∂‰∏éM4 MaxÊåÅÂπ≥„ÄÇ
- haloÁöÑnpuÊÄªÂÖ±Â∞±50topsÔºå8060s‰πüÊ≤°ÊúâÁ°¨‰ª∂wmmaÔºå2.5ghz‰∏ã‰πüÂ∞±51.2topsÔºåÂä†Ëµ∑Êù•‰∏çÂà∞gb10ÁöÑ1/4
- ÂèØÊÉúhalo‰∏çÊòØÁªü‰∏ÄÂÜÖÂ≠òÊû∂ÊûÑ
  - haloÊòØumaÊû∂ÊûÑÔºå15Âπ¥ÂâçÂàù‰ª£apuÂ∞±Â∑≤ÁªèÊòØumaÊû∂ÊûÑ‰∫Ü

- Ëøô‰∏™‰∏úË•ø3000ÂàÄÊòØÁúüÁöÑÂæàË¥µÔºå‰∏çËøáproject digitsÊòØÊåëÊàòÂÜØËØ∫‰æùÊõºÊû∂ÊûÑÔºåCPU ËÆøÈóÆÂÜÖÂ≠ò„ÄÅÁ°¨ÁõòÔºåÊòæÂç°Â§ÑÁêÜÊï∞ÊçÆÈúÄË¶ÅÊääÊï∞ÊçÆÂÖà‰º†Âà∞ÊòæÂ≠ò„ÄÇÁªü‰∏ÄÂÜÖÂ≠òÊû∂ÊûÑÂ∞±ÊòØÊääGPUÊ†∏ÂøÉÁõ¥Êé•‰∏éÂÜÖÂ≠òÁõ∏ËøûÔºåÂºÑÂ§ßÂÜÖÂ≠ò„ÄÇ
  - ÁõÆÂâçÈô§‰∫ÜÂ§ßÂÖ¨Âè∏ÊúâÈí±‰π∞Âá†Áôæ‰∏äÂçÉÂç°Ë∑ëËÆ≠ÁªÉÔºåÊôÆÈÄö‰∫∫ÁúüË∑ë‰∏çËµ∑LLMÂ§ßÊ®°Âûã„ÄÇÂØπ‰∫éÊôÆÈÄö‰∫∫Êù•ËØ¥ÔºåÊ†∏ÂøÉÁÆóÂäõ‰∏çÈáçË¶ÅÔºåÈóÆÈ¢òÊòØÊÄé‰πàÂú®ÊòæÂç°loadÂ§ßÊ®°Âûã„ÄÇËÄåÁªü‰∏ÄÂÜÖÂ≠òÂ∞±ÊòØÁî®Ë∂ÖÈ´òÊÄß‰ª∑ÊØîÁöÑÂÜÖÂ≠ò‰ª£ÊõøÊòæÂ≠òÔºå‰∏çÁî®GDDR7ÔºåÁî®DDR5 „ÄÇ
