---
title: tool-iot-devices
tags: [iot, tool]
created: 2019-08-11T07:36:17.519Z
modified: 2022-01-16T15:52:31.293Z
---

# tool-iot-devices

# watch

- 不使用录音笔，而使用录音手表

- [智能手表录音功能总结 - 知乎](https://zhuanlan.zhihu.com/p/592180358)
  - 社畜们需求很简单 能息屏一键录音 录到没电 能同步到手机 且可以导出文件 有血氧 心率监控等健康功能 手表分运动产品线和商务产品线 质感做到最好 这表我出3000
  - 后台录音
  - [胖表哥小课堂：Amazfit GTS4>R4手表录音功能测评，能否息屏录音？音质如何？\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1vm4y1v7yY/?vd_source=deff4d2e2efa3273948dd6911a08fd39)

- [分析华米Amazfit T-Rex和GTR对比区别？](http://bbs.mydigit.cn/read.php?tid=2831142)
  - GTR支持录音
  - 功能是一样的，就是外观不同的，我自己也是才购的，新品trex的，样式很不错，很运动的感觉
# U盘
- U盘的优点主要在于，
  - U盘的使用不需要依赖网络，
  - 并且U盘能够保护用户的隐私和数据安全。
  - 另外，将电脑文件从本地上传至U盘的传输速度很快。
- U盘的主要缺点在于，
  - 价格可能相对昂贵，能够储存的数据容量有限，通常以GB为单位。
  - 此外，U盘作为一个物理硬件容易被丢失，U盘中的文件也没办法随着本地文件修改进度而进行实时更新。
  - U盘间进行数据传输也比较麻烦。
- 云盘的优点在于，
  - 用户通常可以免费体验云盘所带来的服务。
  - 云盘能够提供较大的数据容量，通常以T为单位。
  - 云盘中的文件可以与电脑中保存的本地文件进行实时同步。（微软的OneDrive和百度云都是很好的例子）
  - 云盘之间想要传输文件也会相对便捷。云盘也会带有一定的网络社交属性。
- 云盘的缺点在于，
  - 云盘的使用需要依赖于网络，并且处于某些原因储存在云盘中的文件会被第三方查阅、修改或者删除。
  - 这样就会导致云盘用户的个人隐私和数据安全得不到保障。
  - 文件上传至云盘的速度还会受到网速的限制。
# screen resolution
- guide
  - 除了分辨率，还要考虑使用场景，如系统分屏、多窗口排列
  - 还要考虑持续更新、消费的能力
  - [Screen Resolution Stats Worldwide](https://gs.statcounter.com/screen-resolution-stats)

- android
  - ldpi: 240x320, 240x432
  - mdpi: 320x480, 480x800, 480x854, 1024x600, 1280x800
  - hdpi: 480x800
  - xhdpi: 720x1280, 1200x1290, 2560x1600
  - xxhdpi: 1080x1920
  - tvdpi: 1280x800
- [Screen sizes and densities](https://developer.android.com/about/dashboards/index.html)
  - ldpi/0.001, mdpi/0.062, tvdpi/0.027, hdpi/0.179, xhdpi/0.45, xxhdpi/0.281

- desktop
  - **1920x1080**/0.215, 1366x768/0.201, 1536x864/0.096, 1440x900/0.064, 1280x720/0.054, 1600x900/0.036

- mobile
  - 360x640/0.106, 414x896/0.072, 360x780/0.057
# android
- vulkan version
  - none: 0.42
  - v1.03: 0.22
  - v1.1: 0.36

- opengel es version
  - v2.0: 0.105
  - v3.0: 0.141
  - v3.1: 0.079
  - v3.2: 0.675
# 设备使用问题
- 强制全家桶
- 手机管家/安全管家
- 推送消息太多
- 隐私数据
# laptop-笔记本
- 要点
  - 键盘的上下左右4个方向键要大小相同
# Fuchsia OS

# 阿里YunOS/AliOS

- 2011年7月28日，阿里巴巴正式推出YunOS，基于linux kernel，设计借鉴了Android
- YunOS兼容Android应用，YunOS的虚拟机还是Dalvik虚拟机修改版
- 魅族支持过YunOS，但坚持使用魅族UI，在一个商业化的操作系统中，核心层并不核心
- YunOS应用在电视盒子上后私自删除用户软件，强迫使用YunOS自带软件
- 改名为AliOS后专注于汽车，与android区别越来越大，linux内核+运行库
# 华为HarmonyOS鸿蒙系统
- 特点：分布架构、高性能、内核安全、生态共享、开源
# discuss-tv
- 参数配置
  - 尺寸
  - 屏幕技术
  - 分辨率
  - 刷新率
  - 亮度
  - 色域
  - CPU
  - RAM
  - 边框

- ## 

- ## [互联网电视不能装网络浏览器…也是醉了 - 小红书](https://www.xiaohongshu.com/explore/6700c411000000001a0223db?xsec_token=ABoe1ug9YJJDbnOMEN4Q6C0fZ0dK1qg9_RVKDLAdWF5b8=&xsec_source=pc_search&source=web_explore_feed)
- 法规要求，电视上的内容必须管起来，浏览器上的内容管不了，不让装。

- ## [Why Chrome browser is not available on Android TV (Google TV)? : r/AndroidTV _202304](https://www.reddit.com/r/AndroidTV/comments/12yc3mi/why_chrome_browser_is_not_available_on_android_tv/)
- I don't know the reason but TV Bro does a great job instead.

- Google doesn't think you need to browse the web in your TV. That's just for consuming media.

- Mozilla used to have a Firefox for Fire TV (and then generic ATV) but they killed that years ago as well.
  - firefox continues to work on ATV but it requires a mouse to operate correctly.

- ## [小米电视不用挂墙，就装个底座为啥还要安装费 - 小红书](https://www.xiaohongshu.com/explore/686c7ba7000000000b02db9e?xsec_token=ABL7n_fleKY2d2yJ0Kl5bHj7o_Vw4sUjrEkqzyYIDNIto=&xsec_source=pc_search&source=web_explore_feed)
- 就拧四颗螺丝这么简单，何须花50块，自己装不就行了吗？

- 其实安装很简单，就是电视太大了要两个人装

- 首先再简单也要时间成本。其次只要过手了就会有风险。

- ## [小米电视没法装第三方软件？看这里！ - 小红书](https://www.xiaohongshu.com/explore/6885f1cc000000001c037e3d?xsec_token=ABFqR2NYsx0tMZ2_1jW9XRC31wt0kSyS-GYe02iCV-ca8=&xsec_source=pc_search&source=web_explore_feed)
- 小米电视安装第三方软件第一步，电视恢复出厂设置。这一步是为了清除电视内的app黑名单。
  - 恢复出厂设置后电视重启，进入新电视设置阶段，一定不要联网，跳过WiFi无线网络选择。保持断网的状态就不会更新黑名单。
  - 打开电视的设置-账号与安全-选择允许安装未知来源的应用。
  - 安装需要的APP：将下载好的app的apk文件拷贝到U盘，把U盘插入电视的USB接口，打开U盘。找到APK文件，点击确定进行安装即可。弹窗提示可能存在未知风险，选择继续安装。软件安装成功。

- ## [红米电视解决底座螺丝难拧的小技巧 - 小红书](https://www.xiaohongshu.com/explore/6794beb00000000029032fe0?xsec_token=ABvhnSCS5ukgPATsbc3yLg8-KBvtXyfeWGHlUGivnnbQQ=&xsec_source=pc_search&source=web_explore_feed)
  - 刚买了个红米电视, 家里人说底座装不上, 查了网上很多人说螺丝难拧
  - 其实只要装支架前螺丝先拧进孔里再拧出来，之后安装就很简单了
- 就是先不带架子单独拧一遍螺丝，然后螺丝拿出来之后再重新安架子不？
  - 对，拧的时候小心点，用巧劲别死摁
- 先别装支架，把孔拧松

- ## [红米电视apro系列和a系列有什么区别？Redmi A55 Pro和a55 2025款怎么选？ - 知乎](https://zhuanlan.zhihu.com/p/721352900)
- 处理器
  - 红米a55pro：四核A55、3GB+64GB
  - 红米a55 2025款：四核A35、2GB+32GB

- 红米a55pro采用NFC遥控器，支持一触投屏；红米a55 2025款采用的是红外遥控器。

- 红米a55pro支持2.4G&5G双频wifi，红米a55 2025款支持2.4G，红米a55pro网速更稳定。

- [小米电视redmi A55和小米电视redmi X50哪个好一点？ - 知乎](https://www.zhihu.com/question/466093554)
  - 参数对比图

- ## [为什么小米电视突然就没人关注了？ - 知乎](https://www.zhihu.com/question/662395651)
- 电视机下一代打法应该是卖65 75寸的显示器，不带任何内容功能，仅提供外接接口，此时合规审查的任务就会落到电视盒子这个东西上面。

- 不是小米电视没人关注了，是整个电视行业都黄了而已，各种会员收费基本把电视这个行业搞残废了。
- 客厅的电视，俩月没打开了，浪费空间

- 小米电视舆论最火的时候, 是华为和荣耀要进军“智慧屏”前后, 现在这摊业务没下文了，小米电视也就没啥声量了。
- 因为智慧屏销量只有小米电视销量的零头，花粉和水军们放弃这个阵地了，目前集火的是小米汽车

- 别家今年的高端：4000nit亮度，4000分区背光，量子点广色域，黑耀屏。 
  - 小米今年的高端：看什么看，还没出呢。
- 在等供应链成熟贴牌呢

- 小米今年有些摆烂，半年过去了才出了一个S Mini LED系列。别家都在搞机海了

- 曾经任职于倒闭互联网电视机品牌微鲸。拆过各个竞品互联网品牌，小米怎么说呢。用料属于还行，但是并非最好。
  - 以16年的产品用料来看。当年最好的其实是乐视和微鲸，小米A系列就差了不少，连外框后盖都不舍得配了，直接裸奔上市pptv, 暴风，风行就更糟糕了。
  - 17年，互联网电视品牌出现了问题，钱烧光，传统品牌等着这笔热钱烧完，开始回来收割市场了。这里面雷鸟依靠自己华星光的面板资源和TCL的规模优势，以及软件运营方面直接用TCL系统把研发成本摊平，物流售后方面完全可以共用TCL母品牌的资源。远比小米的成本优势明显。对小米来说，电视业务是补足自己全屋智能的拼板，可以不赚钱，但绝对不会为了这个业务投钱。因此性价比优势也就没有了

- 小米电视主打的还是低价和米家生态，2021年下半年开始，小米电视被雷鸟电视针对，竞争比较激烈
# discuss-screen
- ## 

- ## 

- ## [为什么现在的智能手机不使用半反半透技术的显示屏？ - 知乎](https://www.zhihu.com/question/264049308)
- 半反半透的好处就是保持彩色屏幕和阳光下可读性的同时还能做到省电。不过这种屏幕色彩还原性不是很好，这种屏幕看个简单的图形或者数字文字还行，要是看图片视频了就不舒服了，色彩还原不好。

- 我想是大量生产问题，始终是少众，人们都习惯了背光，背透在彩色显示上差强人意，所以有些時在沒背光下會使用AMBRITE（虎珀）/黑白模式，在晚间或稍暗时是不方便和尴尬的，省了点电但换来不便，现在多使用在少电的穿戴产品。

- [半透反射式TFT液晶屏为何没有流行？ - 知乎](https://www.zhihu.com/question/20807503/answers/updated)
  - 这种屏幕现在除了一些专业的户外器材（例如运动手表啥的）上有用，已经离大众越来越远了，也算是时代的眼泪了。
# discuss-ai/ml-hardware
- ## 

- ## 

- ## 

- ## 

- ## [4x64 DDR5 - 256GB consumer grade build for LLMs? : r/LocalLLaMA _202504](https://www.reddit.com/r/LocalLLaMA/comments/1k6p20z/4x64_ddr5_256gb_consumer_grade_build_for_llms/)
  - I have recently discovered that there are 64GB single sticks of DDR5 available - unregistered, unbuffered, no ECC, so the should in theory be compatible with our consumer grade gaming PCs.
  - Both AMD 7950x specs and most motherboards (with 4 DDR slots) only list 128GB as their max supported memory - I know for a fact that its possible to go above this, as there are some Ryzen 7950X dedicated servers with 192GB (4x48GB) available.

- I have 64GB of DDR5-6000 and it is great at inference - of models that don't take more than around 16GB (preferably 10GB) - anything bigger becomes too slow to use.

- If you're going for a CPU based build, you want to go for epyc, not a consumer CPU.
  - It won't be super fast; expect memory speed of around 200GB/s, so about 1/5th the performance of a 3090 or 4090 in token generation, and maybe 1/10th in processing speed.

- Yeah, I've got 128Gb of DDR4 3200, now I am running 110Gb models with 0.3t/s

- On desktop Zen 4/Zen 5, I wouldn't recommend doing that.
  - You're quite limited by the Infinity fabric bandwidth, limiting you to a max of 62-68GB/s on DDR5-6000 to 6400, while theoritical DDR5 6000 128-bit is 100GB/s.

- I'm running a Ryzen 9 7900X on MSI PRO B650M-A WIFI AM5 Micro-ATX with 256GB using 4 of those 64GB DDR5 sticks. So it is possible. Your memory bandwidth drops, as you need to slow the memory down to stay stable. If you are building from scratch you may want to use a CPU with more memory channels.

- ## [Did someone ever benchmark how cpu inference performs with quad and eight channel memory ? : r/LocalLLaMA _202401](https://www.reddit.com/r/LocalLLaMA/comments/1920l93/did_someone_ever_benchmark_how_cpu_inference/)
  - Since people always say that bandwidth is the problem. And a full 8 channel memory board with ddr4 3200 would be about 200gb/s per second i was wondering if anyone ever benchmarks that stuff and how it scales with cores ?

- I have a 8ch epyc build, after some experiments i have found that the effective bandwidth is about 135 gb/s, so a 40gb model is ~ 3, 3 t/s, a 20gb is twice the speed.

- Intel Xeon E5-2680 v4, 128GB DDR4 2400 RAM 4 chanels. llama.cpp, model: mixtral8x7b Q5_K_M 6 tokens/sec

- R720 2x xeon 2670, 192gb ddr3-1333 dram, llama.cpp running mixtral q3_k_m quant w/ 10k context, pure cpu inference: 3.6 t/s. If use installed P40: 9.1 t/s

- ## [Thread for CPU-only LLM performance comparison : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nj4axf/thread_for_cpuonly_llm_performance_comparison/)
  - I could not find any recent posts about CPU only performance comparison of different CPUs. 
  - With recent advancements in CPUs, we are seeing incredible memory bandwidth speeds with DDR5 6400 12 channel EPYC 9005 (614.4 GB/s theoretical bw). 
  - AMD also announced that Zen 6 CPUs will have 1.6TB/s memory bw. The future of CPUs looks exciting. 
  - For this CPU only comparison, I want to use ik_llama 
  - use CPU only inference (No APUs, NPUs, or build-in GPUs allowed)
- llama-bench benchmark (make sure GPUs are disabled with CUDA_VISIBLE_DEVICES="" just in case if you compiled for GPUs):
  - qwen3moe 30B Q4_1: 38.9
  - GPT-OSS 120B Q8_0: 24.7

- those dusty EPYCs/Xeons with fat memory channels you see on eBay suddenly look like budget LLM toys..it; s crazy that decommissioned gear can outpace shiny new desktop CPUs for this niche.

- That's my server, I think there are some config issue here as using thread 64 would be much slower, maybe I should enable HT.
  - CPU: 1S Epyc 7B13(64c, HT disabled manually)
  - RAM: 8 x 64GB DDR4 2666
  - Motherboard: Tyan S8030GM2NE
  -  qwen3moe 30B Q4_K: 31.0 
  -  gpt-oss 120B F16: 14.9
-  Yes, there is definitely something wrong with the server in your case. You should get better results than my server.

- ## [Looks like Intel Arrow Lake can support 4 DIMMs @ up to 6400 speeds : r/LocalLLaMA _202411](https://www.reddit.com/r/LocalLLaMA/comments/1gindy1/looks_like_intel_arrow_lake_can_support_4_dimms/)
  - After searching through a few boards, it looks like Arrow Lake can do 4 Dimms @ 6400. For an ASrock example, see below - select vendor "Corsair", and there is a 24GB per DIMM options @ 6400. Crucial and ADATA have 48GB "4 channel" options @ 5600.
  - Anyway just wanted to pass along that we may see "certified" 6400+ speed 4 DIMM setups become common with Arrow Lake (Core Ultra 200 series). An x86 way to have 192GB-256GB (when DIMMs are available) on a standard desktop at reasonable speed.

- But they are only dual channel, so you can expect at most around 100 GB/s of memory bandwidth.
  - Missed opportunity...

- I’m running 6200 stable on 128gb 4 dimms with Ryzen 7950x3D since 1+ year lol

- 4 dimms does not mean 4 channels. MBs have had 4 slots forever. Arrow Lake is dual channel.
- Arrow lake is dual channel. 4 dimms does not mean 4 channels. MBs have had 4 slots forever.

- it actually has 4 channels. It has the ability to address each half of the ram seperately. And it's a 4 channel controller, for 4x 32Bit. It's just that that's the same bandwidth as 2x 64 bit.

- [Is 96GB of DDR5 6800Hz RAM enough for training? : r/LocalLLaMA _202402](https://www.reddit.com/r/LocalLLaMA/comments/1b1e4z8/is_96gb_of_ddr5_6800hz_ram_enough_for_training/)
  - I went with 96GB of fast RAM thinking that it would be more than enough, but I've been seeing that people recommend at least 128GB for training and interfacing.
- What is your ram capacity and ram speed, I'm running 4x32gb at 6200mhz which speeds up inference. You can run AIDA to benchmark your ddr memory speed bandwidth
  - I want to get 128 gb of ram 6000MT/s. Cl30. But most people say that there is no way the ryzen 9 7950x3d could reach that at all without it melting or being unstable. Even 5200 would be a miracle. Apparently for am5, 4 DIMMS should not be used. You shouldn’t quadrank. The most you can go for is 96 gb ram 6400MT/s (2x48). So how did you do it? Please teach me

- ## [CPU RAM only speeds on 65B? : r/LocalLLaMA _202307](https://www.reddit.com/r/LocalLLaMA/comments/14q4d0a/cpu_ram_only_speeds_on_65b/)
- I have both Dual RTX 3090s+NVLink and 128GB RAM (@3200) and for 65B models, using the CPU (i have a 3rd gen 8 core Ryzen) is just too slow. It's around 1 token per second, far from 7/s. From what i've seen getting a better CPU (16 cores) doesn't help much.

- The 7950x with DDR5 6000 on a 65b_4_ks model is 1.75t/s.
  - When it comes to token generation speed, the core count doesn't really matter. What does matter is the RAM bandwidth.
  - However, if you don't have a GPU, then the core count becomes important for prompt evaluation speed. But it's not worth buying a high-end CPU just for this.
  - In my opinion, if you can tolerate a speed of 2t/s on 65b models, the most cost-effective option would be to go for the 13400f($170)processor (disable the E-cores), paired with 64GB of high-frequency DDR5 RAM, and a second hand RTX 2060 for just $100. This GPU can be used for prompt processing and offloading as many layers as possible.

- I have CPU Ryzen 9 3950X and 64Gb RAM at 3600MHZ. The airoboros-65b-gpt4-1.4.ggmlv3.q5_K_M.bin generates at the average of 1077ms per token, with very small variance actually.

- ## [Is RAM latency very relevant for LLMs (Ollama)? : r/LocalLLaMA _202411](https://www.reddit.com/r/LocalLLaMA/comments/1gws9yp/is_ram_latency_very_relevant_for_llms_ollama/)
- No overclocking will not produce notable gains. When you offload to CPU your major bottleneck is the processing not memory bandwidth. You'll do a lot of work and adding potential instability for something that you won't really notice, maybe an extra token per hundred generated.
  - You are confidently wrong. I went from 3.45 tokens/s to 5.29 tokens/s just by enabling XMP profile (2666 MHz to 3600 MHz).

- ## [Will DDR6 be the answer to LLM? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o0i4fz/will_ddr6_be_the_answer_to_llm/)
  - Bandwidth doubles every generation of system memory. And we need that for LLMs.

- I think the combination of smart quantization, smarter small models and rapidly improving RAM will make local LLM's inevitable in 5 years.

- Prompt processing will be even more critical with faster RAM - you need lots of compute for larger models, DDR6 will be used for, and CPUs do not have enough compute. You still absolutely would need GPU.

- Isn't Apple unified memory just multi channel RAM? It does deepseek fairly well.
  - Unified memory without upgradable ram is such a double-edge sword. I want it but I don't want it to be "The future"

- ## [How fast big LLMs can work on consumer CPU and RAM instead of GPU? : r/LocalLLaMA _202407](https://www.reddit.com/r/LocalLLaMA/comments/1edryd2/how_fast_big_llms_can_work_on_consumer_cpu_and/)
  - Would not it be cheaper to build a PC with 256-512 GB of RAM and run very big models on it than buying two Rtx 3090 and having only 48gb of VRAM?

- I'll get some example numbers with Llama 3.1 8B Instruct Q6_K with a context size of 8192 tokens.
  - Running on my RTX 4060 Ti: 25.46 tokens/s
  - Running on my Ryzen 5 7600: 6.66 tokens/s
  - As you can see, CPUs are the devil.
  - The RTX 4060 Ti's memory bandwidth is 288 GB/s, and my RAM is 81.25 GB/s (dual-channel DDR5 5200), and dividing those numbers comes out to close to the same ratio--while the GPU memory is 3.54x as fast, using the GPU for inference is 3.82x as fast.
  - Using shared memory with the GPU is far worse because PCI-e 4.0 x8 is only about 16 GB/s one way (PCI-e 5.0 is only twice that fast).

- I built a pc with 128G RAM , I9 14900K and one 4090 GPU. I have loaded Llama3.1 70B Q2 with Ollama, and test it, the token per second is about 9. Then I loaded Mistral Large 2 123B Q2, the token per second is about 2

- 14900K has only two memory channels which gets bandwidth bottleneck at 6 threads alone! So you get almost same CPU performance as me with 12700H in a laptop. I wish you researched more before building such a system there was no point of buying 14900K at all neither for LLMs or gaming expect you use it for something else ofc..

- For running LLMs on CPU you must buy something with at least 8 memory channels. Dual channel CPUs get bandwidth bottleneck at 6 threads alone and begin loosing performance severely as you increase thread count. Not cores rather only threads! So with 8 memory channels you can use 24 threads and gain around 4 times more performance. (Exact performance depends on clock, memory speed etc ofc but it is roughly like this.)

- ## [Running LLMs partially on cpu. DDR5 R1(single rank) vs R2(dual rank) : r/LocalLLaMA _202403](https://www.reddit.com/r/LocalLLaMA/comments/1b3vhc7/running_llms_partially_on_cpu_ddr5_r1single_rank/)
  - 2x32gb, dual rank(8x32bit/256bit), 6800mhz, 13600 MT/s, total bandwidth 217.6 GBps
  - 2x24gb, single rank(4x32bit/128bit), 7800mhz, 15600 MT/s , total bandwidth 124.8 GBps

- I would go for the first one with much more bandwith. As far as I know this usually is the bottleneck. But I haven't benchmarked anything like it myself.
  - 217 GBps sounds almost too good to be true. Only 3.5x slower than 4080's bandwidth.

- ## [Anyone actully try to run gpt-oss-120b (or 20b) on a Ryzen AI Max+ 395? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nabcek/anyone_actully_try_to_run_gptoss120b_or_20b_on_a/)
- people post actual good benchmarks, 49T/s on TG and 700T/s on PP. 
  - Better than my 14900k (96GB 6800) + RTX3090: (32T/s on TG and 220-280T/s on PP).

- Ryzen 7950X + 3080 + 96GB DDR5-6000 gets me around 330T/s on PP and 15 on TG for the 120b

- ## [Most economical way to run GPT-OSS-120B? : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1n13rsq/most_economical_way_to_run_gptoss120b/)
- I get 100t/s on short (2-3k) context and ~85t/s on longer context (12-14k) using three 3090s.
  - Two Mi50s and a Cascade Lake ES Xeon get me ~25t/s on the same 12-14k context. 

- Framework PC - 128GB LPDDR5x-8000 ! I have 5090 + DDR4-2933 = 18 t/s for GPT-OSS-120B

- Also 5090 + DDR5-6000 (offloading 22 layers) = ~35 t/s for GPT-OSS-120B (full precision ≈ 60GB)

- Given that you already have a PC, I think, the most economical way to run gpt-oss 120b at good speeds is to buy 3 used rtx 3090. It will give you 72gb of vram and it will cost you around 1800 + PSU. It will be roughly the x4 speed of the Framework. 

- I have a Framework Desktop with 64GB. It almost explodes with GPT-OSS-120b loaded but I can actually run the 4bit version for a question or two until it fails and it runs at 50 tokens/s, even with the system having been exiled to SWAP. (openSUSE Tumbleweed, llama.cpp)

- ## [gpt-oss-120b on CPU and 5200Mt/s dual channel memory : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mj6xif/gptoss120b_on_cpu_and_5200mts_dual_channel_memory/)
  - I have run gpt-oss-120b on CPU, I am using 96GB dual channel DDR5 5200Mt/s memory, Ryzen 9 7945HX CPU. I am getting 8-11 tok/s. I am using CPU llama cpp Linux runtime.

- 5800x with 96gb of system ram DDR4 3200 in dual channel. Getting just over 5t/s with the 120, nothing offloaded to GPU

- ## [How to run gpt-oss-120b faster? 4090 and 64GB of RAM. : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mnsg6d/how_to_run_gptoss120b_faster_4090_and_64gb_of_ram/)
  - llama-server --hf-repo unsloth/gpt-oss-120b-GGUF --hf-file gpt-oss-120b-F16.gguf ^ -c 16384 -ngl 99 -ot ".ffn_.*_exps.=CPU" -fa ^
  - with 16k context here, I am getting around 14tps 

- I'm getting 35T/s and 120T/s prefill on a 3090 and 14900K but that is with 96GB of fast DDR5 (6800)

- ## [gpt-oss-120b performance with only 16 GB VRAM- surprisingly decent : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/)
  - GPU: RTX 4070 TI Super (16 GB VRAM)
  - CPU: i7 14700K
  - System RAM: 96 GB DDR5 @ 6200 MT/s (total usage, including all Windows processes, is 61 GB, so only having 64GB RAM is probably sufficient)
  - Model runner: LM Studio
  - 13 t/s is a speed that I'd consider "usable"

- Just posting my numbers too! 5090 + 60GB of DDR5, 22 cpu moe layers offloaded:
  - 36.61 tokens per second

- ## [10.48 tok/sec - GPT-OSS-120B on RTX 5090 32 VRAM + 96 RAM in LM Studio (default settings + FlashAttention + Guardrails: OFF) : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mk9c1u/1048_toksec_gptoss120b_on_rtx_5090_32_vram_96_ram/)
  - Just tested GPT-OSS-120B (MXFP4) locally using LM Studio v0.3.22 (Beta build 2) on my machine with an RTX 5090 (32 GB VRAM) + Ryzen 9 9950X3D + 96 GB RAM.
  - Everything is mostly default. I only enabled Flash Attention manually and adjusted GPU offload to 30/36 layers + Guardrails OFF + Limit Model Offload to dedicated GPU Memory OFF.

- Try llama.cpp with-ot ".ffn_(up|down)_exps.=CPU" This offloads up and down projection MoE layers instead of full MoE layers. You should get 30 t/s
  - I have a budget workstation that costs less than your GPU alone and I get 20 t/s with Unsloth their 120b. That is 20 t/s for the first 1K tokens, it slows down to 13 t/s at 30K context.
  - My specs: 16 GB RTX 5060 Ti + 16 GB P5000 + 64 GB DDR5 6000

- I can second that: With the same (short) prompt I get 17.9 t/s
  - Specs: 16 GB RTX 5060 Ti + 128 GB DDR5 5600 / Ryzen 9 9900X
  - .\llama-server.exe -c 60000 --chat-template-kwargs "{\"reasoning_effort\": \"low\"}" -fa -ctk f16 -ctv f16 -m "c:/....../gpt-oss-120b-GGUF/gpt-oss-120b-BF16.gguf" -ub 512 --temp 1.0 --top-p 1.0 --top-k 0 --min-p 0 --repeat-penalty 1.0 --no-mmap -sm none -ngl 99 --n-cpu-moe 44

- Thats really bad. I get 30T/s for 3090 + 14900K 96GB. 25T/s for the 14900K with just 8GB VRAM usage.
  - This is the trick: 
  - --n-cpu-moe 36 \    #this model has 36 MOE blocks. So cpu-moe 36 means all moe are running on the CPU. You can adjust this to move some MOE to the GPU, but it doesn't even make things that much faster.
  - --n-gpu-layers 999 \   #everything else on the GPU, about 8GB

- I get 9 t/s on integrated gpu in my thinkpad, you are doing something wrong

- I'll trade my 4070 Ti Super that gets 50 tokens/second for your ridiculously slow 5090.

- GPU: NVIDIA RTX 4000 SFF Ada Generation GPU: AMD ATI 04:00.0 Raphael Memory: 54.6GiB / 94.2GiB
  - eval rate: 8.03 tokens/s 

- ## [16→31 Tok/Sec on GPT OSS 120B : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1ndit0a/1631_toksec_on_gpt_oss_120b/)
  - CPU: Intel 13600k
  - GPU: NVIDIA RTX 5090
  - Old RAM: DDR4-3600MHz - 64gb
  - New RAM: DDR5-6000MHz - 96gb
  - Model: unsloth gpt-oss-120b-F16.gguf
  - 16 tok/sec with LM Studio → ~24 tok/sec by switching to llama.cpp → ~31 tok/sec upgrading RAM to DDR5
  - `llama-server --n-gpu-layers 999 --n-cpu-moe 22 --flash-attn on --ctx-size 48768 --jinja --reasoning-format auto -m C:\Users\Path\To\models\unsloth\gpt-oss-120b-F16\gpt-oss-120b-F16.gguf  --host 0.0.0.0 --port 6969 --api-key "redacted" --temp 1.0 --top-p 1.0 --min-p 0.005 --top-k 100  --threads 8 -ub 2048 -b 2048`

- You can get more speed on computers with hybrid cores (a mix of p and e cores) by pinning llama.cpp to p-cores only. 

- Maybe even some more speed to win by offloading only up and down projection MoE layers: https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune#improving-generation-speed
  - In my testing, the suggestion in that link is outdated.

- You should spend money on unified memory systems for models like this instead of on a strong GPU like 5090. For example, M4 Max has GPU equivalent to 4070 mobile, which is not super fast, but it can run this model at 75 t/s on llama.cpp and 95 t/s on mlx (though mlx implementation currently has slow PP speed).

- You want to only use 2 lanes if possible so 48x2. If you use all 4 ram slots your speed will be limited.

- For more "long running" sessions, servers, permanent agent running etc, llama.cpp, vLLM and especially ktransformers are FAAAAR better options.

- I am getting 10-11 tokens per second with GPT-OSS-120b on DDR5 4800, 7940HS CPU, 96GB RAM in LM Studio. Guessing I could get at least another 50% performance based on what you're saying
  - GLM 4.5 Air is running at 4-5 TPS with this setup. Qwen3 30b-A3B runs at about the same speed as GPT-OSS-120b.

- ## [gpt-oss-120b in 7840HS with 96GB DDR5 : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nf3fof/gptoss120b_in_7840hs_with_96gb_ddr5/)
  - With this setting in LM Studio Windows, I am able to get high context length and 7 t/s speed (noy good, but still acceptable for slow reading).
  - Is there a better configuration to make it run faster with iGPU (vulkan) & CPU only? I tried to decrease/increase GPU offload but got similar speed.
  - I read that using llama.cpp will guarantee a better result. Is it significantly faster?

- Don't force the experts onto CPU, just load them all in gpu, that's why you have the iGPU in the first place! You should be able to load ALL the layers on GPU as well.

- Loading all layer to iGPU will result unable to load vulkan0 buffer, I think because only 48GB can be allocated to my iGPU
  - No, . Put them all there, it will work. If dont, put 23 or so, do a tryout load. VRAM is also your shared ram, all equal. I got ryzen 7940hs, runing unsloth Q4-K-XL, with 20K context, its about 63Gb of space, i just put all on the GPU on LMstudio, ans just one processor on inference. I get 11 tokens per second, linux mint.

- Thoughts from someone who has the same iGPU and used to have 96GB memory:
  - Your offload config looks about right for your memory size (I wrote a comment about it on a lower message thread)

- ## [Managed to get GPT-OSS 120B running locally on my mini PC! : r/selfhosted _202508](https://www.reddit.com/r/selfhosted/comments/1mk6jlt/managed_to_get_gptoss_120b_running_locally_on_my/)
  - I was able to get the GPT-OSS 120B model running locally on my mini PC with an Intel U5 125H CPU and 96GB of RAM to run this massive model without a dedicated GPU, and it was a surprisingly straightforward process. The performance is really impressive for a CPU-only setup. 
  - MINIPC: Minisforum UH125 Pro
  - CPU: Intel u5 125H
  - RAM: 96GB
  - Model: GPT-OSS 120B (Ollama)
  - prompt eval rate: 31.83 tokens/s
  - eval rate: 2.77 tokens/s
  - This is running on a mini pc with a total cost of $460 ($300 uh125p + $160 96gb ddr5)

- ## [You can now run OpenAI's gpt-oss model on your local device! (14GB RAM) : r/selfhosted _202508](https://www.reddit.com/r/selfhosted/comments/1mjbwgn/you_can_now_run_openais_gptoss_model_on_your/)
  - The 20B model runs at >10 tokens/s in full precision, with 14GB RAM/unified memory. Smaller versions use 12GB RAM.
  - The 120B model runs in full precision at >40 token/s with ~64GB RAM/unified mem.
  - There is no minimum requirement to run the models as they run even if you only have a 6GB CPU, but it will be slower inference.
  - Thus, no is GPU required, especially for the 20B model, but having one significantly boosts inference speeds (~80 tokens/s). With something like an H100 you can get 140 tokens/s throughput which is way faster than the ChatGPT app

- ## [What hardware to run gpt-oss-120b? : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1miggb2/what_hardware_to_run_gptoss120b/)
- 64GB RAM with ~16GB - 24GB VRAM offloading, or just 128GB RAM.

- Is it really possible to run it with just 128 GB RAM and no VRAM?
  - I'm running it with 96GB RAM and 12GB VRAM and it's usable
  - my setup: 2x RTX 4070, 96GB DDR5 6400MHz RAM, Ryzen 9 7900X
  - It's not a memory issue with WSL (I know that an I've allocated 80 GB of memory to WSL), it just doesn't run very fast. I hit around 10-15 tokens/sec with CPU on Windows using LM Studio, but running it in the same LM Studio (or Ollama) on Linux does not go well. I'm getting around 0.8 tokens/sec, not sure why.
  - I'm running the 20B version at around 200-250 tokens/sec though, which is great
  - I can run it straight from CPU on 96GB and it seems to run about the same. I'm not sure. to stop LM Studio from using my GPUs, I ran it on a WSL instance without GPU access, it got the same-issue Tok/s, maybe slightly lower by the token per second, but really not that big of a difference when it's already that slow

- I was able to run gpt-oss-120b in LM-studio on a 5060ti 16Gb video card + 64Gb DDR4 RAM. I placed 8 layers in video memory, the rest in RAM. The performance was 10 tokens per second, for comparison, the younger model worked at a speed of 85 tokens per second.

- ## 💡🚧 [gpt-oss 120B is running at 20t/s with $500 AMD M780 iGPU mini PC and 96GB DDR5 RAM : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1nxztlx/gptoss_120b_is_running_at_20ts_with_500_amd_m780/)
  - Everyone here is talking about how great AMD Ryzen AI MAX+ 395 128GB is. But mini PCs with those specs cost almost $2k. 
  - I searched for mini PCs that supported removable DDR5 sticks and had PCIE4.0 slots for future external GPU upgrades. 
  - I focused on AMD CPU/iGPU based setups since Intel specs were not as performant as AMD ones. The iGPU that came before AI MAX 395 (8060S iGPU) was AMD Radeon 890M (still RDNA3.5). Mini PCs with 890M iGPU were still expensive.
  - The cheapest I could find was Minisforum EliteMini AI370 (32GB RAM with 1TB SSD) for $600. Otherwise, these AI 370 based mini PCs are still going for around $1000.
  - Next, I looked at previous generation of AMD iGPUs which are based on RDNA3. I found out AMD Radeon 780M iGPU based mini PC start from $300 for barebone setup (no RAM and no SSD). 780M iGPU based mini PCs are 2x times cheaper and is only 20% behind 890M performance metrics. 
  - I checked many online forums if there was ROCm support for 780M. Even though there is no official support for 780M, I found out there were multiple repositories that added ROCm support for 780M (gfx1103)
  - 🖥️ I bought MINISFORUM UM870 Slim Mini PC barebone for $300 and 2x48GB Crucial DDR5 5600Mhz for $200. I already had 2TB SSD, so I paid $500 in total for this setup.
  - There was no guidelines on how to install ROCm or allocate most of the RAM for iGPU for 780M. So, I did the research and this is how I did it.
  - I know ROCm support is not great but vulkan is better at text generation for most models (even though it is 2x slower for prompt processing than ROCm).
  - Mini PCs with 780M are great value and enables us to run large MoE models at acceptable speeds. Overall, this mini PC is more than enough for my daily LLM usage (mostly asking math/CS related questions, coding and brainstorming).
  - I was getting great numbers that aligned with dual DDR5 5600Mhz speeds (~80GB/s).
  - I know ROCm support is not great but vulkan is better at text generation for most models (even though it is 2x slower for prompt processing than ROCm).

- ROCM with gpt-oss 120B mxfp4
  - tg128: 18.7 
- VULKAN (RADV only) all with Flash attention enabled
  - qwen3moe 30B. A3B Q4_1:  32.6, 22.3
  - gpt-oss 20B MXFP4 MoE: 28.1, 24.8
  - gpt-oss 120B MXFP4 MoE:  20.4, 18.1
  - qwen3moe 235B. A22B Q3_K:4.3
  - glm4moe 106B. A12B Q4_1:9.1

- just in case someone wants to compare with strix halo:
- STRIX-HALO @ Debian 13 6.16.3+deb13-amd64 (kernel >= 6.16.x for optimal memory sharing)
- ROCm
  - gpt-oss 120B MXFP4 MoE: 47.8
- Vulkan
  - gpt-oss 120B MXFP4 MoE: 51.5

- For me also same but the problem is when context become big speed decrease
  - I get 18t/s at 8k context 

- DDR5 is almost 2x faster than my DDR4 tower PC with AMD Ryzen 5950x CPU. DDR6 should come soon (2026 or 2027?). Also, It is high time that consumer PC industry embraced quad channel memory setup (e.g. DDR5 with 4 channels in mini PC would be amazing).

- Pretty incredible is 96gb the max or can it go 128?
  - it can potentially go up to 256GB but I could not find SO-DIMM DDR5 with that size. But yes, 2x64GB = 128GB is possible but those sticks are expensive! From $200 for 96GB to $400 for 128GB. So, 96GB is cost effective.

- with 90GB RAM allocated to iGPU, gpt-oss-120b-GGUF should comfortably fit 64k context. Also, running with that context will be slow for the initial cache loading (it may take hours).
  - Update: just laoded gpt-oss 120b with 130k context. With flash attention, that context took extra 5GB only. So, I would say it is possible to load the full context.

- 2x64GB dual channel near or above 6000 mt/s are not seen yet. 2x48GB dual channle can go up to 6800mst/s and some may overclock(超频) it to even higher speed depending your luck, may not be stable.
  - The key is to use 2 slots only. 4 slots will drop the speed significantly even from the exact same brand model spec.

- Is this a one-off for only running gpt-oss 120B or is this platform expected to be somewhat future proof and newer models a likely to work on it?
  - Yes. This is future proof as long as llama.cpp and vulkan exist. Yes, this will run Qwen3 235B. Q3 should run at 6t/s.

- did you also compare the performance against running it on the CPU only, without iGPU? If I remember correctly, using the iGPU mostly improves pp performance while tg is still limited by the (shared) memory bandwidth speed? Is that (still) true?
  - Also, since you seem into getting the most out of (relatively) limited hardware, I think it could be an interesting experiment to run a bigger MoE using mmap and a PCIe Gen 4 NVMe SSD (max. ~8 GB/s). I think this might be surprisingly usable for use cases without limited context, etc.
- Yes, I tested with ik-llama for CPU. The best I got for gpt-oss 120b with CPU was 13t/s. So, iGPU improves TG by ~65-70%. I also tried glm 4.5 air in vulkan. I got 9t/s TG. I haven't tried SSD offloading. But yes, I could try qwen3 235B Q4 for that.

- Excellent results! My M4 Max 128GB was more like $6k and is only about 2.5X faster (55tok/s) with flash attention. Without flash attention, it’s down around <10tok/sec.
  - What a cool budget option you found! gpt-oss-120b is a great tool-using, private, safe LLM.

- I squeeze 11 tokens/ s with mini pc ryzen 7940hs, 780M and 64 GB 5600 mhz ddr5. Vulkan cpp. I fit 21 Layers. The rest goes to cpu. Inference 6 cpu cores. Context 18000. Maybe 20000. Linux mint mate latest version. Do not use last vulcan cpp 1.51. Use 1.50.2
  - I get 13 t/s with CPU only in ik-llama cpp

- Can you give AMDVLK a try in addition to RADV for your Vulkan perf? On my (completely different but still AMD so it may transfer to yours) hardware AMDVLK basically matches ROCm in PP while still being slightly faster than ROCm at TG (not as fast as RADV though).
  - you're right, RADV starts off slower, but then doesn't slow down as much when context increases.

- Can someone do the same with a Ryzen AI HX 370? They are on Alibaba for around 400$ now (Mini PCs incl an Oculink port) and can be equipped with 128GB DDR5.

- Whats the max context it can run?
  - I have not tested it yet. But with 90GB RAM allocated to iGPU, gpt-oss-120b-GGUF should comfortably fit 64k context. Also, running with that context will be slow for the initial cache loading (it may take hours).
  - Update: just laoded gpt-oss 120b with 130k context. With flash attention, that context took extra 5GB only. So, I would say it is possible to load the full context.
- The problem with context is not at the ininitial run, it tends to deminish the infererence speed as the context filled up.

- I wonder what speedup you would get if you slapped a 3060 12gb EGPU onto it
  - much slower. I have a mini PC with an even older iGPU (680m), and I run almost all MoE models > 20B sometimes faster than folks with 12GB vRAM

- Would a similar approach work with say an Intel iGPU on a desktop motherboard using vulkan? I've got an older 12th Gen i7 but 4x64gb DDR4 (will be slow I know) and wondering how it would compare to just CPU-only.
  - It should be possible but ddr4 will be 2 times slower
- No, desktop iGPUs are very weak, usually. They just exist to provide video out. AMD has some large desktop iGPUs (the G series processors), but I don't think Intel does. Intel iGPUs are also generally not as good as AMD's, at least for llama.cpp.

- ## [More RAM or faster RAM? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1nzf0zf/more_ram_or_faster_ram/)
  - If I were to run LLMs off the CPU and had to choose between 48GB 7200MHz RAM (around S$250 to S$280) or 64GB 6400MHz (around S$380 to S$400), which one would give me the better bang for the buck? This will be with an Intel Core Ultra.
- Little to no difference in speed. You need to optimize for the number of memory channels you have to ensure the highest bandwidth possible.
  - This is why folks opt for older Xeon or Epyc machines, because even with slower ram, they have oodles(大量或很多) more ram bandwidth.

- I bought 2x64 sticks at 6400 on paper but only 5600 stable for my system. I can run GLM 4.6 at 5 t/s and q2, but it beats anything else I could run easily. Cost me 380 euros, totally worth it.

- More Ram.. I use all 32 GB Vram from RTX5090 and 50+ GB Ram just to run Wan2.2.
  - I would say at the minimum you should get 128 GB ram if you want to run LLM (so you can offload and run 70B model). 
  - Personally my spec is 5090 + 256 GB ram so I can offloading most mid size LLM.

- VRAM is up to 20 times faster than 6000MHz RAM, there lies your answer
  - Those models are MoE models and will run at usable speed. Even a 355b GLM 4.6 runs at 4 to 5 tokens per second on 128gb ram on my system. 
  - With upcoming implementations of MTP this might get uplifted into the 10 tokens per second range. MoE models are also getting sparser and sparser. 128 GB ram even if it's just dual channel is absolutely worth it in my opinion.

- Neither of those rams will make a significant difference in inference speeds for llms, both are quite slow bandwidth wise. The useful bit of those ram kits is the capacity, could you get a cheaper 64gb kit instead?

- You even sure your CPU and mobo can utilize these speeds?

- 
- 
- 
- 
- 
- 
- 

- ## 🖥️ 我想买个 已装好但可自己配置的台式机工作站 或 自己买机箱/cpu/2张3090显卡自己装台式机, 要求台式机的机箱要尽量小，同时能发挥多张显卡的计算能力，散热要正常
- 铭瑄 ARL-HX 迷你双卡工作站
  - 内部搭载了两张Intel Arc Pro B60显卡，合计提供48GB GDDR6显存

- [求推荐一款小型机箱？ - 知乎](https://www.zhihu.com/question/1945481402184364122)
  - 坚定小型化，就得把预算提到600以上。个人推荐方糖机械大师c28这个型号，建议直接上闲鱼搜二手的
  - 坚持要小机箱不仅是提高预算的问题，后面装机会有一堆麻烦

- ### [2025年5月更新，电脑机箱推荐。推荐一波高颜值的机箱。包含ITX, M-ATX, ATX, E-ATX机箱](https://www.zhihu.com/tardis/zm/art/210537601?source_id=1003)
- ITX：专门为小型电脑机箱设计的主板规格，尺寸 170mm×170mm。
  - mini-itx机箱的特点是比较小巧，携带相比大机箱要方便些，但是散热相对大机箱要差一点点，也有散热很好的ITX机箱。ITX机箱的另一个缺点就是配套的主板及电源要贵不少。
- DTX：尺寸为170mm×203mm。这个尺寸的主板很少见，ROG的部分高端主板是这个规格（如ROG C8I主板），这个规格也是为小钢炮设计的，部分ITX钢炮机箱支持这个尺寸，购买机箱的时候需要仔细看看
- ATX：ATX 可以理解为全尺寸主板，尺寸305mm×244mm。
  - ATX（中塔）机箱是目前大部分用户的选择，这类的机箱尺寸偏大，基本都支持ATX，M-ATX，ITX版型，部分支持包括E-ATX在内的所有版型。
  - 优点在于散热好，ATX机箱基本都支持240及360水冷，对大型双塔风冷散热的支持也更好。
  - 扩展位多，方便安装更多的硬盘等设备。
- M-ATX：就是缩小版的 ATX。长244mm，宽度有多种规格。主流为244mm×244mm。
  - M-ATX机箱的优点在于大小适中，配套的M-ATX主板价格合适，性价比高，扩展性也够用，是大部分用的选择。
  - M-ATX机箱的散热也不错，很多都支持240水冷及中等尺寸的风冷。
  - 但是目前优秀的M-ATX机箱比较少，尤其是高端的M-ATX机箱几乎是空白。
- E-ATX：加大型主板，尺寸305mm × 330 mm。主要用于服务器主板。例如搭配AMD 3990X的主板。
  - E-ATX（中塔，全塔）机箱推荐, 这类型的机箱比较大，通常都支持360水冷，大型风冷，散热基本都非常优秀。硬盘位也很多。
  - 这类型的机箱比较适合游戏发烧友，或服务器工作站。
  - 做服务器或工作站用的朋友建议考虑使用分形工艺的Meshify 2和D7，以及这两款机箱的XL型号（双显卡用）。我个人在使用这两款机箱，散热优秀，设计，用料，做工都非常不错。Meshify 2是突出散热，D7是突出静音。
  - 分形工艺 Torrent: 箱体长宽高（mm）544（长）*242（宽）*530（高）mm

- ### [25L Portable NV-linked Dual 3090 LLM Rig : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1l0zsv7/25l_portable_nvlinked_dual_3090_llm_rig/)
  - cpu: AMD Ryzen 7 5800X 3.8 GHz 8-Core Processor
  - Motherboard: Asus ROG Strix X570-E Gaming ATX AM4 Motherboard
  - NVlink SLI bridge
  - Mechanic Master C34Plus Portable Desktop ATX Case with Aluminum Handle: 5.2 Kilograms, 39.1 x 18.5 x 34.3 cm.
  - CORSAIR RMe Series Fully Modular Low-Noise Power Supplies: 1200 watts
  - ⚠️ WARNING - these components don't fit if you try to copy this build. The bottom GPU is resting on the Arctic p12 slim fans at the bottom of the case and pushing up on the GPU. Also the top arctic p14 Max fans don't have mounting points for half of their screw holes
  - All that being said, with a 300w power limit applied to both gpus in a silent fan profile, this rig has surprisingly good temperatures and noise levels considering how compact it is.
  - During Cinebench 24 with both gpus being 100% utilized, the CPU runs at 63 C and both gpus at 67 Celsius somehow with almost zero gap between them and the glass closed. All the while running at about 37 to 40 decibels from 1 meter away.
- I did a bunch of testing back when I had a 4x 3090 rig. The sweet spot was always between 250-300W for inference. Above that I saw no improvement in inference speed (this was a DDR4 system, YMMV with DDR5). Below 250 speed would start dropping off quite quickly.
  - If memory serves me, I settled on 275W and enjoyed the power savings while not sweating the .05 tokens/sec it cost me for not running over 300W
- Neat build. Even power limiting to 200W doesn't have that big of a hit on inference. (Exl2)
  - I power limit all mine to 200w. It's perfectly fine.

- ### [Smallest case possible for dual gpu? : r/sffpc _202312](https://www.reddit.com/r/sffpc/comments/187og11/smallest_case_possible_for_dual_gpu/)
- Someone else just posted details of their NR200 build with dual 4090s, might be worth a look?
  - 18.49 x 37.49 x 29.18 cm
  - 新款mini-itx是 NR200P V2
- Didn't realise the NR200 had the possibility of going dual GPU. I'mm super new to SFF.
  - pretty sure you would have to do PCIe birfurcation for that
- Yeah, max mobo you're going to fit in there is an ITX, and building round it already a squeeze with one GPU to think about (with any kind of air CPU cooler)

- check out Cerberus (mATX) or Cerberus X (full ATX)

- Generally sff cases are designed around ITX motherboards with only one pcie slot, finding a sff case that will fit an m-ATX motherboard is going to be near I possible. I have a nr200p same as the guy with dual 4090s, it only officially supports ITX boards.. it's also not really small form anymore (I have a tophat with a radiator it)

- [Smallest case for mATX + air cooled cpu + 2 gpu : r/buildapc _202308](https://www.reddit.com/r/buildapc/comments/15x7h9h/smallest_case_for_matx_air_cooled_cpu_2_gpu/)
  - Asus Prime AP201 MicroATX Mini Tower Case: 20 x 35 x 46
  - Sliger Cerberus. mATX case much smaller than the Asus AP201 (huge at 33L, when the Sliger Cerberus is 19.6L).

- [Good pc case for dual gpu setup? : r/VFIO _202402](https://www.reddit.com/r/VFIO/comments/1ao7gsj/good_pc_case_for_dual_gpu_setup/)
  - Take a look at Fractal Design Meshify 2, or even the XL version. It should have plenty room for what you want.

- ### [Small-form-factor all-air-cooled dual RTX 3090 SLI 1000W build : r/sffpc _202107](https://www.reddit.com/r/sffpc/comments/orug4s/smallformfactor_allaircooled_dual_rtx_3090_sli/)
  - Case: Sliger Cerberus X
  - Motherboard: MSI MEG X570 Godlike
  - CPU: AMD Ryzen 5600X
  - RAM: HyperX Predator 32GB 3333Mhz
  - GPU: 2x RTX 3090 Founder's Edition SLI
  - PSU: Silverstone SX1000 Platinum SFX-L
  - CPU Cooler: Cryorig H7
  - Case fans: 2x Arctic P14 140mm, 3x BeQuiet Pure Wings 2 92mm
  - Temps under full load: GPU core 66/50, GPU memory junction 102/102, CPU 85
  - Using software to estimate power draw at full load I'm guessing around 800W. I am confident the PSU can handle a 5900X even without undervolting but temps would be an issue so I'm leaving it with the 5600X for now. The 5600X is slightly undervolted using PPT 70W, negligible performance hit compared to stock but a few degrees lower temps.

- ### [Looking for a 2 x 3090 case : r/LocalLLaMA _202503](https://www.reddit.com/r/LocalLLaMA/comments/1j3h23g/looking_for_a_2_x_3090_case/)
  - I am looking for either a case or an open air rig where I could fit two 3090 FE connected with nvlink.
  - I need to be able to use pcie extender otherwise there is not enough spacing on the Mobo for putting the nvlink because of the 3 slots GPUs.

- I feel like this shouldn't require too much special consideration. I have a standard mid-tower case and it fits 2 3090s in it just fine.

- If you can afford it, phanteks enthoo pro 2 server edition is amazing. Internal fan bracket lets you direct air onto the gpus. My 2x3090s stay below 30 C at idle, and never go above 70 on load.
  - 24 x 56 x 58

- I can just barely fit 3 MSI 3090s in my Corsair 7000D Airflow, 2 horizontal and one vertical with riser. Tons of fans.
  - 24 x 55 x 60

- Almost any full ATX case with 8 pcie slots would work fine. Antec P101 for example.

- [Smallest case for dual air-cooled 3090's? : r/sffpc _202104](https://www.reddit.com/r/sffpc/comments/mzfg3p/smallest_case_for_dual_aircooled_3090s/)
- Wondering if getting a smaller case with Quadro A6000 would be better option than bigger case with Dual 3090?
  - An A6000 would be amazing but it’s just so damn expensive. It doesn’t make sense to me from a value perspective.
  - And since the 3090 and A6000 have the same amount of cuda cores, having dual 3090’s would actually give me up to twice the performance as a single A6000, but with the same amount of total memory.
- Are you sure you can combine the VRAM in your workload? You'd most likely need an NVLink bridge aswell.
  - The 3090 has NVLink, although certain features are disabled in software. For ML training though I’m pretty sure I would be getting the full NVLink bandwidth.

- Fractal R6 has a lot of room. So does the 7

- If you need the NVLink, you'll need one with two 16x slots spaced 4 slots apart (NVidia only sells 4-slot spacing NVLink-bridges). 
  - Also: Do you need the PCIE bandwith for your workload? Most consumer CPUs don't have enough PCIE lanes, so the two GPUs will be running in only 8x each.
- Yeah, Cerberus X won't be an option if you wanna aircool the CPU. The O11D is a pretty giant case (57L).
  - If you want something smaller, I'd recommend the Meshify C. It's just under 40L, has a standard mid-tower layout, can fit big ATX PSUs, a big aircooler like the NH-D15 and has good ventilation (if you add 3x 120mm fans in the front) so your GPUs will get enough airflow.

- ## [一次性价比爆棚的显卡升级之旅：折腾 V100 显卡续集，二手神卡跑ComfyUI！ - 知乎 _202507](https://zhuanlan.zhihu.com/p/1929587520502498303)
- Tesla V100 SXM2	
- V100 是 Volta 架构（sm_70），而现在一堆 AI 框架都要求 Ampere 起步（sm_75 以上）。导致这些模块直接报错
- LTX-Video-Q8-Kernels 的 setup.py 未适配 Tesla V100 的 sm_70 架构，仅支持 Ampere（sm_80+）等较新架构。
- Triton 3.3.0 与 sm_70 兼容性差，sageattention 的 INT8 优化可能不支持 V100。PyTorch 2.7.1+ 要求 sm_75，而 PyTorch 2.4.1 是最后一个支持 sm_70 的版本。 解决： 回退到 PyTorch 2.4.1 和 Triton 3.0.0
- is_flash_attention_available 是 PyTorch 2.5.0+ 的 API，PyTorch 2.4.1 不支持。
- FlashAttention-2 的 flash_attn_func 不存在于 FlashAttention-1（1.0.9），而 V100（sm_70）不支持 FlashAttention-2。 解决： 安装 FlashAttention-1
- PyTorch 的 注意力机制，FlashAttention-1或者xformers内存效率低，视频生成模型（如 Wan2.1、Hunyuan）需 大量显存，V100 的 16GB 不足。而sageattention又不支持V100，所以暂时无解

- ## [Laptop with 32 GB VRAM for Stable Diffusion : r/StableDiffusion _202305](https://www.reddit.com/r/StableDiffusion/comments/1353g26/laptop_with_32_gb_vram_for_stable_diffusion/)
- I don't think so, unless you do an "egpu" with a card in a separate case connected via thunderbolt. My card has 16gb VRAM and I can run a 2048x2048 without tiling. Tiling lets that go much higher, so that's the way to go if you want to do large images. Generally speaking the vram stuff is mostly an issue if you want to train LoRAs/embeddings/hypernetworks/etc.

- Is 16 GB of VRAM on your machine enough to train LoRAs and checkpoints? I can do very limited training with my current 8 GB VRAM machine, and I'd be very interested in training LoRAs and checkpoints to personalize my work. 
  - Yes, 16gb is enough. Until recently I was able to do batch sizes of 8 for Dreambooth fine-tuning, generally in other AI training it's good to do that or accumulate gradients

- ## [AMD 计划于 2026 年推出 Zen 6 架构 Ryzen 处理器，有何亮点值得期待？ - 知乎 _202506](https://www.zhihu.com/question/1944114025425269198)
- 2006年，ATi被AMD以54亿美元收购，“Fusion融合”成为新AMD最高战略。CPU、GPU、主板芯片组三者合一，集成为前所未有的SOC单芯片，即“APU”，异构运算以提升运算效能
- 2017年，“Zen禅”架构的锐龙处理器上市。单芯片设计的APU力挽狂澜，桌面处理器触底反弹、移动处理器迅速崛起、拿下双游戏主机大单。但称雄桌面、制霸服务器端，AMD依靠的是Zen2开始的Chiplet多晶粒模块化设计，多核堆死老师傅。
- 摩尔定律失效，单芯片面积有限，先进制程代价昂贵，剥离了GF格罗方德工厂、几近破产边缘抢救回来的AMD，必须得精打细算
- 新一代Zen6架构，CCD计算核首发2纳米制程，连财大气粗的苹果、高通都没敢下这么重的本。
- AMD选择将Chiplet进行到底： 2nm的Zen6 CCD计算核，2-3种3nm的RDNA5GPU，用4种以上3nm的APU组合在一起
  - Strix Halo架构那颗AI MAX+ 395上取得的经验，成为日后的封装标准

- Strix Halo架构的AI MAX+ 395，性能足以媲美RTX4060；只是双CCD计算核拉高了成本，试水新封装良率不高，又遇上AI推理本地化热潮，造成出厂价奇高，单U价格超过4060游戏本整机，叫好不叫座。

- 2027年末上市的AI MAX 500系列新品，是APU，也是一颗可以做成独显的GPU：
  - 【小杯】Medusa Halo mini，也是 AT4 GPU，12大小核2LP，24CU RDNA5，PTX 1050同芯，对标4060
  - 【大杯】Medusa Halo，也是 AT3 GPU，至少具备12核，可选CCD计算核桥接补齐，48CU RDNA5，PTX 1060同芯，对标4070
  - 小杯使用128bit LPDDR5X内存控制器，显存可达128G，更适合经济游戏本
  - 大杯采用384bit LPDDR6内存控制器，显存可达512G，针对价格不敏感的AI行业用户

- ## [How well does ComfyUI perform on macOS with the M4 Max and 64GB RAM? : r/comfyui _202503](https://www.reddit.com/r/comfyui/comments/1jhifyi/how_well_does_comfyui_perform_on_macos_with_the/)
- TLDR - if you want to work linear on one image, a Mac is a huge waste of time. Maybe 25% of the speed of a decent NVIDIA PC for AI generation. 
  - However, if you know how or want to multitask, it’s easily the best system you can purchase.

- I don't know if you'll find this information but based on estimates comparing to the M4 Pro, it should reach close to half the performance of an $800 PC with an RTX 3060

- I have a Mac Mini M4 Pro with 48GB of unified RAM. As my daily driver for everyday things, it's great, even great at media encoding, but for generative AI stuff, compared with my Linux PC with a RTX 4090 - it pales in comparison. We are talking minutes vs seconds here for a 1024x1024 Flux generation. Most of it is tuned for Nivida CUDA and that is the key. Apple Silicon offers great performance for everyday computing, but its support for machine learning frameworks like PyTorch sucks butt compared to CUDA.

- I transitioned from dabbling with generative AI images on M1 Max Macbook to using a PC that I owned with Nvidia RTX 3060 graphics card. 
  - The PC with ComfyUI was 3 times faster than my Mac which was using DrawThings (I installed ComfyUI on Mac but abandoned it because DrawThings was more convenient and faster). 
  - After getting more involved I ended up buying a PC with Nvidia RTX 4090 graphics card. Very happy with that decision. I love the Mac for most things but most likely even the M4 Max might prove to be frustrating to use to keep up with the rapid advances in the ComfyUI - Stable Diffusion world.

- AI image and video generation require cuda cores to function properly so Apple or even AMD gpus are not recommended.

- I ran ComyUI Flux Schnell, Pro and several LORAs on Mac studio Max4 base model and most models will run fine. I only ran into memory issues with video generation therefore moved to M3 Ultra 96GB. I dont have a reference point with PC but i am getting 15-30sec for 1024x1024 img generation for most workflows.
  - That's pretty slow. A NVIDIA 3090ti 24gb PC can do that in under half that

- my Mac M4 16gb is much more slower than my old PC with the good old Geforce 3060. An Apple a day keeps the Comfyui away.

- ## [Setting up ComfyUI with AI MAX+ 395 in Bazzite : r/StableDiffusion _202510](https://www.reddit.com/r/StableDiffusion/comments/1nux1f0/setting_up_comfyui_with_ai_max_395_in_bazzite/)
  - Qwen took 3 min 20s with fp8 image and clip at 20 steps
  - 1 min 22s for Qwen 4-step lightning lora with 8 steps - 8-step lora doesn't work since it's bf16
  - Flux was super slow first time, running flux-dev at fp8, but after that it was about 1 min 51s
  - WAN 2.2 fp8 high-lo 20 steps took 4 min 14s for an image (no speed up lora)
  - WAN 2.2 fp8 with Lightx2v took 18 seconds for an image

- 3 times more expensive than 4070 but 3 times slower than 4070?
  - The purpose is the 128gb of unified ram, it's meant for LLM use, not image generation. 
  - Obviously if you only care about small models that fit in 16gb vram there are way cheaper and faster methods.

- [System Question: AMD Ryzen AI Max + 395 with 128GB LPDDR5x 8000mhz Memory -- Will this work to run ComfyUI? : r/comfyui](https://www.reddit.com/r/comfyui/comments/1nr9ttv/system_question_amd_ryzen_ai_max_395_with_128gb/)
  - It would be really sloooooow. Assuming you’re talking about ai max. There is a YouTube video of a review in Chinese. He shows running some T2i and t2v using some Chinese software. Regardless it was super slow.
  - TLDR CUDA is still king.
- You can load large LLMs and run them decently on that machine but it is not meant for heavy image and video work. 
  - A dedicated GPU will run rings around that machine for rendering time. With a 5090 I can generate 8 seconds of 720p video with FP16 high and low noise models and Loras using sage attention 2 in about 3 to 5 minutes, you don’t need to be running them as high as I am if you want good results with 16 a 24gb vram. 
  - The main difference is that VRAM is faster (much faster) than ram and the GPU chip turns out many more TFLOPS of 16 floating point precision than the tiny 8060S can, not to mention the LPPDR 8000 ddr ram is much slower than GDDR7. 
  - If you just want to run language models get that machine. Otherwise, you’ll be badly equipped and your render times will be forever

- ## [DIY vs Nvidia dgx spark? : r/StableDiffusion _202509](https://www.reddit.com/r/StableDiffusion/comments/1nfoi9d/diy_vs_nvidia_dgx_spark/)
- As already suggested, go for 5090. Or if you need more VRAM and have the budget, go for RTX PRO 6000 (if you are planning some video work, that may be a better option).

- The spark can be a good option for LLMs but not for image generation. Here the currently best options that you can run in an office are the 5090 and the RTX Pro 6000.
  - When it's coming to training you could (should) even consider multiple 5090 like 2 or 4.

- just buy 5090, DGX spark is 40/5060 Ti ish performance

- Spark can do nvlink? did you mean network based NCCL?
  - You want to train Stable Diffusion, i assume UNET based model like XL and 1.5 variant. The less pain in the ass way to train in multi gpu setup is DDP. splitting unet is pain you need special libs like https://github.com/mit-han-lab/distrifuser
  - Incur communication cost, no free meal, DDP even though less demanding than FSDP in communication, it stilll need allreduce the gradient across rank.
  - Is 128G VRAM really important for your use case? DGX Spark is mainly for LLM workflow, large weight model but low active compute sequence (tokens), since LLM sequence isn't as crazy as diffusion models.
  - Engineering and salary cost. Your engineer need to learn to parallelizing gpu, manage networking, setting up torch distribution
  - Most SD trainer is mainly for single gpu
  - DGX Spark is not GDDR memory but LPDDR, and boy moving tensor from GDDR to share memory (gpu internal memory) is already slow, relative to SM speed, and now LPDDR is much slower

- ## [Will this thing work for Video Generation? NVIDIA DGX Spark with 128GB : r/StableDiffusion _202504](https://www.reddit.com/r/StableDiffusion/comments/1ju2mfk/will_this_thing_work_for_video_generation_nvidia/)
- For a machine with 128 GB of LPDDR5 with 273 GB/s memory bandwidth and a paltry 200 GbE ConnectX-7 I find $4k a bit much.

- 4090 still superior to this despite the 128GB of memory. That memory speed is much slower and the TOPS is lower. A 4090 is still best dollar value if your focus is SD and video.

- Check out the HP Z2 G1a that's dropping on Monday. Allocate up to 96GB of RAM to the GPU (I've heard that on Linux you can allocate even more) and the price has surprised me. I'll be getting one as soon as I know Ollama and SD support its APU.
  - Yes, it's 110GB on Linux. Are you surprised that the price is so high? Other Strix Halo machines also with the same APU and the same 128GB of RAM are much cheaper. The Framework Desktop starts at $2000 or just $1700 to buy the motherboard. The GTK is well spec'ed out for around $1800 ready to run. Those are like half the cost of the HP.

- ## [DGX Spark? : r/comfyui _202506](https://www.reddit.com/r/comfyui/comments/1ljbgxn/dgx_spark/)
- 4090 or 5090 for images. RTX 6000 Pro if you're doing video. DGX Spark is for running medium to large-ish LLMs slowly and does not have the right mix of performance for image/video work.
  - The 6000 is about 3x the price of a 5090 so I'll have to think about that one.
  - 96GB of RAM on one GPU. Video models were mostly engineered to run on 80GB H100s. You can run them on less VRAM but with hokey compromises and limitations. Not saying people don’t do it but I don’t have the appetite, would rather just use models as intended.
- Yes, any RTX *90 series card will be faster. DGX Spark is targeted at large text based models. I would not buy this if you plan on using it for image/video gen.

- Just get a nuc and a pro 6000 Blackwell and you'll be set for a long while.
  - Wow it's at least 10k€ just for the gpu!

- [NVIDIA says DGX Spark releasing in July : r/comfyui](https://www.reddit.com/r/comfyui/comments/1labkat/nvidia_says_dgx_spark_releasing_in_july/)
  - Lmao. It has 1/5 the memory bandwidth and cuda cores of a rtx6000 pro. 
  - comfyUI would need to be able to run on arm CPU. All Mac users have ARM CPU 

- ## 🧮🤔 [NVIDIA says DGX Spark releasing in July : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1kq4ey4/nvidia_says_dgx_spark_releasing_in_july/)
- Let's do some quick napkin math on the expected tokens per second:
  - If you're lucky you might get 80% out of 273 GB/s in practice, so 218 GB/s.
  - Qwen 3 32B Q6_K is 27 GB.
  - A low-context "tell me a joke" will thus give you about 8 t/s.
  - When running with 32K context there's 8 GB KV cache + 4 GB compute buffer on top: 39 GB, so still 5.5 t/s. If you have a larger.
  - If you run a larger (72B) model with long context to fill all the RAM then it drops to 1.8 t/s.
- Yes, these architectures aren't the best for dense models, but they can be quite useful for MoE. Qwen 3 30B A3B should probably yield 40+ t/s. Now we just need a bit more RAM to fit DeepSeek R1.

- Is that how you can calculate the maximum speed? Just bandwidth / model size => tokens / second? I guess it makes sense, I've just never thought about it that way. I didn't realize you would need to transfer the entire model size constantly.
  - You don't transfer the model, but for every token generated it needs to go through the whole model, which is why it is bandwidth limited for single user local inference.
  - As for bandwidth, it's a MT/s multiplied by the bus width. Normally in desktop systems one channel = 64bit so dual channel is 128bit etc. Spark uses 8 of DDR5X chips of which each is connected with 32bits, so 256bit total. The speed is 8533MT/s and that give you the 273GB/s bandwidth. So (256/8)*8533=273056MB/s or 273GB/s.
- > "You don't transfer the model, but for every token generated it needs to go through the whole model"
  - Except when you use models with "sparse" support, apparently. Which is why its a big deal the things have hardware accel for sparse models.

- My MacBook Pro M1 Pro is close to 5yo and it runs qwen3 30B-a3B q4 at 45-47t/s on commands with context. It might drop to 37t/s with long context.
  - when you run a smaller quant like Q4 of the 30B A3B model you might get close to 60 t/s in your not-long-context case.

- Running the same Qwen model with a 32k context size, I can get 13+ tokens a second on my M4 Max.
  - With just 32k context size set, or also mostly filled with text? Anyway, 13 tps * 39 GB gives us about 500 GB/s. The M4 Max has 546GB/s memory bandwidth, so this sounds about right, even though it's a bit higher than expected.

- thank you. those numbers look terrible. I have a 3090, I can easily get 29 t/s for the models you mentioned.
  - I don't think you can fit a 27 GB model file fully into 24 GB VRAM. I think you could fit about Q4_K_M version of Qwen 3 32B (20 GB file) with maybe 8K context into 3090, but it would be really close. So comparison would be more like Q4 quant and 8K context at 30 t/s with risk of slowdown/out of memory vs. Q6 quant and 32K context at 5 t/s and not being near capacity.

- But 128GB of memory will be amazing for ComfyUI. Operating on 12GB is impossible, you can generate a random image, but you can't then take the character created and iterate on it in any way or use it again in another scene without getting an OOM error. At least not within the same workflow. For those of us who don't want an Apple for our desktops this is going to bring a whole new range of desktops we can use alternatively. They are starting at $3k from partnered manufactures and might down to the same price as a good desktop at $1-2k in just another year.
  - You're probably better off with an RTX 4090 (and a full desktop PC to support it, so it is going to be more expensive) for image generation, as the Spark is going to be slower than a gpu. It can run far bigger models, yes. But 128GB is too much for just image generation while the speed will suffer due the limited bandwith. A sweetspot would be half the memory at twice the speed, but that doesn't quite exist, at least in that price range. A modded RTX 4090 with 48GB of ram (and the accompanying desktop) is going to perform better - although the entire thing would probably cost more than twice as much. BUT, if you already have a desktop, upgrading your gpu will give you better bang per buck.
- It likely depends on how big your workflows are. Your right in that if I don't run out of memory on my gaming graphics card, image generation is super fast, but if I do run out of memory all the speed in the world is not going to help me finish my workflow. Also the speed is not as important for developing, since your the only user. I can let this little guy do the work while I game on my gaming card and the power draw is so low it can share the same circuit.
  - This is similar to my thoughts. You have CUDA-capable running in the background and reasonably low wattage. Just throw some stuff at it and come back later to see the results. It won't bog down your main system and hopefully it won't waste too much electricity (maxing at 170 watts? Each, since you could have multiple linked). Having an always available local LLM is also just nice. MSTY or similar can make it available to your whole home network rather easily.

- So, basically like a 128 GB strix halo but almost triple the price. Yawn.
  - But it has CUDA man. CUDA!!!!!

- Just a note that DGX Spark is listed as $3999 on their page currently. There are some licensed competitors that have cheaper machines available but they all sacrifice something to get there.

- You can get an Apple Studio M4 128 GB for a little less than DGX Spark. The Apple device will have slower prompt processing but more memory bandwidth and thus faster token generation. So there is a choice to make there.

- ## [Comparing AI Performance of DGX Spark to Jetson Thor - DGX Spark / GB10 User Forum / DGX Spark / GB10 - NVIDIA Developer Forums _202508](https://forums.developer.nvidia.com/t/comparing-ai-performance-of-dgx-spark-to-jetson-thor/343159)
  - Jetson Thor: “NVIDIA® Jetson Thor™ series modules give you the ultimate platform for physical AI and robotics, delivering up to 2070 FP4 TFLOPS of AI compute and 128 GB of memory with power configurable between 40 W and 130 W.”
  - DGX Spark: “Powered by the NVIDIA GB10 Grace Blackwell Superchip, NVIDIA DGX™Spark delivers 1 petaFLOP of AI performance in a power-efficient, compact form factor.”
  - Is this a case of marketing terminology conflation, or might the Jetson AGX Thor provide better local inference performance compared to DGX Spark?

- The NVIDIA Jetson Thor Developer Kit is a purpose-built developer platform targeted at developers creating robotics and physical AI solutions that deploy with embedded Jetson modules. 
  - DGX Spark is a purpose-build compute to build and run AI, targeted at AI developers and data scientists who need to augment current laptop, desktop, cloud, or data center resources to provide large local memory and access to the NVIDIA AI software stack for their AI prototyping, fine-tuning, inference, data science, and general edge workloads.

- The RAM being similar but performance different could be down to scaling with power draw. The DGX Spark ships with a 240w USB-C brick, and I think it’s specced to draw significantly higher than the Thor at 170W.

- The DGX Spark has 6144 CUDA cores, or just as many as RTX 5070. I believe I saw numbers claiming 1000 TOPS in FP4 sparse mode. I’m not sure how many tensor cores, or what type of tensor cores even, if any?
  - The AGX Thor has 2560 CUDA cores, with 96 fifth-generation Tensor cores; benchmarks I’ve seen so far indicate it performs on LLMs about as fast as an RTX 5070. I’m unsure if these were tests used in FP4 Sparse mode, as NVIDIA rates it for 2070 TOPS in FP4 sparse mode
  - I suppose I can see the Spark using more power due to using CUDA cores instead of Tensor Cores as the main source of processing, offering FP32 performance needed for precision during training and perhaps greatly versatility

- Some other differences that are noteworthy
  - DGX Spark has one NVENC/ NVDEC chip. Thor has Two.
  - DGX Spark has a connect-x nic. Thor is not connect-x but a 4x25g nic. It doesn’t appear to support RDMA among other features you get with connect-x. which also means you probably can’t combine the thor modules very easily.

- ## [AGX Thor LLM Inference Performance & Implications for DGX Spark? : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1n0rheb/agx_thor_llm_inference_performance_implications/)
  - Excited to see the initial benchmarks rolling in for the AGX Thor following yesterday's release. A recent YouTube video showed around 30 tokens/sec generation speed with gpt-oss-120b using llama.cpp
  - This got me thinking about the DGX Spark. NVIDIA advertises the AGX Thor as having 2 PFLOPS of FP4 performance, while the DGX Spark is listed at 1 PFLOP

- Why would LLM inference scale with available FLOPs? 
  - Most of the operations used are dot produces (particularly matrix-matrix products) which are memory bound operations fundamentally; the number of FLOPs don't matter (within reason). Even a 4 core CPU with sufficient bandwidth can still run an LLM like GPT OSS
  - Where the FLOPs might matter is prefill (long context operation) or for concurrent inference serving, like maybe if you were doing async agents or something in parallel. These operations are compute bound, and FLOPs do matter (though in the case of prefill, with KV caching it doesn't matter for iterative workflows like chatting or iterative coding, and you're memory bandwidth bound again).

- I think one of the biggest drawbacks of the Spark (and something that is holding back many current offerings for AI and Handheld gaming) is the memory bandwidth
  - The one benefit that Spark might bring is NVIDIA MIG, which would allow us to partition GB10 into several instances and maybe run several models in parallel. Might be interesting for exploring LM Agents, especially if you got several of them working at the same time.

- I think there are two versions of the future: MXFP4 - will become the standard. MXFP4 - will not become the standard. 
  - In the first case, DGX Spark and Blackwell will make other solutions garbage. In the second case, DGX Spark will be garbage.

- ## [Nvidia DGX Spark | Hacker News _202508](https://news.ycombinator.com/item?id=45008434)

```markdown
<!-- FP4-sparse -->
| GPU Model | FP4-sparse (TFLOPS) | Price ($) | $/TF4s |
|-----------|---------------------|-----------|--------|
| 5090      | 3352                | 1999      | 0.60   |
| Thor      | 2070                | 3499      | 1.69   |
| Spark     | 1000                | 3999      | 4.00   |

<!-- FP8-dense -->
| Model        | FP8-dense (TFLOPS) | Price | $/TF8d (4090s have no FP4) |
|--------------|---------------------|-------|----------------------------|
| 4090         | 661                 | 1599  | 2.42                      |
| 4090 Laptop  | 343                 | vary  | -                         |

<!-- Geekbench -->
| Model      | Geekbench 6 (compute score) | Price | $/100k |
|------------|-----------------------------|-------|--------|
| 4090       | 317800                      | 1599  | 503    |
| 5090       | 387800                      | 1999  | 516    |
| M4 Max     | 180700                      | 1999  | 1106   |
| M3 Ultra   | 259700                      | 3999  | 1540   |

```

- Memory is the bottleneck. It limits the size of the models you can run and what you pay for.
  - Spark: 200B 
  - 5090 : 12B (raw)

- spark is $3, 999 and current M3 Max 28-Core CPU 60-Core GPU is the same price.

- I run 4 Mac Studio ultras at work (they’re pricy when maxed out), for local-first AI dev services. But there’s a few things that make me want to switch to the Spark. 
  - Networking is the biggest one, the Macs have Thunderbolt and Ethernet, but if I run distributed inference with EXO over Thunderbolt; the drop in tokens/second is massive. These Sparks get RDMA and can stack nicely. 
  - The other big one is access to CUDA, MLX has come a long way but being able to have CUDA and GPU access in containers would simplify the stack so nicely. 
  - If I had a USB-C/Thunderbolt backplane it might compare, but scaling with the Spark is likely a lot more straightforward.

- Biggest problem with Macs is that they don't have dedicated tensor cores in the GPU which makes prompt processing very slow compared to Nvidia and AMD.
  - there's been a little speculation that Apple adding TensorOps to Metal 4 suggests M5/M6 may get tensor cores.

- If that would be true why aren't Mac sales banned in China instead of Nvidia GPUs?
  - Because it's only a superior solution if you just want one box, and that mostly for inference. Once you start scaling to larger loads, it's much trickier to get a clusters of Macs to efficiently process them in parallel, whereas datacenter GPUs are designed for clusters.

- ## [What is the estimated token/sec for Nvidia DGX Spark : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1krsast/what_is_the_estimated_tokensec_for_nvidia_dgx/)
- generation rate (tokens / s) are almost always bound by memory bandwidth not compute. 
  - It will be bound by the LPDDR5x memory to 273 GB/s.
  - Of course the compute will help with prompt processing and batching multiple queries, and the huge RAM will allow you to (slowly) run big models

- Slightly more than Strix Halo, due to better GPU/drivers, but nothing major.

- Ohhh.. now i see why they are willing to sell this high memory product to the general public. This is straight up trash tier performance. Fast enough that it will be bought and used by AI developers and enthusiasts... but slow enough as to not be hoarded and abused by cloud providers.

- Strix Halo devices are all at $2000 and are now widely shipping from many manufacturers. These are RDNA3.5 devices and while WIP, have full PyTorch support.

- ## [Nvidia digits specs released and renamed to DGX Spark : r/LocalLLaMA _202503](https://www.reddit.com/r/LocalLLaMA/comments/1jedy17/nvidia_digits_specs_released_and_renamed_to_dgx/)
- Framework Desktop is 256GB/s for $2000… much cheaper for running 70gb - 200 gb models than a Spark.
  - Yup, and being X86 is much more usable. These small AMD APUs are quite nice for a console/multi-media box purposes when not using LLMs. 
  - Nvidia offering is ARM so Linux only and not even X86 Linux so pretty much no gaming will be possible.

- ## [Local inference with Snapdragon X Elite : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1l5k290/local_inference_with_snapdragon_x_elite/)
- I've been using mine (Surface Laptop 7) since it came out. It's good, but not in the exact way marketed.
  - I use it with LM Studio and AnythingLLM running models up to about 21B, the model size is limited by my 32GB integrated RAM. The token rate on an 8B is like 17-20 per second. 
  - But the NPU doesn't seem to have to do with anything. All the inference is on CPU, but not in that bad way people complain about if they have Intel products, more in the good way people talk about if they have Macs.

- I've been using local inference on multiple Snapdragon X Elite and X Plus laptops.
  - In a nutshell, llama.cpp or Ollama or LM Studio for general LLM inference, using ARM accelerated CPU instructions or OpenCL on the Adreno GPU. CPU is faster but uses a ton of power and puts out plenty of heat; the GPU is about 25% slower but uses less than half the power, so that's my usual choice.
  - I can run everything from small 4B and 8B Gemma and Qwen models to 49B Nemotron, as long as it fits completely into unified RAM. 64 GB RAM is the max for this platform.

- NPU support for LLMs is here, at least by Microsoft. 
  - You can download AI Toolkit under Visual Studio Code or Foundry Local. 
  - Both of them allow running of ONNX-format models on the NPU. 
  - Phi-4-mini-reasoning, deepseek-r1-distill-qwen-7b-qnn-npu and deepseek-r1-distill-qwen-14b-qnn-npu are available for now.
- Microsoft has AI Foundry which uses the Hexagon NPU. You're limited to Phi models and DeepSeek Distill Qwen models. Performance is fine but the models are old.
- Microsoft just added Qwen 7B and 14B DeepSeek Distill models that run on NPUs. I think for the moment, only the Snapdragon X Hexagon NPU is supported using the QNN framework. These are ONNX models that require Microsoft's AI Toolkit to run. 

- the X elite, would maybe use the gpu for the transformers i dont know if it would be possible for the llm since im using llama cpp
  - I don't think it's possible. A lot of Python ML-related frameworks don't have ARM64 Windows wheels or they can't be compiled without getting into a dependency nightmare. They won't work under WSL Linux because there's no Vulkan or OpenCL GPU passthrough.
  - If you want to use the Adreno GPU for inference, you're stuck with llama.cpp, LM Studio or Ollama, which all use the same ggml backend code.

- [Snapdragon X CPU inference is fast! (Q\_4\_0\_4\_8 quantization) : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1emd3bg/snapdragon_x_cpu_inference_is_fast_q_4_0_4_8/)
  - On a Surface Pro 11 with a Snapdragon X Plus 10-core chip, running CPU inference for Llama 3.1 8B, I'm getting the following:
  - llama_print_timings: prompt eval time = 895.37 ms / 126 tokens ( 7.11 ms per token, 140.72 tokens per second)
  - llama_print_timings: eval time = 90360.33 ms / 1391 runs ( 64.96 ms per token, 15.39 tokens per second)
- I get ~5.5 tokens/s with Llama 3.1 8B Q8_0 and DDR4-3600 memory, so ~15.5 tokens/s for Q4 and dual-channel LPDDR5X-8400 memory doesn't seem that impressive.
- Well it's not close to m2 levels at all. M2 max gets 66 t/s and 671 pp/s at q4
- Wow, I have ~5.8 tokens/s on 8+gen1 in llama 3.1 8b

- ## [Snapdragon X Elite - local llm? : r/LocalLLaMA _202406](https://www.reddit.com/r/LocalLLaMA/comments/1ddyc51/snapdragon_x_elite_local_llm/)
- I have purchased the new Surface Pro CoPilot+PC and am struggling to get the LLMs to run using the embedded NPU. I guess I was spoiled by using Ollama and Llama.cpp, now I am having to learn the basics of quantizing a model and using HuggingFace APIs to host the models.

- Should be good for 13B. There is a video out there showing it’s real world performance already on a model called “Phi Silica”

- [Snapdragon X Elite llama.cpp ? : r/LocalLLaMA _202406](https://www.reddit.com/r/LocalLLaMA/comments/1dj6h6x/snapdragon_x_elite_llamacpp/)

- [$899 mini PC puts Snapdragon X Elite into a mini desktop for developers (with 32GB RAM) : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1cxhh1q/899_mini_pc_puts_snapdragon_x_elite_into_a_mini/)
  - every modern CPU since AMD phenom is a SoC with unified memory architecture. The key thing is the memory controller and how many steps it needs. CPUs benefit from being able to quickly go back and forth between ram compared to GPUs, which is why RAM performance isnt a direct competitor to VRAM in GB/s because the MT/s is more important for RAM and how efficiently your data and instructions are packed into every transfer.
  - Thanks for correcting that AMD and other competitors have had UMA for years. My mistake. But still stands that apple has 800GB/s memory bandwidth vs things like Snapdragon X Elite’s 136GB/s

- ## [骁龙X Elite再遭痛殴, 第二代酷睿Ultra 英特尔Meteor Lake有多强？ - 知乎 _202406](https://zhuanlan.zhihu.com/p/701851374)
- 第一代酷睿Ultra平台（流星湖，Meteor Lake）的四大模块中，除了计算模块采用了自家的Intel 4工艺，其他三个模块都是台积电代工。
  - 到了【月亮湖】，就连最核心的计算模块也改用了台积电的N3B工艺，英特尔仅保留了自己的先进封装工艺（Foveros）。
  - 在【月亮湖】上，英特尔还带来了类似苹果统一内存架构的设计，直接将LPDDR5X-8533内存封装在了芯片之上，可选16GB或32GB容量。
  - 这种设计的好处是，能使数据传输负载降低大约40%，延迟更低，相较传统的板载内存还能节省大约250平方毫米的主板空间
- 【月亮湖】的核心竞争力，就是计算模块中的CPU、GPU和NPU三大单元都迎来了全面提升。
  - 首先就是NPU，从第一代酷睿Ultra的11TOPS，大涨到48TOPS
  - 【月亮湖】CPU部分的AI算力为5TOPS，GPU为67TOPS，在异构计算的加持下，整体AI算力高达120TOPS，一举超越了骁龙X Elite的整体75TOPS（NPU为45TOPS），和锐龙AI 300的整体80TOPS（NPU为50TOPS）。
  - AI PC时代之所以格外强调AI算力，就是因为微软即将在Windows操作系统层面，就把生成式AI技术应用到基础体验之中。如果使用CPU和GPU进行处理，笔记本的续航会尿崩。此时，唯有低功耗高AI性能的NPU，才能在兼顾续航的同时，随时随地享受AI带来的便利
  - 在GPU方面，【月亮湖】升级到了新一代的Xe2架构，性能有了平均50%的提升，有机会与AMD锐龙AI 9 HX 370集成的Radeon 890M掰掰手腕

- ## 🆚🌰 [AMD AI Max+ 395 CPU 本地大模型推理性能评测报告 - 知乎 _202509](https://zhuanlan.zhihu.com/p/1952045270763283746)
- 针对搭载AMD AI Max+ 395 CPU的零刻GTR9迷你主机进行了一系列严格的大模型推理速度测试。
  - 硬件平台: 零刻 (MINISFORUM) GTR9 迷你主机
  - 核心组件: AMD AI Max+ 395 CPU
  - 任务类型: 本地大语言模型推理
  - 性能指标: Tokens/s (每秒生成Token数) — 该数值越高，代表推理速度越快
- 设计了涵盖多种任务类型的标准化问题：
  - 综合能力: "你是谁？请详细介绍一下你能干什么。"
  - 知识问答: "作为专业人工智能专家，请告诉我如何学习深度学习？"
  - 数学计算: "如果A+B=12, A-B=10，则A的值是？"
  - 自然语言理解: "识别句子‘我将会在明天早上的8点到湖北黄陂的森林公园’中的所有地名。"
  - 代码生成: "请使用Python编写一个贪吃蛇游戏。"

- 参评大模型:
  - deepseek-r1:70b, 30
  - qwen3 系列（32b / 30b / 14b / 8b）
  - gpt-oss（120b / 20b）

```markdown
| Model          | Ollama | LM Studio |
|----------------|--------|-----------|
| deepseek-r1:70b | 4.43  | 4.97      |
| qwen3:32b      | 8.97   | 10.12     |
| qwen3:14b      | 19.47  | 21.70     |
| qwen3:8b       | 29.93  | 35.96     |
| gpt-oss:120b   | 30.84  | 42.07     |
| gpt-oss:20b    | 42.57  | 60.54     |
| qwen3:30b      | 48.93  | 68.70     |
```

- 对比两组数据可见，同一模型在LM-Studio中的推理速度普遍优于Ollama
- AMD AI Max+ 395 CPU采用CPU/GPU共享内存的统一内存架构（UMA），这种设计天然适合运行混合专家（MoE）模型（如gpt-oss系列、qwen3:30b）。
  - MoE模型虽然总参数量庞大，但每次推理仅激活部分"专家"参数，非常契合这种大容量内存但绝对算力相对有限的硬件。
  - 相比之下，对于参数密集的传统稠密模型（如deepseek-r1:70b、qwen3:32b），由于需要更高的绝对算力，该处理器的集成显卡则稍显吃力。

- DFRobot作为在单板计算机（SBC）、AI边缘计算和开源硬件领域的创新者，此次测试结果意义非凡。若未来DFRobot推出基于AMD AI Max+ 395 CPU的单板计算机，将其强大的本地AI推理能力与DFRobot成熟的模块化传感器生态（如Gravity系列）相结合，将催生出更多实时、智能的物联网与机器人应用

- ## 🆚 [AI max+ 395 128gb vs 5090 for beginner with ~$2k budget? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1nunlls/ai_max_395_128gb_vs_5090_for_beginner_with_2k/)
- ComfyUI? 5090.  LLMs? AI Max
  - FWIW, you can also use a thunderbolt eGPU with the AI Max.

- I've been able to run ComfyUI on max+ 395. It's a pain, but it's possible.

- As for ComfyUI, I run that just fine on my Max+ 395 as well. But saying ComfyUI doesn't mean much. It's just a framework to run things. What exactly do you want to do? If it's just image gen, then the Max+ is as good as anything else. If it's video gen, it does work but uses way more memory than Nvidia. If it's voice cloning, that just works too. So other than using way so much memory, which you have plenty of on a Max+ 128GB, what is this bugginess you are talking about?
  - Max+ 395 owner, and I agree, the level of experimenting i've reached has been great with the device.
- I love my 395+ 128 GB, and I've got LLM, image generation, voice, etc. working, but I cannot for the life of me get video working. Mainly trying to experiment with I2V with WAN 2.1 and 2.2, but have never got a successful run. Either get OOM or seemingly infinite time per iteration. Not sure if you can provide any tips. Using the pytorch 2.9.0+rocm7 RC since that was the most stable for everything else.

- I’d personally go 395, because almost all AI labs are going with MoE architecture at this point. So big RAM can accomplish a lot even if it’s not the fastest.

- The AI Max+ 395 handles all my AI needs for now and it’s pretty snappy once you get things working in Linux.
  - The downside is if you need/want to run anything super fast - you will need a different setup. Eventually I might get a setup like that, and use the Max+ 395 for my mobile AI needs.
  - Still, I can get 20+ TPS on GLM Air 4.5, and even faster with the GPT-OSS models.

- How much memory ddr5 with 5090? Just 32gb is limited 128gb can run huge Moe's.
  - If your running 30b models and under 5090 all day. 
  - If your running large 120b moe models or smaller dense models 395plus all day.

- if you want to do some training stuff, CUDA, and ... maybe gaming, go for the 5090
  - if you just want to run some local model to try them or serve them locally 24/7, strix halo is probably a better option, almost all the best latest open model are MoE and they need a lot of ram/vram (glm 4.5 air, gpt oss 120b, some qwant of qwen 235, qwen 3 next, ... will all run better on strix halo vs 5090+128gb ram). Also strix halo is usually a full machine, while the 5090 you still need the rest of the PC

- AMD AI 395 miniPC preferably with Oculink (there are couple) where you can hook later external dGPU like AMD AI PRO R9700.

- If you're interested in image/video gen, you need the 5090. That is going to be miserably slow on CPU.
  - If you just want to run MoE models interactively, the AI 395 will be enough.
  - Personally? I'd go with both. You can in fact attach a 5090 to an AI 395 system. Then you can run LLMs for your prompt generation/improvement workflows while generating video and imagery to your heart's content.
  - I don't think people who have not experienced a Blackwell GPU understand how much faster they are for image/video work. Even against a 4090 my workflow times cut in half.
- I agree, I think I start with 395 and add the 5090 if it becomes necessary.

- if it was only to play around and learn llm stuff i'd lean towards the framework 128.
  - but the moment you mention comfy, amd is off the table. get the 5090 instead.

- If you're looking mostly to be able to inference bigger MoE models with llama.cpp then I think the 395 can be a good choice.
  - However, if you have an interest in playing round w/ image/video generation, doing any further poking around (vLLM/SGLang, voice, PyTorch, model training/fine tuning, etc) then I think the 5090 is the clear way to go.
  - If you're working w/ smaller models btw it's not just about stability either, the 5090 is a whole different animal in performance - as long as you're OK w/ running ~30B/smaller models.

- ## [Advice: 2× RTX 5090 vs RTX Pro 5000 (48GB) for RAG + local LLM + AI development : r/LocalLLM _202509](https://www.reddit.com/r/LocalLLM/comments/1nsyiag/advice_2_rtx_5090_vs_rtx_pro_5000_48gb_for_rag/)
- As someone with 2x 5090s, started from 1.. don't do it. Go straight to the pro 6000. Thank me later. It'll save you time, money, and effort. :) I now have a pro 6000. You can pick one up from Exxact for $7200.
  - Dual 5090s are great… you can run many models. 
  - However, even with quantized KV cache it’ll crash often. You can’t max out the 1million context Qwen coder. You can’t really fine tune models that don’t fit in a single GPU. You’ll have issues with accelerate. 
  - Heat is very hot. At least 10 degrees warmer in a whole room when fine tuning. Power usage is very high. You’ll need to run new electrical or have your machine shut off when someone plugs in a vacuum. You’ll need to run dual psus or buy a 2000w+ psu. 
  - Case options are very limited. The cards are massive in size. You’ll need 8 slot pc cases minimum for 2 cards. 
  - You’ll want to run 70b+ models guaranteed. Even Qwen next 80b. 2x 5090s isn’t enough. 
  - This is all solved with a single pro 6000. So for an extra $1000 you can save yourself a lot of headaches and get access to nvidia enterprise

- Rent GPUs online unless you have a reason other than inference

- Remember the most important thing: whatever you choose initially, you will have to switch to RTX 6000 PRO 96Gb on Epic or Threadripper platrofm eventually if you want to continue running large local llms efficiently. The cost of those things will dwarf your current current setup. So, do not sweat much while deciding on the current config. Just make sure that your case and motherboard allow to connect at least 2 GPUs via riser cards with PCI 5.0 on each, even if at x8 speed. 

- Be sure you get a motherboard with dual x8 pcie slots if you go with two GPUs. Most AM5 boards don’t support that but a handful do. Just wanted to give you a heads up
  - Yeps, Asus Pro Art 2x pice 5.0

- ## [The NVIDIA DGX Spark at $4, 299 can run 200B parameter models locally - This is our PC/Internet/Mobile moment all over again : r/LlamaFarm _202509](https://www.reddit.com/r/LlamaFarm/comments/1nee9fq/the_nvidia_dgx_spark_at_4299_can_run_200b/)
- DGX has 273gb/s versus 1.7Tb/s for a rtx 5090...
  - The bandwidth is terrible. You'll run your models at 5 tps
  - I literally don't see who's gonna buy this aside from people who just got blinded by the unified RAM...
- At 273 GB/s memory bandwidth this unified RAM there will make inference VERY SLOW... For comparison: RTX 4090 has 1008 GB/s and RTX 5090 has 1792 GB/s memory bandwidth

- u can easily get an m1 max with 128 gb of unified memory for 1k cheaper than that and double the memory bandwidth. Don’t see how this is a good deal.
  - It's not a good deal. It is more of a sign. MLX is growing, but the ecosystem is still tiny compared to CUDA. With Blackwells coming to PCs and AMD shipping powerful GPUs, there will be real competition in this area for the first time.

- M3 Ultra with 256GB RAM does cost roughly $1000 more, but has 256GB unified memory
  - And 819 gb/s vs Spark's 273.

- Mac Studio, Ultra series, either M1 or M2 with 96GB or 128GB RAM. And more recently, the M4 Max. I bet by next year the Mini will likely have a 128GB configuration too.
  - What I am NOT saying is that Apple’s offerings are the best local inference for 128GB workloads. I am saying they are relatively cheap and capable, and the inflection point for fairly large local models (>60GB) was two years ago.
  - I bought my M4 Max 128GB last year for local models. It does not disappoint.

- No thanks, I'm not taking my wallet out until I see the AMD Medusa Halo 128GB AI mini-PCs in two years. I CAN WAIT.

- Jetson AGX Thor claims 2000 TOPS of AI at FP4, but it is the same memory bandwidth as DGX spark so I have my doubts. Guess we have to wait for some benchmarks.

- [DGX spark vs MAC studio vs Server (Advice Needed: First Server for a 3D Vision AI Startup (\~$15k-$22k Budget) : r/deeplearning](https://www.reddit.com/r/deeplearning/comments/1lylfuw/dgx_spark_vs_mac_studio_vs_server_advice_needed/)
- Macs are decent for inference, but nobody "real" is training models on Mac. Even Apple was using TPUs earlier (when that team was still run by the ex-Google guy) and grapevine says they're on Nvidia now

- DGX Spark is like a RTX 5060(70?) class GPU with 128GB of slowish (for GPU) memory.
  - The only thing Spark really has going for it is it might be SM100 (real Blackwell with Tensor Memory) instead of SM120 (basically Ada++) which may be useful for developing SM100 CUDA kernels without needing a B200.
  - Much better off I think with a NVIDIA RTX PRO 6000 Blackwell Series (96GB) for most people, or 512GB Mac Studio if you need very large LLMs but less GPU perf.

- ## 🍎 [MacBook M4 Max isn't great for LLMs : r/LocalLLaMA _202503](https://www.reddit.com/r/LocalLLaMA/comments/1jn5uto/macbook_m4_max_isnt_great_for_llms/)
- M4 Max is about 50% faster than an Nvidia P40 (both in compute throughput and memory bandwidth). 
  - It is about 2.5x slower than a 3060 in compute¹ throughput (FP16) and 50% faster in memory bandwidth. 
  - Compared to 3090, it is about 7x slower in compute¹ throughput (FP16) and almost 2x slower in memory bandwidth.
- P40s (and generally Pascal) were the last ones without tensor cores (which increase FP16 throughout by 4x).
  - The lack of tensor cores is also the reason Apple M3 Ultra/M4 Max and AMD 395 Max, lag in Prompt Processing throughput compared to Nvidia, even if the M3 Ultra almost matches a 3080/4070 in raster throughput (FP32).
  - Compared to CPU-only inference, P40s are still great value, since they cost $150-300 and are only matched by dual 96-core Epycs with 8-12 channel DDR5 which start from $5000 used.
- Pascal doesn't even have FP16 support, all the operations are done through fp32 units afaik so throughput is effectively halved. It wasn't until Ampere that NVidia had FP16 support.

- Try swapping to serving with LMStudio - then use MLX, and speculative decoding with 0.5b as draft for 14b! Tripled my speed on my M1 Max
  - Speculative decoding really is great. It at least doubled my speeds. In token generation. Prompt processing didn't get and bump though. I'd love to have a 128gb+ RAM machine to also activate KV Cache

- ## [本地部署大模型性价比之王真的是Apple Mac Studio M3 Ultra 192或512吗？ - 知乎 _202503](https://www.zhihu.com/question/14548406514)
- deepseek r1 用mac studio测试
  - ollama: 16 tops
  - lmstudio: 18 tops
  - 512Gb内存还是只能部署4bit量化版本，8bit不行
- 如果按照18.11token/s的输出速度，不考虑其他，全天24小时运行（不考虑prefill等时间）18.11×86400s=156.5万tokens，而官网百万tokens售价才16块，156.5万售价25块钱
  - 而Mac studio 512G内存，1t硬盘版本售价7.3w教育优惠也要6.7w，需要365×24h不停运行8年/7.3年才能回本

- 那要看你怎么定义性价比了，[ktransformers] + [epyc] 现在也已经玩得很强了，苹果胜在上手就能用不用折腾装，在那个价位算一个可行方案，但并非算唯一性价比方案

- ## [MAC mini M4芯片32G+256能跑大模型吗？ - 知乎 _202503](https://www.zhihu.com/question/14795834393)
- 真跑ai只推荐Pro以上的芯片。
  - m4的内存带宽其实和普通核显win机子差不多。而到了Pro，内存带宽就达到256了，达到4060的水准。
  - max和ultra带宽分别是400＋和800＋，基本是显卡级别的带宽，用来跑ai推理很合适。

- 主要是内存大啊，我用一万出头的mac能跑27B的gemma3 8bit量化，如果用显卡的话至少得是个3090级别的

- Mac Mini 当副机可以，统一内存看着香，但很多模型不支持 MPS，或者用 MPS 跑得比 CPU 还慢。

- ## [Apple M5 could ditch unified memory architecture for split CPU and GPU designs | Hacker News _202412](https://news.ycombinator.com/item?id=42552494)
- UMA hurts the GPU too much. Widely parallel processing wants to access memory in bigger chunks than a CPU. If you try to mix access and modification, you lose the benefit of widely parallel processing. 
  - Other GPU designers have considered and eschewed unified memory models, to the tune of hundreds of millions in research dollars.
- I agree that single cache-line fetches are pretty poor for parallel vector units, but supporting the former in an environment designed for the latter doesn't seem to off-putting (the CM-5 did this).

- You can split the CPU and GPU and still have UMA. Splitting CPU/GPU is a packaging and interconnect concern and is not mutually exclusive with UMA.

- ## [求推荐！想组一台256G➕笔记本跑大模型🥹 - 小红书](https://www.xiaohongshu.com/explore/6892017700000000030274de?xsec_token=ABHUpmj6nmLewRxGBOYsEYt2FVRxDHjqOHSnhQfgUJJnc=&xsec_source=pc_search&source=web_explore_feed)
- 普通笔记本不支持256，别异想天开
  - 笔记本内存有是有，但是笔记本不支持，买了也没用，到时候插上去不识别就老实了
- 别想了，笔记本不支持ecc内存，跑时间长容易出错
- 哪个苹果笔记本有512内存 你找出来我看看，我就知道推出过2t内存的mac pro还有512内存的mac studio，mbp最大的就128g内存吧 m4max

- 散热压不住的

- 笔记本目前能本地部署大模型的只有m系芯片的mbp内存拉满（仍然是勉强够用水平）和ryzen ai max+395板载内存拉满128（这个的内存和效率估计不太够，唯一优势x86日常方便），因此主要往itx上面去凑，现有产品也就是我说的这两套即苹果m系和amd ai，因为有统一内存可以用核显来跑ai免去多路gpu的麻烦。

- 如果你需要cpu高性能，那么可以考虑用比较新的服务器平台（因为我估计连hedt都无法满足你的内存需求）和itx主板（当然，华擎不一定还在做这类奇特的产品）。你只需求单卡的话可以勉强塞一套itx，但是各方面都很极限了，包括供电和散热，大概是跑不满的。如果你需求多卡的话，可能得几张卡用扩展卡连接以分着用itx主板仅有的那一两个pcie x16接口，这样做性能肯定会有折损，并且100%需要做分体的两个itx机箱，这种情况你需要自己画设计图去tb之类的平台找工厂定做板材，好处是供电和散热没之前那么极限，坏处是这一套下来估计不比eatx机箱轻便多少。

- ## [选个能本地跑70B大模型的笔记本当主力机试 - 小红书](https://www.xiaohongshu.com/explore/6811b115000000002301de1a?xsec_token=ABZEBEajWMF79jFUPp-RsmTEccFriosXMc79IFdgiGHWo=&xsec_source=pc_search&source=web_explore_feed)
  - 之前一直用灵刃16 2024版的当主力机，跑起来风扇实在是响，开个线上会别人都听不清，而且重量实在无法移动。
  - 最近看到HP 战99 Ultra到货就直接拿下了 128G。用了一天，感觉AMD AI395+跟之前i9-14900hx性能差别不大 不过噪音好了很多，集显8060s的性能虽然不如之前4070，但是跑个本地小模型也没什么压力。
  - 重量上1.6KG稍微重了一点点，跟X1C没法比，跟macbook pro 14差不多，比灵刃强太多了。
  - 触屏的

- pd充电能有原厂充电器几成的性能？
  - 原厂130w的，65w的PD会提示慢速充电器，只有一半不到的速度

- 本地70B速度怎么样？
  - 有个20-30 token/s 凑合用
- 我qwrn32量化8也才5个token，你这个70b 20-30token是怎么来的
  - 70量化4可以的，量化8肯定不行

- ## 🆚 [关于几个桌面级的AI统一内存集成方案的对比 - 知乎 _202503](https://zhuanlan.zhihu.com/p/31599083340)
- 目前桌面级别的AI方案，除了nv的独显外，还有几个统一内存的集成方案
  - 苹果Mac mini和Mac studio
  - AMD AI 395 MAX
  - 英伟达 DGX Spark

- 带宽 ：(❓推理/部署时重要)
  - m4pro 64G 和nv dgx spark 128G都是 273 GB/s
  - amd AI 395 max+128G 是            256 GB/s
  - m3ultra 96G是                     400 GB/s (m3u无128G，256G以上才有800GB/s)
  - m4max 128G 是                     546 GB/s

- AI算力：(❓训练时重要)
  - nv dgx spark是  1000 TOPS (FP4)
  - amd AI395max是  126 TOPS（int4）
  - m3ultra是       72 TOPS(int4)
  - m4max和m4pro都是 38 TOPS(int4)

- 价格：
  - nv dgx spark  3000 美元（估计23000人民币？）
  - amd ai395max  25999 人民币
  - m3ultra 96G   32999 人民币
  - m4max 128G    29249 人民币
  - m4pro 64G+1T  16999 人民币

- 综合看来如果性价比和通用性比较好的选择应该是nv DGX Spark（生图，生视频之类的算力比带宽更重要），
  - 如果单纯为了LLM性能（统一内存带宽比算力更重要）m3ultra 96G 可能是比较好的选择。
- 至于之前有看到的一些用mac mini通过雷电口堆叠的虽然可以低成本做到大显存，但是几乎没啥实用价值，因为雷电口带宽只有15GB/s。。。
  - 雷电堆叠是为了低延迟跑tensor parallelism吧，又不是remote访问内存。PCIE带宽也不如内存，但多卡并行还是有效的

- 现在网上ai max 395的小主机已经卖到14000左右了，这样比下来，感觉ai max 395性价比还不错。

- ## [如何评价售价 18999 元的惠普暗影精灵 MAX 游戏本? 哪些亮点值得关注? - 知乎 _202503](https://www.zhihu.com/question/15023061538/answers/updated)
- 暗影精灵MAX这个新模具就用来取代暗影精灵Plus的，依然是主打一个“一线品牌中的性价比”定位。
- 一线品牌高端游戏本的守门员，原来暗影精灵PLUS的替代者。

- 这代Max整体的升级点：
  - 散热规格升级：采用了VC均热板设计，加上液金散热与4出风口设计，整体可以做到250W+的性能释放，双烤75+175W；
  - 屏幕改为更主流的16寸：2.5K分辨率16:10比例高分屏，500nit亮度240Hz刷新率；
  - 新增大师模式，允许用户手动超频。

- 其他配置只能说中规中矩了：
  - 32GB DDR5-5600内存+1TB硬盘，双硬盘位双内存插槽；
  - 只给了2A2C（2个雷电4）+HDMI+RJ45的接口，在游戏本里面算比较少的了；
  - 给了RGB背光键盘，但目前来看还是四区RGB而非单键RGB，并且方向键依旧为半高；
  - 电池容量83Wh，主流旗舰定位的游戏本都90Wh+了，有的甚至99Wh

- 目前惠普精灵遇到一个比较尴尬的问题，比上没有品牌力，比下没有性价比：
  - 加3000可以上逼格更高、更有氛围感、可玩性更高的且品牌更强的ROG枪神9系列；
  - 预算更低，等二线品牌上了之后，大概率1W4不到就可以拿下低U高显的RTX5080游戏本了，性价比更高。
- 国内游戏本市场目前还是以性价比为主导，就连ROG这两年都变得有性价比起来，所以对于暗影精灵MAX这类的不上不下的主流系列旗舰本，我是谨慎看好的，比上无品牌力，比下无性价比。

- 小米G的目标客户是追求性价比的用户，但这帮用户会因为小米G没有性价比不选择小米G……

- ## 🆚📈 [2025买个RTX 5090笔记本，有什么推荐吗？ - 知乎](https://www.zhihu.com/question/1890528801219405195)

> 不太在意便携，主要是看性能+性价比。神舟性价比太高了，但是又有点害怕翻船。

- 在圈定要RTX5090游戏本的前提下，题主的最佳选择肯定是一线品牌高端游戏本一直以来的性价比扛把子，暗影精灵MAX啊
  - U9-275HX+RTX5090+2.5k 240hz 500nit IPS+32/1T PCIe 5.0的核心配置，250w+的整机性能释放表现中等偏上，拓展性一个5.0 M.2一个4.0 M.2，重量2.75kg，接口是俩雷电4，俩USB-A，HDMI和rj45。
  - 上面这个配置也就22999，国补后才20000出头
  - ROG也完全没必要看枪神9 Plus超竞版吧，捆绑了64G内存后价格直接33999了，实在是太夸张。枪神9 超竞版除了内存硬盘都缩小一半以及尺寸为16吋之外，几乎没啥区别，价格则直接下降到了29999，立省4000.

- ## [有没有24g显存的笔记本电脑推荐？ - 知乎 _202409](https://www.zhihu.com/question/666131987)

> 想要买个新电脑来玩AI。主要还是想考虑笔记本，但是看了一圈，好像都是8g显存。最大的也就16g显存。没有找到有24g显存。

- 目前做AI推荐N卡，N卡24G显存只有4090，4090做成便携式只有这一个方法

- 有三个方案：
- 第一个方案是内置显卡的笔记本电脑，优点是便携性好，缺点是性能差点，也得做好散热。
  - 想要16G显存只能RTX4090的型号，绝大部分RTX4080移动端是12G显存，目前使用RTX4090的笔记本电脑不算多，想要性价比就机械革命耀世16Super，想要更强的整机性能释放可以考虑ROG枪神。
- 第二个方案是拓展坞外接桌面显卡，优点是性能强不用考虑散热，并且后期还能随时无痛升级显卡，缺点是丧失了便携性，并且要注意别没断电就拔显卡。
  - 笔记本有雷电3/4或者USB4接口就行，RTX4080会有性能损失，但是能接受，毕竟怎么都比移动端强
- 第三个方案就是放弃繁文缛节，直接上台式机吧。

- ## [纠结选哪款笔记本电脑？主要用于stable diffusion? - 知乎](https://www.zhihu.com/question/620893866)
- 看到楼下有个好点的建议，雷电接口的笔记本+显卡拓展坞+独立显卡
  - 这个方案的话，拓展坞不做推荐确实不知道怎么选，但显卡4090普遍10000以上了

- ## [AI绘画（Stable Diffusion）用什么显卡比较好？ - 知乎](https://www.zhihu.com/question/638915747)
- 在你能承受的范围内，选显存最大的

- 目前的条件下只有N卡能正常玩AI。
- 想要低价拉满AI。就用RTX TITAN 24GB（不到5000，但需要加水冷压，或者魔改的RTX 2080Ti 22G（不到3000）非涡轮卡的版本。但现在目前看24G的3090性价比最高。

- 推荐一块性价比高的显卡，RTX 2080ti 22g。ai画图性能接近RTX 4090的一半，价格只要五分之一。而且22g大内存，对模型训练也很友好。不过RTX 2080ti 已经上市很多年了，要淘到一块好卡不容易，最好有几年保修的更好

- 跑4K以上稳稳的，显存为32G的显卡一定是首选，不是魔改款，那就只有即将上架的5090了。
  - 跑2K以上的话，12G-16G的卡都可以，不过12G跑某些大模型可能不够用
  - 2K将就用，目前选4060TI 16G的用户多点，不过这款显卡只是预算少的选择，毕竟显存位宽被阉割，GPU性能也一般。
  - 没短板，出图快，首推4070TIS 16G，这款卡，我用下来很满意，缺点就是溢价高。
  - 跑1K以上的话，3060 12G，2060 12G，都已经可以跑了

- 经过不断的优化，现在的Stable Diffusion对显卡要求不算太高，
  - 如果只是跑图，4060Ti 16G就够用，炼丹的话，4090也足够了。

- 至于也是3000多的2080Ti-22G，魔改的有风险，多几G显存对这个软件来说用处不大，有新还是买新。
  - 之后就是7000这一档的，5070Ti，为什么不是4070Tis呢，因为50系显卡可以用4位模型，40系只能8位。

- 10系显卡不支持4bit（其实也不支持8bit、16bit），但是Q4能跑啊，就是慢呗。工作原理是按原位数大小装显存中，跑的时候分段转换成fp32来跑。

- ## [如何评价HP最新发布的搭载AI MAX 300系列处理器的战99 Ultra？ “战 99 Ultra”移动工作站已于3月17号上架京东  - 知乎](https://www.zhihu.com/question/15255805038)
- 今年唯二的Strix Halo笔记本（另一个是幻X），同时可能是唯一的常规形态产品。
  - 这机器在海外的名称是Zbook Ultra G1a，实质上是EliteBook X G1a的复用模具但加厚+增强散热的版本。定位就是旗舰轻薄移动工作站的小尺寸版本，类似ThinkPad P1，但是做了14吋的版本。
  - CPU方面，除了和之前已经上市的幻X一样的AI MAX 390/395（分别对应16C+40CU/12C+32CU）之外，还多了一高一低两个新的配置，更低的AI MAX 385是8C+32CU的规格，同时还有个顶配的商用版AI MAX+ Pro 395，这颗CPU是惠普独占的。
- 其他地方就基本和原版的EliteBook一致了：
  - 2.8k 120hz的OLED触摸屏
  - 74.5wh电池，支持PD 3.1快充
  - 单硬盘位

- 这价格只能说是好家伙了，55w性能释放的常规形态机器，卖的比隔壁幻X的平板形态+80w性能的还贵... 如果一定买考虑，只推荐顶配版本，起码还是有独占CPU的

- 如果主要是冲着Strix Halo这颗CPU来的，那还是更推荐幻X，因为性价比更高，AI MAX+ 395+128/1T的版本也就20999，如果是32G的版本，就只要14999了。

- 为什么STX Halo这颗CPU看起来很好，但实际上无OEM愿意用？个人认为主要原因是两点：
  - 一个是这颗处理器原本应该在2024年和STX Point一起上，却因为种种原因延误到了今年，因此对标的对象也从原来的RTX 4060变成了RTX 5050，这时候STX Halo的核显性能实际上就没有太大的吸引力了
  - 另一点在于STX Halo产生的最大意义在于在空间尺寸受限的平台上做到尽可能高的性能，但现在ROG已经在14.0吋的轻薄平台上做出来BD1的卡了（RTX 5080 on 幻14 Air），幻X的上一代也早已经挑战过在13吋的平板上做H45+BD2显卡
- STX Halo这么个怪物CPU本身存在的意义就已经受到了严重的动摇了。继续往小尺寸做？那散热更完蛋，性能也跟着寄。继续往大尺寸做？做到15吋及以上的平台上，传统的CPU+GPU方案成本低，散热更好做，整体效果还更好
  - 不会有人真的觉得A卡游戏体验好吧，尤其是移动端A卡

- 最大的缺点还是价格。2.5万的价格，如果以他对标的7945HX+RTX 4060的笔记本电脑，都可以买3台了，选择R9000P也就8000多出头。那就看你愿不愿意为了轻薄付出溢价了。

- 惠普战99 Ultra，海外对应型号Zbook Ultra G1a，便携移动工作站定位。常规的配置例如全金属机身、2.8K OLED屏幕、惠普工作站的祖传接口3C+HDMI+1A、74.5Wh电池就不再赘述了。
  - 最大的亮点就是AMD锐龙AI MAX 300系列的处理器。AI MAX+ 395处理器是惠普独占，CPU规格和桌面端9950X相当，只不过受制于笔记本电脑散热不能完全发挥；说是集显，但是性能堪比RTX 4060的独显。
  - 同时另一大好处在于实现类似于苹果一样的128GB统一内存，最多可以分配96GB给显存。128GB内存版本为4通道，内存吞吐速度能够达到200GB/s出头，比常规的双通道100GB/s直接翻倍了
- 14寸的笔记本电脑，重量到了1.6kg，属于是偏重的。性能释放也就在55W，不能发挥出完整性能。作为移动工作站只有单硬盘位。

- 问题是性能释放还不如幻x，这个只有55w。

- ## [ROG幻X 2025对于运行AI应用（如本地大模型）有没有显著优势？能否助力用户提升AI体验？ - 知乎 _202503](https://www.zhihu.com/question/14670046303/answers/updated)
- 属于什么都能干，什么都干不好的富哥玩具。
  - 便携和离电性能比不过几千块的mac air。
  - 游戏性能比不过同价位的4080m（马上就5080m了）
  - 大模型被同价位mac studio m4打爆。

- 这玩意跑大模型的能力就是个3060 12g或者最多4060ti 16g的水平，反正12g～16g显存跑不起来的东西395也没速度了，而只要用显存能跑起来，速度绝对吊打395。
- 按我理解这玩意的意义是具备轻薄本少有的同时具备64g以上内存、不尿崩的续航、足够强的显卡以应付图片和视频编辑。然而在这个场合下，却被MacBook pro全方位爆杀。

- [独家首发AI Max+395处理器 ROG 幻X 2025跑分解禁，是否值得购买？ - 知乎 _202502](https://www.zhihu.com/question/12616419565)
- 最近我看到业内新出现了2种桌面级AI计算/PC类产品，一个是在NVIDIA GTC大会上正式发布的DGX Spark（芯片代号GB10），还有基于AMD Ryzen AI MAX PRO处理器的笔记本/移动工作站/台式机，都宣称能支持70B乃至更高参数的模型。
  - NVIDIA DGX Spark号称“最小的 AI 超级计算机”，它的处理器有点像微缩版的DGX计算系统（参考下图），在GB10单芯片上集成了Grace CPU——20个Arm Core，以及Blackwell架构的GPU。
  - AMD Ryzen AI MAX PRO系列（代号Stirx Halo），更接近传统集成显卡的x86 CPU，但整合GPU的性能却比较强。其默认TDP功耗55W，根据不同系统设计，cTDP可调功耗在45-120W范围。
  - 无论使用CPU还是GPU做AI计算，在LLM推理的Prefill（内容输入理解）阶段的瓶颈是算力；而在Decode输出时的性能（Token/s）则主要受制于内存带宽。
  - 我们看到上面2款产品都使用了256位LPDDR5x-8533内存（AMD的实际运行速率为8000），比传统AI PC的64位双通道内存提高了一倍，相当于4通道。
- 由于Grace ARM CPU只认证了DGX™ OS操作系统，应该只能跑Linux（不兼容Windows），所以DGX Spark主要就是用于计算，图形性能方面不知是否做了优化？
  - DGX Spark的AI性能，与GeForce RTX 5070桌面显卡较为接近。不过有一点，5070的显存带宽高达672GB/s，这一点即使是256bit LPDDR5x内存的集显也忘尘莫及。毕竟一块5070独显就是250W TGP功耗，其空间占用也很难做到Mini机箱/轻薄笔记本里面。

- ## [AI绘画（Stable Diffusion）用什么显卡比较好？ - 知乎](https://www.zhihu.com/question/638915747)

- ## [不计预算，帮我买一台性能超强，外观简约的笔记本电脑？用于本地部署各种大模型，暂定微软，还有别的选吗？ - 知乎](https://www.zhihu.com/question/624402976)
- Windows笔记本这边的性能天花板是 i9-13980HX/R9-7945HX +RTX4090（175W）
  - 这俩CPU性能差不多，7945HS便宜点
  - 推荐你选i9-13980HX，因为Intel对各种生产力工具的兼容性更好。
  - 移动端RTX4090的性能相当于桌面端的4070Ti，但是它有16G显存，比12G显存的4070Ti更适合跑模型。
  - 目前，搭载13980HS和RTX 4090笔记本的价格普遍在22000元以上。比如ROG枪神7和微星GP78HS
  - 同样的价格你可以买到i7 13700K和RTX4090 24G的台式机（惠普暗影精灵9plus）

- 当前笔记本端最强的4090笔记本显卡，规格上持平桌面4080，跑分性能上与桌面4070ti相当
  - 笔记本端次强的4080笔记本显卡，规格上接近于桌面4070ti，跑分性能与桌面3090相当

- 建议还是戴尔，惠普，联想这三家的工作站笔记本吧。工作站配置的独立图形显卡适合的要求的大量模型，再就是工作站的内部结构、配置都是针对处理大型3D模型做了加强、优化的，其工作稳定性是普通笔记本无法比拟的。

- ## [amd发布新的芯片，这次芯片跟苹果的m4一样，将gpu和npu集成到同一块芯片上去，说明了什么？ - 知乎 _202501](https://www.zhihu.com/question/9253691862)
  - AMD 在 CES 2025 上发布了锐龙 AI Max 300“Strix Halo”系列 APU，搭载了最高 40CU 的超强核显，此外集成了 50 TOPS“XDNA 2” NPU。
  - 这种将强力 CPU、GPU、NPU 等集成到一个芯片的做法，看起来有点像苹果在 M 系列芯片中的设计。

- 苏妈看到apple M系列芯片被拿来跑AI大模型推理的时候，也鼓捣了一个Ryzen AI Max 300 系列处理器规格曝光，最高16核心、40CU核显 。最高可以从内存中分配96GB超大显存，结合ROCm（开放计算平台）系统支持，或能变成新一代小型工作站神U

- ## [如何看待英伟达公司发布的桌面级AI超算? - 知乎](https://www.zhihu.com/question/8970511370)
- Project Digits 现在改名叫：NVIDIA DGX™ Spark 。
  - 其定位是：个人桌面端的 AI 超级计算机。
  - 由 GB10 超级芯片驱动
  - 使用 FP4，AI 性能达 1000 TOPS
  - 配备 NVIDIA Blackwell GPU，⽀持第五代 Tensor Core 技术
  - NVIDIA Grace CPU 实现，采用 20-core 高性能 Arm 架构
  - 128 GB 统一寻址系统内存
  - 支持高达 4 TB 的 NVMe 存储
  - 支持高达 200B 参数的大语言模型
  - 通过 NVIDIA Connect-X 网络进行连接，可连接两个 DGX Spark，支持高达 405B 参数的模型

- ## 🚀 [如何评价英伟达新发布的桌面 AI 超级电脑 Project Digits？ - 知乎 _202501](https://www.zhihu.com/question/8953765123)
- 上午还在看AMD strix halo，下午Nvidia突然就放了一个相同定位的...

- 想买的同学注意下这个设备的内存，它是统一内存，即CPU和GPU共享LPDDR5X. 它不是GDDR6，也不是HBM2的。
  - 虽然有 128GB，但是根据 Grace 架构 CPU 的 Product Brief，单 CPU 的内存带宽最大只有512GB/s
  - 所以如果用这个设备来运行大语言模型，瓶颈就会变成这个内存带宽。
  - 简单来讲，大语言模型每生成一个token，就需要将整个模型扫一遍进行计算（实际上比这个描述复杂很多）。这意味着，当浮点算力充裕的时候，扫描的速度就决定了生成文本的速度上限。
  - 目前这个设备的内存带宽水平跟 M4 Max 的 MacBook 没什么区别（Apple MacBook Pro M4 Max 128GB 内存带宽是546GB/s）
- 拿 [Llama-3.3-70b-instruct-4bit] 举例，这个4bit量化模型大小约为40GB，那么扫一遍就意味着GPU要处理40GB的数据，如果想要每秒钟生成10 token，简单计算可得，40GB\*10 = 400GB, 这意味着内存带宽至少有 400GB/s 才能保证每秒钟能生成 10 token.
  - 回到 digits 这个设备，在512GB/s 的情况下，**运行 70b-4bit 规模的模型，生成速度理论最大值是 512/40 = 12.8 token/s**

- nv版的mac mini。当然mac mini有macos，project digits只有linux，操作系统生态上以及桌面级定位上估计会导致拉胯掉，nv虽然有cuda生态，但是在桌面级相比os的生态，还是有点困难的。
- 大模型推理其实是个很微妙的产品需求，大显存容量很重要，memory bound也是事实。
  - 总体来讲，容量比带宽重要，毕竟容量决定了yes/no，带宽决定了token/s的体验。但是体验差到一定程度也是个yes/no的问题。
- 以ai pc今天事实上的超级应用为例ai编程而言，10 token/s以下基本就是玩票，加上ai啰哩啰嗦要分析一大堆，基本要做到可用还是得30～40 token/s
  - 按照激活量算，30～40token/s，如果10GB的激活量就300～400GB/s的内存带宽
  - dense模型10B上下做ai编程几乎没法用，moe可以搏一搏。
  - 目前的模型主流的基本都是70b以下的dense，以及200b以上的moe，70b以下的dense很尴尬，效果上比较难接近200b以上的moe，容量需求小，但带宽需求可是实打实的超级高。比较适配[gddr显存] 的正经游戏卡。

- GB10的芯片，应该是从服务器的GB100当中砍一刀下来用的。CPU是联发科定制的一颗ARM，GPU部分算力达到1个P，fp算力。最大的亮点直接拿了128GB的 LPDDR 5x内存做显存。
  - 很多人其实不知道，现在跑大模型无论是训练还是推理，最大的瓶颈是显存容量不足（Memory bound），而不是算力不足(Compute bound)。
  - 比如一个200B的模型，在fp4或者int 4的前提下，光是显存占用就要有100GB大小。运行起来之后还要有kv cache随着上下文长度占用而增大。一张显卡装不下，就要分布在多张卡上，那么就会产生通信开销从而导致算力无法被充分利用，不得不等待通信完成之后再进行计算。
  - 之前消费级的RTX 4090，最大的显存只有24GB。RTX 5090，也只有32GB显存而已。数据中心卡例如A100有40G和80G，但价格又会显著比消费级显卡贵。
  - 所以现在这个128GB的内存作为统一显存使用，至少解决了显存不够用的问题。3000美元的售价，甚至可以说良心了。

- CPU是联发科定制的一颗ARM，跑Linux玩毛游戏

- 其实前两年AI刚开始起头的时候，local LLM和文生图爱好者就发现多数任务都是memory bound的了，市面上除了比汽车还贵N家推理卡，只有Mac的统一内存能装的下。
- 这个具有可用性，cuda生态与性能是不用担心的，mac跑ai就是行为艺术，速度慢到爆炸，生态也是残缺的
- 只是为了 128G 统一内存现在也不需要买这玩意儿啊.. $4799 的 Mac Studio 或者 $4999 的 MacBook Pro 就行. 当然只是为了 AI 目的的话, 老黄这个盒子确实更有性价比.

- 三千刀，128GB，这个价格其实不比苹果好多少。而且三千刀是starting price

- 经典的72B的Llama模型，8比特精度需要84GB的显存，那就需要2块A100或者4块24GB的4090/3090，这两种方案都要比3000美元多且复杂。
  - 要知道Project digits是一整个机器，72B的经典模型可以直接跑，这样就基本上可以做绝大多数的微调工作。
  - 甚至两台机器，就可以跑4比特精度的200B模型，这么大的模型放到之前基本上只有大的公司或者实验室才有可能跑的起来，而现在6000美元就能完美解决，这对于绝大多数的穷Lab来说都是天大的福音。

- Strix Halo和M4 Max是高规格的消费级产品，搭载Strix Halo的是各种品牌开发的笔记本电脑、SFF主机，甚至是准系统、MoDT主板，它们搭载Windows 11操作系统，也可以安装任意GNU/Linux操作系统，就像一个普通的x86电脑一样
  - Project Digits的CPU，是联发科定制的20核ARM CPU，10大核为Cortex-X925，10中核为Cortex-A725。A725和M4小核大性能接近，而X925则不如M4大核和Zen5。另一方面，ARM Linux的生态并不如x86，这和ARM macOS完全不同。Strix Halo的16核32线程的zen5 CPU，基本上是顶级的CPU配置，其多线程性能是拉满的。
  - Project Digits的内存，虽然官方没有说明，但很大可能也是256位的，此时Project Digits在使用LPDDR5X的8533MT/s的内存时，其速率同样为272GB/s，而这一带宽，仅与Strix Halo/M4 Pro相同，是M4 Max的一半；如果是512位，则强于Strix Halo一倍，并与M4 Max持平。
- halo的npu总共就50tops，8060s也没有硬件wmma，2.5ghz下也就51.2tops，加起来不到gb10的1/4
- 可惜halo不是统一内存架构
  - halo是uma架构，15年前初代apu就已经是uma架构了

- 这个东西3000刀是真的很贵，不过project digits是挑战冯诺依曼架构，CPU 访问内存、硬盘，显卡处理数据需要把数据先传到显存。统一内存架构就是把GPU核心直接与内存相连，弄大内存。
  - 目前除了大公司有钱买几百上千卡跑训练，普通人真跑不起LLM大模型。对于普通人来说，核心算力不重要，问题是怎么在显卡load大模型。而统一内存就是用超高性价比的内存代替显存，不用GDDR7，用DDR5 。
# discuss-gpu
- tips-gpu
  - 使用场景的要求: 🤔 文多还是图多, 显存大(VRAM), 速度快(带宽)
    - 大显存的方案: mac, amd-ai-max, nvidia, intel-arc
  - 显存、带宽、位宽, 显存够大才能运行模型，运行模型时的速度主要考虑内存带宽
  - 估算文本模型速度用 `内存带宽如260GBpSec / 模型实际体积如13GB`, 还要考虑context的影响
    - MoE模型对内存带宽的要求会低很多
    - 文本大模型的免费api更容易获取
  - ❓ 文生图的场景是否也用此公式计算, 
    - 注意文生图能通过lora加速，所以内存带宽重要性降低
    - 文生图时经常需要VLM视觉模型辅助，所以VRAM越大越好
    - 常见文生图模型的大小在20GB左右，大显存的单卡也可运行
  - 考虑软件支持度, comfyui在arm/linux平台的支持度, dgx-spark默认linux
  - 计算集群: nvlink
  - 主力工具不要用AMD的CPU/GPU, 因为linux需要特殊配置, 部分软件也需要特殊配置如pytorch
  - 模型选择: 支持int4、fp8、fp4，能否用nanchaku加速, 支持flash-attention、bf16、awq、sglang

- 多显卡
  - 多显卡的机箱太大，不便携, 配置和维护成本高，不如用主流台式机箱或某宝成品可定制工作站
  - 多显卡/多硬盘都需要大体积的机箱，itx一般不能满足，需要使用matx
  - 配置nvlink需要单独的bridge连接线
  - 选显卡要注意尺寸，公版一般尺寸适中，其他场景的定制显卡有的长宽比较大，如三风扇卡会明显大雨涡轮卡

- 可考虑用大容量单卡如4090-48gb配合20L以下的小机箱, 
  - 4090公版尺寸304x137mm, 涡轮版267x111x38mm, ⚡️ 三风扇版350x140x53mm
  - 可考虑itx机箱包括, 机械大师c28(18)/cmax(20), 闪鳞g300(17)/g350(20), 乔思伯z20(20)
  - 机箱散热要注意cpu功耗和显卡功耗，可选用低功耗cpu+高功耗显卡，双塔风冷
  - 暂时选择机械大师cmax(392*185*284mm, 20.5L), 因为能支持较长的三风扇显卡，显卡支持 385*160mm 以内
    - 乔思伯, 公开资料最多, t6(13.6L), tk-o(16.45L),c6(18.4L), z20(20.2L)
  - 想要128GB的内存，机箱的空间够大且满足散热需求是前提，还需要主板提供4个内存插槽，cpu的和内存的频率是能和谐工作，频率都不能太高
    - 内存条的频率要考虑cpu支持、主板支持
    - 大内存对跑MoE模型有用

- nvidia性能对比
  - [大模型GPU算力卡汇总 - 知乎](https://zhuanlan.zhihu.com/p/1904206218748236301)
  - [Sable Diffusion WebUI Benchmark Data: nvidia/amd/torch](https://vladmandic.github.io/sd-extension-system-info/pages/benchmark.html)
  - [Which GPU should I buy for ComfyUI · comfyanonymous/ComfyUI Wiki](https://github.com/comfyanonymous/ComfyUI/wiki/Which-GPU-should-I-buy-for-ComfyUI)
  - https://www.zhihu.com/question/615946801/answer/3156016610

- 🆚🔥 [英伟达热门 GPU 对比：H100、A6000、L40S、A100 - 知乎](https://zhuanlan.zhihu.com/p/5041686924)
  - [Memory Bandwidth Comparisons - Planning Ahead : r/LocalLLaMA _202402](https://www.reddit.com/r/LocalLLaMA/comments/1amepgy/memory_bandwidth_comparisons_planning_ahead/)

- 参数对比
  - gpu-arch: 2020-ampere(a100/a6000), 2022-ada-Lovelace(L20/L40s/6000ada/4090/4090d), 2022-hopper(h100), 2024-blackwell(5090)
  - fp16/tflops
  - 3090是最后一代支持nvlink的消费级显卡，低端专业级显卡如5880/L20也不支持nvlink

- gpu-specs

- Memory Bandwidth:
  - Nvidia DGX Spark: 273 GB/s
  - AMD AI Max 300: 256GB/s
  - M1(68)/M2/M3: 100 GB/s
  - M4: 120 GB/, 10-cpu, 10-gpu, 16-neural
  - M1/M2/M3 Pro: 150/200 GB/s
  - M1/M2/M3 Max: 300/400 GB/s
  - M4 Max: 546 GB/s
  - M1/M2/M3 Ultra: 819 GB/s
  - RTX 5090: ~1.8 TB/s
  - RTX PRO 6000 Blackwell: ~1.8 TB/s

```markdown
| GPU    | VRAM       | fp16 | v-bandwidth | v-bit | cu-core | power | note                          |
|--------|------------|------|-------------|-------|---------|-------|-------------------------------|
| A100   | 40GB HBM2  | 312  | 2039gb/s    | ?     | ?       | 400W  | 40g-9w                        |
| A6000  | 48GB GDDR6 | 77   | 768gb/s     | ?     | ?       | 300W  | 48g-3w3                       |
| 6000ad | 48GB GDDR? | ?    | 960/s       | ?     | ?       | ?     | 48g-4.8w                      |
| PR6000 | 96GB GDDR7 | ?    | 1792/s      | ?     | ?       | 600W  | 96g-6.6w                      |
| L20    | 48GB GDDR6 | 119  | 854gb/s     | 384   | 1.02w   | 350W  | no-nvlink                     |
| L40    | 48GB GDDR6 | 147  | ?           | ?     | ?       | 350W  | ee                            |
| L40s   | 48GB GDDR6 | 731  | 864gb/s     | ?     | ?       | 350W  | 48g-4.4w                      |
| 5880ad | 48GB GDDR6 | 69   | 960/s       | 384   | 1.28w   | 285w  | 48g-2.8w,no-nvlik,6000Ad阉割  |
| 5000ad | 32GB GDDR6 | 65   | 576/s       | 256   | 1.41w   | 250w  | no-nvlink                     |
| 5090   | 32GB GDDR7 | 3352 | 1800gb/s    | 512   | 2.18w   | 450W  | 32g-2.3w, no-nvlink           |
| 4090   | 24GB GDDR6X| 330  | 1008gb/s    | 384   | 1.64w   | 450W  | 48g-2.4w🌹, no-nvlink, 850wP  |
| 4090d  | 24GB GDDR6X| 330  | 1008gb/s    | 384   | 1.46w   | 425W  | 48g-1.9w,频率锁且不超频         |
| 3090   | 24GB GDDR6X| ?    | 912gb/s     | 384   | 1.05w   | 350W  | 24g-8.3k, nvlk/VulkRT/OpenGL4 |
| 3090ti | 24GB GDDR6X| ?    | ?gb/s       | 384   | 1.08w   | 750W  | 24g-8.3k, nvlink              |
```

- ## 

- ## 

- ## 

- ## [rtx 4090 vs rtx 5090 vs rtx 4090 48gb vram? : r/StableDiffusion _202505](https://www.reddit.com/r/StableDiffusion/comments/1km4snx/rtx_4090_vs_rtx_5090_vs_rtx_4090_48gb_vram/)
- across various benchmarks rtx5090 is around 30% faster than rtx4090 in terms of "raw power". It also has native FP4 support, which can increase speed by ~x2 and reduce memory usage ~x2 for use-cases where fp4 models available.
  - So, choice depends really on your use-case.
  - If you run repetitive tasks and it fits into rtx5090 32GB VRAM, especially in fp4 format, then, this is the way.
  - as an all rounder and for different experiments, rtx4090 with 48GB can be more interesting.

- If you have a workflow with multiples models, vram is king as it allows you to avoid loading / unloading models. It's the slowest part of the process in generation. With enough vram, you can keep all models loaded, gaining a tremendous amount of time each generation.
  - Example: generating with model A which has very good prompt understanding but shit styles, glossy skin etc...then inpainting / upscaling with model B which has better style and so on.

- ## 🆚 [RTX 5090 vs RTX 4090 48GB (or RTX 6000) : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mo92ou/rtx_5090_vs_rtx_4090_48gb_or_rtx_6000/)
- The jump from 32GB to 48GB is a big jump, but the question is whether it's actually relevant today. 
  - With 32GB, you can run 24B, 27B, and 30B MoE at 8 bit and blazing fast speeds. You can run 32B at Q6, and even 49B at Q4. You can run basically any diffusion model as fast as possible with no limits at 8 bit.
  - 48GB lets you run the same models with higher context length, or lets you run 70B at Q4KM. The problem with this however, is that the 70B class of models has been stagnant for a very long time, so it's questionable if there's actually any benefit of running models in that size class right now. Any model larger than that is generally an MoE and will need partial offloading to RAM anyway. For example, if running GLM Air 106B, there's not going to be a massive speed difference between 32-48GB, because the bottleneck is the RAM.
  - The RTX 6000 Pro 96GB is an amazing card, but it generally has the same problem as the 48GB. With 96GB, that opens up the path for 70B and 100B, but not really any larger. You still won't be able to run 200B+ without partial offloading, which means that again, RAM is the bottleneck.

- One question I have is the context length. Using this calculator, for a 32B model at Q8, I'd have less than 4K context even with the KV cache at Q8. So if I'm adding files of any significant length to the context, then that also skews for more VRAM? Or can the context also be offloaded to RAM?
  - Yes, context does take up VRAM, and VRAM consumption scales linearly in proportion to context length. The exact amount of VRAM for a certain amount of context varies based on the size of the model, and architectural choices like Sliding Window Attention and Grouped Query Attention. Hence, the exact memory usage footprint is unique to the exact model, so that website is likely not perfectly accurate. That said, the context length takes up VRAM whether you use it or not, and does not increase automatically if you exceed it.
  - You can offload context to RAM, but that will cause a massive slowdown overall, and I can't really recommend it except for very specific use cases.

- If you're running multiple AI applications simultaneously, then yeah probably.

- FLUX for image generation needs something like 12GB.
  - WAN2.2 for video generation can be ran on as little as 8GB.
  - Qwen3-30B at Q4 requires 18.6GB, the Q8 is 36GB.
  - Blender for 3D assets recommends >8GB.
  - Photoshop recommends >4GB.

- ## [单卡双芯48GB 铭瑄Arc Pro B60 Dual Turbo专业卡规格详解 - 知乎 _202505](https://zhuanlan.zhihu.com/p/1908505488590636946)
- 第二代锐炫Pro专业卡，同样基于“BattleMage战斗法师”架构，有两款产品
  - Pro B50 16GB，16组Xe核心，2048sp，128bit显存带宽，TBP整卡功耗70w
  - Pro B60 24GB，20组Xe核心，2560sp，192bit显存带宽，TBP整卡功耗120-200w

- B580 24GB的传闻已久，不算新鲜；最出人意料的是铭瑄，拿出来的Pro B60产品是单卡双芯
  - 双DP 2.1 + 双HDMI 2.1a满血输出，均可支持8K 60hz或4K 240hz；由上至下，分属于两个不同的GPU
  - 这张显卡并未集成PCIe桥接芯片，PCIe Gen5 x8 + PCIe Gen5 x8，需要主板本身支持PCIe x16通道拆分。
  - 所以电脑设备管理器内显示，应该显示为两张Pro B60 24GB，而非单张Pro B60 48GB，只有通过相应程序，才能实现GPU、显存统一调用，料想不会有驱动程序层面的特殊支持，和其他品牌的单卡单芯无异。

- 这么说就是两个显卡，插在一个插槽，并且两个卡都是pcie5.08 这样每个卡的带宽都砍掉了pcie 5.0 *16 的一半。 这种东西有鬼用啊。还不如插两个显卡
  - 1，现在的GPU性能跑不满带宽，包括5090用PCIE x8都没问题；
  - 2，AI模型推理其实对GPU性能要求不高，主要是卡显存。
  - 3，B580原生Gen4 x8，B60升到Gen5 x8，两GPU集成到一块儿并没阉割带宽。

- 后续所有的锐炫中高端家用显卡都会复刻，Q3发布的B770 16GB当然也会有相应的32GB专业卡；连AMD也表示会跟进产品。

- 牙膏要是有类似nvlink的东西那价值还能上升，毕竟甭管训练还是推理，瓶颈都在这个pcie 5.0x16的64GB/s 上。

- 48G显存玩SD画图不得爽死。话说现在SD对intel的支持咋样了？能玩不？
  - 不怎么行，只能pf16否则加lora就会黑图，训练目前我还没跑通过。而且双芯卡实际是两张卡，常见的部署框架comfyUI不支持多卡

- ## [显卡日报10月8日｜笔电RTX5090显存容量24G？ - 知乎 _202410](https://zhuanlan.zhihu.com/p/853223258)

- 整理下笔记本RTX5090的可能参数，可以看到，和台式机的RTX5090差距还是比较远的，连台式机显卡一半的规格都没有，可能是有史以来笔记本和台式机差距最大的一张旗舰显卡
  - 此外，笔记本RTX5090相比于上代笔记本RTX4090进步也不大，最大的进步可能就是换了镁光GDDR7显存，显存带宽明显提升

- CPU能加个12400不带K不？
  - 已经不带K了

- [笔记本电脑5090和5080游戏差距较小｜显卡日报4月3日 - 知乎](https://zhuanlan.zhihu.com/p/1890877968840119676)
  - 根据VideoCardZ报道，最近著名硬件媒体Notebook Check测试了相同配置的RTX5090笔记本和RTX5080笔记本，这两台笔记本用了相同的模具，显卡功耗上限都是175瓦，都搭配锐龙9955HX CPU，据说这款XMG是机械革命的海外贴牌厂。
  - 在《赛博朋克2077》中，4K分辨率下两款笔记本的平均帧率差距仅为12%，远远小于两者的价格差距
  - 根据VideoCardZ统计，目前北美市面上最便宜的RTX5090笔记本比最便宜的RTX5080笔记本还贵了72%，所以在游戏差距这么小的情况下，毫无疑问RTX5080笔记本更值得入手。
- 多8g显存利好生产力，游戏性能的话功率给不足的情况也不大好提升

- ## [当前4090笔记本显存为什么是16g？ - 知乎](https://www.zhihu.com/question/599743802)
- 移动版4090的本质: 用了桌面4090的名字，使用了桌面4080的规格，跑出了桌面4070 Ti的性能

- 移动版的4090硬件参数基本上就是台式机4080
  - 台式机的4080最高功耗在400w左右（不同品牌型号略有差异），移动版的4090最高功耗限制在175w左右（不同散热规模笔记本可能略有差异）

- 256Bit的显存位宽咋上24GB？

- ## [笔记本端RTX4090和RTX4080差距到底有多大？ - 知乎](https://www.zhihu.com/question/581865855)
- 2024年8月更新，以华硕ROG枪神8 超竞版系列作为参考
  - 从各项跑分中可以看到RTX4090相比RTX4080，高出10-16%
  - RTX 4090 Laptop拥有完整的256位带宽，RTX 4080 Laptop其显存位宽降低到192

- ## [笔记本的4090显卡相当于台式机的什么显卡？ - 知乎](https://www.zhihu.com/question/594215351)
- 该显卡用了台式机RTX 4090的名字，硬件规格上和台式机RTX 4080相似，性能上和台式机RTX 4070 TI相近。
  - 硬件规格上，笔记本RTX 4090 Mobile使用AD103核心，采用台积电N4制程工艺，搭载9728个CUDA单元，16 GB GDDR6显存，显存位宽为256 bit；
  - 同样的，台式机RTX 4080也使用AD103核心，采用台积电N4制程工艺，搭载9728个CUDA单元，16 GB GDDR6X显存，显存位宽为256 bit。与笔记本RTX 4090 Mobile在硬件规格上代的区别就是一个是GDDR6一个是GDDR6X显存。
- 但是，笔记本RTX 4090 Mobile的性能还是远不如台式机RTX 4080，主要因为功耗喂不饱。台式机RTX 4080的TDP为320 W，对应的Boost频率为2505 MHz，而笔记本被锁了功耗墙，不同功耗限制下RTX 4090的Boost频率如下：
  - RTX 4090 Mobile 120W = 1335-1695 MHz
  - RTX 4090 Mobile 130W = 1425-1815 MHz
  - RTX 4090 Mobile 140W = 1500-1950 MHz
  - RTX 4090 Mobile 150W = 1620-2040 MHz
  - RTX 4090 Mobile 175W = 2040-2040 MHz
- 因此，最后满血的RTX 4090单精度浮点算力大概是39.69 TFLOPS的水平，和台式机RTX 4070 TI的40.09 TFLOPS差不多，未来要是RTX 4090 Mobile解锁200 W功耗墙可能有希望超过台式机RTX 4070 TI

- ## [芯动科技发布「风华 3 号」全功能 GPU，该产品都有哪些亮眼性能？ - 知乎 _20250923](https://www.zhihu.com/question/1953762444674565105)
  - 芯动科技“风华 3 号”全功能 GPU 昨日在珠海香山会议中心正式发布，其采用全国产底层设计，同时拥有 AI 智算算力和 8K 重度渲染算力，兼容 DirectX12、Vulkan1.2 等图形接口，并适配统信、Windows 等系统。
  - 芯动科技“风华 3 号”全功能 GPU 在行业内率先实现国产开源 RISC-V CPU 与 CUDA 兼容 GPU 的深度融合，可一站式覆盖大模型训推、垂类多模态应用、科学计算与重度图形渲染
  - 在生态方面，“风华 3 号”在软件端兼容 PyTorch、CUDA、Triton、OpenCL 等主流 AI 和计算生态、和 DirectX、OpenGL、VulKan 等渲染生态，适配统信、麒麟、Windows、Android 等操作系统。
  - 在大模型方面，“风华 3 号”是国内首款单卡配备 112GB + 大容量高带宽显存和自研 IP 的全功能 GPU，突破目前国产 GPU 显存和多卡搬运的上限，单卡可支持多用户 32B / 72B 大模型；单机八卡能直驱 DeepSeek 671B / 685B 满血版大模型。

- [GPU Fenghua No.3, 112GB HBM, DX12, Vulcan 1.2, Claims to Support CUDA : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1noru3p/gpu_fenghua_no3_112gb_hbm_dx12_vulcan_12_claims/)

- ## [如何使用intel的gpu和npu跑大模型？ - 知乎](https://www.zhihu.com/question/5293002844)
- 去下载ollama intel版，运行，然后就能调用核显跑模型了，但32g本子核显只能调用20G

- 目前比较适合半小白的办法，就是下载intel的ollama portable zip
  - 以上这俩应该都是基于llama.cpp的sycl后端优化的。
  - 其实在NPU或GPU上运行大语言模型还可以用OpenVINO。但不是很适合小白，OpenVINO更适合有一定开发能力的用户。

- 你需要用openvino，推理的时候指定推理的设备npu，gpu还是cpu，就可以把推理运行到你想要的设备上。你可以到openvino的notebook找到很多例子。

- intel自带的gpu不是普通显卡，不能用gpu这个选项，

- ## [3090 相当于 40 系的什么显卡呢？这款产品的优缺点分别是什么呢？ - 知乎](https://www.zhihu.com/question/681021828)
- 3090资深用户，首发购入一直用到4090首发, 
  - 3090理论跑分性能等于4070tis super。
  - 但是3090显存更大，在ai方面性能更强，可以跑一些量化大模型，4090能跑的3090都能跑。
  - 3090的<能耗比>比4090差很多，因为3090是三星8nm工艺，相对台积电5nm差距非常大。相同功耗下4090性能超3090 60%。
  - 3090功耗上限非常高，最高功耗产品为evga 1000瓦水冷款，从400瓦到1000瓦性能提升10%。由此可见能耗比之差。

- AI算力来说，cude算力表现，跟RTX4070差不多；

- ## [3090 24g和3090ti 24g从哪能买到新的? - 知乎](https://www.zhihu.com/question/585256727)
- 3090/ti基本上不会有新的了，ti还好点，但是货本来也不多，标新的店价格都高，甚至高过4080钙中钙
  - 你的情况，要么4080，要么4090，要么二手3090/ti将就点，4070ti的带宽太低了，显存12G其实也能用。
  - 其实3070、3080/ti也都是可以生产力、渲染的。非特殊场景不会随随便便爆显存的。只是4070ti的显存低、带宽低了不说，cuda核心较3090少了30%

- ## [2x4090 vs 6000 ada vs L20 vs L40s: what is the bottleneck for llm inference/finetuning? : r/LocalLLaMA _202408](https://www.reddit.com/r/LocalLLaMA/comments/1exwc04/2x4090_vs_6000_ada_vs_l20_vs_l40s_what_is_the/)
- a 6000 > 4090, less power, no moving data between PCI bus/CPU/cache to GPUs. 1 48gb will often beat 2 24gb for fine tuning.

- The l40s is essentially the 4090 with more vram. It is exactly the same chip. Nvidia released the l40s because the 4090 chips were available and demand on H100 was too high / H100 production volume too low. The 4090 has transformer engine which boosts fp8 performance. If you want inference speed on fp8 and don't need advance nccl support I would go for the 4090s

- ## [网上传的沸沸扬扬的96GB显存的4090魔改版是真实存在的么？是怎么做到的呢，有人用过么？ - 知乎 _202502](https://www.zhihu.com/question/13164350111)
- 因为有全新L20 48G（可近似当作4090D 48G）在2.5万的价位压着，实际上现在4090的价格已经涨无可涨。
  - 👀 L20不支持nvlink

- 以上这些东西你都解决了，那你准备怎么过VBIOS检测？

- ## [2w左右的预算炼丹，2张3090矿卡还是一张4090? - 知乎](https://www.zhihu.com/question/592038342)
- 两年前帮同学装过双路RTX 3090炼丹炉，个人建议还是用单卡RTX 4090。
  - 双卡RTX 3090矿渣不容易安装，市面上开放式散热的RTX 3090的厚度普遍接近3-slot，至少要预留1-slot的散热空间，也就是说每张显卡要占用4-slot，目前消费级主板常见的PCIe区域宽度是7-slot，安装两张RTX 3090有点困难。而且桥接器NVLink Bridge的常见规格是2-slot或者3-slot，和上面说的4-slot占用空间有矛盾
  - 所以，要组双卡RTX 3090优先考虑涡轮版，一般厚度只有2-slot，体积更小，但是RTX 3090涡轮版矿渣的价格比非公版贵了1000元，再加上NVLink Bridge的价格，即便你捡矿渣，两张涡轮RTX 3090的成本也超过单张全新RTX 4090了。
  - 另外，双卡RTX 3090矿渣需要的额定功率更大。单卡RTX 4090的TDP是450 W，双卡RTX 3090的TDP是700 W，电源成本也是要考虑的，而且大功率电源满载的时候挺吵。

- 两张3090矿卡+nvlink (相对于pcie，大模型DP TP 2-3倍速度提升)
  - 性能对比（gpt2 1.3B 8batch 2TP，测试环境alpa/jax）
  - TP模式：结论带nvlink在148个all reduce的条件下，可以提升200%的速度 12.95s vs 38.53
  - DP模式：结论带nvlink的条件下，可以提升150%的速度 12.03s vs 31.15s

- 2张3090要组建带nvlink的平台比你想像的还要麻烦，和单卡4090比较一下
  - 主板: 双卡3090需要一个至少atx的带双PCIEx16插槽的主板，而且两个显卡插槽距离要是标准的3槽间距才行，而显卡厂商不会给你说这些数据很麻烦，很容易买错，4090就一般的matx就行，主板价格就能便宜几百
  - 电源: 双卡3090一般要买1400w左右的电源，4090一般850w就很够了，价格估计差价300到500左右
  - 显卡: 要组双卡考虑到nvlink一般最大3槽，只能选涡轮版或水冷版，比一般3风扇风冷单卡贵2000以上
  - 机箱: 双卡3090一般要支持atx的大机箱才能保证扇热, 4090一般的matx机箱就行，差价一般不会特别大
- 另外nvlink也要1500左右
  - 价格: 实际上要比单卡4090的价格贵大概4000以上
  - 功耗: 双卡3090烤鸡功耗估计要超过800w要远远大于单卡4090的450w，整机双烤估计要突破1000w
  - 质保: 双卡3090都是矿卡无质保，能用多久看脸，而4090全新带质保
  - 保值:30系默认矿，30系已经臭大街了，以后如果用不上了要出掉，30系不好出手，而40系名声好要比30系保值多了

- 两张3090都没有一张4090贵，一张4090是一张3090的三倍有多。

- 4090没有nvlink，大显存还是选3090ti多卡

- 有无nvlink到底有多大区别
  - 模型大小不超过24g 区别不大。 2块4090一般相当于2倍的batch size, Nvlink解决的主要是——能不能的问题。 比如模型大小超24g时
- nvlink只是增大带宽，并不会增大显存，如果你不人为把模型写到两张卡上，每个iteration两张卡各跑个的，然后梯度回传的时候两张卡要同步梯度，有了nvlink这个过程会更快。如果24g不够，你不人为写到两张卡上，两张卡也跑不了。

- 3090只能双卡下nvlink，没有nvlink的卡或跨两组用nvlink连接的卡会走pcie到cpu进行数据交互。你用的pytorch会用nccl自动选择合适的方式交互。

- 关于“3090只能双卡下nvlink”，这个资料您是从来看到的？
  - 当然所有PCIE GPU应该都受这个限制，唯一例外可能是DGX station上的四块A100
  - 3090只有双卡的bridge

- ## [NVIDIA Tesla V100 SXM2 - 2025年AI本地部署性价比之王！双卡V100！32G显存，低价高能，碾压2080Ti 22G - 知乎](https://zhuanlan.zhihu.com/p/1927666998030078159)
- 年初16GB显存的V100售价1000元，如今已降至600元，远超Mai50成为性价比之王。其算力可与1500元左右的RTX 2080 Ti 22G相媲美。
  - 唯一的门槛在于其接口设计：V100采用服务器专用的SXM2接口，而非常见的PCIe x16金手指。SXM2接口一侧为PCIe，另一侧为NVLink，支持多卡互联。
  - 通过200元左右的转接板和80元的散热器改装，仅需900元即可获得2080 Ti级别的性能。
- 单张V100显卡的显存为16GB，双卡配置可达32GB。目前消费级显卡中仅有RTX 5090具备32GB显存，但价格高达2万元起。相比之下，双V100方案仅需2000余元，使得22G显存的2080Ti瞬间失去性价比优势。

- 测试平台采用华擎B650M主板，因其仅有一个PCIe插槽，需在BIOS中启用通道拆分功能以支持双显卡。安装完成后，我们将对双V100系统进行性能测试。
  - 通过NVIDIA SMI工具检测NVLINK状态，可见GPU0与GPU1的链接状态均显示相同带宽值，表明两片GPU已成功互联且NVLINK功能正常启用。
  - 在参数相同的情况下，2080Ti的每次迭代耗时约为2.17秒，而V100的每次迭代耗时约为1.39秒。

- V100不支持flash- attention、bf16、awq、sglang新版本也不支持v100了，显卡太老了，新的加速策略用不了，速度还不如消费卡呢。
  - 对SD跑图来说，不支持int4、fp8、fp4，不能用nanchaku加速。SD也不能多卡显存叠加。

- 本质上还是单卡16G，没什么卵用，而且很多推理特性不支持，远远没有2080ti好

- ## [主要做生成模型，自用深度学习服务器买双卡3090还是买单卡4090？ - 知乎](https://www.zhihu.com/question/9062530414)

- 对于低算力场景，最大的性能瓶颈永远是浮点性能，因为这会决定你的训练时长。
  - RTX3090的浮点性能（FP32/FP16）35.6TFLOPS，RTX4090则高达82.6TFLOPS，几乎高出了3090一倍，显存带宽也略高，同时TensorCore的版本也更新支持更多数据格式。
  - RTX3090看起来有NV LINK，但只是个阉割版，用户确实可以在代码中将整个模型分配在两张卡上（需要完全手动实现），但是其带宽只有112.5Gbps，不到显存带宽的1/8，训练时会出现严重的性能瓶颈。买2张RTX3090 24GB并使用NVLINK桥连接并不意味着你能将其当作一张48GB显存的卡使用。
  - 另外RTX3090是挖矿重灾区，甚至曾经有一段时间单卡日收入能到120RMB，很容易买到矿卡。

- 双卡算力是逼近单卡的，虽然有代差，熟悉双卡后，对多卡也有了经验，很容易扩展到多机多卡。生成模型的训练离不开多机多卡。

- 3090ti没有性价比，双卡3090加nvlink也不错的记得内存配尽量大。4090不支持nvlink的，训练不推荐。

- 2080ti 22gb也是香的不行。追求价格，就买4张2080ti 22gb的更爽。不过不支持flash attention和bf16数据格式。

- 如果是训练为主，毫无疑问选3090x2。
  - 如果是推理为主，且显存占用超过24G，选3090x2；显存占用小于24G，选4090。

- 双卡吧，怎么也得让自己的代码支持一下ddp，不然以后卡多了还得重新踩坑。

- 去年我给某高校实验室攒机那会儿，恰巧把两套配置都折腾过。
- 先说双3090这茬儿，乍看显存怼到48G挺唬人，但您知道跑Stable Diffusion时显存带宽被PCIE通道卡脖子的滋味吗？
  - 上个月拿双卡跑1280x720图生视频，显存倒是富裕得能养鱼，可实际吞吐量比单卡就多出23%，GPU利用率曲线跟过山车似的…
- 不过您要是搞大语言模型微调，单张4090的24G确实容易把裤子卡到脚脖子。可别听论坛那帮敲锣边儿的瞎忽悠，实测用QLoRA技术能把70亿参数模型压进18G显存，这节骨眼儿上4090的第三代Tensor Core直接让训练速度飞起～
- 前些天给某美院动画系那帮哥们装机器，他们拿双3090跑渲染以为捡着宝，结果电源三天两头跳闸——整机850W电源满载时滋啦作响的动静，跟二踢脚在机箱里开party似的
- 要是铁了心玩分布式训练且预算绷得住电费，双卡能省下17%的迭代时间；可要是就图个痛快跑单卡大模型，4090那9%的FP32性能提升配上Dlss3技术，渲染输出时绝对能让您体验什么叫 原地起飞

- NVLink桥接器可实现相邻两张3090的并联，双向带宽最高600 Gbit/s（约75 GB/s），但仅支持两卡间点对点通信，无法实现多卡全互联（如GPU0与GPU1互联后，GPU1与GPU2仍需通过PCIe通信）；
  - 可买专用NVLink桥接器（如华硕ROG NVLink Bridge）并确保主板支持；同时需在Linux系统中启用TCC模式（关闭显示输出），并依赖第三方显存管理工具（如NVIDIA MPS）。

- ## [已有3090Ti一张，再增加一张卡，是选择增加3090Ti+NVLINK，还是增加4090？ - 知乎](https://www.zhihu.com/question/623563385)
- 3090.nvlink, 提前学习一下双卡怎么用，未来大项目都是多卡，纯粹的单卡做不了什么特别大的项目，纵使H100也是大把人用8卡。

- 炼丹是自娱自乐还是将来想拿来谋生？谋生的话大显存多卡任务协同是必修课，一张卡怎么学？

- 3090Ti的Nvlink和专业版的不一样，带宽减半, 也没有显存真正池化

- ## [4090不支持nvlink，在训练深度学习模型时具体的表现是什么？ - 知乎](https://www.zhihu.com/question/603522002)
- 没有[NVLink] 功能只是不能通过桥接器进行两两互联(即不能通过NVLink两两进行p2p)，但是对于多卡并行训练是支持的，可以通过device to host to device这种走[PCIe链路] 进行数据并行处理.
  - nvlink最高可以做到800GB/S，PCIE只有128GB/S
- pcie慢很多而且受制于主板

- ## 👷 [4090 48G涡轮版深度体验 - 知乎 _202502](https://zhuanlan.zhihu.com/p/22691935175)
  - ComfyUI跑flux fp16模型，图像宽高1000*1000，step20，跑完22秒左右，显存占用36G，大显存就是好。

- 能不能测一下与双3090 NVlink的差距
  - 看拼几块3090吧，你要2块肯定比不过4090 48G，NVLINk拼显卡还是有损耗的

- 现在有4090 48g三风扇版不吵了
- 一直正常用，就是显卡温度一直上90度，暴力风扇转速没法调小

- 改完还能打游戏吗
  - 可以正常打游戏

- 哪有靠谱销售渠道啊？
  - 想玩魔改 哪会有售后啊 不都是自己玩吗

- 性价比最高的是2080ti 22g，不过要换电源

- 这种售后怎么搞呢
  - 我是拿自己的4090去改的，花了5千5， 店家说有三年质保哎

- b站修电脑的张哥能修，只要不是核心坏了都能修

- [魔改的RTX 4090 48G卡值得选吗？ - 知乎](https://zhuanlan.zhihu.com/p/1888965700464406937)
  - 这卡我买了一张用了一个月了，分了540gddr5内存跑了满血版q2半个月了，没啥问题，基本在15到20tokens

- [技嘉RTX4090涡轮卡太吵怎么办？改这个水冷 - 小红书](https://www.xiaohongshu.com/explore/6713b9e70000000026037edd?xsec_token=AB3p2aSFwZpNJgLEWUwjbY4ZRQfalBGT1nX8ghdKLP2ls=&xsec_source=pc_search&source=unknown)
  - 涡轮卡，都放在服务器里面用的，如过你想放到PC机箱里面使用，那要做的生理克服首先就是噪音
- 那你把转速调低呗
  - 涡轮版本身散热就不如风扇的，调低有风险啊

- ## [丐卡值不值得买? - 知乎](https://www.zhihu.com/question/596939079)
- 要看具体型号。 举个例子，RTX 40系首发RTX 4090、RTX 4080、RTX 4070 TI三款型号，散热模具都超规格了，即便是丐版型号都用料十足，所以一二线品牌的丐卡是最值得买的
  - 但是，从RTX 4070开始情况发生了变化，“RTX 40系显卡散热过剩”的规律也被打破了。市面上的丐版RTX 4070普遍采用低廉的悬臂式散热方案，这种散热不能完全覆盖供电区域，容易导致VRM的个别地方过热。

- 当时初衷也是想看看丐版究竟够不够用，高阶版本拥有更帅气的外观（材质），更好的散热（风扇、热管的数量等）、更好的超频性能（benchmark可能还能看，具体到游戏帧数只能说：你懂的），但真正影响显卡性能的，是GPU和显存。不过，如果高阶版本溢价可以接受，可以选，比如更好的散热性能带来的是温度的降低，温度的降低更有利于整机的稳定性。

- 微星买万图师。现在万图师比超龙性能低不到3%价格整整多一千。微星万图师和[微星超龙] 差距就是游戏满载温度高13℃。

- 一定要相信老黄和苏妈的刀法，4070你就是芯片体质再好、供电再强、散热再稳，给它超冒烟了最多能刚刚好赶上默频的4070Ti
  - 显卡最重要的是里面的PCB那一小部分，核心、显存等等，外面那3P大空调之类的，对于纯粹的性价比爱好者来说，应该放到最次一级来考虑

- 我只知道第一次买显卡买最便宜的丐图师，直接让我桌子附近温度从春天变成夏天。自此下张显卡的温度是我考虑的因素之一。

- 丐卡再丐，理论性能都超越下级[显卡]，无非是供电和散热做的差点，达不到[功耗墙]没法超频，但是芯片总归还是那颗芯片。
  - 反过来说4070做的再好，再堆用料都不可能超越[4070ti]。只要售后质保能保证，在金额预算里面，买一块高级的丐卡也是可以考虑的。

- 够用就好吧，我买的4070s万图师的卡，感觉价位挺合适的，用着也稳定。

- ## 💡 [为什么50系列显卡出来，性价比没有提升反而40系列显卡涨价了？ - 知乎](https://www.zhihu.com/question/1902322217175463161)
- - 40系涨价是因为商家都在清库存，库存减少。仅剩的40系显卡集中到少数几个商家手中。但是市场的需求没有减少。需求不变，供应减少，价格必然提高。而且这几个商家的最佳策略就是涨价，因为以后肯定是要跌的，趁现在能卖高价必须卖高价。
  - 40系是不可能再大降价了。因为没有巨量的显卡下来击穿商家的库存。现在反而是商家那里没有库存，剩下的显卡只能惜售。

- 以前老黄的产能全部由消费级市场承担，所以每次新卡一发布，就会有海量的新卡砸盘，旧卡必须降价清仓否则会烂手里。
  - 现在可不一样了，老黄8成的产能被AI计算卡吃掉，投放给消费级市场的产能寥寥无几，新卡别说砸盘了，填补市场缺口都不够，所以老卡就有了奇货可居的价值，涨价也是理所当然的了。
- 为什么不增加产能？
  - 老黄是fabless，自己没有工厂，完全依赖代工，主要是找台积电，然而台积电每年的产能是固定的，全球大客户那么多，就算是老黄也做不到产能包圆，所以老黄能分到的产能是有限的，他想增产也做不到。
  - 这也能解释为什么有时候老黄会找代工，最主要的原因是台积电那边分到的产能不足以覆盖产品线。
- 拿着设计图纸多找几家代工厂不就有产能了吗？
  - 芯片这东西，比较特殊，他必须在设计立项的时候就敲定代工厂。
  - 想要找代工厂代工，自己是必须设计到门电路一级的布线，然而不同的代工厂，他的元件布局是不一样的，比如A厂的与非门是2x2布局，那B厂的与非门布局可能就是1x4了，换厂就没法做，因此一款芯片必须在立项的时候就决定好找哪家代工，设计好原理图，然后使用代工厂提供的EDA软件进行布线。
  - 大家想象中的像机加工一样，有了设计图就能随便找个工厂加工，在芯片领域是不存在的，哪怕你拿到了完整的4090布线图，找中芯代工，也是做不了的，除非你愿意重新设计布线，那就要先掏几千万美元的设计费，再掏上亿美元的流片费用，并且有流片失败全部费用打水漂的风险，非常划不来。

- 原则上是这样的，不过换厂也未必就代价很大。8Gen1和8+就是一年内从三星换成了台积电。
  - 早就曝光了，按台积电的生产周期，8gen1找台积电做，高通会有大半年的产品真空期，所以他一开始就立项做两款，三星工艺的8gen1先顶上，半年后再推出8+。台积电的产能预定起码提前2年，发现8gen1不行再改8+你这产能都排到2年以后去了。
  - 好像是细节记不清楚了，刚开始高通就是找了台积电和三星分别做一部分，哪知道三星做的发热量严重，最后不得已订单全给台积电了.
- 苹果6S上的A9处理器直接俩版本，三星和台积电的都有

- 国内还算是有良心的，50系出来以后，40系可以买到，稍涨一点价但还算有性价比。
  - 美国市场自从50系上市以后，正价的40系一夜之间全下架了，只有黄牛的高价卡在卖。
  - 在美国，NV 50系和AMD 9070/9070xt上市以来一直处于缺货状态，都被黄牛用bot买走了，普通人很难买到。比如Amazon上面正常价格的一直缺货，一堆非自营黄牛挂高价。
- 普通商家也不允许卖吗
  - 普通商家的都被黄牛买完了

- 40系出来的时候AI热刚刚兴起，[4090] 之所以价格爆炸，也是因为很多人都拿4090当生产力卡用，炼AI
  - 老黄是看到了40系，尤其是4090卖爆了的情况下，才决定50系挤牙膏，全力押宝计算卡上
  - AI卡卖的比游戏卡贵多了，价格可以说突破天际
  - 人家资本也不是傻子，比起花真金白银买算力卡（还可能用不了几年就被淘汰），还不如卷一卷算力，用便宜卡搞大模型。

- ## [5090D显卡比4090强的话为何没有被禁？ - 知乎](https://www.zhihu.com/question/9802980796)
- 5090d是nvidia创立以来性价比最差的显卡。
  - 5090d和4090d算力一样，但比4090d还差，因为4090d可以魔改48g显存，5090d不仅不能扩显存还禁止多卡互联，跑ai软件3秒锁死，要重启才能继续用，可以说只能玩游戏

- 中国特供版旗舰显卡RTX 5090D，虽然其AI算力削减了约29%，但是游戏性能却几乎并未缩水，价格为16, 499元起（略高于RTX 5090的1999美元，即约14648元的价格），将于1月30日上市。？性能差些，价格还贵些

- ## 🚀 [如何看待新推出的Nvidia推出的4090d？ - 知乎 _202312](https://www.zhihu.com/question/637218617)
  - 12999和4090价格一样。看了cuda规格是完整的ad102的百分80

- 老黄的刀法还是很精准的，这次RTX 4090D刚好卡在3A090出口管制条例的界限上。
  - “数据中心芯片”生效范围比较复杂，但是“非数据中心芯片”一刀切在TPP = 4800上，也就是说对消费级RTX 40显卡，TPP限制是4800上。
  - 而这张RTX 4090 D，该显卡搭载14592个CUDA 核心，加速频率 2.52GHz，该显卡搭载14592个CUDA 核心，加速频率 2.52GHz，显存为 24GB 384bit GDDR6X，显卡总功耗 425W，常规游戏功耗 302W。

- 这玩意有4090九成规格，应该还是在禁售范围内的（所以nv还限制了超频）

- 4090D这张卡本身，流处理器规模是4090的89%，Tensor Core和RT Core也是等比低削减，并未专门缩掉了Tensor Core的规模，显存也未阉割，L2甚至可能不变，估计综合游戏性能可以达到4090的90%-95%吧

- 都2023年了，还能在消费市场上见到大公司减量不减价的操作，只能说垄断还是厉害
  - 技术性垄断总比政策性垄断要好，最起码人家是凭真本事垄断的

- 这小砍一刀无伤大雅，依然有73.54TFLOPs➕CUDA➕24GB GDDR6X，吊打7900XTX跟4080以及新的4080S

- 全球芯片开始转向Risc结构，美国的大部分企业选择arm， 只有NVDIA目前的GPU和谷歌的TPU不是，但谷歌似乎也拥抱arm， 至少是用在其手机部分。 中国大企业却是拥抱RISC-V，这种开源的硬体架构规格，因为不需要给arm专利。英特尔也拥抱RISC-V， 表示与arm竞争。

- ## [魔改版4090 48G显卡性价比大探讨：真的值得入手吗？ - 知乎](https://www.zhihu.com/question/14343647195)
- 40系显卡的主要性能瓶颈是[显存位宽] ，和显存容量关系不大。这一点我们可以从4090和rtx6000ada两张卡得到验证。
  - 这一点我们可以从4090和rtx6000ada两张卡得到验证。
  - 4090和rtx6000ada拥有相同的架构的gpu核心，基本相等的cuda核心数量（一个是16384的cuda核心，一个是18176的cuda核心），基本相同的显存位宽（4090是1008GB/S，rtx6000ada是960GB/S）。
  - 两张卡只有显存容量不同（4090的显存有24g，rtx6000ada显存48g）。经过测试，两张卡无论是游戏性能，ai推理，结果都是差不多的。rtx6000ada并没有因为48g的显存容量而有更加优秀的性能。
  - 同样的结论还可以分析4090和4070得到。4090拥有16384的cuda核心，4070拥有5880个cuda核心。4090的cuda核心数量是4070的将近三倍。按理说，将近三倍的显存容量应该带来三倍的性能，但是4090的实测性能只有4070的将近2倍。4090的位宽384bit，4070的位宽192bit。4090刚好是4070的两倍。

- 粗略计算，每10亿个参数大约需要4G显存来加载，所以48G显存能跑120亿参数的大模型，如果是半精度的最多可以跑240亿参数的模型，显存不够大，模型都加载不进去，48G魔改出来的都是涡轮卡，插机架服务器用的，就不是给游戏玩家折腾出来的

- 我下周去找个工厂做一下测试，魔改的原理就是在[4090主板] 上加[显存粒] ，这个分两种一种是他们买的[3090主板]，换成4090芯片，然后加焊接24G显存粒，加两个。这种俗称消费级，就是需要每天关机一次，要不然估计主板顶不住，毕竟是特么的3090改
  - 另外一种就是基于4090主板，加焊显存粒，这种理论上比较OK，但是还没有实测，准备带上显卡去深圳找人魔改一下，要是能成了，那就我就开辟一个魔改业务，本来南京也有，但是南京没有家里创啊，产业链有点拉。只能招聘那些修手机来干

- ## [导师给30w预算装4-6卡服务器，目前打算上5880ada，要噪音较低、不要液冷，求合适方案？ - 知乎](https://www.zhihu.com/question/1939707476724389292)
- 5880满载功耗仅285w，比3090都低，涡轮也很静音。

- https://zhuanlan.zhihu.com/p/1939741761053364343
  - 之前我分享过，在RTX Pro 6000 上执行ollama与gpt-oss-120b，效率麻麻哋，不过，用它来运行vLLM却是一把好手，特别是主流的32B模型: Qwen3-32B，能达到22 token/s的速度，
  - 对比上一代卡皇5880(6000 ada的小兄弟)怎么样呢？实测过，2x5880也就才24 token/s。

- [英伟达中国特供版RTX 5880发布！性能比旗舰大砍近25%，比RTX 5000只高6% - 知乎 _202401](https://zhuanlan.zhihu.com/p/676491377)
  - 相比于旗舰级RTX 6000，定制版5880在性能方面可谓是大幅降级——CUDA核心少了23%，单精度浮点性能低了24%。
  - 实际上，它的表现更加接近RTX 5000——两项参数分别提升了10%和6%。

- ## [双RTX A6000显卡做深度学习，使用nvlink桥接器能实现显存共享成96G吗？ - 知乎](https://www.zhihu.com/question/455953236)
  - 双RTX A6000（显存是48G）显卡做深度学习，使用nvlink桥接器能实现显存共享成96G吗？插上了A6000的安培架构的桥接器，输入nvidia-smi nvlink -i 0 -s 显示的是速度14GB/S，不是显示的active，然后跑训练，还是只能使用48G的显存。
- NV的桥接器功能其实在宣传上有些问题，加了桥接器其实也完全不可能二卡合一、显存倍增，还是两块完全独立的显卡，桥接器只是让两块显卡之间可以快速交换数据，不用再从CPU那边绕一大圈；至于用了双卡以后提升了多少性能，主要看应用软件本身对多GPU优化的怎样，不同的应用软件的表现完全不一样，可能1+1接近2，也可能1+1≈1.5, 也可能1+1=1，甚至可能1+1<1。
  - 最新一代的Ada Lovelace架构的旗舰卡RTX4090, RTX6000Ada和L40干脆把对NVLink的支持彻底取消了，可见目前这个功能在图形类应用的领域有多拉胯，这个功能现在成了ai计算领域的专有功能了

- 可以直接在显存间传输数据，不需要再经过内存了是吗
  - 理论上是，但是也需要应用软件支持此功能才行，这个要看算法的，实际上多卡的算法没那么简单

- NVLink不是单纯的显存叠加，48G变96G，而是会增加两张显卡之间的显存交互带宽。一般来说，多卡跑深度学习，一般是指数据并行，即每个显卡处理一部分数据。

- ## 🚀 [如何评价Nvidia A6000显卡？ - 知乎 _202010](https://www.zhihu.com/question/424306404)
  - 取消Quadro命名，采用10752 CUDA满血GA102核心，48GB的GDDR6不带X显存，供电接口为新8Pin接口（EPS-12V与传统Pcie-8Pin显卡供电口不兼容）

- 虽然说取消了Quadro卡的命名，但是这东西看起来依旧是和Quadro一脉相传，当然叫他“专业卡”或许更直观，
  - 一直以来，游戏卡和专业卡都有着一道非常明显的区别，那就是OpenGL驱动，专业卡能打，但是游戏卡不能打，虽然老黄有放出过Studio驱动出来，但是依旧是无法取代OpenGL驱动的地位

- A6000采用的还是N卡30系的安培架构，所以注意安装cuda还是需要cuda11.1及以上版本。
  - 一个有意思的点是功率300w，比3090的350w要低，可谓是低功耗高效能了。
  - A6000的48G大显存两倍于3090还是比较适合上大模型的，看过网上评价A6000的性能优势主要体现在transformer双精度推理和分布并卡训练，由于目前炼丹的模型要求real-time比较小，单卡足以

- ## [RTX A6000存在的意义是什么？ 同样的价钱为什么不买两块3090交火呢？ - 知乎](https://www.zhihu.com/question/483799457)
- 英伟达把消费级显卡和专业级显卡区分得还是很开的，游戏卡不支持多路NVENC流、不支持vGPU，不支持ECC自动纠错，openGL性能较差。
  - 如果你有跑科学计算的需求的话，ECC显存是非常重要的，可以说必备，这种情况下你只能用专业图形卡或者计算卡来跑。
  - 多路NVENC也可以很好地用在大型广播控制台内，用于录制推流。这也是游戏卡应用不到的领域。

- RTX A6000是基于NVIDIA Ampere架构的超高端专业显卡，搭载最新一代的 RT Core、Tensor Core 和 CUDA Core
  - 兼具 ECC校验、GPUDirect for Video、多GPU 支持、四重立体缓冲、Mosaic多显示器、Quadro Sync等功能。

- 因为老黄禁止数据中心使用游戏卡

- ## [8w左右的双卡4090或单卡A6000服务器，有什么好的推荐？ - 知乎](https://www.zhihu.com/question/646105848)
- 至强W的T7960塔式工作站，保守最大4*A6000的支持。
  - 上限为56C的W9-3495X；DDR5-4800内存；PCIE 5.0；应该够用的存储空间（可选RAID/NVME等高级选项）
  - W7-3465X+128GB内存+RTX4090*2/一个A6000，咬咬牙也不是搞不定。
- 搭载至强可扩展三代的双路塔式T550服务器，最大两个A6000。
  - 上限就不说了，DDR4-3200; PCIE 4.0；标配独立阵列卡以及150TB+存储空间也相当不错。
  - 这个性价比较低，不是很推荐。
- 主流的机架式解决方案，也就是PE R750服务器，支持最大3*A6000，但是建议尽量控制在两张，
  - 它有个姊妹型号叫做750XA，倒是可以支持到4*A6000，相应的也要稍微贵那么一点点。。。
- A6000优势在于桥接扩容显存，所以还是尽量选择塔式，可能用不上，但是必须要支持对不啦。
  - 所以我认为最优选择还是塔式工作站T7960, 8通道DDR5-4800+PCIE5.0啊，何其先进。

- 7960能只买机架和电源吗, 其它的想自己配
  - 最低最低，得带上处理器。内存硬盘显卡自己配去吧。

- [导师给了10w买深度学习的服务器，只要求2张a6000的卡，其他配置有什么推荐么？ - 知乎](https://www.zhihu.com/question/628269514)
  - 推荐用各个厂家的单路旗舰解决方案直接适配。 也就是基于AMD Threadripper PRO或者全新至强W相关的最新平台。
  - 例如业内非常优秀的戴尔Precision 7960 塔式。 性能上限应该是 56C112T；4TB DDR5内存（16个内存插槽）；满配10个3.5硬盘槽位；满配最大四张A6000, 属实的优秀
  - 或者说7960的小弟5860也不是不行 ，扩展能力稍差，但是支持两张A6000运行，应该也是没啥问题的。
  - 或者上一代Precision 7920也行，虽说是末期稍有涨幅，但是性价比依旧是强无敌，旗舰毕竟是旗舰，虽然说是上一代。不过两张A6000。。。。 肯定行。

- ## [装机配置讨论，单卡A6000or双卡4090？ - 知乎](https://www.zhihu.com/question/599204652)
- 6000ADA太贵，6000感觉太老，4090显存比较紧张，马上5090出来之后，肯定6000和6000ADA会降价，甚至届时5090说不定有32G显存，可能是更合适的选择。很纠结，于是目前拿之前入门时候购买的4060Ti16G在顶着用。

- 据我所知 A6000唯一的优势就是显存大，实际上考虑大显存都是考虑降低多卡并行通信开销的，一张也体现不了大显存优势...

- ## [实验室配置服务器，4090，a100和a800选哪个？ - 知乎](https://www.zhihu.com/question/595107162/answer/4153401353)
- 测试发现，L40速度与4090D相当，表现令人满意。
  - A100, 40GB, 312 TFLOPS, ¥90, 000+, 400W, 
  - A6000, 48GB GDDR6, 79 TFLOPS, 300W, ¥33, 000, 300W
  - 4090, 24GB GDDR6X, 330 TFLOPS, ¥12, 000, 450W
  - L40, 48GB GDDR6, 147 TFLOPS, ¥44, 000+, 350W
  - 厂商解释，由于中美博弈的原因，L40宣传上未定位为训练卡，但实际用于训练完全没问题。
- 虽然L40 的 FP16 性能（约 147 TFLOPS）在理论上优于 A6000（约 78.75 TFLOPS），但在实际训练大型模型时，它并不被广泛推荐，原因如下:
  - 优化与软件支持：深度学习框架（如 TensorFlow 和 PyTorch）通常会针对特定的 GPU 进行深度优化，特别是 A100 和 A6000。NVIDIA 的 Ampere 架构（如 A100 和 A6000）在训练性能上得到了广泛认可，许多训练算法和优化器都针对这些显卡进行了调整。而 L40 的推理优化可能不适合训练任务。
  - 计算架构：尽管 L40 的 FP16 性能较高，其计算架构和硬件设计可能不具备 A6000 的特性，例如 Tensor Cores 的优化使用。Tensor Cores 可以大幅提高训练过程中的计算效率，尤其在大型深度学习模型中。

- L40单卡训练是没问题，最大问题是不支持Nvlink
- a6000不比4090高多少，价格高这么多
  - 显存大

- 4090不支持多卡并联，也就是大模型是根本没法训得，它是单卡性能强，同时性价比高(阉割NVLink了，多卡带宽严重不足)。
  - a800是a100的阉割版，也就是说，有a100选，肯定优先a100。因为是要训练大模型，建议购买sxm版本的a100。

- 要玩stable diffusion，目前的版本4090足够用了，其他大模型先明白搞什么，自己训练还是跑跑lamma，glm6b之类的，自己训练稍微大点的模型基本都不够，先算清楚对显存的需求一般就能有决定了。所以个人建议，不知道跑什么或者只跑stable diffusion先上个4090，反正便宜。明确知道搞什么，根据需要可以测算出来
- 4090从头训练图片生成够用吗？有人实践过吗。这些。
  - 按原版模型原版训练方式从0训不太够，原版是A100X8X32训了15万GPU Hours，每张卡batch size=8，总批次8X256，粗暴点算24g的4090一个批次只能2~3，就算不考虑慢的问题，批次依赖也需要另外解决
- 问题的关键是为什么要从头训练，基础模型上微调耗费很小的资源
  - 有创新的诉求，然后微调不够用(具体面临的问题可能和大神们的不同)

- ## [双a6000和双4090哪个计算更快呢？ - 知乎](https://www.zhihu.com/question/664827604)
- 小规模数据，单卡能跑的情况，显然4090比A6000快的多。
  - 数据规模再大点，超出了双卡的存储空间，4090就跑不了了，毕竟A6000一块就能顶2/4块4090啊。

- [渲染建模，是一个A6000牛逼还是两个4090牛逼? - 知乎](https://www.zhihu.com/question/658932305)
  - 在不溢出显存的情况下渲染单4090比A6000起码牛逼2倍，双4090就是4倍，
  - gpu-z上面有数据，一个4090是一个a6000的140%

- ## [深度学习用什么卡比较给力？ - 知乎](https://www.zhihu.com/question/612568623)
- Nvidia的PC机显卡，分为三个系列，Tesla系列，Quadro系列，Geforce系列。。
  - 原先分别对应专业科学计算，专业图形显卡，个人消费级显卡领域等。
  - 从价格上看，同等算力的价格差不多是4：2：1。
- 在算力差不多的情况下，Tesla和Quadro系列的溢价溢在哪里了呢？
  - 主要是稳定性（加了ECC纠错机制），显存效率更高（HBM2 vs GDDR）, 显存容量更大（一般Geforce卡显存在10G左右，但Quadro和Tesla一般都是24G，甚至48G，还有卡间互联技术NVlink等）。
  - 个人用的话，Geforce就差不多了，如上一代的3090，价格～13000¥，算力也足
  - 可以上A6000，～30000¥，也还不错
- 在CES 2025展览会上，英伟达发布的采用Blackwell架构的显卡是GeForce RTX 50系列，包括RTX 5090、RTX 5080、RTX 5070Ti和RTX 5070等型号，对标40xx的显卡
  - Tesla系列，推出了B200，给数据中心用的，死贵死贵。
  - 目前暂未有Quadro系列的最新架构显卡发布，这个系列国内可用的最高配置是Ada5880, 大概5万块一张吧，次一点可用Ada 5000, 3万多一张吧。。

- 现在行情不好，价格丑陋。别说4090了，就连3090也涨价了不少。干脆搞魔改2080ti吧22G的。改好的一张2500块钱，买3张也就现在（2023.11.4号）一张3090的钱。然后上洋垃圾至强x99平台，服务器内存随便插满128g。电源用矿电源1600w，反正放实验室，不心疼。
  - 我半个月前想买4090，商家不发货。只能退款，当天晚上立马去问rtx a6000能不能发货，当时2w6的价格。那会商家还没反应过来。只是晚上不发货，凉凉。第二天商家已经改价成3w了。

- H100 ￥23.5W, A100 ￥13W，但现在一般人或公司不好买了，已经被禁购了

- ## [有没有便宜点的AI算力显卡? - 知乎](https://www.zhihu.com/question/634145498)
- 2080ti 22G，仅需2000出头
- 3080 20G，3090 24G 便宜且支持bf16

- 要便宜，大显存，L40系列看看。英伟达Ada Lovelace 架构，48GB支持ECC的GDDR6显存，两者的显存带宽都是864GB/S, 
  - L40S作为L40的升级版本，主要在FP32运算能力提示幅度为1.1TFLOPS，在TF32 Tensor Core TFLOPS、FP16 Tensor Core、FP8 Tensor Core、INT8 Tensor Core运算能力均提升 一倍左右。

- 要便宜，大显存，p40最合适。24g显存，价格800左右就可以入手。记得一起买风扇和电源转接线。
  - 900块拿下过p100，感觉已经是平民价里面最好性能的了，还有hmb显存。

- 租云服务呀，能选的专业卡多还能开票让老师走经费

- ## [4090 魔改 48g 显存是怎么做到的？ - 知乎 _202502](https://www.zhihu.com/question/11803840385)
- 4090显卡有两个兄弟，叫做l40s和a6000 ada，都用的ad102内核，这两个兄弟显存都是48g的。
  - 没有a6000 ada，要么是30系的a6000，要么是40系的6000 ada。我之前也搞错过。

- 魔改的4090 48G价格是2万2左右（给料的加工费是5.5k左右），同一代（40系显卡）核心的6000 ada 48G要5万2左右，上一代（30系显卡）核心的A6000 48G是3万2左右

- 3090芯片发售时，显存颗粒最大1GB，24GB显存需要24颗，PCB板正反面都有。
  - 4090芯片发售时，显存颗粒达到2GB，24GB显存只需要12颗，PCB板只有一面有焊盘。
  - 流出的4090 48GB改版显卡bios，正好发现4090针脚定义和3090一样，可以焊在3090PCB上
  - 这样4090芯片+3090PCB+24颗2GB显存+流出魔改显卡bios=4090 48GB显卡。
  - 显卡bios大概率不是x86指令，否则早就有个人魔改版，诱惑太大了。
- 有能力改的，大概率不想得罪英伟达。所以这种只能偷偷搞，没保障

- 它就是个混合体：4090核心+3090的PCB板+24颗2GB显存颗粒，而且还有下面几个巧合，任何一个失效都不成立：
  - 4090的核心和3090的核心针脚一样，所以它可以焊到3090的PCB板子上并被识别；
  - 3090用的是24*1GB的显存颗粒，板子没有限制显存颗粒容量，可以换成2GB的；
  - N厂内部流出了显存驱动、显卡固件居然能完美支持。

- 网上测评显示4090 48G 显卡还可以支持 FP8，甚至这款显卡也已经出走海外，来自加拿大的小哥在平台上晒出了自己在 eBay 上买的 RTX 4090 48G，售价要 3 万人民币起步。

- 不得不佩服皮衣刀客的刀法了。科技是第一生产力，刀客在逆天而行。
  - 皮刀客目标在于赚钱， 发展生产力只是你想法而已，人家才不管你发展狗屁生产力呢

- 和3090同一级别的专业卡是RTX A6000，48G显存，其它参数和3090差不多的，现在的售价仍然是3万元
  - 和4090同一级别的专业卡是RTX 6000ADA，48G显存，Cuda核心比4090多约2000个，售价现在6万元；
  - 略低一个级别的RTX 5880ADA，Cuda核心比4090少约2000个，售价在5万左右； 
  - 所以4090 48G 2.3W的售价，性价比极高

- a100性能远不如4090
  - 4090推理王者

- 驱动？卡的bios才更重要
  - 你说得对

- 其实就是传言流出来那版vbios，没有那版vbios，就没有后续48G。
  - vbios有数字签名会和芯片内的安全芯片作相互校验，因此绕不过去，而在2023年流出来了一个工具，可以把不同品牌的vbios（有数字签名版）互刷，所以拿到48G的vbios就等于有了48G的4090，无非是如何搬板，甚至有能力可以重新设计一张pcb来扩张，换句话说，如果未来有更大显存容量的bios流出，原则上也可以做更大显存的卡。
  - vbios签名是与GPU芯片内安全组件进行校验，校验通过时，GPU才会完全初始化。

- 既然4090 48G是用3090的PCB板魔改的 那为什么市面上都没见到魔改3090 48G的？按理来说3090成本比4090低得多啊
  - 30系没有对位的48g计算卡，bios会卡住点不亮。闲鱼上有很多3090芯片转移到4090pcb上的卡，价格还挺实惠，敢买的人不多。
- 因为3090没有48G的BIOS流出。4090的AD102核心还用于ada6000等专业显卡，出厂显存就是48G，说明AD102是可以兼容48G显存的，

- ## [为什么N卡一定要带cuda? - 知乎](https://www.zhihu.com/question/592464568)
- cuda又不是硬件……toolkit想装就装，不带也不会影响打游戏
  - 对于n卡来说，cuda本来就算是增值服务，对于很多开发者来说提供了好用又高效的开发接口。
  - 我是不认同把n卡和cuda划等号。但是cuda明显已经形成了开发者好开发-使用者体验高-N卡占有率高的良性循环。

- 因为现在的显卡的可编程管线都是通用的。除去光栅器等少数部分，像顶点着色器等等，驱动底层和硬件计算单元和CUDA其实都是共用一套东西。

- 至于说AMD没有CUDA，但是AMD有ROCm和OpenCL啊。只不过因为软件支持差，不好用，用的人少而已。

- ## [是否存在支持cuda的核显轻薄本？ - 知乎](https://www.zhihu.com/question/663409299)
- 不存在。cuda是英伟达独显的，N卡独占，是独显的功能。核显是基于CPU的，而CPU是因特尔和AMD这两家的。
- 不存在。Cuda是英伟达自研的只支持自家显卡的算法。核显是Intel和AMD CPU自带的。英伟达并不生产笔记本的CPU。

- CUDA是英伟达独家，目前核显轻薄本是Intel和AMD，自然现在市面上找不到支持CUDA的核显轻薄本。
  - 现在的Intel、AMD甚至高通主推的AI PC全靠NPU，但是目前应用软件层面能够调用NPU的寥寥无几。
  - 而CUDA支持各个软件应用的支持显然更加广泛。到时候大概率会出现英伟达的核显本才是真正意义上的AI PC。
  - 目前英伟达的GPU一大问题是显存容量较小，比如主流的甜点游戏卡RTX 4060就是8GB显存。现阶段跑本地的LLM、Stable diffusion瓶颈并不在多少多少TOPS的算力，而在于很容易爆显存，你连跑都跑不起来。
  - 未来英伟达自己的核显本，可以共享内存，本地的大模型和AI应用可能才会真正发展起来。

- ## [为什么说CUDA是NVIDIA的护城河? - 知乎](https://www.zhihu.com/question/564812763)
- 英伟达从cuda里学到的最重要的一课，就是软硬件捆绑。
  - 计算界cuda之所以厉害，不仅仅是因为它可以调用GPU计算，而是它可以调用GPU硬件加速。
  - physX也是，N卡限定。 甚至于说，如果需求量够大，英伟达把三维体积的有限差分操作，有限元的检测函数积分操作，全做成“电路板计算”也不是不可以。 同时配合着自己的软件体系一起往外推。
  - 这才是英伟达真正的组合拳。 在这套组合拳体系下，cuda扮演着胶水级核心航母的角色。还有其它护卫舰，而这些护卫舰都绕不开。

- 2007年6月，CUDA发布。在Nvidia的持续精耕细作下，CUDA已经成了科学计算领域的事实标准。
  - 等神经网络算法火了，Nvidia又大力支持，各大AI框架因此优先支持CUDA，形成了正循环。
  - 之后，Nvidia又发力与AI关系紧密的自动驾驶和机器人领域，推出了针对汽车的Drive系列芯片和针对工业机器人的Jetson系列。
  - 这一切，都是以CUDA作为软件切入点，最终，CUDA就成了今天的样子，变成了又深又宽的护城河。

- amd抛弃opencl了，推他自家的rocm

- 用fpga、甚至是更定制化的asic来加速，早就有了，性能超过nvidia的同类产品比比皆是，问题还是生态和总拥有成本。google也做了TPU，在内部用得也不少，生态还是干不过CUDA。性能有时候并不是决定性因素。
# discuss-laptop-win/linux
- tips
  - 选择笔记本的一个重要因素是售后维修，大厂会容易很多如联想、惠普

- ## 

- ## 

- ## 

- ## 

- ## 

- ## 

- ## 🧩 [为什么都没有人推荐准系统笔记本？ - 知乎](https://www.zhihu.com/question/432305670)
- 先说结论：当前环境下，已经没必要再推荐准系统笔记本
  - 售后稀烂，全看商家良心，准系统要求用户有一定解决问题的能力
  - CPU来路不正，暴利且不稳定，准系统因为不是正规的OEM厂商，无法从英特尔采购正规的笔记本CPU，因此只能使用测试版
  - 厂家准备亲自下场角逐，当时准系统主要为微星和蓝天两家，看到高性能笔记本这么一块大蛋糕，有研发实力又有渠道又有推广能力的厂家自然不甘心给人做嫁衣。后来的微星大家也看到了，已经成为几大Gaming笔记本品牌之一，蓝天则抱紧神舟大腿。这些原本的准系统供货商开始更新自有品牌，对公模的投入越来越少，以至逐渐衰落。
  - 笔记本产品的拓展性趋近于0，现在除极少数笔记本，CPU和显卡已经不可更换，准系统也是如此，曾经引以为傲的拓展性也风光不再。
  - 价格优势不复，拿题主的参考，我找到了淘宝某准系统店家模具为NH55(是同级别散热最好的模具，堆热管加暴力扇嘛)，以R7-3700X（8核16线程，对位R7-4800H） 16GB内存 512G固态 RTX 2060配置举例，这个价位的产品都有哪些呢

- 现在国内蓝天准的代理只有船准和掏粪准，前者同模具，神舟拼好的sku比准划算不少，神船虽然也是喜欢抠固态，但至少也给你保修。tf准就不说了只有更贵。
  - 这年头良心准商不是没有，某个温州萌妹和南京海鲜似乎还在坚持，但是价格你也看得到，大多数准商破产的破产跑路的跑路，转型台式机的也有，自己申请了一个牌子试图洗白的也有。
  - 总而言之，就是时代变了，有靠谱正规蓝天代理商，为啥要去买连税都不交的没有保障的小店产品。。。

- 准系统的死亡阶段主要经历了2次暴击。
  - 第一次暴击，来自芯片厂商。有心的玩家应该都知道，从酷睿4代之后，再无PGA封装的CPU，全部焊接在主板上，而MXM的显卡，从GTX980M之后，也慢慢退出了历史的舞台。
    - 虽然之后蓝天另辟蹊径将桌面级CPU搞到了笔记本上，使笔记本有了LGA插槽，然而并没有什么卵用，英特尔2年一换接口，这谁顶得住。笔记本的BIOS更新还不如台式主板来的快。纵然LGA1151撑了6、7、8、9这4代，但是其中的幺蛾子与骚操作，只有经历的人才懂那种痛。
    - 而虽然10系显卡还有MXM的，但是异型卡大行其道，几乎没有标准版的卡型了。以前可升级的显卡一去不复返。
  - 第二个暴击，来自神船。从前的准系统厂商，以搞蓝天和微星准系统为主，准系统商家的蓝天的货源主要来自神船与未来人类，微星的大概就是来自微盟，还有独立一派的镭波、海鲅，现在也死了吧？没咋关注。
    - 纵然准商可以提供的定制化需求多样，但是架不住神船下场打这个架。准商只能靠着比神船更好的屏幕、网卡、SSD来抗衡，但是越精细的定制，越要花钱。神船就用一招便宜，打的准商毫无还手之力。神船的提货量本身就庞大，自然会拿到更好的价格，神船还吃着英特尔给的饭，准商哪个打得过价格战？
- 买个笔记本，花这么多钱，承担这么大的风险，哪个小白愿意铤而走险？隔壁神船好歹还有个售后，好歹还是个国内“知名二线品牌”。
- 现阶段，神船也算是吧准系统的那碗饭抢过去了。题主贴的图，本身问题也不小。还是老生常谈的杂牌配件问题，但凡标明好点型号的，都要加钱。其实一点不算便宜。
  - 精打细算一下，甚至跟买个神船没啥区别，神船至少还有个7天15天退换和相对完整的售后服务。

- 中肯，现在想入蓝天模具最好就是上船
  - 但是神船基本都是低端模具上高配。。。实际性能并不行。。。高端模具神船以前有，现在基本绝迹
- Gx10 x170不过价格嘛望而却步

- ## [为什么所有笔记本电脑都设计成不能升级cpu？ - 知乎](https://www.zhihu.com/question/666682172/answers/updated)
- 最精髓的两条，一是不省空间很难压薄；二是有金属盖导热效率更低。

- 不划算，台式到了换cpu的时候基本主板和u都一起换了，
  - 笔记本你换主板，每个牌子的主板还要匹配对应牌子外模，而且主板扩展性很低，硬盘内存接口就那么多，升级也只是用回原来的配件。
  - 如果只是单纯换cpu，相当于为了醋包饺子。不像台式你一个机箱什么主板都能安装。

- 以前是可以的，后来发现没意义，你看现在大家升级台式机，都不像以前，慢慢升级了。基本上换就是一套全换了。

- 其实台式机也不能，因为每几代[CPU架构]会换，等你想换了CPU了，多半主板上的 [CPU插槽] 已经不适配最新的CPU了，而且CPU很可能是电脑最不容易坏的玩意儿了

- 以前的很多笔记本电脑都能换cpu。我大学的时候本来想买台式电脑，但是寝室空间太小而且台式电脑不方便携带，考虑再三入了准系统笔记本的坑。
  - 当时买的是神舟战神zx7sp5d1，蓝天模具准系统笔记本电脑。大学期间我给这台电脑加了个固态硬盘当系统盘，加了条8g内存条，又加了个1T机械硬盘用来给老师们安家（这个笔记本电脑有两个固态接口，两个机械硬盘接口，四个内存条接口）。
  - 这台笔记本电脑我一直用到现在，1060显卡玩巫师3、老头环还都可以中低画质60帧，因为太耐用了导致我组台式电脑的计划一直没能落实。

- 2020年左右，我买的炫龙M7，cpu是台式机的 cpu，可以更换的。

- 当年蓝天模具可是整过逆天操作的，设计了带CPU插槽的主板，笔记本自带，用户可以买到手以后更换i5-9400
  - 更多的原因是英特尔太鸡贼了。例如新出12代处理器，就要用新的，和旧的不通用。例如你真的弄到手一个笔记本，处理器是11400，你想把处理器换成12400？没门！

- 2014年之前，大部分笔记本电脑都是可升级CPU的
  - 真正把行业取消可更换插槽式CPU的，一是上游的推动，二是笔记本整体设计趋向于轻薄化
  - 上游的推动很简单粗暴，从酷睿5代开始，英特尔就取消了笔记本产品线所有的PGA封装插槽式CPU，全部改用[BGA封装] ，直接焊在主板上
  - AMD也在类似的时间点砍掉了所有插槽式产品线
  - 这时候才只剩下蓝天、微星等具备LGA/PGA插槽，使用桌面处理器的准系统机型
  - 另一方面，这种可更换的设计需要在主板上设置一个基座，再把CPU装到上面，整体占用的厚度就会有所增加

- 2015年之前的还是都是可以换cpu的，英特尔最后一代移动端可换CPU是第四代酷睿，采用了FCPGA946插槽

- 蓝天以前有的是。不做的原因有很多，一是Intel和AMD强推hx55取代了原来桌面u生态位，不给笔记本厂插槽了，二是蓝天一条懒狗摆烂基本上不更bios，出后续u也不支持。

- 准系统笔记本都是可以的
  - 不光CPU，显卡都能换
  - 但不管是换CPU还是换显卡，这样的准系统的笔记本，包括IBM的老本子，模块化设计的都有一个共同特点就是厚、沉

- ## [有没有什么笔记本是和台式机一样可以进行扩展升级的？ - 知乎](https://www.zhihu.com/question/567261427)
- 蓝天NH50JNR了解一下？X170了解一下？
  - 这才叫和台式机一样：CPU [显卡] 内存硬盘全都可以换，甚至你够闲够有钱，还可以把风扇换了，散热改VC甚至上水，当然，后面那些要自己去定制/DIY。
  - 话说X170的正统继任者是啥？似乎没下文了？（好久没去了解准了）别说是X270，那破烂不配。
- 可惜显卡这块，老黄已经不给做MXM了，显卡可更换大概要成为历史了。

- X170之后确实就再也没有顶级准系统机了老黄毕竟不做mxm了。
  - 事实上我觉得应该说，超级肌肉本这种产品本身就已经可以说濒临死亡了，毕竟受众只有生产力狂人和玩机佬。
- 一眼望去，现在还能坚持做出真正强大的超级肌肉本的也只有微星一家了，而尽管如此今年的超级旗舰GT77虽然堪比X170，但是天生330W单孔电源直接就限制了不额外扩适配器的情况下她的上限，更不要说GT77的核心组件都没法更换了，一代末代机皇的痛啊。

- 市场决定了厚重的模具越来越少，不管是蓝天的厚重x170设计，rog的超神，微星的gt8，都已经没有新品好多年了
  - 蓝天新模具越来越倾向于薄，rog高端交给了轻薄的冰刃系列，微星只有gt7系列，rog也没有厚重设计了，估计日后也不会有了
  - 主要是成本太大 销量太差，这种顶级本动辄三万往上，本来市场就小，投入进去基本回不来

- 一般的轻薄本、超极本部分可以替换硬盘，或者添加内存。
  - 标压笔记本电脑基本都可以自行替换硬盘，部分可以添加内存。早期CPU为[BGA接口]，能够自行替换，近年的已经改成板载，不能替换。
- 使用桌面CPU的肌肉本，可以基于主板，自己更换CPU、[MXM显卡] 、硬盘和内存条。这种肌肉本可以买成品，典型例如神舟笔记本的GX、TX、K系列等。
- 还有一些只有主板和外壳，一般称为[准系统笔记本]，要自己选配桌面CPU、MXM显卡、硬盘、内存条、屏幕等等。
- 采用蓝天模具的游戏本厂商有雕牌，未来人类，神舟战神，炫龙，雷神（海尔），机械师（海尔）等等。
  - 而采用微星模具的则有机械革命（清华同方），火影（清华同方），镭波等等。
  - 游戏本厂商除了贴牌出售整机，有些还会直接出售准系统，其中大部分以蓝天模具为主。
  - 国外有不少可选零配件的蓝天模具准系统销售网站，而国内只有铁头（未来人类）和神船（神舟）在出货。
- 如果想购买能够像台式一样升级的笔记本电脑，就选准系统好了。

- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 
- 

- ## [后期升级空间大的笔记本有哪些？ - 知乎](https://www.zhihu.com/question/314565920)
- 笔记本只能升级硬盘和内存。约等于没有升级空间。

- 你有没有听过MXM笔记本？

- 以前的很多封装称之为pga封装 已经不在生产了 现在的笔记本除了部分是台式机使用的lga封装 其他的都是bag封装了 也就是焊锡的 pga封装是插针的 比较老了 可以轻易更换CPU

- 外星人m51笔记本啊，台式机的构架

- 据说后来能换cpu的笔记本很少了。将来再买笔记本，第一考虑可升级性

- ## [惠普暗影精灵10slim对比上一代9slim有什么提升吗？ - 知乎](https://www.zhihu.com/question/661490891)
- 暗影精灵10 Slim 16等于便携、拓展、性能哪一样都想要，但哪一样都没顾上。说到底还是上代的产品思路，在2024年跟不上市场了，毕竟竞品有后发优势。
  - 要性能+拓展，有重量差不多、干到200W性能释放、拓展性更好的ThinkBook 16p
  - 要便携+续航，酷睿Ultra、1.85kg更轻薄、质感更好的ROG幻16 Air

- ## [如何评价惠普发布的轻薄游戏本暗影精灵9 Slim，值得购买吗？ - 知乎](https://www.zhihu.com/question/593557269/answers/updated)
- 优点：轻薄，颜值在线，屏幕顶，超大电池。
- 缺点：单硬盘位，祖传大下巴。
- 自使用独显直连屏蔽核显后，直到现在没有再次出现蓝屏问题。

- Slim版则实现了纤薄便携与性能之间的平衡，更适合注重颜值与移动性的玩家

- ## [有哪些笔记本比较完美地支持ubuntu？ - 知乎](https://www.zhihu.com/question/286150644/answers/updated)
- 如果不打算跑cuda或者其他需要Nvidia显卡的程序的话，我一般没遇到过什么特别大的问题。但如果需要用Nvidia的显卡，那就是问题不断，经常要看运气了。
  - Ubuntu本身经常有Nvidia显卡驱动问题，但换成基于Ubuntu的那些以user friendly为指导思想的发行版本，很可能就不需要自己折腾，完全没问题了。这里，我强烈推荐Linux Mint，在Dell XPS上一扫最基本Ubuntu的各种显卡驱动问题。

- ## [笔记本电脑适合安装哪个Linux？ - 知乎](https://www.zhihu.com/question/520319837)
- 笔记本不建议折腾Linux，老老实实当客户端用吧。笔记本换了没适配过的Linux，电源管理，屏幕亮度，无线驱动都可能是问题。买官方支持的Linux本价格可不低。

- 笔记本来安装Linux，会让你用这个笔记本的初衷迷失在各种研究尝试上。

- ## [求推荐适合linux系统的笔记本? - 知乎](https://www.zhihu.com/question/1100336631)

- 台式机建议用Linus配的那个线程撕裂者主机，b站有配置单。显卡建议换成rx5700xt或者rx6700xt。其他的直接复制粘贴就行。
  - 笔记本电脑，i家选十代酷睿（含十代）及以下的酷睿核显本，a家选zen或ryzen架构的到最新的核显本都可以。十一代以上（含十一代）Intel处理器硬件有bug。
  - 独显一般是n卡，能不用就不用，笔记本电脑上Linux用n卡有点自找难受。

- 大部分主流品牌的都可以。

- 最正确的难道不是随便买一台win本子，虚拟机下面跑Linux吗？

- 如果不能忍受需要折腾，Ubuntu certified laptops 大大方方买就是了，主要以Dell、HP、Lenovo御三家为主

- 这玩意不是取决于你愿不愿意折腾驱动么...
  - 如果不愿意折腾就买个专业的移动工作站这种比较稳定，带驱动支持的，
  - 如果愿意折腾与喜爱，那性价比高的游戏本随便找一个就好，CPU都可以顶满到14900HX...

- ThinkPad太贵了，御三家工作站都没问题，惠普性价比最好

- 能不用笔记本别用笔记本。Linux对硬件性能没有太大限制，所以，你的硬件越强~你得到的就越多。
  - 显卡，如果搞AI的话，只选NVIDIA独立显卡，现在3090二手货也不贵了，千万别信什么N卡驱动难搞，那都是不会英文的原始人才会信的话术。Linux用其他卡才叫灾难，要啥啥没有，只能显示，还不如亮机卡。
# discuss-laptop-mac
- ## 

- ## 

- ## 

- ## [M4 Max 128 GB vs Binned M3 Ultra 96 GB Mac Studio? : r/LocalLLM _202503](https://www.reddit.com/r/LocalLLM/comments/1j8tyjr/m4_max_128_gb_vs_binned_m3_ultra_96_gb_mac_studio/)
- What do you mean “binned”?
  - Some of the GPU cores are not enabled. M3 Ultra has 60 core and 80 core GPU versions. The first one is 4 grand and second one costs 5.3 grand

- i think for these models, 96GB is more than enough and the GPU, NPU and RAM bandwidth are doubled on the M3 ultra vs M4 max. you would get 1.5-2X the tokens per second with the M3 ultra.

- Go with the M3 Ultra 60 core GPU 96 G 1TB variant. It is faster for LLM inference compared to M4 Max and has more memory bandwidth.
  - Note: Don't go for 80 core GPU variant. It is not value for money deal. Also go with 1TB variant. If you need more storage space, get an external SSD.

- ## [Is mac best for local llm and ML? : r/LocalLLM _202509](https://www.reddit.com/r/LocalLLM/comments/1ndhu25/is_mac_best_for_local_llm_and_ml/)
- Mac is best for energy efficiency for sure. Idle power is super low.
  - even at inference you're using less than 300W on a top of the line Mac Studio

- For casual, chat style inference it's hard to beat. However for training, high context processing and diffusion image generation Nvidia is still king and Mac will be quite slow for this.

- macs are pretty good for LLMs but as far as i know they struggle with vision nets and diffusion models 

- Bro, running models you should be good. However, training/fine-tuning (sometimes like 10% of the time works decent) other 90% you will find yourself crying watching your MAC train, slower than Dial-up internet back in the 90s.

- ## [How does M2 Ultra compare with M3 Ultra for LLM? : r/MacStudio _202508](https://www.reddit.com/r/MacStudio/comments/1mr09e2/how_does_m2_ultra_compare_with_m3_ultra_for_llm/)
- Llama 7b F16 TG
  - M1U/64 - 37.0
  - M2U/60 - 39.9
  - M3U/60 - 42.2

- ## [M3 Ultra Mac Studio Benchmarks (96gb VRAM, 60 GPU cores) : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1kvd0jr/m3_ultra_mac_studio_benchmarks_96gb_vram_60_gpu/)
  - I loaded each model freshly in LMStudio, and input 30-40k tokens of Lorem Ipsum text (the text itself shouldn't matter, all that matters is token counts)

- I have the same machine as the OP (96GB, 60 cores) and am running Qwen3-30B-A3B 8bit and Qwen3-32b 6bit concurrently - great combo to use in Aider architect mode. Which two models have you chosen to work with in Roo Code? What has been your experience?
  - I typically use Qwen3 32b as Orchestrator and Architect, Qwen 2.5 32b 128 K as coder and debugger. I use Unsloth versions of all. They can handle certain projects just fine. Especially languages like python. If I run into issues, I mix in deepseek r1 or v3 from openrouter.

- Can you explain how to load several models?
  - I use LM Studio, and you just keep loading models until you’ve either loaded all of the ones that you need, or when you reach 85% of your memory capacity. It’s a good practice not to fill more than that.

- ## [Mac Studio for LLMs: M4 Max (64GB, 40c GPU) vs M2 Ultra (64GB, 60c GPU) : r/LocalLLM _202506](https://www.reddit.com/r/LocalLLM/comments/1l702x2/mac_studio_for_llms_m4_max_64gb_40c_gpu_vs_m2/)
- The ultimate decision comes down to: (a) Do you want to run smaller models a little faster or (b) Do you want to be able to run larger models for less money? 
  - In my experience, the larger models are almost ALWAYS better. They just win.
  - Now, if you really do care about speed, and you plan on using LONG contexts, and you are content running smaller models, then by all means get a 3090 PC and call it a day. 
  - Just understand that the speed gaps tend to disappear at the larger models, and many of the nvidia fan boys cannot run the larger models at all due to lack of VRAM.

- M2 ultra gets destroyedy by AMD instinct Mi50 , especially if we're talking about value because we can pick up 1X 32GB GPU @ $250-300. Couple that with old mining rig motherboard or 7002-7003 EPYC CPU + cheap mobo
  - It's not easy because there's a ton of configuration needed . The MI50 is not supported by rocm anymore which means that we need to use older versions and compile it all ourselves. 

- LLMs are horribly memory bandwidth and memory size limited. Not as compute limited.
  - Forget the M4, it has MAX chip with half bandwidth.
  - The choice would be between M2/M3 Ultra at around 800GB/s. Unfortunately Apple asks for extreme premiums on memory capacity.
  - The AMD AI Max boxes go for around 2 000 with 128GB of unified memory, that alone makes them a better value, but it's slower memory at 250 GB/s
  - With AI Max 395 you spend less to run much bigger models, or regular models slower.
  - Personally I have a 7900XTX 24GB that is really competent due to the high memory bandwidth. I can run 30B models competently on my GPU with ROCm or Vulkan runtimes.

- Memory bandwidth of 3090 is also 800GB/sec right, so in terms of new machine M2/M3 Ultra is relatively similar performance with ability to use bigger model?
  - If you are serious about machine learning, not, and it's not even close. It's just LLM inference that it can work because they are very forgiving on compute, and just require huge slabs of fast memory. There are people using 12 channel EPYC processors and running LLMs on 1.5 Terabytes of memory that way.
  - Nvidia has CUDA, and there aren't words to describe how far other vendors are from providing something that works like CUDA does. I use ROCm and took a month to get acceleration running on Comfy UI. With metal my collogue gets maybe 5 % of the theoretical performance, it takes minutes to diffuse somethings that take literally 2 seconds on my 7900XTX.
- No, the RTX 3090 is 4X faster for prompt evaluation and 2X faster for token generation. Because whatever they say, compute power is important too.

- ## [What models can't I run with 128gb (M4 Max) vs 256gb (M3 Ultra)? : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mkip7t/mac_llm_users_what_models_cant_i_run_with_128gb/)
- I got the 256GB
  - Remember that the Ultra will have twice the memory bandwidth of the M4 for more speed.
  - I hate running the very largest, because it makes it so I can't run image generation at the same time with a decent sized context.

- Technically the M3 Ultra has only about 1.5x more the mem bandwidth than M4 Max (812 vs 546 GB/s)

- I have the M3 Ultra 256 gb. The problem with really large models is that inference speed is super slow. However, you can load several smaller models concurrently. Which can be useful.

- ## [need a “M4 Max 128GB” vs “M3 Ultra 96GB” comparison please! : r/MacStudio _202503](https://www.reddit.com/r/MacStudio/comments/1ja5xyc/need_a_m4_max_128gb_vs_m3_ultra_96gb_comparison/)
  - M4 Max [unbinned] with 16‑core CPU, 40‑core GPU, 16‑core Neural Engine, 128GB unified memory.
  - M3 Ultra [binned] with with 28-core CPU, 60-core GPU, 32-core Neural Engine, 96GB unified memory.

- Just to confirm, M3 Ultra (28CPU, 60GPU, 32NE) has the same memory bandwidth as M4 Max (16CPU, 40GPU, 16NE) ?
  - Apple is confusing the heck out of people - putting “up to 819” but without providing a real figure for binned chip.
- No both M3s have the same bandwidth. From apples page “You can choose between two M3 Ultra chips for this Mac Studio. Both are phenomenally powerful, with a 32-core Neural Engine. Both feature 819GB/s of memory bandwidth.”

- the M3 Ultra single-core performance is over 20% less than the M4 Max, and a fair amount of my work is heavily dependent on single-core performance

- M4 Max does seem like the best value for money in this comparison, but I want to know if am I going to lose on Ultra’s 800+Gb/s memory bandwidth which seems like an important factor when it comes to 3D rendering and video editing

- [Mac Studio (M4 Max 128GB Vs M3 Ultra 96GB-60GPU) : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1koxr32/mac_studio_m4_max_128gb_vs_m3_ultra_96gb60gpu/)
  - M3 Ultra will perform better of course. It is two cpu running in one pc. But it also means 2x more power consumption. Personally I'd pick M4 Max even though it's a bit slower

- ## 🆚 [So the M3 Ultra is better than the M4 Max? Why do that? : r/MacStudio _202503](https://www.reddit.com/r/MacStudio/comments/1j45hnw/so_the_m3_ultra_is_better_than_the_m4_max_why_do/)
- the M3 Ultra is different than the prior iterations of M1 and M2 Ultra. It is not two Max chip SoC’s fused together with a common I/O like the M2 Ultra, rather, its a single SoC on a megachip with the performance and experience cores of two Max chips but without the limitations in memory access across the prior I/O interface. The speed and performance improvements are substantial; 
  - You are correct that the M3 Ultra does not have the single core performance of the M4 Max, but it massively outperforms the M4 Max in multicore performance; 
  - The ability to outfit the M3 Ultra Mac Studio with 512Gbs of VRAM shared unified memory is a game changer for machine learning and large model.

- Because the M2 Ultra is better than the M3 Max, and the M1 Ultra is better than the M2 Max. That’s how it’s always been

- [why don't we have m4 ultra it's much worse than you think : r/mac](https://www.reddit.com/r/mac/comments/1ld47pn/why_dont_we_have_m4_ultra_its_much_worse_than_you/)
  - In a recent video, we see that it's already possible to make the Mac Studio with an M4 Max chip overheat—even with the fans running at full speed. When both the CPU and GPU are pushed to their limits simultaneously, the system draws over 330 watts of power. That’s already enough to overwhelm the Studio’s cooling system, forcing it to throttle performance to avoid overheating.
  - if you imagine an M4 Ultra—which would essentially be two M4 Max chips fused together—you’re looking at a theoretical power draw of over 670 watts. That’s more than even a high-end GPU like the RTX 5090 consumes.
  - But it’s not just about cooling. The current Mac Studio’s power supply is only rated for 480 watts.
  - The Mac Studio, in its current form, simply isn’t built to handle that kind of power. But there is one Mac that could handle it: the Mac Pro. 
  - m3 ultra was chosen which consumes around 450w peak

- ## [苹果推出 M4 Max / M3 Ultra 芯片 Mac Studio，哪些亮点值得关注？ - 知乎 _202503](https://www.zhihu.com/question/14167658813)
- 在今天之前，最适合个人用户跑大模型的机器是 M2 Ultra + 192G 内存的 Mac Studio，而在今天之后，我单方面宣布 Mac Studio M3 Ultra 512G 版本

- b站张黑黑：[DeepSeek R1] [Q4 MLX]  19.17 token/sec，适合个人用户，但是不适合大规模并发的商业场景
  - 512GB统一内存，本地跑Q4的671b R1！！8550刀！甚至不到一张 A100 40GB
  - 内存带宽819GB/s！（4090是1008GB/s，5090是1792GB/s，买了一堆商单吹上天的AI Max+395的带宽是256GB/s）
- 不过现阶段llm还是买不如租，租不如白嫖
  - 除非是专业相关需要自己调模型发paper，或者是网上流出个671b的NSFW的model，否则本地部署671b没有太大意义（70b缺的通识知识可以通过允许联网搜索来弥补）

- 其实，最应该干这个活的是AMD手里有CPU，有GPU，有缓存，有高速总线。而且x86生态好。AMD弄一个8核处理器，GPU尽可能多的芯片，支持多通道lpddr5很方便。结果，还是苹果先弄出来了。

- AMD AI Max，4070级核显，128G片上内存，但内存带宽就256，苹果是500/800

- LLM一次推理可以分为prefill和generate两个阶段。
  - prefill阶段读入所有token，然后计算他们的内部表示。你可以理解为写文章前先读一遍书，把书上的字转换为自己理解的内容。
  - generate阶段则是根据prefill的计算结果，生成新的token。这个可以理解为是看完书开始写，一次写一个字，写完一个字后再写下一个。
  - prefill是计算密集型任务，因为prefill时所有要计算的token都已经确定，可以并行地同时计算所有token对应的内部状态。
  - generate是内存密集型任务，因为generate时每个token的计算都依赖于前一个token的计算结果，所以只能一个一个地计算，这时候带宽就成了瓶颈
  - prefill影响的是你看到第一个返回的字之前要等多久，generate影响的是后续token的生成速度。
- 大家都会忽略prefill的速度只看generate的速度。这个在gpu上是合理的，因为gpu的计算速度相对能容纳的模型大小是足够的。但是在mac上，这个情况就反过来了，因为mac的内存可以很大，但是计算性能比较差，这就导致prefill会成为瓶颈。

- 芯片不是M4 Ultra，而是M3 Ultra，其实本来就只有M3 Ultra，这在一年半前就有消息了，内部代号是T6032/T6033
  - 至于M4 Ultra目前消息中显示是不会有的，不是为了给 Mac Pro 去拉开产品差异，而是M4 Max 没有 UltraFusion连接器，M3 Max 是有 UltraFusion 的，只是切割芯片的时候把 UltraFusion 连接器切掉了，所以放到网上看到的 M3 Max 的 Die 图找不到 UltraFusion 连接器。
  - M3 Ultra 雷雳5的驱动应该是重新设计，符合它本身的定位。
- 看内部代号 M5 Ultra 为 T6052，也是使用 UltraFusion 连接两块 M5 Max。
- m4max相比m3max核心提升不大。做m4ultra提升也不大，研发成本还高。干脆直接m5ultra惊艳一点

- 
- 
- 
- 
- 

- 
- 
- 
- 

- ## [苹果产品中的“统一内存（unified memory）“与以往的”内存（memory）“有何不同？ - 知乎](https://www.zhihu.com/question/429727608)
- 统一架构的好处，就是把以前内存和显存（显卡的内存，gpu用的memory）做了统一
  - 这样就减少了cpu和gpu通信时候，在内存和显存之间数据通信时候的拷贝

- 这个统一内存就是把内存、GPU的显存和神经网络处理器的缓存放到一起，通过Fabric和CPU/GPU/神经网络处理器相连。
  - 好处是等于在CPU与GPU间加了个缓存区，它们之间的数据传输更容易；
  - 坏处就是三个公用那么大统一内存（比如有16GB），等于给CPU的内存实际不到16GB，其中有部分不得不分给GPU和神经处理器。
- 换个思路，cpu其实用到了一部分显存

- UMA这东西早八百年用烂了，linux的kernel里有个dma-buf子系统专门用来做这个东西
  - dma-buf可以理解为一个在驱动间共享的可被不同硬件共享的内存，当然这个内存属于device memory，有读写限制，和CPU实现有关，这个子系统至少零几年就有了

- 要说统一内存amd走得更前，索尼在ps4就用上了

- [苹果发布会统一内存是什么意思？ - 知乎](https://www.zhihu.com/question/429767639)
  - 以前cpu有自己的内存，gpu也有自己的内存叫显存，苹果直接统一起来了所以叫统一内存。好处是避免了内存和显存之间的数据搬运，坏处：贵。
  - 贵体现在需要更多的lpddr控制器导致芯片面积大，而且lpddr本身就贵一点，焊接一起的lpddr几乎不能更换。
  - 至于有个回答说什么soc的，现在所有的cpu都是soc，gpu也是soc。soc跟是什么芯片没关系，只要不是单一模块都是soc。现在的cpu也有dsp（视频硬件编解码）这些模块所以都是soc。
- UMA不是什么发明，低端PC/游戏机上早就有了。苹果只是把内存带宽整的比较夸张（通过封装）。

- ## [如何评价苹果 3 月 5 日发布的 MacBook Air M4，相比前代有哪些提升？ - 知乎](https://www.zhihu.com/question/14167413067)
- M4 相较于 M3 有 18～25% 的提升
- 显著影响你的电脑的使用年限的，是硬盘和内存，而不是处理器，还是那句话：慢不慢和能不能是两个概念

- 其实M4的单核提升不太重要，因为M4相比M3单核能效提升很小（3%多一点），多出的那些性能都是拉高频率拿功耗换的。反而多核提升是很重要的，像我写论文一开就是十几个网页+PDF文献+编译器，M4多出的两个小核能极大缓解大核的压力从而提升续航

- M4 的 GPU 性能比 M3 提升约 21%，更适合处理3D图形渲染相关的任务
  - 处理速度提高：单核任务快约 25%，多核任务快约 30%

- ## 👷🏻 [macbook air m4 敲代码是否够用？ - 知乎](https://www.zhihu.com/question/15035632763)
- 在使用 MacBook Air M4 24 + 512 近一个月后，我可以说这已经是我最喜欢的电脑了。我用一周时间，在不关机的情况下用 mactop 对上述工况下的系统使用数据进行采集并进行简单的统计分析，结果如下：
  - 整个系统负载最大的指标是 RAM 和 SWAP。我的内存是 24GB，但是在开启所有常用程序后 SWAP 依旧来到了 8GB 左右，说明其实 32GB 是有必要的（不过当时没有国补所以买了 24GB）。不过即使如此，整个系统依旧丝滑，用起来完全没有卡顿。
  - CPU 和 GPU 的均值不超过 20%，略高于我上次在 M4 Pro 上测试的结果，这是合理的，毕竟差了 2 - 4个核心，但是依旧没有成为性能瓶颈。95% 分位数也在 30% 左右，与上次相差不大。说明 M4 这颗芯片提供的性能其实依旧完全满足了我的日常需求。
  - 日常使用时几乎感受不到发热。
- 我经常需要拿着电脑去会议室开会，每一次从桌上拿起 MacBook Pro 时，侧面的出风口都会给我很强的感知，这一点上 Air 的手感完胜。

- 不能用windows 不能做安卓逆向 淘汰 啥时候回归 windows 再买，现在用2019款装的 win10

- 开了内存泄漏大户zotero和arc游览器之后，24g运存就开始不够了

- 个人测试mac mini m4 16g版本，运行20w行代码的java项目，docker运行 mongo + redis+mysql，开七八个网页，内存压力就已经开始黄色，对我来说这还是轻度测试，我个人工作流四五十个网页都很常见，后台还需要postman，微信，qq等，按我用法得32g以上才够用。。

- 如果只是为了外接设备的话，一个几百块的转接器就搞定了.

- 512GB硬盘必备：Xcode、Android Studio、Docker镜像随便一个都能吃50GB，256GB分分钟爆仓

- 真要散热焦虑，淘宝30块买个笔记本支架，温度还能再降5℃

- ## [是选M4 MacBook Air还是M4 MacBook Pro? - 知乎](https://www.zhihu.com/question/14423191905)
- MacBook Air M4：有2个雷雳4接口、MagSafe充电接口和一个耳机插孔。
- MacBook Pro M4：有3个雷雳4接口，还增加了HDMI接口和SD卡槽

- Pro搭载M4 Pro/M4 Max 芯片，性能核心更多（如 M4 Pro 14核CPU + 20核GPU，M4 Max 16核CPU + 40核GPU），内存带宽高达 512GB/s（M4 Max），适合多轨道视频渲染、复杂特效喝多图层设计。

- Pro，屏和喇叭差不少呢 ，值回差价。

- ## [Mac mini可以将笔记本电脑作为显示器吗？ - 知乎](https://www.zhihu.com/question/11673595878)
- 可以，适用于所有笔记本电脑的方法是，通过网络连接远程桌面---特别是两台电脑都处于同一局域网的情形下，效果非常可以。
- 如果用自带的屏幕共享，Mac自己的客户端连接速度尚可，其他VNC客户端倒是一言难尽。
- 可以用HDMI采集卡。Mac mini从2010款开始都标配了HDMI接口。
  - 采集卡不能共享键盘鼠标
  - 或者KVM over USB的，然后自己用一套鼠标键盘
- 直接连接不太现实，但是可以用屏幕共享工具，把 Mac mini 投屏到笔记本上，就可以相当于使用笔记本的键盘和触控板来控制Mac。但要在笔记本上装支持 VNC 协议的客户端，再通过 IP 地址或 Apple ID来 连接Macmini。但是这个操作非常吃网络稳定性，而且画面延迟有点小高。你不介意的话就可以一试，最好还是直接给 Mac mini 配独立的显示器。

- 笔记本电脑没有视频输入功能，要当显示器也不是不可以，可以去买个采集卡，将就可以用了。

- 不过采集卡，好的还是挺贵的，还不如去买个便携屏幕，方便携带，还能给笔记本电脑用也可以。

- 如果需要携带，有便携屏可以选择，配合Mac mini M4用可以一线通，还是比较省心的。特别是有些有一些支持触控的，可以给macOS触控操作
  - 便携屏的话，我喜欢16英寸的，能携带，看着也舒服。像维辰思这个，4K分辨率，是100% DCI-P3广色域，500尼特亮度，日常够用了。

- ## [Mac mini的显示器推荐：便携显示器能做什么，平板能做屏幕用吗？ - 知乎](https://zhuanlan.zhihu.com/p/31411990698)
- 什么叫一线通？就是Mac mini通过雷雳4接口，和便携屏连接就可以实现给屏幕供电和信号。便携屏不需要另外插电源了。

- 为啥不把平板当显示器用？
  - 各种平板电脑不支持视频输入，只能通过远程桌面，或者推流方式实现。但是这样方案一个是延迟很低，分辨率也只是到1080P而已，更高分辨率挺费钱，没必要。
  - 像iPad，现在安卓平板，都没有视频输入能力。被商家限制了功能，或者不想做。

- [MacBook配件之便携屏选购攻略 - 知乎](https://zhuanlan.zhihu.com/p/628909420)

- ## [Mac mini配可随身带显示器如何，是不是比笔记本还自由？ - 知乎](https://www.zhihu.com/question/51732960)
- 笔记本移动电源（or太阳能电池板）+转接口+耳机+便携式显示器。
- 随身显示器，还要带上随身键盘，还有随身排插。还有随身音响的亲。

- 随身携带的显示器有很多选择，我觉得主要问题是电源，如果能搞一个随身携带电池，我觉得让 Mac mini 变成笔记本就不难了。

- ## [入门macOS系统，选air还是pro? - 知乎](https://www.zhihu.com/question/1895920692941263011)
- Pro多出来的性能，普通人根本用不上，就像你买个跑车但天天堵在三环

- air还有个好处，年头久了也不用清灰，省事。

- MacBook Air M4芯片和Mac mini，以及MacBook Pro有区别，它的CPU是10核心，但是GPU是8核心的，比那两个核心数少了两个。在图形秘籍的因公，比如一些游戏，会有一些影响。

- ## [Macbook Air M4 24+512还是 Macbook Pro M4 Pro 24+512? - 知乎](https://www.zhihu.com/question/1893050996537668535)
- 有条件还是尽量选pro，pro的接口和屏幕更适合生产力，
  - 它的mini led屏在写代码时，120Hz Promotion带来的跟手感是用了就回不去的。
  - 而且pro默认支持同时外接2台6K显示器，air需开盖才能外接双屏，pro对于多窗口工作更灵活。
  - 像你如果经常同时进行多任务，比如后台跑Stata+开PyCharm+Chrome多标签，需要外接多屏或高速接口，那必须是pro

- Air 和 pro 我觉得体验上的最大差异是屏幕观感，如果你需要高亮度，高对比度，高刷新率，那无疑只有 pro 满足，如果对这些没要求，那就图 Air 的轻薄方便携带吧；

- 作为维修人员来说，PRO的故障率比AIR高很多，10台坏掉的机器7成是PRO，AIR故障率非常低，苹果历代的AIR都是这样的，苹果的AIR用个几年，顶多电池扛不住了换个电池就好了，PRO就不一样，主板故障率对比AIR高太多了

- ## 🆚️ [MacBook Air 与 MacBook Pro 差别多大？ - 知乎](https://www.zhihu.com/question/20385806)
- MacBook Pro 与 MacBook Air差别有多大，要大可以很大，要小也就个风扇的差距。

- Air没有配风扇，大型软件只能轻度使用。外接显示器超过一定时间发热处理器就会降频。
  - Pro有风扇，大型软件耗电较大。但Pro的性能除Mac pro和studio外最强！

- 同配置的M1 芯片 MacBook Air 和 M1 芯片 MacBook Pro，差距就在于MacBook Pro有风扇，MacBook Air没有风扇，导致MacBook Pro在持续高性能工作下表现更优异。

- [现在适合入手Macbook吗？Pro和Air怎么选？冲M3还是等M4？ - 知乎](https://www.zhihu.com/question/655713407)
- macbook 我只推荐 pro: air 并不轻，不是叫 air 就轻，air 还有很多问题，散热差，屏幕差，而且没法接多个显示器，只能同时点亮两个屏幕，意思就是你接了两个屏幕的情况，air 本体的屏幕就没显示了，我的 macbook pro 上面接了 3 个屏幕，本体屏幕也正常使用 

- Air无风扇设计：适合短时间高负载任务（如轻度视频剪辑、编程编译），但持续高负载（如长时间渲染、3D建模）可能导致降频，性能下降。

- Pro配备主动散热风扇：可维持长时间高性能输出，更适合专业级工作流（如4K/8K视频剪辑、复杂建模）。

- 我写代码以前是iOS开发为主，现在是小程序开发。之前用的是MacBook Pro，现在用的是Air，以后应该都会用Air了。既然选了笔记本，就是为了轻便去的。 我现在用的是16G+1T。 真正考验性能的可能就是视频剪辑类的了，写代码Pro与Air都能对付的。

- pro版多了一个HDMI接口，如果你需要外接显示器或者投影仪之类的东西，这个接口会方便很多，如果不需要的话，那还是建议省下一千块。
# discuss-pc-vendor
- ## 

- ## 

- ## 

- ## [如何评价高通发布的新一代旗舰处理器骁龙8 Elite/至尊版，是今年最强SOC吗？ - 知乎 _202410](https://www.zhihu.com/question/1651714161)
- 自2015年11月11日高通发布了搭载自研 [Kryo CPU] 的骁龙820以后，高通就放弃了自研CPU，随后的处理器CPU都是基于 [ARM公版架构] 魔改，这一次在骁龙8 Elite身上，自研CPU架构终于回归
  - 骁龙处理器这么多年以来经历了数次改名，从最开始的MSMxxxx到骁龙xxx再到骁龙x Gen x，今天又出来个新名字
  - 工艺制程上了台积电3nm，和之前发布的天玑9400以及苹果A18/A18 Pro相同。

- 2023年10月24日，高通发布了NUVIA的第一个产品——骁龙 X Elite，这是一款用于笔记本的处理器，它就搭载了Oryon CPU架构
  - 这一次Oryon架构用在了手机处理器上，并且高通宣称骁龙8 Elite搭载的是第2代Oryon架构

- 目前首发机型已经确认是小米15系列

- CPU部分，取消了三缓，换上了大二缓，和苹果一样的思路，延迟会好看很多。
  - 8elite在10瓦功耗，就能达到9400 18瓦功耗下的成绩，此时同性能下45%功耗优势。同功耗10w情况下，有11%的性能差距。

- ## [高通骁龙 X Elite 很厉害吗？ - 知乎 _202402](https://www.zhihu.com/question/644273100)
- 要性能没性能, 要生态没生态, 要性价比没性价比, 要续航没续航, 唯一能吹的马上也要被i和a反超了

- 转译执行 [x86 程序] 的折损还是有点大，这导致整体的性能和13代酷睿基本持平，30W以内，残血版对应x86的跑分还低一些
- 转译运行X86/X64软件，不考虑不兼容的那部分，性能会被AMD/Intel原生运行吊打。

- 没有体会到转译和原生的区别，都很顺畅
- 事实上所谓的兼容性问题起码有一半以上是伪命题——因为绝大多数传统x86应用都不是性能密集型应用。我自己用真机实测，x64转译版微信，steam和小黄油，完全感受不到任何卡顿，续航看起来没有明显异常——最重要的是跟运行时跟原生应用一样完全没有发热

- 在同样的模具下，高通X Elite 在NBC中的续航也没跑过Lunar Lake

- 和苹果的Apple Silicon M系列芯片相比，那就是能效和性能都赢不了，但是价格可以更便宜。

- 对于一些小众需求的用户，比如工业用的软件，ARM的生态目前应该是不太适合的。重度游戏用户也是如此。

- 
- 
- 
- 

# discuss-pc-mini/diy
- tips
  - 不推荐买整机工作站，购买单独的算力卡即可

- ## 

- ## 

- ## 

- ## 

- ## [Upgraded self-hosted AI server - Epyc, Supermicro, RTX3090x3, 256GB : r/LocalLLaMA _202405](https://www.reddit.com/r/LocalLLaMA/comments/1d3dh4c/upgraded_selfhosted_ai_server_epyc_supermicro/)
  - moving from AM4 to Epyc. CPU/mb/GPU/RAM/frame purchased on Ebay. 
  - CPU - AMD Epyc 7F52 CPU
  - Motherboard - Supermicro H12SSL-i
  - RAM - 8x32GB DDR4 ECC Reg 3200 Mhz, DDR5 is only supported on Epyc 8000/9000 series, the Epyc 7001/7002/7003 lines only support DDR4.
  - 3 x RTX3090 Founders Edition, all on PCIE 4.0 x16 risers
  - Veddha 6GPU miner open frame
  - Proxmox RAID Z1 (mirrored) on 2 x Kingston 256GB SSD
  - Samsung 1TB m.2 NVME for VMs
  - Samsung 4TB SSD for models & data
  - Intel X540 T2 2 x RJ45 10GBe nic
  - Corsair HX1500i, 34.6 x 23.8 x 13.2 cm, 10.8L
  - Ollama running command-r-plus:latest eval rate 12 t/s, llama3:70b Q4_0 17 t/s, llama3:70b-instruct-q6_K 13.06 t/s

- ## [What’s the deal with all the cheap EPYC mobo +cpu + ram bundles selling from China? : r/homelab _202210](https://www.reddit.com/r/homelab/comments/yemiap/whats_the_deal_with_all_the_cheap_epyc_mobo_cpu/)
  - There are hundreds of these kinds of offers on eBay and elsewhere - at ridiculously low seeming prices Eg: AMD EPYC 7551P CPU 32 Cores + Supermicro H11SSL-i Motherboard +8x 8GB 2133P RAM
  - Are these a scam?
- More likely they're decommissioned hardware from data centre.
  - This has been asked multiple times. The answer is they're old and being decommissioned. 1st gen stuff has been available for cheap for a couple years, while second gen is starting to get cheap. It's not like people are shocked when they see a 1950x for less than a quarter of the original MSRP.

- ## [AMD EPYC 9004 series 1U chassis : r/homelab _202311](https://www.reddit.com/r/homelab/comments/17nopmx/amd_epyc_9004_series_1u_chassis/)
  - EPYC 4th AMD EPYC™ 9354P
  - Motherboard Supermicro H13SSL-NT
  - 256 GB RAM + M2 storage.
  - [The new EPCY 9004 CPU's look like a great option for a low(ish) power build. : r/homelab](https://www.reddit.com/r/homelab/comments/181e2u2/the_new_epcy_9004_cpus_look_like_a_great_option/)

- The board is listed as ATX so any case thats takes that size motherboard is on the table except for one thing - trying to find a suitable case.

- ## [Xeon 6230 or Epyc 8124P or 9124 for Homelab? - Hardware Hub / Build a PC - Level1Techs Forums _202502](https://forum.level1techs.com/t/xeon-6230-or-epyc-8124p-or-9124-for-homelab/225592)
  - I’m looking to build a server for my homelab (HV + NAS). I don’t see many comparisons or benchmarks for the 8004’s and lower end 9004’s. 
  - Epyc 8004 motherboards are still pretty limited. For various IO reasons, I don’t really love the current 8004 options (ie: ME03-CE0 & SIENAD8-2L2T), making me lean toward the 9124.
  - Used DDR4 ECC is so cheap that I can max out all six memory channels (6x64GB) right away. Which would be great for TrueNAS and my VM’s. With the Epyc’s, I would start with 2x64GB and wait for DDR5 ECC prices to drop, meaning more cost later. And who knows how long until they actually drop.

- ASUS S14NA-U12 is another 8004 motherboard option. I’ve no experience with any Epyc motherboard, but this looks nice and is a bit cheaper in EU

- I have a 8024P AMD EPYC running in my home lab on a Gigabyte ME03-CE0 motherboard with 6 16GB ECC DDR5 RDIMMs and storage.
  - It’s a bit weak (wish I had found a 8124P or even a 8324P if it was somewhat affordable), but gets the job done.

- ## [Asrock vs Supermicro - AMD Epyc Genoa : r/truenas _202406](https://www.reddit.com/r/truenas/comments/1dekxey/asrock_vs_supermicro_amd_epyc_genoa/)
  - I am looking to buy a new motherboard for my server (TrueNAS Scale) and am considering the following models: ASRock GENOAD8UD-2T/X550, Supermicro H13SSL-NT
  - My previous experiences with Supermicro motherboards have been very unpleasant. The IPMI port didn’t work, the PCIe card I installed for the network didn’t appear in BIOS, and a RAID card (purchased from a trusted company) had the same issue.

- I use the Supermicro H13SSL-NT with windows at this point, in a desktop case and everything is working fine. I have the epyc 9124. And windows desktop doesn't support the 10gbe broadcom adapter.  

- ## [What are the cheapest Intel/AMD CPUs supporting Quad-channel RAM in 2023? - Quora](https://www.quora.com/What-are-the-cheapest-Intel-AMD-CPUs-supporting-Quad-channel-RAM-in-2023)
- xeon w3 2423 the latest its like 400$ but has 4 channel memory

- There are a few different candidates, depending on whether you include old or second-hand components, and how you define “quad-channel”.
- Technically the Xeon E5 2620 is the cheapest CPU you can buy in 2023 with quad-channel support.
  - It’s a very weak 6-core server CPU from 2012, and you can buy one second-hand from Aliexpress for about $2 or £1.50
  - It supports quad-channel DDR3, both registered and unbuffered, but because it only supports DDR3, it’s not a good option
- Most modern laptop CPUs support quad-channel LPDDR RAM, because LPDDR channels are half as wide as DDR channels (32 bits vs 64 bits), so you can fit twice as many channels on a given CPU, though each channel has half as much bandwidth. 

- If you want a current-gen CPU with support for at least 4 full memory channels (at least 256-bit memory bus, at least 8 DDR5 sub-channels), the cheapest option is a 6-core Intel Xeon W3-2423 (4 channels) or 8-core AMD EPYC 8024P (6 channels) which both cost about $400 or £400.

- ## [Ryzen DDR5 Quad channel resources - Hardware Hub - Level1Techs Forums _202412](https://forum.level1techs.com/t/ryzen-ddr5-quad-channel-resources/221787)
- Check your motherboard’s QVL list for compatible memory kits.
  - It’s not the number of sticks, it’s the number of memory ranks. So you can typically do 4 single rank sticks or 2 dual rank sticks at EXPO speeds. 
  - The problem is consumer memory rarely specifies the number of ranks as they may swap suppliers based on what’s cheap/available.

- ## [Does a CPU with 2 memory channels support using 4 sticks of RAM? : r/buildapc](https://www.reddit.com/r/buildapc/comments/13d83mt/does_a_cpu_with_2_memory_channels_support_using_4/)
- It CAN use up to four sticks, yes. It's still dual channel, it's just double-populated dual-channel (true quad-channel RAM CPUs do exist, but it's highly unlikely you have one).
  - Finally, if it's a situation of DDR4, RAM is so goddamn cheap right now that if you want to go from 16GB (2x8GB) to 32GB (2x16GB), you can usually just sell your old kit on like ebay, and even after the fees, it's within a few dollars of the same net cost as if you bought another 2x8GB kit.

- ## 🆚 [Framework strix halo vs Epyc 9115 -- is Epyc better value? : r/LocalLLaMA _202503](https://www.reddit.com/r/LocalLLaMA/comments/1jo50iz/framework_strix_halo_vs_epyc_9115_is_epyc_better/)
  - I've put in a reservation for the Framework desktop motherboard, which is about $1800 with 128GiB ram, 256 GiB/sec bandwidth. 
- However, I was going through some server configurations, and found this:
  - Epyc 9115 -- 16-core, 12-channel memory, $799
  - Supermicro Motherboard w/ 12 DIMM slots -- $639
  - DDR5 6400 16GiB x 12 -- $1400
  - That would give me (12 channel x 64 bit wide per channel * 6400) 614.4 GiB/sec bandwidth, about 2.5x the Strix Halo motherboard configuration. Cost would be about 1k more, but getting 50% more memory too.
  - Now this would be doing CPU only inference, which I understand is mostly memory bandwidth bound anyway. Prompt processing would suffer, but I can also throw in a smaller sized GPU to use for prompt processing.

- That Epyc has just two CCD. Even if it uses 2 GMI3 links per CCD, it would be limited to 240 GB/s.
  - AMD has this fraud of advertising bandwidth between RAM and memory controller, even when it's severly bottlenecked by bandwidth between CPU and controller.
  - In order to obtain the advertised 9005 bandwidth you need to use the 12 or 16 CCD SKUs.

- 9115 costs less than 1k, not 10k. But it has just 2 CCD and probably its actual performance is much lower than expected. A way better choice would be the 9175F, which goes for 4k.
  - Or I would consider 9274F, if the price has reduced after 9275F launch.

- I was just going to reply 9175F is $2650 on Newegg and 16CCD. I might go for that instead. Thanks for the reply! Any RAM recommendations? And amount?
  - 9175F supports 12 channel memory, so you should look for a single-CPU 12-channel mobo. Theoretically, it should be able to benefit of memory up to 7200 MT/s, but without benchmarks is hard to say.

- The cheapest Epyc Turin that will give you more bandwidth would the 9225 at 2, 500 usd. (4 ccd)

- I regularly find the previous EPYC 9554 64-core CPU with 8x CCDs (measured at 390GB/sec on STREAM) listed for $2500-2900. This is the most value for money configuration I believe and it offers more computation FLOPs for prompt processing (~5 TFLOPS in AVX2 is my estimate).

- The reason to choose Epyc is for the PCIe lanes or amount of memory, i.e. you are planning to load up GPUs in it or want to run something that requires a lot of memory.
  - 🤔 If you only want the lanes you can go cheaper with an SP3, something like 7532 which is the cheapest 8 CCD second gen you can buy, an ATX motherboard gets you 7 total PCIe slots, and in something like the H12SSL 5 of which are 16x - you could put in 5 x16 GPUs and still have lanes to spare. Placing them will be tricky because two would have to be single width but you can use cables.
  - If amount of memory matters and you can live with lower speed then SP3 wins again, because you can easily get 512Gb RAM+ for less than the cost of pretty much anything in DDR5 - DDR5 RDIMMs are eye wateringly expensive. Depending on what you're doing you may get similar or better memory bandwidth out of a 7532 compared to the low CCD count SP5.

- If you want to get a versatile CPU for other tasks as well, the previous gen AMD Epyc 9554 is better value imo (regularly under $2900) with 64 cores (~5 TFLOPS) and measured at 390GB/sec on STREAM. It achieves this with DDR5 4800 which is cheaper as well.

- Just get 5 7900 XTX. Strix Halo sucks big time in those machine where the motherboard does not support ECC. I hope some Strix Halo provider will finally create a ECC memory supported device
  - The HP Z2 G1a has ECC

- Early leaks point to Medusa Halo using a 384 bit bus, posdibly even LPDDR6.

- The 9015 was tested at 240GB/s on STREAM. I doubt that the 9115 will be much higher than that so you should temper your expectations

- ## 🧮💡 [Is DDR4 3200 MHz Any Good for Local LLMs, or It's Just Too Slow Compared to GDDR6X/7 VRAM and DDR5 RAM? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1ndg19v/is_ddr4_3200_mhz_any_good_for_local_llms_or_its/)
- For those wondering DRAM memory bandwidth can usually be calculated using following formula:
  - MT/s x 8bytes x memory channels / 1024 
  - So 3200MT/s x 8bytes x 2 channels / 1024 = 50GB/s

- Well, not only is DDR4 considered slow, it's more a question of how many memory channels you have got. 
  - Chances are that it's not going to be very many, it but really depends on the hardware you have. 
  - The rule is that each transaction is 64 bits or 8 bytes wide, so 3200 MT/s where T is for transactions means 3.2 GT/s can be performed by the hardware, and 3.2 GT/s * 8 B/T gives you 25.6 GB/s per channel. 
  - DDR5 is not hugely faster in this scale, e.g. you might have 6400 MT/s DDR5, which also has 64-bit transactions and thus double the data rate. 
  - Multiple memory channels is how you take these raw results in 50 GB/s order to the 200+ GB/s data transfer rates where inference starts to become practical. 4 are probably required, 8 or 12 is better.

- A common mistake is that something like am5 has only 2 memory channels but 4 slots for ram. This means that even with 4 sticks you only get 2 stick bandwidth (probably at an even lower speed). Make sure you check what your cpu is capable of.

- It isn't going to be as fast as pure vram obviously, but if you're using moe models it works better than you'd think. Having 256GB of ram with a large moe will do good, even with just 24GB of vram. You also can partially offload a dense model but still have super fast input prompt processing. 
  - My desktop computer for example is 96GB of system ram and 24GB of vram. I can run a 70B or a 100B model, process 200, 000 tokens of input within a couple minutes, and wait for slower tk/s but much shorter output.
  - I also use llms on my laptop that only has 2 channel 64GB ddr4 at 3200mhz. No gpu. I'm getting 2.5 tokens/sec with GLM 4.5 air @ 3Q right now. Not fast by any means but not impractical. I just let it do it's thing for a couple minutes and come back for the answer.

- I've was here not long ago as I upgraded to 128 GB of ddr4 3200 ram. The Qwen3 235B Q2 (Need 96GB ram) runs at 3.7 tk/s and with the Qwen3 235B Q4 (128GB ram) I get 2.2 tk/s. So yes very slow.

- VRAM > [LPDDR5X > DDR5 > DDR4] > Disk

- [What is the theoretical data rate limit of a DDR5 DIMM slot? : r/hardware _202106](https://www.reddit.com/r/hardware/comments/nxvsk7/what_is_the_theoretical_data_rate_limit_of_a_ddr5/)
- ram is overclockable. But first theres something you need to understand about ram speed: the MHZ speed you see listed on ram is not the actual clockspeed , the real clock is half but since modern ram is DDR (Double Data Rate), everyone advertises the 'effective clockspeed ' , which is actually the transfer rate as measured in MT/s (Mega transfers per second).
- Now each dimm slot is a 64 bit wide bus so to get the theoretical data rate, you simply take the transfer rate times the bus width , so for example with 1 dimm of '3200 mhz ram' aka 3200 MT/s ram, thats 3200 * 64 = 204, 800 Mbps or 25, 600 MBps.

- ## [Which motherboards support quad channel memory : r/buildapc](https://www.reddit.com/r/buildapc/comments/sw22cg/which_motherboards_support_quad_channel_memory/)
- Note that it's not enough for the board to support it. The CPU has to as well. Most consumer ones do not.
  - Boards that support quad channel only support CPUs that also support quad channel. 

- ## [主流的 ITX 主板有哪些，那一个牌子或者型号比较好? - 知乎](https://www.zhihu.com/question/350786479)
  - 微星技嘉华硕华擎这四家
- 华擎是所有品牌里价位最低的，平均低100元左右，这个要说不是优势，那绝对是胡说。
  - 供电上比微星少了一相，但考虑到itx不需要考虑超频需求，上个3700X甚至更高也毫无压力，因此没有必要考虑这一点。
  - 该有的全都有，不该有的，可能它也有，比如光纤音频
- 华擎B450I最好？？看得我一头问号，供电比不上微星，无线比不上技嘉，信仰比不上ROG，还没灯，就一并联“3+2”，也就忽悠忽悠小白吧

- 微星的B450i卖的比华擎B450i多得多啊，不知道啥原因，难道只是为了内存超频？
  - 华擎挖矿板出的很多，消费市场这几年很疲软，都打算进军商用产品了。我说的是防掉压，不过3代AU超频性能很一般，确实没必要，所以说不考虑pcie4.0，X570都没啥意思。
- 华擎没出过多少挖矿版。他比别人可以便宜也赚钱的原因是工厂在越南，而且不需要营销。ITX独孤求败。你说的防掉电我刚才没看到，那个是处理器部分的防掉压吧？

- itx 首推妖板 华勤····· 价格便宜 华硕兄弟品牌

- ## [FormD T1 - sub-10L 64-core Epyc with 7x Gen4 NVMe : r/sffpc _202506](https://www.reddit.com/r/sffpc/comments/1l48tsm/formd_t1_sub10l_64core_epyc_with_7x_gen4_nvme/)
  - FormD T1 Sandwich V2.1 (silver, CNC): 13.5 x 22 x 33.5cm, 10L
  - AsRock Rack ROMED4ID-2T deep-ITX motherboard
  - AMD Epyc Milan 7Y83 64 cores 128 threads (OEM of 7763)
  - NVIDIA Quadro RTX A4000 16GB
  - Samsung 2S2Rx4 DDR4 ECC RDIMM 256GB (4x64GB)
  - Samsung PM9A1 2TB (OEM 980 Pro, heatsink)
  - 6x Gen4 M.2 NVMe on 3x carrier cards/heatsinks connected via SlimSAS (PCIe Gen4 8x per card)
  - Custom GPU-side fan bracket (Xianyu)
  - Corsair SF850

- ## 🌰 [An Epyc FormD T1 : r/sffpc _202504](https://www.reddit.com/r/sffpc/comments/1k1grum/an_epyc_formd_t1/)
  - Asrock Rack ROMED4ID-2T
  - Epyc 7532
  - 4 x 64GB 3200MHz RDIMMs
  - Corsair H100i Elite Capellix with slim Noctua A12x15s
  - Runs Proxmox with a bunch of VMs
  - Despite having no GPU, this leaves a lot of room for more U.2 drives

- Be careful with U.2s, you usually need some decent airflow to keep them cool. I would use the gpu hole to push or pull some more airflow.

- ## [Built a Powerful and Silent AMD EPYC Home Server with My Kids (for a Fraction of the Price!) : r/homelab _202412](https://www.reddit.com/r/homelab/comments/1hmnnwg/built_a_powerful_and_silent_amd_epyc_home_server/)
  - we built a beast of a home server powered by an AMD EPYC 7C13 (3rd gen).
  - CPU - AMD EPYC Milan 7C13 64C/128T 2.2GHz SP3 (100-000000335 7763 7713)	
  - Motherboard - Supermicro H12SSL-NT SP3 AMD EPYC DDR4 ECC	
  - RAM - Samsung 64GB DDR4 LRDIMM ECC x8 (512GB Total), DDR4 RAM: Delivers 130GB/sec bandwidth.
  - Case - Fractal Design North (White/Oak)	
  - CPU Cooler - Noctua NH-U14S TR4-SP3 (Premium-Grade)	
  - PSU - 850W SFX (ATX 3.0, PCIE 5.0 Ready, 80 Plus Gold)	
  - SSD - Samsung 990 Pro 1TB (7450 MB/s Read)	

- Regarding the CPU, I see some active listings on eBay – try searching for "AMD EPYC Milan 7B13" (or 7C13) for the same price range. Just a heads-up, though – there are engineering sample (ES) listings on eBay. Keep in mind that confidential computing (AMD SEV-SNP) won’t work on those CPUs, and there might be other feature limitations or performance issues that I’m not fully aware of.

- ## [Mini ITX EPYC 64 core 128 thread SFF Build : r/homelab _202311](https://www.reddit.com/r/homelab/comments/182i7k3/mini_itx_epyc_64_core_128_thread_sff_build/)
  - Case Cooler Master Nr200: 360 x 185 x 274mm, 18.25L
  - ASRock Rack ROMED4ID-2T Deep Mini ITX motherboard.
  - Noctua NH-U12S TR4-SP3 cooler. 猫头鹰 风扇
  - AMD EPYC ROME SP3 ZEN2 7662 64-Core 128 thread
  - 256gb DDR4 RAM.
  - 4tb NVME Drive.
  - Running ESXI vSphere 8 with VCenter Server 8.
  - Ruining like a champ temps in the 40-50c range, headless server with IPMI OOB management and remote console.
  - Putting these similar spec parts in a cart on Newegg says $5101.52.
- I’m guessing the cpu / mb / memory probably cost around $1000 on the used market. I just built a similar system second hand.. hard to beat the value!

- They make Mini ITX EPIC boards? That is wild.
  - They’re not true ITX. They are Deep ITX (wider than ITX, closer to mATX, but the same height as ITX).

- 🆚 what's the difference between RDIMM and LRDIMM?
  - [UDIMM, RDIMM, and LRDIMM | Exxact Blog](https://www.exxactcorp.com/blog/HPC/differences-between-dual-in-line-memory-modules-rdimm-vs-lrdimm)
  - A DIMM (Dual In-line Memory Module) is the physical memory stick that houses DRAM (Dynamic Random-Access Memory) chips. These chips serve as RAM, the volatile memory pool CPUs use to quickly access data during tasks.
  - UDIMM (Unbuffered DIMM): Standard in desktops and laptops. Affordable, simple, and designed for everyday tasks like browsing, content consumption, and productivity.
  - RDIMM & LRDIMM: Specialized modules designed for servers and enterprise workloads, offering higher capacity and stability. These will be the focus of this guide.
  - Registered DIMMs (RDIMMs) are designed for greater stability and scalability than standard UDIMMs. They include a register buffer that improves signal integrity and reduces the electrical load on the memory controller. 
  - Load-Reduced DIMMs (LRDIMMs) push performance further by using advanced buffering to minimize electrical load and maximize capacity. They are built for systems that demand extreme memory density and efficiency.

- For the price of this even 2nd hand (~$1500 for 7662, $300 mobo) -- you could build three separate i9-12900k/ryzen-7900x systems which combined would be about 2.5x-3x the total compute of this
  - This motherboard doesn't even get you the PCIe lanes.
  - 3x the compute but also a lot more power and maintenance. There's a reason hyperscalers use Epycs for x86.

- How's the noise?
  - I put in 4 noctua fans, and it's silent. Along with the 2 cooler master fans that came with the case and its dead silent. Although I need to put more VMS and put it to the test.

- ## [[PC] AMD EPYC Milan (7763) 64c/128t Build : r/homelabsales _202501](https://www.reddit.com/r/homelabsales/comments/1ht1fdu/pc_amd_epyc_milan_7763_64c128t_build/)
  - Got an email yesterday saying electricity rates in my area are increasing. My wife gave me the look after looking at the power bill for this month (and looking over previous months...) so I'm looking into downsizing my homelab.
  - AMD EPYC 7J13 (Oracle rebrand of the 7763 with slightly higher clocks) 64 core / 128 thread CPU
  - ASRock Rack ROMED8-2T motherboard (with Intel NICs)
  - 512GB (8x64GB) DDR4 3200 Registered ECC RAM (MICRON MTA36ASF8G72PZ)
  - ARCTIC Freezer 4U heatsink
  - ICY DOCK ToughArmor MB720MK-B 4x NVMe enclosure w/ OCuLink PCIe splitter card
  - ASUS Hyper M2 PCIe 4.0 card with 2x Optane P1600X 118GB + 2x Samsung 970 EVO 1TB
  - Intel X710-DA2 10G card
  - Intel Arc A380
  - Corsair HX1000i 1000W power supply
  - Prices seem to be all over the place for the CPU and motherboard, but the RAM prices seem to be stable. I was thinking in the ballpark of $3000 for everything?

- My energy rates went up too. My solution was to buy 8 more solar panels.

- ["Sleepy Chungus"... AMD EPYC 7763 w/ 64x cores, 128 threads @ 2.450GHz in a GEEEK A30 V2 && 128GB of Quad Channel, Dual Rank, 3200MT ECC REG RAM (linux sffpc, btw) : r/sffpc _202209](https://www.reddit.com/r/sffpc/comments/x6phtn/sleepy_chungus_amd_epyc_7763_w_64x_cores_128/)
- I'm not sure if it is called sffpc with that psu and chonky cooler.

- ## 🌰 [AMD EPYC mini-ITX build : r/sffpc _202207](https://www.reddit.com/r/sffpc/comments/w81afy/amd_epyc_miniitx_build/)
  - Case: Streacom DA2 V2, 340 x 286 x 180mm, 17.5L
  - Motherboard: Asrock ROMED4ID-2T
  - CPU: AMD EPYC 7443P, 24 Core, 48 Thread
  - RAM: 4x 64GB DDR4 PC4-25600 3200MHz LRDIMM ECC
  - PSU: Corsair SF Series SF600 SFX 600W
  - Boot SSD: Micron 3400 512GB NVMe M.2
  - Storage SSD: Samsung PM1723b 15.36TB NVMe U.2
  - CPU Cooler: Noctua NH-D9 DX-3647 w/ NM-AFB7b bracket for SP3
  - Exhaust Fan: Noctua NF-A9
  - Side Intake Fan: Noctua NF-F12
  - M.2 Cooler: Sabrent SB-HTSK
  - U.2 Cable: HighPoint Slim SAS SFF-8654 to 2x SFF-8639
  - TPM: Asrock TPM2-SLI
- About $5650 USD new, plus the cost of the bracket which I couldn't find. OP mentioned they bought the SSD slightly used, so probably closer to $5500

- With that PSU where it is, is it even possible to fit a GPU in there?
  - It's all been measured, 2x 40Gbps QSFP+ card is coming later

- Those are some absolutely eye watering specs (24 cores, 256gb, and a 15.36 TB ssd, I'm so jealous lmao). Truly a cut above. It's so cool to see enterprise hardware crammed into a boutique SFF.

- Only ASRock would make a mini ITX SP3 motherboard
  - Asrock rack is a true miracle, at least when I did my epyc server build, it was the only company that was selling mobos for epyc that wasn't stupidly overpriced, or required you to at least buy a barebones server (looking at you supermicro).

- Does epyc have igpu?
  - No, but the motherboard has a basic onboard GPU

- [AMD EPYC mini-ITX home hypervisor : r/homelab _202207](https://www.reddit.com/r/homelab/comments/w81bxe/amd_epyc_miniitx_home_hypervisor/)
- Some of the best features of Epyc are wasted in this configuration though. PCIE lanes, 8 channel memory etc..

- What is the power requirement for that?
  - The verdict @ 240V AC (I'm in the UK): 60-70W idle and 235W maxed out

- [如何在本地部署DeepSeek-R1模型？ - 知乎](https://www.zhihu.com/question/10630134422/answer/89240187608)
- 国外这个博主Matthew Carrigan 提供了在本地运行 Deepseek-R1 的完整硬件和软件配置，成本差不多是6000美元，token的output大概在6-8个每秒。
- 注意：这是纯CPU版本，GPU版本得10万美元+，所以也可以理解为穷鬼套餐。
  - 主板：技嘉MZ73-LM0/LM1（双路EPYC插槽，解锁24通道DDR5带宽）
  - 处理器：双路AMD EPYC 9004/9005系列（推荐9115/9015，性价比之选）
  - 内存：24×32GB DDR5-RDIMM（总量768GB，必须占满24通道！
  - 参考型号：vColor/Neumax）
  - 机箱：Enthoo Pro 2 Server版(支持服务器主板安装的消费级方案), 240 x 580 x 560 mm, 77L
  - 电源：实测功耗<400W，但需双CPU供电。海盗船HX1000i为稳妥选择（兼容型号可平替）
  - 散热：SP5插槽需特殊散热器（Ebay/Aliexpress专供型号实测可用），静音改造方案见Newegg
  - 存储：1TB+ NVMe SSD（700GB模型加载考验持续读写，PCIe4.0为佳）
  - BIOS设置：NUMA组数量设为0 → 内存全交错访问 → 吞吐性能翻倍

- ## [Is quad channel simply 4 sticks of ram together? | guru3D Forums _201203](https://forums.guru3d.com/threads/is-quad-channel-simply-4-sticks-of-ram-together.360222/)
- No, dual channel and quad channel are different.
  - To put this very simply, think of it like a road, dual channel gives you 2 lanes and quad channel gives you 4 lanes. This enables twice the traffic (data) to be sent because you've doubled the number of lanes (channels) the traffic can use to get where it's going.
  - For memory to work in dual channel you need a pair (or 2 pairs) of memory sticks and for quad channel you need 4 memory sticks (or 8 if there will be any motherboards that support 8 sticks?)

- but as you know lga775 and 1155 for eg have 4 dimm slots so if you put in 4 sticks of ram why is that still dual and not quad channel?
  - That's because it's what's supported by the chipset. X79 will be able to run quad channel, just like my x58 chipset supports tri channel and your x48 chipset only supports dual channel.
  - Just because you have 4 dimm slots doesn't mean you can run quad channel.

- quad is not just 4 sticks.

- ## [Memory Channels: Single, Dual, Triple, and Quad Channel Memory](https://storedbits.com/types-of-memory-channels/)
- Memory channels matter mainly for bandwidth and not directly the speed. It also doesn’t double your RAM size — but it can double the bandwidth, meaning more data can travel at once. That helps especially in tasks like gaming, video editing, or multitasking.

- What is a Memory Channel?
  - The CPU connects to the memory through a pathway. It is a dedicated data path between the CPU (or memory controller) and RAM modules (DIMMs). On the physical level, it is composed of copper traces (wires) etched into the motherboard, connecting the CPU socket (specifically, the memory controller) to the RAM slots (DIMMs).

- ## [为什么AMD霄龙EPYC最多只支持双路运行？ - 知乎](https://www.zhihu.com/question/7936434502)
- 处理器之间是需要高速信号互联的，每个都要和其他所有处理器有直接联系线（否则就准备好延迟爆表吧）。双路只要一根联系线。四路需要6根。八路需要。。。。每个u和其他7个一根，8*7根，由于A到B和B到A重复计算了，除以2，得到28根。这些连接线都是和内存布线一个难度的高速信号线，而且由于CPU插座的物理尺寸巨大，这些信号线还不得不变得极长，并且8路布线几乎无可避免地要与内存布线发生冲突，以至于根本没办法控制主板的成本。 

- 因为OEM厂家都反馈 4路，八路系统，全球一年卖不出100套。

- 霄龙单片处理器就是十二通道二十四条内存，俩处理器就是四十八条
  - 单个处理器就是128条pcie通道，两个就是256条，哪怕全塞都能塞32个，如果做成常规pciex16接口能塞16根，一般是不会配置这么多x16
  - 再多……主板塞不下啊，霄龙的设计本来就是单处理器塞海量核心和海量扩展，多ccd设计它单个处理器就类似多路处理器了

- ## 💡 [Local LLM Build with CPU and DDR5: Thoughts on how to build a Cost Effective Server : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1kjvo1t/local_llm_build_with_cpu_and_ddr5_thoughts_on_how/)
  - DDR5 RAM: 576GB (4800MHz, 6 lanes) - Total Cost: $3, 500(230.4 gb of bandwidth)
  - CPU: AMD Epyc 8534p (64-core) - Cost: $2, 000 USD
  - 8xx Gen EPYC CPUs: Chosen for low TDP (thermal design power), resulting in minimal monthly electricity costs.
  - ASUS S14NA-U12 (imported from Germany) Features include 2x 25GB NICs for future-proof networking.
  - qwen3:32b-fp16: 1.14 tokens/s
  - qwen3:235b-a22b-q8_0: 2.70 tokens/s
  - deepseek-r1:671b_1.58bit: 3.26 tokens/s

- Older EPYC models (e.g., 9124) offer a balance between PCIe lane support and affordability.

- The GPU could be used to run attention layers and host kv cache. llama.cpp's -ot 'ffn=CPU' (or -ot 'exps=CPU' for MoE) is worth a try.

- While the hexa-channel DDR5 4800MHz configuration provides similar bandwidth to Epyc Milan's 8x 3200MHz DDR4 (204.8GB/s), Zen4 (c) might offers faster prefill performance due to AVX512 support.
  - You might want to offload MoE tensors using: `--override-tensor 'blk\.\d?\d\.ffn_.*_exps.weight=CPU'`

- Additionally, frameworks like ktransformers or ik-llamacpp are optimized for CPU-GPU hybrid inference.

- In China, there are OEM server processor options that offer comparable pricing to entry-level models while delivering significantly better prefill performance than 16-core alternatives. Notable affordable options include:
  - - **Intel**: Xeon 8455C (48C SPR 8xDDR5 4800MHz ~¥6, 000), 8481C (56C SPR ~¥7, 500), 8581C (60C EMR 8xDDR5 5600MHz ~¥9, 000)
 - **AMD**: Epyc 7B13 (64C Zen3 8xDDR4 3200MHz ~¥3, 800), Epyc 9v74 (80C Zen4 12xDDR5 4800MHz ~¥8, 500)
  - Beyond OEM models, the Epyc 9375F (32C Zen5 12x6000MHz) offers exceptional single-core performance and memory bandwidth, making it better for some tasks. 
- When comparing Intel and AMD platforms:
  - Intel's advantage lies in AMX instructions for prefill acceleration
  - AMD offers 12-channel DDR5 bandwidth and more PCIe lanes for multi-GPU setups

- ## 🆚📌 [Memory Bandwidth Comparisons - Planning Ahead : r/LocalLLaMA _202402](https://www.reddit.com/r/LocalLLaMA/comments/1amepgy/memory_bandwidth_comparisons_planning_ahead/)
- Epyc actually has 12 channels of ram. The latest 9004 series has 460.8 GB/s. Threadripper is the one that comes with quad and octa channel variants.
  - Note: The upcoming Epycs(zen 5) are supposed to have even more bandwidth due to the new out-of-the-box ram speed being 6000mhz instead of the current 4800mhz
- 6000 MT/s would be nice, giving 4090-like memory bandwidth over 24 channels in a 2P system, but with a minimum of 384GB instead of a maximum of 24GB. That's assuming all else is equal/negligible, which isn't quite the case.
- Yeah, an ideal environment for sparse MoE like Mixtral

- Lpddr5x at 120gb/s I have a core ultra 7 155h with lpddr5 at 100gb/s. You can ask me for some tests if you want

- Is there any reason that regular consumer motherboards can't support quad or 8 channel RAM? I feel like if we can have 8 channels DDR6, we'd be at around 600 to 800GB/s, which is very similar to gpu vram speeds. Maybe this is what we should ask AMD to do instead of GPU's with 46gb or 96gb RAM for consumers at reasonable prices.
  - It would normalize everyone potentially having great bandwidth for local inference, wouldn't require a GPU at all, and would basically explode the number of devices that could locally inference at reasonable speed. This would open the flood gates for local llm's - open or closed source, because now everyone and their grandma would be able to use it effectively.
  - And unlike GPU's, you'd never be limited by how many GB's of RAM you want to install, and therefore not be dependent on NVIDIA (or whomever) to hopefully one day release a card with more VRAM. The power would go back to consumer. And the bandwidth would double again for DDR7 and so on.
  - I just don't know if putting quad or 8 channels on a motherboard is somehow difficult and can only done at high price to the consumer, which is why only pro-sumer or server level mobos do it.
- They could, but the main limiting factor is the memory controllers are on the CPU. Intel, AMD, and the others use number of channels as a market segmentation method. But ultimately it boils down to memory channels equal $$.

- The bandwidth numbers for the Apple M1/2/3 SoC are just the raw totals from the memory, but depending one which cluster is using it (P-cores, E-cores, GPU) they have their own limitations. Here is the explanation for the M1 series
  - On the M1 Max with 400GB/s the CPU can get maximum 204GB/s when using the P cores only or 243GB/s when using both the P and E cores.

- ## ⚡️📊 [有人可以做一个epyc服务器CPU的天梯榜吗？ - 知乎](https://www.zhihu.com/question/596966739)
- cpu, cinebench-r23, pricing
  - AMD Ryzen 9 7900x, 3.0w, 2469
  - AMD Ryzen 5 7500F, 1.4w, 938
  - AMD EPYC 7742/7B12, 5w, 6k
  - AMD EPYC 7552, 4.4w, 6k
  - AMD EPYC 7702, 4.9w, 7k
  - AMD EPYC 7b13/7c13/7v13/7763/7j13, 6.1w, 5850
  - AMD EPYC 9V74, 10.5w, 1.2w 
  - AMD EPYC 9654 ES, 11w, 1w 

- [AMD Server Processor Specifications](https://www.amd.com/en/products/specifications/server-processor.html)

- [Ryzen Threadripper - AMD - WikiChip](https://en.wikichip.org/wiki/amd/ryzen_threadripper)
  - 🎯 zen2(2019):
    - [Template:AMD Epyc 7002 series - Wikipedia](https://en.wikipedia.org/wiki/Template:AMD_Epyc_7002_series)
  - 🎯 zen3(202103): uni epyc 7313p/7443p/7543p/7713p; 
    - multi epyc 7313/7343/7443/7543/7713/7763
    - EPYC 7003 "Milan"
    - 8 channels per socket, up to 16 DIMMs, max. 4 TiB
    - Up to PC4-25600L (DDR4-3200)
    - [Template:AMD Epyc 7003 series - Wikipedia](https://en.wikipedia.org/wiki/Template:AMD_Epyc_7003_series)
  - 🎯 zen4(202211): uni epyc 9354P/9554p/9654p; 
    - multi epyc 9124/9174F/9224/9254/9454/9634/9654
    - lp/edge: 8324p/8324pn/8434p/8534p
    - EPYC 9004 "Genoa": 12 channels per socket, two 40-bit (32 data, 8 ECC)
    - DDR5 subchannels per channel
    - Up to 24 DIMMs, max. 6 TiB
    - Ryzen 7000 "Raphael"
    - 9124: 16-core (32-threads), 4 × CCD, I/OD, Base Clock3.0GHz, DDR5-4800, p-200w
    - 9224: 24-core (48-threads), 4 × CCD, I/OD, Base Clock2.5GHz, DDR5-4800, p-200w
    - 9254: 24-core (48-threads), 4 × CCD, I/OD, Base Clock2.9GHz, DDR5-4800, p-220w
    - EPYC 4004: 16 AMD ”Zen 4” cores, 32 threads, an L3 cache of 128MB, DDR5 memory support, and 28 PCIe® 5 lanes
      - Max DDR5 Freq (MHz) (1DPC): 5200
      - EPYC 4004 is just a rebrand of the Ryzen 7000 series, designed to make it clearer about ECC support mainly.
    - [Template:AMD Epyc 9004 Genoa - Wikipedia](https://en.wikipedia.org/wiki/Template:AMD_Epyc_9004_Genoa)
  - 🎯 zen5(202411): uni epyc 9015p/9125p/9355p/9755p
    - multi epyc 9005/9015/9115/9125/9175F/9335/9665/9755F
    - The series offers core counts ranging from 8 cores to 192 cores, 
    - with support for up to 12 channels of DDR5-6000 memory (up to 6 TiB per socket) and 128 PCIe 5.0 lanes
    - [Template:AMD Epyc 9005 series - Wikipedia](https://en.wikipedia.org/wiki/Template:AMD_Epyc_9005_series)
    - [Template:AMD EPYC 9000 Series - Wikipedia](https://en.wikipedia.org/wiki/Template:AMD_EPYC_9000_Series)

- [如何评价AMD EPYC 7B13, 7K83, 7T83, 7763? - 知乎](https://www.zhihu.com/question/542414897)
  - 这些都是7763的马甲，另外还有7J13，7V13等等。。。。可能频率设置上略有不同，TDP也都是280W，基本上满载性能是差不多的。
  - 各种云的定制版，云厂商有功耗，性能的需求，amd就帮他们定制

- 性能真强，说起来7r13最近好像降价了，感觉好像比7c13还强点。

- EPYC服务器基本上不需要调优，7003还有NPS设置，9004开始就没有了，内存频率只需要自动，自己调高必蓝屏。

- epyc很多都是用来跑cfd的，用这种跑分软件得出的结论完全是错的。都是用openfoam做benchmark对比，别说7950x了，我的9950x和我的epyc工作站比起来，都被秒到渣都不剩

- epyc系列的CPU，根本不需要做天梯图，性价比最高的应该就是7？83（7T83、7W83等）。1w出头就能拿下板+64核U。

- [EPYC或线程撕裂者或XEON有没有单核性能强又廉价的U? - 知乎](https://www.zhihu.com/question/1947237447466464324)
  - 目前是看了q30h/q2t7和ms03还有256g d内存，预算刚好卡的很死，不知道有没有啥更优的选择

- ## [DDR5 2 vs 4 sticks, different speeds, same bandwidth? Confused : r/overclocking _202502](https://www.reddit.com/r/overclocking/comments/1ih0rfe/ddr5_2_vs_4_sticks_different_speeds_same/)
- 4 sticks will still run in dual channel. So 4x sticks @3600 will always be much slower than 2x sticks @6000
  - There are thousand of posts like this one Every week.
  - Just return 2x sticks or all 4x and just get 2x48 6000-6400MTs if you need high capacity & speed

- 2 sticks is the same bandwidth as 4 because each channel shares 2 lanes. You’re only getting half the bandwidth on each stick with 4. Not really all that much to it.

- ## [do you think i could run the new Qwen3-235B-A22B-Instruct-2507 quantised with 128gb ram + 24gb vram? : r/LocalLLM _202507](https://www.reddit.com/r/LocalLLM/comments/1m5wgcg/do_you_think_i_could_run_the_new/)
  - i am thinking about upgarding my pc from 96gb ram to 128gb ram. do you think i could run the new Qwen3-235B-A22B-Instruct-2507 quantised with 128gb ram + 24gb vram? it would be cool to run such a good model locally

- The old qwen3 235b model ran, at UD-Q4_K_XL, on my system with a R9 7950x and 96gb ram and a 4090 with 24 gb vram. ~5 t/s once it was warmed up. Processing speed was about the same though (X_X).
  - That's the best I got, so far. I tried a few different off-loading strategies, but just offloading to cpu for most of it and MMAPing the file was what did the best on my system with its constraints.

- You should be able to run the Q4 with that. How fast will it be will depend on what speed RAM you have.

- Someone ran qwen235b at iq4 on 2 sticks of 64gb ddr5 5600 with 3.5-4 tokens/s on cpu only (7950X).

- two amd mi50 & 128 gb ddr5 could gen 7 t/s With Qwen235b-Q4

- ## [How large models can I run with 128GB RAM? : r/LocalLLaMA _202405](https://www.reddit.com/r/LocalLLaMA/comments/1cjtzft/how_large_models_can_i_run_with_128gb_ram/)
  - I currently have 16GB VRAM and 32GB RAM. I would be fine upgrading to 128GB RAM.
  - I can run 8B/13B-models without issues. Can I run larger models if I upgrade to 128GB? I would want to avoid buying more RAM unless it has any effect.

- I get 1.9 tokens/sec generation speed on wizardlm2 8x22b q5_k_m 128gb quad channel ddr 4 2133. No offloading to gpu.

- Adding more RAM will just allow you to run larger models ...on the CPU, which will be incredibly slow.

- ## [Mini-PC Dilemma: 96GB vs 128GB. How Much RAM is it worth buying? : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nmlluu/minipc_dilemma_96gb_vs_128gb_how_much_ram_is_it/)
  - I'm planning to pick up one of the new mini-PCs powered by the AMD Ryzen AI Max+ 395 CPU, specifically the Bosgame M5. The 96GB RAM model looks more cost-effective, but I'm weighing whether it's worth spending ~15% more for the 128GB version.

- Get the 128 GB one. It's soldered RAM so not upgradable.
- Under Linux, you can use all the RAM with the GPU at full speed (215 GB/s).
  - That means qwen 235B A22B at Q3_K_XL at 15 tokens per second at 54 Watts.
- The ability to run up to 120 GB MoE models is likely to be increasingly useful as more are released.
  - It's also possible to run other models in Comfy UI but slow and unstable. This may improve when AMD makes rocm less uncompetitive with cuda.

- I have a pc with 192GB and I would use 256 if I would have purchase 4x64 instead. I run huge model (like DeepSeek-V3.1-UD-TQ1_0.gguf) on a 3090 + 172gb pc ram (ddr5) and I can get 'decent' around 3T/s, which I find acceptable since I can configure many parameters.
  - If you use GGUF versions of a model, you don't need 'pc' ram to be dedicated as a gpu, since the model can put part of the model on the pc ram by itself.

- ## 🆚 [Any downside to having 128GB of RAM on two 64GB sticks? : r/buildapc _202510](https://www.reddit.com/r/buildapc/comments/1nyso1u/any_downside_to_having_128gb_of_ram_on_two_64gb/)
  - should I split it into four 32GB sitcks or get two 64GB sticks? Is there a difference to performance? 
  - The specific product I am looking at is: Corsair Vengeance 128 GB (2 x 64 GB) DDR5-6400 CL42 Memory
  - I could get 4 32GB sticks instead. But I read that DDDR5 RAM doesn't play well on AM5 if all 4 sticks are utilized.(AM5 is the latest CPU socket from AMD for their Ryzen 7000, 8000, and 9000 series desktop processors, replacing the long-lived AM4 socket)

- You likely won't be able to run four sticks at 6400MHz, but there's a higher chance you will with two. It depends on your CPUs memory controller. I have 4x32GB 6000 CL30 and I regret not getting 2x64GB. You live and you learn, I guess.

- Much better performance-wise on two sticks than four for DDR5. Downside is lighter wallet lol

- You can buy a motherboard with 4 slots and only populate two of them. Dual slots ATX mobo are very rare
  - I have built and bought parts for at least a dozen pcs and i am pretty sure i have never seen a motherboard with 2 slots in my life
- It’s much more common in ITX boards though.
  - Only itx and ultra cheapo boards have 2 slots.
- All mini-ITX form factor board, a good chunk of the lowest end budget boards (ie stuff like A620 boards generally intended for low power office machines, that kind of thing), and a tiny few extremely high end OC boards (two slots to use larger traces and maximize stability at extreme OCs). You can just set the number of slots to 2 on PCPartPicker to see.

- If only people listened to Apple, and just accept whatever memory came on the device when they bought it
  - Shit, big Repair got to him in the middle of his sentence

- Maybe look into 2x48GB: 6000MT/s CL30 isn't extortionately priced.
  - 2x48 is your best bet. You won't have any performance penalties with that setup. 2x48 is no harder on the memory controller than 2x32.
  - I wouldn't take the performance hit of 128GB of RAM, when 96GB works so well. Unless you really need it, that is. I use my PC as a workstation for processing drone imagery into orthomosaic maps and 64GB is enough for mapping up to 150 acres. OpenDroneMap is pretty damn RAM hungry.
  - If you really need 128GB, you should still stick to 2 DIMMs. 2x64 is going to be much easier to get working and perform better than 4x32.

- 2 sticks have better performance than 4 sticks.

- As others are saying, dual channel is going to be significantly more stable (as opposed to 4x32).
  - If you’re just tinkering and have the funds, there are boards that support 196GB. Or you could jump up into professional stuff and go for 256GB.
  - All of this is assuming DDR5.

- There's not any specific significant downside, but some motherboards may not support a RAM with 64 GB per stick without a BIOS update.
  - those 64 GB sticks are slower than 32 GB sticks (6000 CL30 > 6400 CL42), as small as it might be.
  - If your PC is AM5, 6400 is highly not likely to boot with 1:1 setting. There's also going to be longer RAM training initially but that's just one time thing.
  - 99% of AM5 CPUs support 6000 at 1:1 speed, but any higher than that, the chance lowers noticeably, and 6400 1:1 is a crap shoot. It doesn't help it's a high-capacity 64 GB, dual rank stick, which puts even more stress on to the memory controller.
  - It's just the way AM5 CPU memory controllers roll. There's a G. Skill 128 GB set (2x 64GB) that is EXPO certified, at 6000 CL32, 34, or 36 that you can get instead. I have the CL34 one, then tightened RAM timings to get it down to 32-40-40-76.

- Most consumer motherboards are going to be duel channel RAM, which means there's only 2 lanes between the RAM and CPU. Duel channel motherboards with 4 RAM slots will have 2 slots share a lane, so in most cases you will get more performance out of 2 sticks then 4, as long as you follow the instruction in the manual and use the slots it tells you to put the RAM in (Often A2 and B2, but can be board specific).
  - Worth keeping in mind a lot of CPU/motherboards don't seem to like running large amounts of RAM at higher speeds, so while it might be rated for 6400MHz you might have to run it at 6000, so if there's a cheaper 6000MHz kit, especially one with a lower latency (CL42 part of the product you listed) I'd probably go for that. Having a quick look it seems common for CL40+ on 128GB kits though.
  - If you must have 128GB, look at something like the G. Skill Flare X5 128 GB (2 x 64 GB) DDR5-6000 CL32 Memory for lower latency, though depending on where you are lower latency kits might not be available. I can see a few CL32 128GB kits available on US pc part picker, but the lowest it shows in stock for the UK is CL34, still be an improvement over CL42.

- for some reason, 2x48gb 6400 CL32, is relatively easy to find
  - but once you go for 2x64gb, it's hard to find good timings.

- it's possible to get even 256GB of DDR5 to run on AM5, so 128GB should be no problem.. not at the same speeds you'll get with the "generic gaming choice of 32GB", ofc, but still.

- if you want 6400MHz, shoot for CL32
  - I believe that is the correct spread change for DDR5 above 6000MHz CL30

- I got 4 sticks CL30 Corsair Vengeance 4 x 32, could not get >5200Mhz stable with MSI Mag 870E + 9950x3D. Though running benchmarks, I ended up staying with this config instead of 2x48 @ 6000. There was negligible difference in any of my workloads, for work or for play, but I did benefit from the at the time, very drastic price difference, and the additional ram is useful in my use-cases.

- Why stop at 128gb when you can now do 256gb 6000MT? See Level1Techs video from 2 weeks ago.

- ## [求推荐mATX非侧透散热好的机箱 - 小红书](https://www.xiaohongshu.com/explore/688e90bf00000000250132d4?xsec_token=ABB5_uSFZrF4Fb2p-cIYUQAnzEybe1BZQKTuCoX9z-RXw=&xsec_source=pc_search&source=unknown)
- 看标题第一反应就是追风者xtm3了，水冷可以选华硕的那个，风冷对风道要求高只有追风者合适了。
  - 如果你的配件都是无光的，也无所谓非侧透了

- 酷冷至尊 MASTERBOX NR200 📌
  - [机箱_产品_酷冷至尊 - NR200](https://www.coolermaster.com.cn/product_detail/335.html)
  - 185x292x376mm, 20.3L 
  - 支持显卡尺寸 330x156x60mm

- [ASUS Prime AP201 MicroATX Case｜機殼｜ASUS 香港](https://www.asus.com/hk/motherboards-components/cases/prime/asus-prime-ap201-microatx-case/)
  - 33 公升的時尚 MicroATX
  - 可容納 360 mm 水冷散熱器、長達 338 mm 顯示卡及標準 ATX 電源供應器

- ## [有没有单条64G或者更高的内存条？ - 知乎](https://www.zhihu.com/question/518047029)
- 普通的DDR4内存单条32G到顶，要服务器用的RECC内存有单条256G, 512G的，当然，价格美丽，且需要主板支持
- 普通条也有64g
  - ddr4 有，ddr5不确定因为还没见到

- ## [为什么市面上没有单条64G的高频DDR5（8000MHz以上）？ - 知乎](https://www.zhihu.com/question/1932456730404587491)
- 你内存频率上去了，主板和CPU跟不上，白搭
- 4月份，芝奇就发布了一款DDR5-8000 128GB内存套装，也就是DDR5-8000 128GB（64GB x2），时序为CL44-58-58-127，不过这款内存条暂时还没有上市，属于第一款64G的DDR5-8000频率内存。
  - 4月份，芝奇就发布了一款DDR5-8000 128GB内存套装，也就是DDR5-8000 128GB（64GB x2），时序为CL44-58-58-127，不过这款内存条暂时还没有上市，属于第一款64G的DDR5-8000频率内存。
  - 现在DIY装机的主流依然是32G（16x2）DDR5 6000MT/s及以上频率和48G（24x2）DDR5 6000MT/s的内存条，即使你在某些生产力上面需要，也64G（32Gx2）DDR5就很足够了。
  - 正常你要是AMD的话，上套6000MT/s的DDR5就可以，而英特尔处理器的话，实际上套6800MT/s的DDR5就可以，所以不用纠结太多。

- 事实上呢，GSK4月份就发表了8000MHz的128GB套装和64GB的DDR5-9000MHz特挑套装，然而具体的供货呢？还是那些渠道才有，官方不会随便给价格，好方便他们漫天要价
  - 8000MHz超高容量比6400s的贵一倍或者两倍都是渠道商们随便喊。
  - 你要搞这种东西，其他部分可以先满上，然后坐等上货，毕竟芝奇的测试平台用的就是：
  - 处理器：Intel-265K/AMD 9950X3D
  - 主板：华硕玩家国度MAXIMUS Z890 APEX
  - 显卡：NV XPG RTX5090-24G

- ## [感觉华擎的东西做工用料都很扎实呀，为什么都说是二线？ - 知乎](https://www.zhihu.com/question/354822608)
- 华擎是现在公认的四大家，所有的跟主板相关的认证，一定是会通过华擎的认证的。
  - 在硬件支持上也一直保留自己的特色，接口和设计上一直都算是比较全且相当友好的。
  - 他最辉煌的时候应该是2011年，当年的销量冲破了1000万片，达到全球第三。
  - 前几年跟华硕分手导致的阵痛比较严重，不过自身产业转型升级很快，渠道上不太待见华擎，主要原因是不赚钱，没有补贴力度（深有体会，没有会议没有钱……）
  - 现在华擎在国内的份额只有2.8%，属实可怜，基本看不到了，而且华擎不接受个人送保，主板性价比极佳，但除非是京东自营，否则自己买，售后属实麻烦。
  - 代理商是不给予代保的（比如大仙，不是自己出的华擎主板坏了，是真的会拒绝代保）
  - 不过总销量依旧可观，全球主板出货量在2018年的时候是400万片左右，2019年好像有接近600万的出货量（数据存疑，因为找不到之前看到的数据了），相当可观，逆市增长那种。
  - 保持出货不跌的只有微星和华擎两个品牌。
  - 华擎是迄今为止，最友好的台湾系品牌商之一，中文网站访问和资料搜索都是最友好的，说出来一般人都可能不太相信。。

- 华擎支持个人送保，只需出来回运费

- 我个人感觉其实就是售后率，售后效率太扯淡了点。1所有主板售后得通过省代资格的手寄到深圳，再返回来，和微星有的一拼。
  - 从矿板就可看出售后率太高，前几年矿潮也就几K的矿板售后率硬是能大几百片，果真一分钱一分货。至今仓库里还10片矿板懒得售后也没人要，等过保修了卖垃圾。

- [如何评价华擎主板？ - 知乎](https://www.zhihu.com/question/27738363/answers/updated)

- ## [Mini server for virtualization with 128 GB ECC RAM, many CPU cores : r/selfhosted _202412](https://www.reddit.com/r/selfhosted/comments/1hbfay9/mini_server_for_virtualization_with_128_gb_ecc/)
- DDR5 is inherently ECC. You do not need explicit ECC, though the on-die ECC only protects the DRAM cells, not the link between the CPU and the RAM chips. So, run the bus slower (say, if you have LPDDR5X 7500, run it at 6400, if you have DDR5 5600, run it at 5200), and you will be fine.
  - with on-die ECC, you do not get early warning on ECC events, and you do not get unrecovered 2-bit ECC reports, so it's safer than no ECC, but not as safe as true ECC. You should not do ZFS RAM caching, for instance, with this setup. But most home server applications should be fine, though.
  - As for capacity, you can get 96GB easily. 128GB would be hard until vendors start making 64GB sticks. So far, the largest consumer grade sticks are 48GB. My setup is MS-A1+7950X+2x48GB+2x2TB+1x118GB Optane. It's been rock solid far.

- Minisforum MS01 ticks all the boxes except RAM. It can go up to 96GB, has 3 m.2 slots, a spare pcie slot, dual 10gbe SFP, dual 2.5gbe with one offering IPMI.

- I have been eyeing the Asrock B650M PG Riptide mATX board.
  - Supports AMD Ryzen™ 9000, 8000 and 7000 Series Processors
  - Supports 256GB DDR5 ECC/non-ECC, un-buffered memory up to 7200+(OC)
  - 2.5G Ethernet
  - 2 x 16x PCIe slots (One of which may only support x4)
  - 2 x M.2 Gen 4 slots
- AsRock DeskMeet seems similar. The motherboard doesn’t seem to support bifurcation to make use of PCIe slot for 4 NVMe, and there is no IPMI. Otherwise OKish, albeit 8 liters
  - I think you have to compromise if you want a small formfactor / non-server board. Have been struggling with the same compromises. But at least this have 2.5G Lan.
  - Bifurcation would have been nice.
  - I don't think it is going to be easy to find a small/non-server board with IPMI.

- ## [Mini ITX AM5 mobo that supports 2x64GB RAM sticks? : r/sffpc _202507](https://www.reddit.com/r/sffpc/comments/1m8wexd/mini_itx_am5_mobo_that_supports_2x64gb_ram_sticks/)
- check the SFF masterlist > mITX boards tab. Lots of boards support 128GB
  - https://docs.google.com/spreadsheets/d/1AddRvGWJ_f4B6UC7_IftDiVudVc8CJ8sxLUqlxVsCz4/edit?gid=523597416#gid=523597416
  - I would still double check the motherboard's website just to be sure. As this is still a community maintained page so info isn't guaranteed to be 100% accurate.

- The Asus B650E-I has a maximum capacity of 64GB per RAM slot, with 128GB total
  - Cheaper models like the MSI 650I Edge and the AsRock B650I Lightning only have a maximum capacity of 48GB per slot

- Gigabyte X870I AORUS PRO ICE supports 128GB. Saw it on Microcenter's website and doublechecked on the Gigabyte site
- If you really want to be sure you have enough RAM capacity, you can always go for an ITX X870 board, but that's a prety big upcost for pretty much no benefits in terms of features (outside of maybe more USB ports in the rear I/O)

- ## [DDR5 128GB on ITX possible nowadays? Experience? : r/sffpc _202506](https://www.reddit.com/r/sffpc/comments/1lgu72h/ddr5_128gb_on_itx_possible_nowadays_experience/)
  - I couldn't find new information on anyone trying out 64GB DDR5 Sticks in their ITX build.
  - Would two sticks work to yield 128GB DDR5?
  - Kingston FURY Beast schwarz DIMM 64GB, DDR5-5600, CL40-40-40
  - there is also a Crucial 64GB stick and an even faster Kingston 64GB stick.
  - ✅ Update: Received my 2x Kingston 64GB RAM sticks and after a 1min initial boot, setting the right EXPO profile in the BIOS, it works perfectly without any issues. To whoever reads this in the future, good luck on your build!

- If you want small with 128GB of fast RAM, then something built with the AMD AI Max 395 might be of interest? Framework are going to sell an ITX motherboard (and SFF PC) with the same chip, but it isn't available yet
  - Yes I am aware of its existence and the advantages soldered RAM has to reduce latency and improve memory bandwidth, but I already got a rig and am happy with the general performance of my 7900. 
  - I was considering upgrading form my measly 2*16GB and my most recent info is that optimum uses 2*48GB sticks, hence 96GB, but I couldn't find further info on 2*64GB.

- Even though only 96 GB of RAM is listed there, it is possible to use 128 GB. I've seen it in YouTube videos, but only with mini PCs, and the guy used SO DIMMs.

- Just did this on my homelab server. The board claims to only support 64GB but I put this 128GB kit in and all is well. (Passed prime95, etc tests. All 128GB is usable.)

- I think the biggest problem is there hasn't actually been 64GB sticks out for very long and with the price not many people have actually tried it. I would just go with a motherboard that states it supports 128GB and make sure you buy the ram from a retailer with a good return policy.

- ## [六联智能推出 AMD "Strix Halo" Thin Mini ITX 主板，板载内存设计 - IT之家 _202507](https://www.ithome.com/0/869/805.htm)
- AMD 的锐龙 AI Max 300 "Strix Halo" 平台 ODM 伙伴六联智能推出了一款板载该系列处理器和 DRAM 内存颗粒的 Thin Mini ITX 主板 STHT1。
- 这一主板目前已被六联智能的 2L 迷你主机、8L 紧凑型台式机、一体机解决方案采用，而其兼容外形规格使之存在直接安装于标准台式机机箱的可能。
- 该主板包含 8 个 LPDDR5x 焊盘，支持至高 128GB 内存容量；配备 2 个 M.2 2280 PCIe 4.0×4 盘位；提供 1 个 M.2 2230 无线网卡位。

- ## [Is a mini-itx first homelab a good idea? : r/homelab _202207](https://www.reddit.com/r/homelab/comments/w6dzji/is_a_miniitx_first_homelab_a_good_idea/)
- At the time I was fine with just one PCIe and 2 memory slots but now the lack of expansion is a pain.
  - Was also limited in the number of drives that could be connected and has a distinct lack of fan headers (not sure if newer boards would have any more - there's really no space).

- Unless you absolutely need the smallest footprint, Mini-ITX is always a bad choice, regardless of application. 
  - There's always a hardware tax, you generally lose out on features (e.g. you get fewer PCIe slots, DIMM slots, M.2 slots), you (almost) always have worse cooling, and the actual build or modifying process is more painful (literally and figuratively).
  - I'd advise going for microATX instead, which allows you to put together a relatively compact build, but should give you more value for money and more freedom than Mini-ITX.

- ## [Are there any ITX motherboards that can handle 128GB RAM? : r/buildapc _202307](https://www.reddit.com/r/buildapc/comments/15cazh5/are_there_any_itx_motherboards_that_can_handle/)
- Are there ones that you, right now, as an average consumer or even business can buy? No.
  - There are custom ITX server boards that have 4 DIMM slots, but they're for fairly old and slow server CPUs using DDR3, and have no PCIe slots.
  - DDR5 can technically support up to 512GB on a single DIMM, so an Intel 13th gen or Ryzen 7000 ITX motherboard could theoretically support up to 1TB of RAM! But the largest consumer DIMM is 48GB, so the most you can get on an ITX motherboard right now is 96GB. 

- If you need 128GB right now, you have to get an mATX motherboard. There is no other option.
- That said, the SSUPD Meshroom S can take an mATX motherboard, even a full sized ATX motherboard, and isn't much bigger than the Terra.

- ## [Can you guys recommend me a motherboard which can support 128gb ram? : r/buildapc _202412](https://www.reddit.com/r/buildapc/comments/1hbspr8/can_you_guys_recommend_me_a_motherboard_which_can/)
  - I'm looking for a motherboard which can support a lot of ram for programming. Preferably it should be mATX. I've heard that ITX boards aren't really great for that soft of thing. edit : CPU is Ryzen 9 7950x.
- ASUS rog-strix-b650e-f has 128 GB configurations on their Memory QVL for Ryzen 9 7950x. According to them it'll run at 5200.
  - I'm considering such a configuration now. I've not yet found an ASROCK Motherboard with any 128GB configurations on their QVL.

- All of the mid tier boards with 4 memory slots will support minimum 128GB.
  - First choice you have to make is which cpu you want, after that you can look for mobo recommendations.

- Since you're 7950X (though I'd strongly recommend getting a 7900X or 9700X as they're far better value - www.bestvaluecpu.com exists, filter for AM5), just get one with decent VRMs. This probably means going ATX as high end boards are usually this size.
  - I would not recommend putting a 7900X in a ITX build, it'll get spicy in there!
  - 64gb is enough even for most video editing, 128gb is "I do programming for my job and my job paid for this PC" kind of levels.

- [128GB of RAM in a tiny box? : r/buildmeapc](https://www.reddit.com/r/buildmeapc/comments/17lxgsj/128gb_of_ram_in_a_tiny_box/)
- The smallest motherboard with 4 dimms would be a micro ATX. Mini ITX only have 2 slots, that's why you can't find options for 128 GB.
  - I thought maybe DDR5 had been out long enough to be able to find a 2x64GB kit, but looks like maybe not. But, Crucial does have a 2x48GB DDR5 kit, and 96GB would be alright.

- I'd suggest the following that gets you 128GB of DDR5 RAM, a 14-core 13600K, compact 17.9L case, with very good CPU cooling capacity that will allow the CPU to run under heavy multi-core load without thermal throttling.
  - CPU: the 13600K provides a good number of cores for a lot of VM's, 
  - Motherboard: mATX motherboard with wifi and bluetooth, plus 4 RAM slots.
  - Memory: 128GB of RAM (four sticks of 32GB).
  - Storage: Good sped 2TB PCie 4 NVME.
  - Video Card: It wasn't clear if you need a separate GPU or not. Added in a decent GPU for not too much money - though if you can just use integrated graphics, you could upgrade to a 13700 for more cores.
  - Power Supply: SFX unit for space savings in the compact case.
  - Case: The Mechanic Master C28, despite being mATX, is smaller than several mITX cases 

- [Please advise on a mATX build with 128GB RAM : r/buildapc _202307](https://www.reddit.com/r/buildapc/comments/15e1r2j/please_advise_on_a_matx_build_with_128gb_ram/)
  - So far my only criteria is: - AMD 7950X, 128GB RAM, RTX4090. Likely the CPU would be under a lot most stress than the GPU for most of my use cases, 
- Asus Prime AP201 MicroATX Mini Tower Case	
  - Motherboard	MSI MAG B650M MORTAR WIFI Micro ATX AM5 Motherboard
  - Memory	G. Skill Trident Z5 Neo 64 GB (2 x 32 GB) DDR5-6000 CL30 Memory x2

- 4x DDR5 configurations barelly run with 4800MT/s and even that might need some DDR5 kit swaps till you have 4 working DIMMs.
  - Getting DDR5 6000MT/s CL30 is ridiculous.

- [Is it possible to have 128GB RAM with SFFPC? : r/sffpc _202210](https://www.reddit.com/r/sffpc/comments/yd2z0b/is_it_possible_to_have_128gb_ram_with_sffpc/)
- Matx board, 25L matax case and 4x32gb dimms?
  - Silverstone Alta g1m or asus ap201 are ones I want since they support a 360 AIO

- Should i maybe get c26 or c28 cases? It looks like even c26 can fit an ATX board so with that it should solve these issues?
  - You will not be able to properly cool the components you are aiming for in a case like the C26, which by the way will not be able to fit a 4090. C28 maybe, but I wouldn't want to travel with a case that has a TG panel. And you need all the airflow you can get.
  - The new Asus Prime AP201 looks promising. I would not advise to go smaller than that.

- [Smallest possible m-atx + 7950x + 4090.... advice appreciated! : r/sffpc _202301](https://www.reddit.com/r/sffpc/comments/107jq5o/smallest_possible_matx_7950x_4090_advice/)
- I just received C26plus, bought B660 mATX MB and 12600K, pending to get a 4090 and PSU. Will get back to you if the case is good fit for 4090 FE
  - Nope, side panel cannot close even with native 16pin cable

- AP201 is the current matx meta imho but the d31 is nice

- ## [Should I Choose a Motherboard with 128GB or 256GB RAM Max? : r/buildapc _202410](https://www.reddit.com/r/buildapc/comments/1fvoskv/should_i_choose_a_motherboard_with_128gb_or_256gb/)
  - is there a significant performance difference if I opt for the 128GB option?

- No. Chance is that you won't even reach the 128GB mark long before you make another full upgrade certain years down the road. 

- ## [64G内存+纯CPU裸跑gpt-oss:120b - 小红书](https://www.xiaohongshu.com/explore/68a6ee02000000001c03f964?xsec_token=ABIl2xpN-BcJcn0jmdi2k1RkNoJcXZiMAtj5QFwnpIark=&xsec_source=pc_search&source=unknown)
  - 64G内存+纯CPU裸跑gpt-oss:120b，一秒钟几个字儿往外蹦
- 慢主要是因为用的DDR4，内存带宽太低

- 换AMD，7840H＋96G内存，纯CPU跑30B，30t/s, Q4，我需要长上下文，Q4拉满256k要79G内存。量化再往上就加载不上了
  - 7840H没有npu

- 组一个一千多块的洋垃圾，256gddr3内存跑，效果看着还行，b站别人有类似视频

- ## [多么痛的领悟，上百台ITX电脑总结出的经验 - 小红书](https://www.xiaohongshu.com/explore/64d4825a000000000c034f36?xsec_token=ABRh3ACWg_DKSLuH2VaseP_A7Mn7MpO5RifkuaugGycJM=&xsec_source=pc_search&source=unknown)
- 第一种机箱类型：ITX主板+小1U电源（个人非常不推荐）
  - 优点：体积可以做到很小，很薄。
  - 坑点：小1U电源噪音大且是你受不了的那种、CPU散热器难以选择、需要显卡延长线、需要半高显卡。
  - 建议：非必要，直接不要选择

- 第二种机箱类型：ITX主板+SFX电源（比较推荐，但也不是没有问题。）
  - 优点：可选择的电源比较多、结构合理、很多品牌的电源也可以做到没什么噪音。
  - 坑点：假的SFX电源比较多且虚标、有些也需要显卡延长线
  - 建议：优先选择类似机械大师C24这种显卡直插形式的。SFX电源尽量选择长城、TT这些品牌。

- 第三种机箱类型：小尺寸MATX主板+SFX电源（比较推荐，主要这类机箱较少，好看的不多）
  - 优点：主板选择空间大，有很多便宜的主板，可扩展空间大，装了显卡还能装网卡，费用少。
  - 坑点：没啥坑点，就是可选择的机箱有点少。
  - 建议：就选艾罗拉SS31/30、笨牛U45等，如有其他建议评论区留言帮助大家。

- 第四种机箱类型：ITX主板+MATX电源（非常推荐，但这类机箱更少了）
  - 优点：电源可选择性很多，空间富裕，显卡可直插主板等。
  - 坑点：同样没啥坑点，也是可选择的机箱有点少。
  - 建议：目前我见过的机箱有闪鳞G200，同样建议有好心人评论区留言帮助一下大家。

- 装了好多种itx，小1u那种适合功耗比较低的，而且大多数小1u电源机箱都要买显卡延长线，而且好的静音电源很贵，建议还是装那种sfx电源的，省事，也比较便宜，后期扩展性也比较大，显卡选择也更多样一些，前面提到的那种一般都是短卡，价格又上去了

- ## [u7 265k用什么风冷能压住 pa140可以吗？还是p60t，或者fc140？  ](https://www.xiaohongshu.com/explore/68d1b0a30000000013007154?xsec_token=AB9X5HvN7d3sRdmBFeEqk4N5mZwaaHkl_t8SjBmCKQI7c=&xsec_source=pc_feed)
  - 开始看到有人说pa120就可以，然后140差不了多钱性能比较好，但是去问客服，客服说不行，说fc140可以。然后也看到推荐p60tv3的，但也有说阿萨辛都压不下265k的。
- 默频满载功耗也不低啊，官标250w，我的满载跑默频260w，最好的风冷也不建议去压240w以上的u。除非你买265k就为了刷刷视频，而且对游戏帧数没要求，没生产力需求。

- 正常用随便一个设计没硬伤的双塔都能压，甚至非生产场景单塔都行，因为ultra系列的优势就是超高能耗比，低功耗也能发挥绝大部分性能，
  - 但你要以长期满载和喜欢烤鸡看温度为要求，这u满载250w风冷干不动只能水，我会选择pa120这种中小尺寸双塔或者厚单塔，相对好装省事儿，fc140这种大驼子除了安装和更新硅脂时带来痛苦带不来什么
- 这位解释的很好了。满载风冷不够，但大多数情况下双塔风冷随便选
- 降个压吧，损失一点点换个好功耗用
- 不超频降压用风冷就没问题，如果要超频那功耗就冲着300w去了，必须水冷
- 你要是做视频解码啥的还是水冷吧，撑的时间长些，不然降频了
- 可以限制啊。你玩itx追求完美释放性能吗？

- 我这pa120换扇p12e对我这个用于编译的工况是够的，p60t没对比过

- 265跟147比就是，性能一样，不缩缸，功耗还低，6热管双塔风冷就行。147一般都是360水冷

- pa140，换俩12cm的风压扇，记得额外买个12cm风扇的扣具

- 如果你会设置cpu，265k，190瓦左右，就可以发挥出97%的性能，也就是说你用b60t也行，甚至大霜塔

- ## 🤔 [itx主机CPU选哪个（AMD还是intel）？ - 知乎](https://www.zhihu.com/question/444107051/answers/updated)
- 11代是牙膏倒吸，没必要。AMD带G的也在涨，轻度使用3400G完全足够，还可以兼顾点游戏，不过还是看预算吧，肯定是预算内堆核心。
  - itx主机麻烦在散热器和机箱的选择上，体积压缩到极致还要散热强，不然容易积热。当然如果不在乎体积的话另说

- 在ITX环境中，还是intel有点优势 
  - 因为ITX优点和缺点一样明显 所以选择intel的平台，办公尽量选择低功耗的, 桌面atx/matx的diy平台，选择amd ryzen

- [10l以下ITX可以支持最高什么cpu？ - 知乎](https://www.zhihu.com/question/7439796831)

- 10L刚好能装双塔，双塔风冷散热器所能支持最高CPU是 EPYC 9004系列和R9 9950X系列。

- 现在有的10L的机箱可以装135mm的散热器。135mm双塔的解热能力在200W出头。
  - 如果以生产力需求为主，那么可以上新的ultra7 265k，新的ultra系列虽然游戏性能开倒车，但是能耗比比上一代有大幅度的提升，比较符合你的情况，但是性价比不高，按需选择。
  - 如果以打游戏为主，那么amd X3D系列的CPU可以随便用。因为工艺问题，除了最新出的9000系列x3d其他的都是限制功率的。功率比非x3d型号的CPU要低。
  - 其他的amd系列的处理器。基本可以随便上，即使是解锁了功耗的9950x也只有230W。

- 10L以下的机箱上不了三槽的标准显卡。只能核显或者有些可以用专门的itx短卡。如果是用风冷，基本就不要考虑intel带k的型号和amd的高端型号了。可能有用120水冷的，上个i5带k的就最多了。
  - 10L左右能上240水冷的机箱我以前还真研究过，这种的基本最高能跑两百瓦出头，上i7带k的型号稍微降一点电压基本能发挥出来，顶级的i9带k跑不满，但也不是彻底不能用。

- 默认用的话其实目前 [Ultra200S系列] 或者 AMD 9000系列 都对散热要求较低了，考虑10升以内的旗舰处理器也都问题不大，基本不会有i9性能损失后只有i7甚至i5性能的问题。
  - 所以不论针对游戏或者生产力主机，配置上处理器都可以放心的选择9950X或者9800X3D或者是U9 285K都是OK的。

- [U7 265K和9700X到底么选 - 小红书](https://www.xiaohongshu.com/explore/68a883b8000000001c03c2ab?xsec_token=AB2GCeMJb66g6AJnAmLcHLOqUT9Bzg3qO1V9MsHhNoFL4=&xsec_source=pc_search&source=web_search_result_notes)
  - 首先9700x有积热问题又或者说这个U太挑散热器也很难压温度 我风冷双塔散热器九州风神AK620默认频率待机温度57度 随便开个软件60来度 打3A游戏甚至70多度 散热器和机箱风扇忽吵忽静 严重影响我正常体验
  - 其次我的机箱是Z20 显卡5070Adoc 这么小的机箱 我稍微玩一下3A游戏这俩货给我机箱搞的跟个炉子似的 
  - 原先14600KF江西夏天不开空调游戏温度58度不到、显卡最高65度、换了9700X游戏温度极度不稳定平均73度 偶尔抽搐一下温度能干到80度 显卡也更热了
- 烤鸡40分钟都没你打游戏热
  - 我各种试过了 问了商家今年批次的9700X电压默认给的高 但是我不开PBO然后负压30温度还是很高 我见过最低温度53度差不多
- 你这温度应该开pbo而且还没降压吧，9700x默认88w温度不高啊，9700x不开pbo，用个百元的风冷随便压啊，打游戏噪音更多来自显卡吧

- 主要265k没法升级了 am5起码能用到2027年 但是Intel明天又要换接口 太逆天了

- 俩都不是一个等级的，97x能配几百块入门的b650，265k搭配的z板最差的也接近1k，稍微好点的b板也这个价，总拿货价比97x高一个数量级了，本来就不该拿来比，265真正的强势来源于能跟更贵的79x99x掰手腕

- 生产力265 游戏97x，还有70度对于cpu来说还太低了，85度都很正常，你这实属温度瞎焦虑

- ## [为什么ITX架构的短显卡大部分都是Nvidia的，而AMD的高端ITX显卡则一卡难求？ - 知乎](https://www.zhihu.com/question/318806879)
- ITX 短卡就是一个细分市场, 事实上除了我国有一堆 K39, 鱼潮S3 这样的机箱, 其他国家基本都找不到类似的机箱.
  - 目前 N 卡阵营里的短卡主要以 [技嘉] 为代表, 包含 1060/1070/1080, 2060/2070 和 1660. 所以除了 1080Ti/2080 这些顶级旗舰货, 基本上甜品卡都有了.

- 最根本的原因在我看来是源自于n卡对笔记本市场的重视。笔记本受制于电源适配器，拢共就200w左右的供电能力，CPU和GPU（显卡核心）都要吃电的情况下，两位大哥都得控制住自己的功耗。
  - 一般就分给显卡90-140w，给cpu 60-90w，多了要么电源扛不住，要么散热顶不住。
  - 有了笔记本控制散热和电源的前提，设计芯片时，才会控制能耗比和发热，最终也就能做出适合itx的短卡、刀卡。
  - 由于nv在笔记本市场的排挤，amd很难把自己的芯片推销给成熟、有名气的笔记本厂商，并做出调教完美的好产品。

- 成本，公版vega就用上了均热板(而且大多数高端卡也都用上均热板了)，所以NANO用均热板是不会有什么成本增加的。

- ITX显卡市场是显卡市场下的另一个细分市场，太小众，AMD作为弱势方也没余力发展。
  - 而且ITX卡对能耗比要求较高，AMD在这方面没有优势。

- ## [5090靠边让让，真神降临——48G的4090！ - 小红书](https://www.xiaohongshu.com/explore/67b5809e000000000e006d0c?xsec_token=ABGeo98UKSG8fjOMNixsHiBRqoxj7iqPLj_-v2GsF7NMk=&xsec_source=pc_search&source=web_search_result_notes)
  - 【CPU】AMD R9 9950X
  - 【主板】技嘉 B650M AORUS PRO AX
  - 【内存】阿斯加特 TUF 64G(32GX2) D5 6400
  - 【显卡】英伟达 4090 48G 涡轮卡
  - 【固态】三星 990PRO 2T+1T
  - 【机箱】机械大师 CMAX
  - 【散热】九州风神 冰暴 240 LCD屏
  - 【电源】全汉Hydro PTM X PRO 1200
  - 【风扇】棱镜 4PRO
  - 【定制】CMAX硅胶线

- ## [ITX | 塞个RTX5080？没问题！ - 小红书](https://www.xiaohongshu.com/explore/67bc32a90000000009039a96?xsec_token=ABBUmmsfcqVOegTyFI0DTJ4zLCSJXlG34hPsmQuX6DX3M=&xsec_source=pc_search&source=web_search_result_notes)
  - 背靠背结构的ITX机箱刚好塞入了一张三风扇的RTX5080，海韵SPX750作为扎实耐用的经典sfx电源轻松带动了i5+RTX5080的组合，顶部两个安静优雅的海韵Magflow风扇为机箱带来了更优化的风道散热
  - 机箱：FORMD T1
  - 显卡：影驰RTX5080-16G魔刃
  - 主板：微星B760i D4 Wi-Fi
  - 内存：皇家戟ddr4-4000-16g*2
  - CPU：i5-14600K
  - 散热器：超频三RC600黑色电源：海韵SPX750W
  - 风扇：海韵MagFlow RGB 120*2
  - 电源：海韵SPX750W

- [高颜值、能装三扇长显卡的13升itx主机 - 小红书](https://www.xiaohongshu.com/explore/676e9a4f0000000014026954?xsec_token=AB_LcGHuSS1a9ZxmBLSWAdGMd-OlAMXO2f9f82oya7dHg=&xsec_source=pc_search&source=web_search_result_notes)

- ## [小机箱 5090d+9950x3d 拿下！ - 小红书](https://www.xiaohongshu.com/explore/67f779020000000007036603?xsec_token=ABegebsB80NsOXLuiVvWiW-lQszDh0QuWayKWLU0ABe5c=&xsec_source=pc_search&source=web_search_result_notes)
  - 【CPU】AMD RyzenTm 9 9950X3D
  - 【显卡】微星 5090d超龙
  - 【机箱】方糖机械大师C+MAX银色
  - 【主板】微星 MAG B850M MORTAR WIFI
  - 【散热】TRYX PANORAMA展域240水冷ARGB 黑色
  - 【内存】佰维 128G（32Gx4）DDR5 6400 C34
  - 【固态】三星 9100PRO 4TB PCIe5.0*4 M.2接口
  - 【电源】 洛基1200w
  - 【风扇】 联立积木三代

- 我和你一样的卡和U，这样装冷排吸显卡尾气再加上9953这个火炉得多热啊我用的还是大机箱和展域360性能版水冷，待机就要40多度了，稍微打开几个网页登一下steam就50+了
  - 我感觉还能接受哎，待机五十多，打开 C4d 轻量干个活 70 多度，CPU 反正也烧不坏
- 我是14900k换过来的之前没用过x3d的CPU。149k虽然耐电但是待机温度不高的，浏览网页不会超过35度。还是心理上没适应高温的CPU
  - 那我老适应了之前用的 i9+3080ti 笔记本

- 如果是 LOL 这种游戏的话显卡三十多度，cpu 六十多，3a 游戏大概都在七八十度左右

- 换个联力a3就可以了，侧面还可以加三把风扇，5面打孔散热板

- 机械大师c系列侧边都是玻璃，要是换成网散热能更好点

- [C+MAX金属大师 GT银灵感 - 小红书](https://www.xiaohongshu.com/explore/65f446c20000000014005ff3?xsec_token=ABxIEa1q74egE8STTJo9DqtGHzgiBycDD0OoPvjAjGLJA=&xsec_source=pc_search&source=web_search_result_notes)
  - CPU：12600KF
  - GPU：影驰 金属大师 4080super
  - 机箱：方糖机械大师 C+max 月光银Air版
  - 主板：技嘉 B760M A ELITE AX D5
  - 散热：奥斯艾i240-B 一体式水冷
  - 硬盘：光威 弈系列 旗舰版 2T
  - 电源：海盗船SF850L

- [机械大师 CMax 老铁抄作业了 9700X搭 5070T - 小红书](https://www.xiaohongshu.com/explore/67caf20b0000000029029b83?xsec_token=ABqsyDIYHWnZyXfQUxpaLc8dJaKdhuBLrkKua73Sg89IQ=&xsec_source=pc_search&source=web_search_result_notes)
  - CPU：AMD 9700X 中文盒（全程烤机不超 70 度）
  - 显卡：技嘉 RTX5070TI 雪鹰 16G
  - 主板：技嘉 B850M 冰雕 WiFi6E
  - 散热器：酷里奥 p60t Pro 白色性能版
  - 机箱：机械大师 Cmax
  - 阿斯加特女武神二代 32G 套 6000 c28 
  - 硬盘：三星 990 Pro 4TB 固态
  - 电源：航嘉 sfx 额定 850w 白金全模 atx3.1

- ## [itx机箱有啥很难受的缺点吗，散热到底能有多差？ - 知乎](https://www.zhihu.com/question/337421059)
- 缺点是空间太小，走线麻烦，换零件麻烦(换的频率不高就不算是缺点了
  - 散热也是看机箱而定的
- 散热也是看机箱而定的, 目前装过三台c24 qbx geeek a4的温度都没啥问题
  - 现在很多a4 机箱的风道都是从下到上，肯花钱买风扇的话，散热效果还是可以的
  - 实在不行就把侧透换成防尘网呗，又能低五度了

- 风道设计在 ITX 里虽然重要，但是实际上最影响散热的是硬件选择，比如你非要在 ITX 里放 i9 + 旗舰或次旗舰的显卡注定好不了。
  - 几年前为了装个 Ubuntu 小主机，搞了一套 11900K + 6600XT 的 ITX 主机，开机就60度以上，后来给 CPU 降压降频，每个核心都降了 0.2GHz，电压 1.1v 才勉强压下来
  - 另外一点就是 ITX 对 SSD 不友好，常见 ITX 主板的 M.2 位少，且散热差，刚才提到的那机器，玩游戏的场景下不一会 SSD 就能上 70 度。背面的 SSD 就好很多，只有 50 度左右
  - 如果想要 ITX 的“小”，那么推荐全部用中端的硬件，i5 级别左右 + 中端显卡，不常移动且能接受水冷的话可以上 240 尺寸的水冷，显卡可以看看品牌拆机的涡轮卡，能够有效的降低箱内温度。

- 我之前搞了个12500H+P4的itx，热的根本没法用，CPU和显卡一直100度，最后无奈换了个ATX机箱，解决，温度没高于60度过

- 当温度上去后，带来的最直观后果就是风扇转速过高引起的噪音很大。
  - 另外itx机箱和主板对于硬盘控非常不友好。M.2一般不超过两个且温度较高，3.5机械硬盘安装困难即使装上去也一般会占用一个风扇位间接影响散热。
- 确实, 我光看小巧便携去了, 几年也搬不了一次, itx换下来比常规贵两千, 而且性能还差

- [【8.1L】 便携科研ITX本地小主机推荐1️⃣ - 小红书](https://www.xiaohongshu.com/explore/67adb5420000000029034581?xsec_token=ABakFdkTIJ1ZcM6p0CG5TD5J9V9fUMmia67ixkR6mRgtE=&xsec_source=pc_search&source=unknown)
  - 机箱：闪鳞s300 tc+usb PCIE4.0延长线（444）
  - CPU：12600kf｛也可14600kf（板u：1889）｝
  - 主板：铭瑄b760i WIFI D4（1669）
  - 散热：利民AXP90-x53 4热管（164）
  - 内存：金百达银爵3600 D4 32G*2 （741）
  - 硬盘：金士顿kc3000 1T PCIE4.0 1G独缓（473）
  - 显卡：技嘉风魔RTX4060ti-16G（3689）毫无性价比，可以考虑二手大显存的卡。
  - 电源：玄武650sfx 金牌全模组（359）
- 12600kf已经可以满足很多日常的生产力任务同时不用担心缩肛，但是和14600kf仅有200元差距，说实话我更推荐146kf，但是无奈缩肛…有动手能力的小伙伴可以上146kf
- 铭瑄b760主板性价比首选。给到9个USB接口其中有2个20Gbps的c口，带WIFI6E网卡，同时给到2个m2 2280的插槽，支持PCIE4.0的速率。再搭配4个SATA接口给硬盘扩容，满足高存储的需求。同时对显卡是PCIE5.0x16插槽。

- ## 🧩💡 [ITX机箱推荐 2025年版 80起步 ［支持MATX主板、ITX主板、常规ATX电源、SFX电源 15L 20L体积］300以内 高性价比 台式电脑机箱 K88 i8 P90 趣造等 - 知乎](https://zhuanlan.zhihu.com/p/677817218)

- resources
  - [CaseEnd filters](https://caseend.com/)

- 15升、20升机箱的结构类型：常见结构，基本是前置电源，横放显卡
  - 有一些特殊的机箱，结构：下置电源、需要显卡延长线、下压式风冷。类似酷鱼T40，但本文不做推荐（太少了，花费也可能比较高）

- 玻璃侧板：常见的侧板材质，安装时，需要注意边角不能被磕到，否则它会碎掉。
- 亚克力侧板：透明塑料侧板，如果温度较高，可能会有塑料味，时间长了，也会发黄。
- 铁侧板：相对来说，比较其他类型的侧板，这款是最好的，碎不了，也不会有塑料味。但有些厂家，铁侧板甚至不开孔，这就会让散热效果差。
- 防尘网侧板：这种侧板，一般自己DIY的比较多，多为塑料材质。部分机箱，是金属侧板+防尘网的组合。
  - 所有机箱，都可以自制防尘网侧板，成本不到10块：

- 本文推荐的所有机箱，个人都只推荐装双风扇显卡。 双风扇显卡，长度一般不会超过300毫米，多数机箱，都可以放下。

- 机械大师 cmax ◇99
  - 尺寸：392*185*284mm, 20.5L
  - 显卡：385*160mm以内
  - 散热：162毫米风冷 或 240水冷
  - 材质：内框：1mmSPCC钢材 ；外框：2mm铝合金
  - 主板支持 MATX/ITX
  - 电源支持 SFX/SFX-L/（140mm以内）ATX
  - 风扇支持 顶部：12cm*2；尾部：12cm*1；前面板：8/9cm*1
  - 2.5寸硬盘位 至多3个
  - 3.5寸硬盘位 至多1个

- 鱼巢 S9 ◇99
  - 尺寸：375×188×300, 21.15L
  - 显卡：360x150限长
  - 散热：160毫米风冷 或 240水冷
  - 材质：1.2毫米 镀锌钢板
  - 侧板：~~玻璃~~
  - 电源：ATX电源，170mm以内
  - 主打的就是便宜，99，还能走背线，让机箱内更美观一些，虽然能走的不多

- 铝小宝 E90 PRO
  - 尺寸：377×187×298, 21L
  - 显卡：355x150x80mm限长
  - 散热：160毫米风冷 或 240水冷
  - 电源：ATX电源，160mm以内, sfx/fiex电源需自备转接板
  - 铝小宝ALBOX P90 ◇158, 被E90Pro取代
    - 尺寸：375×188×300, 21L
    - 显卡：360x150x80mm限长
    - 散热：160毫米风冷 或 240水冷
    - 材质：1.5毫米+1.0毫米 冷轧钢板
    - 侧板：铝 或 亚克力
    - Motherboard(MM): 245 x 245
    - Power Supply(MM): 150 x 160 x 86
    - 木质提手，算是这个机箱的特色了。
    - 电源延长线位置，在机箱顶部，装出来的效果，就参考官方这个图吧。

- 笨牛 N20 ◇199
  - 尺寸：381×199×300mm, 22L
  - 显卡：限长364x160x85mm, 16cm的电源长度支持315mm显卡长度, 16cm以上电源仅支持290mm显卡
  - 材质：1.5毫米+0.8毫米 冷轧钢板
  - 侧板：3毫米 玻璃侧板
  - 散热：164毫米风冷 或 240水冷
  - 能走背线的小机箱，不多，也不少。不过都只能走一点点。前面板有大面积开孔，散热通风好，电源位置有一个挡板，装出来，那堆线就看不到了

- 包子星人 A88Pro ◇183
  - 尺寸：380×195×305, 22.6L
  - 显卡：360毫米限长
  - 材质：1.2毫米 镀锌钢板
  - 侧板：铁 或 玻璃

- 先马趣造 ◇139
  - 尺寸：391×185×303
  - 显卡：335毫米限长
  - 散热：155毫米风冷
  - 材质：1.5毫米+1.0毫米 冷轧钢板
  - 侧板：铝 或 亚克力
  - 趣造还有个兄弟，叫做蓝宝石银角大王，但目前只有库存货了，而且卖的比趣造贵……

- 文中提到的这些品牌，多数都是有10升的机箱，可以去搜搜看。

- 有没有多盘位（3-5个3.5HDD）小机箱推荐
  - 无

- 如果拒绝光污染，非透明侧板的15升，有推荐吗？
  - A88

- ## [有哪些可容纳 4090 显卡的 ITX 机箱？ - 知乎](https://www.zhihu.com/question/559771286/answers/updated)
- 极限尺寸itx不是不能搞，问题是溢价太高，定制的机箱就要1k-2k左右，主板（itx板型），电源（），显卡（最完美的是，三风扇的不用想了）小尺寸的全都有溢价。总体比正常配要贵30%-40%左右。
  - 4090还算好，你降频到3090这个水平玩没有散热问题，但是itx正常用散热问题很大，建议开盖，不然CPU得上水，显卡4090降频用问题不大，还有背部m2的散热，风切声音问题等等。

- 公版4090和3090一个大小，买公版然后基本上原本能装3090的都能装

- 在reddit上看到了几个案例，不提Nr200、Meshlicious那些大号的itx机箱，就看10L左右的：
  - Formd T1，这是个10L的机箱，这个机箱我也在用，外形我非常喜欢。我自己的T1里面塞了个公版3090。
  - 联力A4-H2O，这是个11L的机箱，继承了Dan A4的好传统，增加了兼容性，成本也降下来了
  - Jimu D+ V2.0，10.5L的机箱，同样塞进去了一个4090FE+AIO，这个机箱外观和内部规划跟Formd T1几乎如出一辙，但价格要便宜些
  - Ncase M1，不到13L，因为不是三明治格局，这货为了不憋死4090把机箱垫起来了，而且如果不用个90度转接头的话4090的电源线没法插。
- 基本上看到的小于14L的4090案例就这些了，如果放宽要求，reddit上有大量的ssupd meshlicious的案例，虽然体积要大不少，但因为是竖直放置的，占地空间非常小
  - 除了公版卡之外第三方的卡基本都塞不进去

- ## [各位心目中最好的ITX机箱是什么？ - 知乎](https://www.zhihu.com/question/55014071/answers/updated)
- 我自己DIY主机，喜欢在有限的空间里，充分发挥各部件的性能
  - PS：我从不考虑水冷，因此不做水冷推荐。

- 高性能兼顾小体积的选择：乔斯伯的Z20机箱（20L）
  - 高性能主机的显卡是一定要上三风扇的，基本上是4080以上级别，CPU也是13600以上的规格，这对机箱的散热有不小的要求。
  - 乔斯伯Z20机箱顶部和底部可以支持4个14cm风扇，尾部还可以装一个12cm风扇，合理规划下进上出的风道，足够压得住4080和13700的组合。
- 最佳性价比小机箱的选择：机械大师C26（12.9L）和笨牛U45（11L）
  - 2024年这个时候，三大厂的ITX主板比MATX主板贵了将近一倍，同样的配置把MATX主板换成ITX主板至少贵600块，实在没什么性价比。因此选择MAXT主板+SFX电源的组合，便成了兼顾性价比的最小机箱搭配
  - 2.1更强散热，推荐机械大师C26。对于AMD显卡+CPU组合或者13700+4070ti高性能组合的，更推荐带前面板风扇位的机械大师C26。这个规格的机箱，顶部和底部只支持9cm风扇了，这个大小的风扇还带侧边RGB样式的，我只找到鱼巢的9cm风扇。
  - 为了压13700或5800X这样的CPU，135mm高度散热器只能选利民的SS135了，但这款不带RGB风扇，为了和9cm鱼巢的风扇rgb风格统一，建议自行更换成鱼巢的12cm风扇。如果是13400规格的CPU，那么利民的AK120mini就能压得住，RGB风扇的风格就自行发挥了
  - 2.2更具性价比且更紧凑，推荐U45。U45有个缺点就是前面板没有风扇位，所以建议CPU和显卡的规格要下降一些，以防机箱闷罐。此外，U45右边那个挡板设计，虽然方便了理线和藏线，但我是真的欣赏不来这设计。

- ## [Quad 4090 48GB + 768GB DDR5 in Jonsbo N5 case : r/LocalLLaMA _202507](https://www.reddit.com/r/LocalLLaMA/comments/1m9uwxg/quad_4090_48gb_768gb_ddr5_in_jonsbo_n5_case/)
  - JONSBO N5 NAS Pc Case: W355*D403*H350mm
  - GPUs -- Quad 4090 48GB (Roughly 3200 USD each, 450 watts max energy use)
  - CPUs -- Intel 6530 32 Cores Emerald Rapids (1350 USD)
  - So some additional information. I'm located in China, where "top end" PC hardware can be purchased quite easily.

- Do they "just work" or do you need sketchy drivers?
  - On Ubuntu they just work with official drivers (either Ubuntu PPA or Nvidia). 
  - On Windows I've seen reports that they do not work with official drivers, but I have not tried (nor do I plan to run windows on this machine)

- ## [乔思伯Z20机箱小白首次装机分享 - 小红书](https://www.xiaohongshu.com/explore/67eadb49000000001c00c0db?xsec_token=ABhwvh4o79bh1iZJod5ETLKzRMlnZYs2oM-5p3wHmKM24=&xsec_source=pc_search&source=unknown)

- [乔思伯Z20装机分享 - 小红书](https://www.xiaohongshu.com/explore/67e7b093000000001201ed15?xsec_token=ABhmT245aHYddZ3_znj1lt9rAgzysn8wPmPsb_KU5jWH0=&xsec_source=pc_search&source=unknown)

- ## [DIY装机乔思伯z20 9600x + 5060建议帖 - 小红书](https://www.xiaohongshu.com/explore/688c38a60000000025022daa?xsec_token=ABH_IcKm-CurbXXiUGZUe0iNzqulTk75YHR2Lmgz9lyY4=&xsec_source=pc_search&source=unknown)
  - 【散热】 利民PA120 MINI BLACK双塔6热管黑色
  - 【CPU】 AMD 锐龙 5 9600X
  - 【主板】 微星 B650M GAMING PLUS WIFI
  - 【显卡】 七彩虹 战斧 RTX 5060 豪华版 8GB
  - 【内存】 金百达 32GB(16GBX2) DDR5 6000 海力士A-die颗粒 星刃
  - 【电源】 安耐美 金竞蝠GM650 黑色  ¥329 @京东

- 内存买了国产的 其他的太贵了
  - 确实是这样！我内存也准备换c30了
- 我是c36 拼多多买的

- 高u低卡了，换7500f，电源选个大厂的

- 9600x加5060高u低卡了？5060配7500f？
  - 7500f感觉都高了，但考虑到网友玩家那差不多了。
- 除非你只玩1080p的网游。但现在主流都是2k显示器或者是4k双模了。而且u还可以小超一下，显卡老黄不给你超的。

- [年轻人的第一台生产力主机乔思伯Z20 - 小红书](https://www.xiaohongshu.com/explore/6777b7c8000000000900d498?xsec_token=ABnLOUzIXMWLH-jTphfigaEnUEg3qXD4FdObm05VCiLdg=&xsec_source=pc_search&source=unknown)
  - 散热：超频三 RZ620臻+追风者M25无光

- [Jonsbo Z20 build : r/mffpc _202506](https://www.reddit.com/r/mffpc/comments/1lo5xwl/jonsbo_z20_build/)
  - CPU: AMD 9600x; 
  - GPU: Powercolor 9070xt reaper 
  - PSU: seasonic focus gx850 
  - Motherboard: Asus B850M plus wifi 
  - Memory: gskill trident 32GBx2 6000MHz CPU 
  - cooler: thermalright frozen notte 240

- [Jonsbo Z20 Build. Intel Core Ultra 7 265K + RTX 5070 : r/mffpc _202506](https://www.reddit.com/r/mffpc/comments/1lfgii7/jonsbo_z20_build_intel_core_ultra_7_265k_rtx_5070/)
  - CPU: Intel Core Ultra 7 265K
  - GPU: ASUS GeForce RTX 5070 TUF GAMING OC 12GB
  - Motherboard: MSI B860M GAMING PLUS WIFI
  - PSU: Corsair SF 850W 80 PLUS Platinum
  - RAM: 64GB Kingston Fury Beast Black RGB EXPO [2x32GB 6000MHz DDR5 CL30 DIMM]
  - SSD: Kingston FURY Renegade SSD 2TB M.2 2280
  - CPU Cooler: THERMALRIGHT Peerless Assassin 120 Digital ARGB
  - Case Fan: DEEPCOOL FC120 x3 Set

- [Jonsbo Z20 Build - RTX 5090 FE, 9800X3D : r/mffpc _202510](https://www.reddit.com/r/mffpc/comments/1nur3dl/jonsbo_z20_build_rtx_5090_fe_9800x3d/)
  - CPU - AMD Ryzen 7 9800X3D
  - GPU - RTX 5090 Founder's Edition
  - RAM - 32GB x2 Corsair Vengeance CL32 6400Mhz DDR5 RGB RAM
  - Cooler - NZXT Kraken Elite 240 RGB AIO (2024)
  - Motherboard - Asrock B650M Pro RS WiFi
  - PSU - Corsair SF1000
  - Storage - Samsung 980 Pro Gen 4 2TB, Lexar NM790 Gen 4 1TB

- [Jonsbo Z20 All white build with Gigabyte Aorus Master ICE 5090 : r/sffpc _202509](https://www.reddit.com/r/sffpc/comments/1nhzeu0/jonsbo_z20_all_white_build_with_gigabyte_aorus/)
  - Build: Mobo: Asus rog strix B850G 
  - Processor: 9950x3d 
  - GPU: Gigabyte Aorus Master ICE 5090 
  - Ram: CL28 96gb kit (2x48gb) 
  - AIO: ROG Ryujin 240mm 
  - PSU: rog loki sfx-L M.2 - 2 wd black one is 2tb gen 5 and the other is 8tb Fans: right now a mix of lian li and rog and thermaltake.

- ## [用乔思伯Z20装一台5090“手提电脑” - 小红书](https://www.xiaohongshu.com/explore/6831b59b000000002202a719?xsec_token=AB8c-JeBH72uEhbyvwZNwZ7CJy32kV-b7wXvQrGV6x6G4=&xsec_source=pc_search&source=web_search_result_notes)
  - 机箱：乔思伯 Z20
  - 显卡：华硕 TUF 5090D
  - CPU：AMD 9700X
  - 主板：微星 B850M 迫击炮
  - 内存：威刚 D300 6400 32G x2
  - 显卡：华硕 TUF 5090D
  - 固态硬盘1：三星 990 Pro 2T
  - 固态硬盘2：致态 7100 2T
  - 固态硬盘3：海力士 U.2 7.68T
  - 电源：华硕 洛基 SFX-L 1200W
  - CPU散热：酷冷至尊 Hyper 612
  - 风扇：追风者 T30 x3

- 3.5寸机械盘就不适合这种机箱。既要保证封道畅通，CPU和显卡好散热；又要保证机械盘温度不超标；还要确保机械盘防震，难以兼得。

- 显卡插几槽啊，5070ti还有机会装底部进风吗
  - 一槽。用5070ti可以装风扇，起码薄扇肯定没问题。

- [风冷极限稳压14600kf！ - 小红书](https://www.xiaohongshu.com/explore/682c48f5000000000f038872?xsec_token=ABkkbE8DDCL643O12-dpNtOGU7Vi3PMVqtoIMW9TX50Xg=&xsec_source=pc_search&source=web_search_result_notes)
  - 散热：利民 pa120
  - 显卡：讯景 rx 9070xt 16g 海外版
  - 最近14600kf价格很香 所以替换掉12600kf 经过两天的调试 双塔6铜管风冷也能极限压住这颗cpu啦
  - 2k分辨率下 这套配置打怪物猎人荒野最高画质平均帧200出头 也算比较舒服了 本来都打算放弃治疗上240水冷了 最后挣扎一下抢救回来了 风冷万岁！机箱风噪是真的大
- 14600kf 还是要降压的 负载要控制1.2v以下，高温度就控制住了
- 单塔风冷随便压，平时都不带转的

- 你加了几个风扇压住的
  - 我itx机箱没啥空间 就顶部两个 左侧一个 我显卡太厚 底部两个风扇都装不上

- 这u降压单塔也能压，一般双塔都是够的，只要不降频没啥好焦虑 你这个温度高主要是机箱差+乱装风扇，前置电源小箱本身进风受阻风道难构，底下也没有风扇进风纯靠显卡抽风(显卡下超过3cm，完全可以尝试上15mm薄扇进风)，上部的风扇尤其是前部的更是把cpu所剩不多的冷气抽走，cpu直接无气可用巧妇难为无米之炊，pa120的塔体是很优秀的

- [黑橙风冷手提式小电脑 - 小红书](https://www.xiaohongshu.com/explore/66d19e28000000001d01b98b?xsec_token=AB38kMIt8DJB5wbhI53QOtqJMabGeUPzQy1DfS21GmOko=&xsec_source=pc_search&source=web_search_result_notes)
- 底部可以同时装两个风扇和一个3.5寸硬盘
- 我装了这个机箱，提醒一下，买主板看一下 pcie 槽的位置，微星迫击炮有点偏下，显卡厚了就贴底了，我看别人华硕 tuf 都挺好

- 🤔 [小闷罐乔思伯Z20的散热方案及双烤测试 - 小红书](https://www.xiaohongshu.com/explore/67b067bf0000000028028ee2?xsec_token=AB37FEjnwyYBwILr2RQtHgse2ohed1b-PXLNaQdYNbzj4=&xsec_source=pc_search&source=web_search_result_notes)
  - 散热器：利民pa120se

- 冷空气在底部，热空气在上部，还是更加科学一点

- 感觉可以尾部扇改进风，CPU散热方向改成左到右，正好可以让显卡和CPU都吸到一手冷空气然后统一在右上排出
  - 我记得有up主做过测试，这种类型的机箱这样装温度表现比传统的安装方式更好。主要还是这类机箱没有前面板进风，CPU散热吸的都是显卡尾气。其实改完之后还是下进上出，只是水平方向反一下。
- 这个一个问题就是进风量＞出风量，热量会堆积

- 其实底部进风用处不算大，倒是对pcie槽和IO槽的积灰有帮助，原因是底部有风送进来就不需要四处借风了
  - 非常有帮助，底部加了两个3000转的P12 max暴力进风扇，烤鸡显卡温度直接从76℃降到70℃以下了，最高不超过70。
- 亲测无用，我还是买的4900转工业风扇，两个加到底部发现没任何卵用，然后又费劲力气换回普通rhb

- pa120se 会跟内存条冲突嘛
  - 有一点点，装散热器内存条的风扇会略高1cm

- 普通内存应该没什么问题？多高比较好？
  - 40mm

- 天选主板三个机箱风扇接口都用上了，这样能单独控制上面，底部和后面的转速
  - 明白了，主要是同时考虑藏线，技嘉小雕主板有一个风扇接线槽太明显了，不好接线

- 没有用集线器啊 主板上的风扇位足够了

- [乔思伯Z20装机分享 - 小红书](https://www.xiaohongshu.com/explore/67e7b093000000001201ed15?xsec_token=ABhmT245aHYddZ3_znj1lt9rAgzysn8wPmPsb_KU5jWH0=&xsec_source=pc_search&source=web_search_result_notes)
  - 散热不用担心，我只装了一个单塔双风扇，室温15度玩游戏大概60度。夏天可以把玻璃面板换成散热孔的。

- [关于matx机箱乔斯伯z20的一些疑问 - 小红书](https://www.xiaohongshu.com/explore/67c32e800000000007036281?xsec_token=ABhMyCZP5HBuwkcA-x3g3Zb7JVfz1CaYgxzSIyUO-3Mgs=&xsec_source=pc_search&source=unknown)

- ## [乔思伯z20风道 - 小红书](https://www.xiaohongshu.com/explore/67a8bfb8000000002803576a?xsec_token=ABg64t-BlcmW2roSur4qqjBKnd-Tlemzg7qdIBUy2o41w=&xsec_source=pc_search&source=web_search_result_notes)
- 实际上都差不多，这机箱就是个大闷罐，侧板建议去搞块开孔的亚克力
  - 我的c26 夏天直接开侧板
- 最好的风道就是去掉侧板

- 之前 B 站看过每个位置的风扇散热效果，后出那个位置是散热效果最好的，其他的偏向是辅助而已，所以后面一定要出风效果会好点
- 其实不用想太多，我水冷大显卡，底下装不了风扇，就靠后面出风和水冷两个风扇，吹出来的风很热，顶部提手都有些烫手，但是开机玩一天也没事，这东西80来度不是正常吗
- 上出，热气默认是往上飘的，你这么会扰乱热流，除非你背面是超大工业扇
- 热气上升冷气下沉，遵循
- 上面全出风，进风从下面
- 后出上出，可以不用进风，负压下散热最好。
- 下进上出 前进后出

- 这种机箱下进风，背后排风最好，显卡温度低。上面不建议装，或者只装水冷。

- [风道求助 - 小红书](https://www.xiaohongshu.com/explore/662673150000000004018542?xsec_token=ABw3jEHfBhULZbVbZwZh-SoQYwidgEI4Cvil6t9yvhd28=&xsec_source=pc_search&source=web_search_result_notes)
- 这个机箱用单塔还是双塔散热合适啊
  - Cpu发热量高一些的话就要上双塔了。限高是指塔的高度，160能塞的应该。
- 顶部靠电源的风扇建议换个方向，对cpu散热很有好处，亲测

- [乔思伯 Z20 风冷，以上两种风道那种更合理，实际效果更好呢？ - 小红书](https://www.xiaohongshu.com/explore/673ef0a000000000060178a9?xsec_token=ABXPeGOfIrwFw2csvcwJomQbdXOQnxRyaYf8eRgRHxm0A=&xsec_source=pc_search&source=web_search_result_notes)

- ## [关于方糖机械大师c28装机时踩的一些坑 - 小红书](https://www.xiaohongshu.com/explore/686dd55d0000000017034ee3?xsec_token=ABosdtjmZ4IdMHAXii79bUC4njWXjrUmL9ULRNaDHexTw=&xsec_source=pc_search&source=web_search_result_notes)
  - 一开始买错电源，上240水冷的话只能搭配SFX小电源，普通ATX会卡水冷风扇
  - 好再发现及时，退了ATX，但是买的时候SFX电源也有个坑，因为我的显卡需要16pin接口，要求电源也必须要有16pin，于是又退了买错的SFX，重新购入鑫谷KL-M750G冰山版
  - 但是这个电源也有一个问题，跟机箱的电源支架有点不适配，电源开关的边缘会卡到支架导致无法放正（图5），最后支架是强行压着电源开关边缘装下去的。
  - 内存限高44mm，常规带灯条的基本都被限高了，只有少数符合要求，一开始也没注意看，后面及时重买了。买的没有灯带的宏基掠夺者银翼凌霜48g 6000 c28, 44mm以上的会把240水冷的两个散热风扇卡住
  - 然后就是水冷，买的是九州风神冰域240，但是这货的两个风扇接口上还有额外的接线

- 一定要注意主板显卡是一槽还是二槽 二槽底部完全不能装风扇 其次显卡长度太极限的话 9cm的风扇是装不了的
  - 这种小机箱 主板建议选技嘉b650m小雕 显卡位是一槽 下面有有空可以放风扇
- 我选的是电竞雕850，和650小雕布局好像一样的吧。包括大小
  - 对 850就是比650的槽位多一点 正常650就ok 850的贵个一两百

- 散热怎么样啊
  - 没啥大问题，待机60内，游戏90度内都没问题

- 能装双塔风冷吗
  - 要看下尺寸，说明书里是要求162mm以内，双塔风冷应该大部分都能满足

- 我那个利民电源也这样
  - 那看来机箱的电源支架设计也有问题。不过螺丝拧松一点盖上去也能固定好
- 电源支架有2款，用另一款的电源支架

- 装c26的坑： 1买风冷没注意大小，放不进去 2对风扇太过自信，买多了，上面的风扇位和风冷打架 3机箱内电源线很怪只能往内排风，因为不敢随便弯折线 4买了个显卡支架，位置不够装

- ## [纯白色美学装机小巅峰，机械大师C28 AI“学习机”配置分享](https://www.zhihu.com/tardis/zm/art/691493855?source_id=1003)
- 彩虹的主板绝对是最高性价比白色主题装机的选择。
- 内存是来自宏碁掠夺者的冰刃系列，内存顶部是哑光半透导光条，相当漂亮。
  - 冰刃的灯光很柔美，能够被七彩虹的iGame Center统一控制
- 显卡是索泰的RTX 4070 Ti Super月白
- 水冷是利民冰封视界240水冷，一块LCD屏幕给机箱增加了几分灵动。
- 由于机械大师C28机箱内存限高最多44mm，这里不得不将利民冰封视界240水冷当作120水冷来用
- 电源规格上支持140mm的ATX电源搭配ITX主板使用，也支持SFX/SFX-L搭配Matx主板使用，电源支架安装起来也蛮方便，其实机械大师的机箱只要把铝壳子拆下装机就很简单。
  - 由于超级紧凑的机箱设计，电源位被设计机箱前侧，通过电源转接线进行连接。
- 机箱设计有4个PCIe槽位，只要满足限长的显卡统统拿捏，机箱前部可以上一个9cm的风扇。

- 前置接口上给到了1只USB-A与1只Type-C接口

- 主板：七彩虹CVN B760M FROZEN WIFI D5 战列舰
  - 主板采用了12+1相直出式加强供电，CPU核心供电为12相，核显供电为1相，最大电流55A，CPU供电接口为8+4设计。B760芯片组本就是不支持CPU超频的，
  - 相对B760I推荐上14600K来说，CVN B760M FROZEN WIFI D5 V20 就更推荐配合i5-13490F或者是带核显的i5-13500使用了。

- 主板板载了4个DDR5内存插槽，单槽容量最大支持32GB，总共最多可以支持到128GB内存，内存频率最高支持XMP 6600MHz（OC），目前市面上的高性价比D5内存普遍以6000MHz、6400MHz居多，选B760M刚刚好。
- 内存时序CL32，工作电压1.4，内存PMIC不锁电压，用户可以自行体验超频的快乐，毕竟6800MHz的内存就要加400块呢

- 主板提供了1个采用了合金强化设计的PCIe 5.0 x16插槽以及1个PCIe3.0 x4插槽，其中x16插槽由CPU提供，x4插槽由PCH提供，可以用来扩展万兆网卡、采集器等PCIe配件。

- 主板提供了3个M.2接口，均为PCIe 4.0 x4，其中靠近CPU的M.2接口由CPU提供，剩余2个M.2接口由PCH提供，三个M.2接口均配备了便捷锁扣，靠近CPU的两个M.2插槽有散热装甲覆盖。正值双十一大促，加满M.2 SSD也花不了多少钱。

- 机械大师C28配240水冷+Matx主板，SFX电源是唯一选择。鑫谷SFX昆仑KL-750G冰山版ATX3.0全模组电源主打一个纯白配色，1块出头1W也还能接受。

- [小钢炮也有大能量｜13600K+B760+机械大师C28 装机实录](https://www.zhihu.com/tardis/zm/art/616613140?source_id=1003)
- 选择 C28 的理由是相比C26plus 宽度高度大了 20mm。实物并没有太大的尺寸差距，但换来更宽裕内部空间，对于机箱内部散热还是很有好处的。
  - 配件方面，13600K+ B760M +6600XT 硬件组合，日常应用不成问题，玩游戏跑个 2K@144 绰绰有余，特效稍微调低也可以流畅运行 4K 游戏。
  - 散热是超频三的风冷 K4，电源因为要考虑到后续要升级 4060ti，选用振华刚出的 13cm 白金电源 VP 1000W。 
- 机械大师的箱子都超级紧凑，尺寸较小，颜值也高，很适合桌面摆放，配合便携屏效果一流。
- 机箱可以支持 335mm 以内的显卡，6600XT 作为过渡显卡，后续会上 4060ti。
- 机箱虽然小巧，但可以支持到 280 水冷。其实我本来也是建议他直接上水冷，奈何朋友总是担心漏液，死活要上风冷。
  - 风冷的话散热高度限制在 162mm，超频三 K4 的高度是 156mm，可以说是刚刚好。
  - 四热管 + 130mm 高性能风扇，官方宣称能压住 12900K，压个 13600K 应该没啥问题。
- 振华 VP1000W 是刚出的新品，主打 13cm 短机身、全日系电容、高转换率。振华是典型的反向虚标专业户，额定 1000W 完全可以当 1200W 来用。而且标配的线材都是柔性软线，很适合用在这种紧凑型机箱里。

- 装机屏幕用的 INNOCN 15Q1F ，屏幕采用的 OLED 技术，显示效果出色，最关键的是这货支持触控，而且自带电池，装机调试都特别方便。

- 英特尔 酷睿 i5-13600K CPU处理器采用性能核+能效核混合CPU架构，拥有6个性能核和8个能效核。
  - 拥有 14 核心 20 线程，主频至高可达 5.1GHz，PCIe 5.0 至高可达20通道，DDR5 至高可达 5600MHz，24MB 的 L3和 20MB L2高速缓存。
  - 搭载英特尔 770核芯显卡，支持超频和WIFI6E网络。

- GIGABYTE B760M AORUS ELITE AX 主板
  - AORUS 系列属于技嘉产品高端阵营，定位追求极致的电竞玩家。
  - 主板采用了14+1+1相核心供电，辅助供电为 8 Pin + 4Pin 12V输入，60A 供电晶体管， 2 盎司铜电路板 + 6 层 PCB 设计，供电、SSD、芯片组等关键位置，甚至连南桥都覆盖了银白色的散热装甲，配合 6mm 热管以及高品质导热垫，让主板散热更为均匀，实现高效的散热能力。
  - 主板为标准的四槽内存设计，有D4\D5 两个版本可选。D5 版最高可支持 7600MHz（O. C），最高兼容 128GB 内存容量，其独有的超频黑科技（高带宽低延迟模式）目前也只搭载在 D5 版上。
  - 此外主板提供了双 PCI-E 插槽，靠近 CPU 的插槽采用合金装甲加固，支持 PCI-E 4.0 x16速率。下方则是普通插槽，速率也只有PCI-E4.0 x4，适合插一些扩展卡之类的。
  - 一体式挡板接口丰富，技嘉B760M小雕WIFI 提供了4个USB 2.0 接口、1个USB3.2 Gen2 Type-A接口（10Gbps）、3个USB3.2 Gen1接口（5Gbps）、1个USB 3.2 Gen2 Type-C接口（20Gbps），HDMI/DP 接口各1、以及2.5G网络接口。当然WiFi6无线网卡天线接口以及音频输入/输出接口、光纤接口都一一完备。
  - 技嘉 13代 主板开始推出的“高带宽低延迟模式”，对于内存性能提升效果明显，特别是高频内存，我之前曾经用它将 7200MHz 内存超到了 7800MHz，读写拷贝超过 10GB/s，写入更是逼近12GB/s（119.18 GB/s），延迟更是压到了 55.7ns，特别适合喜欢玩超频的朋友，有兴趣的朋友可以看看我之前做的测评。

- 技嘉从 b760 D5 主板开始，推出了个“高带宽低延迟模式”，同样的 DDR5 内存条在这块板能跑出更高的读写，更低的延迟，就拿雷克沙这套内存来说，同样的 5200MHz 开启这个模式后，相比正常XMP模式下数据都有不小的提升。

- SSD 是雷克沙定位高端的 NM800PRO 1TB。单面PCB 底板，厚度控制不错，笔记本或者游戏主机都能适配。SSD 表面覆盖新一代纳米铜箔复合材料，搭载智能温度检测模式，有助于快速降温，长时间使用不用担心高温引发的掉素问题。

- 散热用的手头的 超频三 K4。其实我本来建议上水冷，奈何朋友一直对水冷有偏见，总担心漏液的情况。K4 是 超频三的高端风冷，全金属扣具，支持 I家 / A家 全平台处理器，标配 GT-3 高导热硅脂，能够快速将 CPU 产生的热量进行传递。
  - K4 采用单塔单风扇设计，塔身为全黑色电泳工艺，独立装饰性顶盖，四热管触底导热、50 片130mm 铝制散热鳍片进行热量散发，两者通过穿Fin工艺组成鳍片群加大散热速度。整体工艺还是蛮精细的。

- 电源安排的振华 VP1000W。这款电源最大的亮点就是采用了创新的微型化技术，提高性能的同时尺寸缩减到 13cm 短机身、可以释放不少空间给其他硬件。电源本身通过了 80plus 白金认证，转换效率高，加上额定 1000W 的功率，别看身材小，拖 13900 + 4090都不成问题。

- 机械大师在紧凑型机箱里拥有不错的口碑，之前我就装过一次 C26 plus，体验极好。这次选的稍大一些的 C28
  - 机箱灵活度极高，除了基本框架，大部分面板都是可拆式组合，不过大量螺丝拆卸起来还是比较繁琐，这也是其被广大网友戏称为“螺丝大师”的原因。
- 背板可拆，方便走线的同时安装配件更加轻松。有些主板 m.2 是在主板背面，用这个机箱就可以在不拆主板的情况下加装 SSD。
- 顶部还支持 双 12cm 风扇或者 240 水冷。话说别看 C28 体积就比 C26plus 大了一点，但装机体验好了不少。

- 便携屏｜INNOCN 15Q1F
  - 装机我都是习惯用便携屏。INNOCN 15Q1F采用的 OLED 屏幕
  - 重量 0.94kg ，单手握持毫无压力，长时间使用有专用的皮套提供倾斜角度。接口是 Type C X 2 +Mini HDMI 组合，可以实现一线连功能（供电与信号传输一线达成）。
  - 这款便携屏支持手指直接操作，十点触控+内置的触摸OSD面板
  - 甚至还支持无线连接。INNOCN 15Q1F自身搭载了 WiFi 模块，通过它可以将主机、手机或者笔电的画面直接投屏显示，控制则通过触摸实现
  - 便携屏还内置 5000mAH 电池，可以脱离线材连接的不便，续航时间能达到 4小时左右，完全就是台平板电脑。

- [【17.9L】C28迷你手提主机 - 小红书](https://www.xiaohongshu.com/explore/664881a50000000014018c4a?xsec_token=ABjcFKpzYqPfIG6C1AnlqDTFCINPxDCJ7dN5lu4n9saWI=&xsec_source=pc_search&source=web_search_result_notes)
  - 开盖风冷都行

- ## [闪鳞G300支持164mm高度风冷 - 小红书](https://www.xiaohongshu.com/explore/66cd695b000000001f03a713?xsec_token=ABX2Viv4Qebp0rD6T3QkbAt8L2lTvl7qp8ds1dlZ-amvc=&xsec_source=pc_search&source=web_search_result_notes)
  - matx, 显卡340mm，没有玻璃侧板
- 如果用itx电源，电源底下能加12cm风扇吗
  - 不能
  - 因为这个如果在显卡底下装风扇噪音会比较高，前后各一比较合理的

- [有没有闪鳞 g300 平替 - 小红书](https://www.xiaohongshu.com/explore/68bfeedd000000001b03777b?xsec_token=AB7fK2DtVB9-Uox-1NRYZeZJRfF0RaYQ3O2_pLevARsAQ=&xsec_source=pc_search&source=web_search_result_notes)
  - 有没有比闪鳞 300 一样但是是铝的机箱呀，需要极限尺寸的机箱，主板标准 matx，sfx 电源，显卡是 7800xt 超白金版，
  - 不需要装水冷（g300 因为可以装水冷，所以机箱比较厚），
  - 需要背部理线，可拆卸 pcie 挡板（能拆卸后板也行，因为显卡长 320）

- 已经看了小喆的，b3，c2p，但是没有背部理线，c2p 不知道可不可以拆后板，都不大行

- G300就挺好的, 底部风扇和屁股风扇我都没见，散热完全足够

- 底部能装三把风扇吗
  - 只能装2个

- 铁的不行吗？
  - g300，3.2kg，小哲 b3，1.8kg

- Ncase M3.. 应该是综合性比较好的了

- [闪鳞G300 9700x➕5070ti风冷改造方案 - 小红书](https://www.xiaohongshu.com/explore/68be261e000000001c03c453?xsec_token=ABL-IFl_t7vNlynLFA54jExvARox4t1Q4sD_V7oRMh-_U=&xsec_source=pc_search&source=web_search_result_notes)
  - 第一次装机没经验，风冷买了乔思伯CR1000max，结果发现根本压不住，随随便便撞95度墙。
  - 后来机箱尾部加了个利民c12pro，散热塔风扇换成了零度世家风尊t30co（塔还是乔思伯的没换）。现在散热表现如下（烤鸡10分钟，均为CPU温度）：单烤CPU77
- 乔思伯的散热器拉完了
- 这个箱子没啥加的余地了，下面加风扇离显卡太近了，属于是副优化了，顶部加风扇要自己配磁吸螺丝，还是散热器换好点的改善最明显

- 换个双塔不就好了。而且不吃显卡尾气不可能的，这机箱也就这样了。
  - 好的，双塔有人说p60t最强（买塔自己配风扇），这个是真的吗
- 可以的，放远点其实也不是很吵

- [终于赶在双十一末尾买齐了 - 小红书](https://www.xiaohongshu.com/explore/673030e6000000001b012544?xsec_token=ABwBx_0AZ3zhOiWwQ_EVFjfonKMxsn0Fki4JP09JVTi8g=&xsec_source=pc_search&source=web_search_result_notes)
- 4070s功率那么高你就买个650w的电源？
  - 哪怕想后续升级，至少买个850w的也行啊
- g300通风做还行，但是加个风扇保险点
- 尾部风扇最重要……4070s厚度还好，底部也能按，不过整套功耗不算很高，有个尾扇足够用

- [闪鳞g300改装思路 - 小红书](https://www.xiaohongshu.com/explore/6810d0cf000000002100613c?xsec_token=AB_QqkK7_ZF3_KbwyW1ZB1W22MKjj0BCjU-aVf69KLO5s=&xsec_source=pc_search&source=web_search_result_notes)
  - 闪鳞g300挺好玩，在做到尽可能多兼容，支持340mm长度显卡，14cm atx电源和164mm风冷的情况下，能尽可能缩小体积，做到媲美itx机箱的体积，是我最爱的机箱。
  - 在换装闪鳞g300机箱后，我先是上了双塔散热器ak620pro，然而整机将近20斤的质量，让他变得不那么可移动，
  - 对这个窘境，我将目光移向下压式散热，为此更换idcooling-isxt67，并更换散热为12025的九州风神ft12。isxt67塔体兼容性确实不错，在做到尽可能美观的同时也不遮挡任何内存槽。后置风扇位做进风处理，为避免风切声，需要将风扇移到外面
  - idcooling-isxt67塔体优秀，更换风扇之后能稳定压制180w的13600kf（渲染最高温度80℃），同时由于风扇数量少，做到更静音，是一个不错的改装思路。

- 技嘉m650小雕主板，用哪款双塔不挡内存呢，我看您这个风冷好像很小
  - 我这个是下压式风冷，基本上用matx主板都可以不挡内存，如果你是9700x以下的cpu默频应该都是能压得住的，开pbo的话表现可能不好。双塔风冷，你可以看看九州风神阿萨辛四或者阿萨辛4s。

- 同款机箱，请问之后换的下压式，比620pro散热如何呀
  - 同一个CPU相比一下高了大概10度

- [第一台matx，闪麟g300，就喜欢这种塞满满的感觉，下一步准备换个大显卡，然后把机箱立起来摆放 - 小红书](https://www.xiaohongshu.com/explore/687222ef000000001203f6cb?xsec_token=ABMrApDT4vX1zr_5qUgsvW1ie38sgqbr-KgLksPrLadcA=&xsec_source=pc_search&source=web_search_result_notes)
- 请问右下9cm风扇是正叶还是反叶，进风的吗？叠加三个为啥
  - 出风，主要侧面换成亚克力透明板，显卡温度不好从侧面出去，所以靠这三个风扇把风从前面引出去，其实两个就够了，三个是为了饱满好看
- 塞这么满会不会影响散热啊？
  - 有风道的，侧进后出
- 右下角风扇是啥
  - 利民9cm风扇， tl-p9w-s

- 300是不是没有顶部风扇位
  - 没有，但是空间够装薄扇，就是如何固定要自己想办法

- 大佬用的内存风扇是用的什么支架？
  - 乔思伯NF-1内存条风扇自带的

- 显卡吊装温度会升高很多，尤其是小机箱大显卡
  - 这也没吊装啊……直插的
- 他说要换大显卡然后把机箱立起来

- 电源那里网孔挡板哪来的啊
  - 前面是订做透明亚克力板，电源旁边小风扇网格板是内存风扇自带的

- 这个机箱支持非模组电源么
  - 支持的，但线堆在一起比较难关侧板

- 这么紧凑散热跟得上吗？
  - 4060和12490F，配置不高，目前温度稳定，显卡最高到72

- ## [新品预告：闪鳞G350机箱 - 小红书 _202504](https://www.xiaohongshu.com/explore/680b49e4000000000f03025e?xsec_token=ABmVEaGHnouuW4by4cF_K2PztIm78YN2rgioigqEQn8-4=&xsec_source=pc_search&source=web_search_result_notes)
- g300加顶部风扇支持吗，太心动啦
  - g350支持两个顶部风扇
  - 电源底下没有，机箱一共5个风扇位

- 和g300啥区别
  - 多了背插，16厘米电源兼容，顶部两个风扇位，侧透

- 电源支持15？
  - 16cm

- 能不能出个mesh板，把玻璃换成网孔

- ## [机箱说¹ - 乔思伯和机械大师谁赢了 - 小红书](https://www.xiaohongshu.com/explore/66a61571000000002701010c?xsec_token=AB-NVtDH_xyoN48IVgN-otrQDhbPhG6D8S77xA6fATSDM=&xsec_source=pc_search&source=web_search_result_notes)
  - c+max尺寸:392mm*284mm*185mm  20.5l体积
  - z20  尺寸:370mm*295mm*186mm  20l体积
  - z20最好的方案是风冷+atx的组合
  - c+max最佳方案是水冷+atx的组合
  - c28在安装240水冷的情况下，大部分主板不支持atx电源。

- 组装的话我感觉差不多，z20对新手更好吧，防尘同一水平。

- 比较喜欢机械大师，用料是真厚实，配件相对来说也多一点，不过性价比来看比不了一点
  - 铝合金和钢的，材料就不一样，重量也轻了。不然咋降不来价格

- 两款都用过，个人倾向于z20, 超级好看

- [机械大师 c28 - 小红书](https://www.xiaohongshu.com/explore/6717559e0000000016021389?xsec_token=ABE6Ld3Qrhhpu6tuNb5p51L1Q3oe3WokQOP0_Eksi_ke4=&xsec_source=pc_search&source=web_search_result_notes)
  - 选择了 c28，装机成功
  - 和朋友的 z20 对比小那么一丢丢，c28 还是建议用 sfx 电源，这样好藏线些！atx 电源确实大了点极限安装
- 风扇是哪个型号
  - 利民s12

- [机械大师c28帅无敌，9700x+5070？ - 小红书](https://www.xiaohongshu.com/explore/68af1a1d000000001d02d18f?xsec_token=ABx7Z2Y7-KnOKMOASaKY8sp7br1TF0G7T40iVw6hcK7ww=&xsec_source=pc_search&source=web_search_result_notes)
  - 首先说一下为啥选择这个机箱。深度400mm桌面，找了很久唯一兼容风冷水冷的机箱。考虑的背后插电源线和出风。抛弃了z20，选用长度350mm左右机箱实施证明刚刚好。还有两款机箱是这个平替但是不支持水冷
  - 再说散热，散热其实hyp612，ak620，p60t都挺好。本来想要数显版本的，后面刷到目前数显bug太多，放弃了。
  - 电源这个纠结最久，按理说考虑兼容性应该一步到位sfx。但是sfx850w坑太多了。最后选了这个鑫谷的，这个电源好处是120mm，和sfxl一样长，电源线出口不太容易和显卡干上

- ## [乔思伯Z20 or 机械大师C26 - 小红书](https://www.xiaohongshu.com/explore/66d70459000000001f01e720?xsec_token=ABFpKqSuGzHOELX8EkCIc8ArGFoGzwbK104D0gsq2YzyU=&xsec_source=pc_search&source=web_search_result_notes)
  - 机械大师C26: 315*160*265mm, 13.3L
  - 乔思伯T6: 13.7L
  - 就玩下黑悟空是选择12600kf还是12490f，然后就是机箱乔思伯Z20 or 机械大师C26，怎么看都好看就是不知道怎么选了

- 玩黑神话124就足够了，现在126太贵了

- c28 大一些可以用 atx 电源，c26 装 sfx 电源和 c28 装 atx 电源差不多，藏线有些困难，看中机械大师便携和颜值，散热加装五把风扇完全够用

- z20可以atx电源+240水冷 c28只能sfx+240
  - 不用itx的哇他俩都支持matx的哇

- 5把风扇，上下14cm，后置12cm，刚刚好
- 主板微星B650M GAMING WIFI

- 下面放两个14就没地方放显卡支架了吧
  - 是的，小显卡也不需要

- z20可不闷，不是以前的炼丹炉了

- Z20好，底部可以装2把12cm风扇和1块3.5英寸机械盘。装机也比螺丝大师省力。颜值上我个人是喜欢Z20的简洁感

- ## [闪鳞g350还是乔思伯z20 - 小红书](https://www.xiaohongshu.com/explore/68b462bb000000001d02a8a5?xsec_token=ABHRfK0CgtuFDQhWIxlZF5BI37zGCq6uwnRe3ZkLYVd3I=&xsec_source=pc_search&source=web_search_result_notes)
  - 显卡29cm涡轮3090，计划用风冷。外观上偏向g350。有散热焦虑，考虑cpu和电源这一串的散热，
- 质感做工是z20更好一些，散热我用着感觉差不多，闪鳞优势在前置接口太顶了

- 我都用过散热基本没区别

- sfx电源加短显卡可以参考这个，我g350塞了个338的显卡就没辙了
  - 我用的146kf加微星b760m刀锋钛，板u套装2300
  - 另外涡轮卡不用有散热焦虑，还是比较依赖自身涡轮风扇的
- 我想用14700cpu有必要用z主板吗想买一个b760m的算了
  - 稳定运行需要挑块供电好的主板，看超不超频和差价吧

- 前置电源的就别想着散热好，没有前进风的都是焖罐
  - 九州的ch160有前进风，稍微比你发的这两大一点

- g350的话最好买背插主板，看着好看一点，散热感觉用哪个都行不用担心

- 这俩其实差不多，看你是想要UI多一点儿还是能替换玻璃板，Z20的玻璃板是可以换成私人定制的亚克力板这个是需要自己买，G350他的这个板子是没法替换的

- [想换小机箱 - 小红书](https://www.xiaohongshu.com/explore/68ce8bd3000000001300fcdc?xsec_token=ABp12HvDLqm3bcBXz75MLag9SHKyM4ALJFCDd6jzfp9_c=&xsec_source=pc_search&source=web_search_result_notes)
- g300 也行吗，就一个风扇位哎，感觉会很闷啊，散热性的话其实想选 g300，因为最小
- g300的问题是电源出风在机箱内，容易被cpu散热器抽走

- g350 有点大，我现在有点纠结 z20 和 g300
  - 一样长，高了1.5cm底下可以多塞两个风扇，宽多了2.4cm增加背线空间还可以装机械硬盘。
  - 300风扇少各有优势。选300你可以对比乔思伯c6吧，Z20是和g350同级别体积20L啊

- g350太大了 买了有点后悔 ，之前的机箱又盖不上盖子 风冷太大 机械大师c28又太贵了

- 小机箱风道都差，要舍才有得，这款机箱能放 240 水冷和 atx 电源，忘记有没有理线空间了，我就选了 ch160plus，不是很推荐 ch160plus，电源线材太长太硬就很难塞，装机贼难受

- 乔思伯z20的散热你完全不用担心！我就在用！！
- 散热没问题的 我146+5080用的z20

- ## [小机箱乔思伯z20和闪鳞g300选哪个 ](https://www.xiaohongshu.com/explore/66f7512b000000001b022ed4?xsec_token=ABeOr8Ft0HNW3FbtvsjZTd5rWYn4gt7MDLAuGVYnlQiIQ=&xsec_source=pc_search&source=web_search_result_notes)
- 乔思伯Z20, 风扇利民TL-S12，正反叶都有
  - 买的二手散热不怕坏，然后新的风扇搭一下
  - 主要是用了利民的pa120风冷和tl-s12光圈风扇
  - 黑色电源随便上，不挑

- z20底下不准备装风扇了吗
  - 我是单塔风冷，cpu单烤半个钟不超过95度，平常待机看视频等34度，玩黑悟空70度左右，感觉没什么问题
- 这种机箱打开侧板比装风扇有用
  - 侧板没开的，因为养了猫，毛太多不敢开
  - 散热器九州风神，具体型号忘了，你搜一下有数显的应该很好找

- 看你配置，z20的缺点是侧透不是快拆设计，经常想打开散热就很痛苦

- 15×15 好像塞不进 G300
- 是的，只能用利民750的，但是性价比不如玄武850 
  - 我也准备装，打算玄武 650k 的，结果大了 1 厘米
- 卧槽利民pa120普通版+鑫谷冰山版+z20跟我一样诶
  - 是，利民pa120mini+追风者650W

- z20吧 g300没有背部走线感觉太难装了 虽然这个机箱也很难装

- 风冷g300，水冷z20
  - z20比g300大一点，我用的风冷，比较偏向于g300塞满的感觉，如果用240水的话z20比较好看

- 我现在觉得无脑g300 闪鳞的mesh板实在太好看了 而且体积小上面两风扇加不加也没那么重要
  - 感觉你是对的，关键在于风道，g300用风冷的话，上风扇不重要，关键在下风扇的进风能力和侧面排风能力

- 我选的g300，不喜欢玻璃侧板的，容易爆，而且散热差

- Z20好看，质感很好

- 我也是在这两个之间犹豫，我之前淘汰下来的板U想装个小机箱带到单位摸鱼用，因为电源是SFX的，G300想装sfx电源，还要另外购买套件，注意这个套件是水冷支架+电源转换套件，是捆绑销售的，我又不用水冷我干嘛掏这个钱，这一点让我很不爽，我就买了Z20

- ## [个人购机，主观评价，欢迎提问！ - 小红书](https://www.xiaohongshu.com/explore/68c0238f000000001d004524?xsec_token=ABFuZAkrrSKYsMlqCenxgM1Y6uNePfwC_HGRX06HHxyp4=&xsec_source=pc_search&source=web_search_result_notes)
  - 1. 做功用料最好的是乔思伯的Z20，板材最厚最有质感。
  - D32PRD与CH260尺寸都非常接近。
  - D32PRO的理线布局绝对是这四台小机箱中最合理的，这一个电源挡板可以挡很多线。走线也会更轻机。我个人认为最难的是CH160plus，前面没挡板. 后面也沒有，走线全暴露。
  - CH260和CH160plus是不能装底部风扇的。

- 六台里Z20是最厚的，有些比他大的还没它重呢。闷不至于，可以底进上出。前面板有空洞的，留给电源散热的。
- 一般都是后面都是出风的，出的多进的少，就会造成负压，机箱内就会自己吸进空气

- 侧板金属板是不是好点
  - 是的

- ## 📌 [itx机箱选哪个 - 小红书](https://www.xiaohongshu.com/explore/67fa9cc6000000000b02cb14?xsec_token=ABufSAS9QghohUw1fzyX0iVNfOBOR7ae1CsQsryEQvDrQ=&xsec_source=pc_search&source=web_note_detail_r10)
  - 目前市面上的闪鳞L300/400，酷冷ncore，九州风神ch170/270等都看过了，感觉不是颜值不够就是体积太大，其他的itx只能横放，所以选了这两个进决赛圈。
  - 但这两位hyte revolt3的可玩性更高，可以装240水冷，风冷也有140mm充足的小双塔或者下压散热空间，但体积就比fdt1大了3L而且颜值肯定还是formd好看
  - formd t1 v2.5价格相对高，但是体积小，投影面积也小，颜值高。相对散热能选很薄的下压风冷，240冷排倒是也能安，烦恼是我觉得cpu散热会有风切声，而且98x3d的功耗也不低。

- [便携主机小机箱推荐！带配置推荐 - 小红书](https://www.xiaohongshu.com/explore/67ccf00b000000000603b048?xsec_token=ABb9QlEEv4edez8zXqcCpR1c6L8yELfSKA3oNbbrletUs=&xsec_source=pc_search&source=web_search_result_notes)
  - 后面附带两套性能非常不错的配置方案。
  - 小主机的安装难度会比较大，紧凑的机箱空间对于配件的安装顺序以及理线都是比较讲究的，有时候可能安装顺序错了就得拆开重来，会有一定的折腾属性，无论新手还是有经验老手都要做好心理准备

- [cpu散热搭配 一看就会 - 小红书](https://www.xiaohongshu.com/explore/68bd273b000000001c006781?xsec_token=AB3IlccEHpLqWO68RcyJXC3wJqTzyJEBj3EjpFo1z1Bhg=&xsec_source=pc_search&source=web_note_detail_r10)
  - 只要不超频 原装机够用

- [formd t1 itx风冷配置分享+经验 - 小红书](https://www.xiaohongshu.com/explore/66d53972000000001f03a988?xsec_token=AB36raZON7rhJOGKgOTp2CP7HXu79wn9eyAjSxKYmydXE=&xsec_source=pc_search&source=web_note_detail_r10)
  - cpu+显卡：12600kf+4070s公版（t1可2.25槽，散热限高68）（cpu怕压弯可以买个扣具）
  - 风扇：cpu是散热原装，机箱是2个T30，小机箱吧热量排除才是关键，嫌吵可以换猫扇？也可以用fan control（开源的）自己精确设定转速曲线。

- [极致itx风冷主机！ - 小红书](https://www.xiaohongshu.com/explore/680779ab000000001c01d6ea?xsec_token=AB4Cx2A5MWHbcyCyxcg0yIjXwSumOkgsgXOeULyDbKBQE=&xsec_source=pc_search&source=web_note_detail_r10)
  - ncase m2
  - 9950x3d➕5080公版
  - 这是啥箱子？六面全透啊，不错不错，其实下面的风扇个人觉得没啥太大必要加，下面基本贴地了
  - 六面全透，最好的是华硕AP201冰立方机箱。
  - 风扇干了小两千

- [十代CPU，黑苹果最后的倔强 - 小红书](https://www.xiaohongshu.com/explore/65499801000000002202c90b?xsec_token=AB-kwWy_ndn0QOUFCU234Q6f2t_yjutR9JJFSe8XxQYy4=&xsec_source=pc_search&source=web_search_result_notes)
  - 机械大师c28: 342*185*284mm, 18L 📌
  - 这个机箱不错啊，进3出3，外加塔式，其实塔式后面还能放个风扇组成9风扇，起码散热不用愁
  - 只是在itx里散热还不错而已，跟塔式没得比。风扇多噪音也大，小机箱理线也难（这机箱装机半小时理线3小时），一切都是为了颜值的妥协
  - 这个正好，比很多大闷罐好看多了，这个散热不差，你在给散热器加个风扇双塔三风扇，兼顾美观和散热，挺好的，我现在机箱就是9个风扇的matx, 酷冷至尊 MCB-Q500L-KANN-S00 : 386 x 230 x 381mm, 33L, 也就打游戏散热还行，topaz该螺旋桨还是螺旋桨，还是得开侧板

- [9950X用风冷实际体验 - 小红书](https://www.xiaohongshu.com/explore/68415da50000000022005234?xsec_token=ABni7f4b-fffCPxt0H6aLouKtK6gxBT1LF2_CCDAkgMZo=&xsec_source=pc_search&source=web_search_result_notes)
  - 对大多数人来说都看不大出来，或者像我这种不怎么玩FPS的，就不用考虑超频这些了，默频完全够用
  - 我感觉更重要的是配一个风道机箱(我用的是联力207)，然后上个双塔应该就够用了(单塔里也有Hyper 612 Apex这样的高手在)，我用的是160+淘来的二手利民PS120EVO，七热管双塔，甚至还有点歪歪的，不过100+买来的便宜货我也就随便了，能用就行

- [小机箱绝配，双塔逆重力热管，利民小银魂 - 小红书](https://www.xiaohongshu.com/explore/6389ca410000000022028074?xsec_token=ABFaT5DIfPsJIcxbjZdwSLk05zjfIEy_AtMNX-25fkBN0=&xsec_source=pc_search&source=web_search_result_notes)
  - 机箱是机械大师C26 plus，水冷兼容性极差，风冷也限制在140mm以内，看来看去还是利民这款小银魂最合适，顺手入了个白色的
  - 散热器尺寸为120 X 135 X 94（mm），迷你双塔，白色涂装，搭配 TL-D12PRO-G 伺服级风扇，不管是性能还是尺寸都很满足我这套主机的散热需求

- [电脑小机箱该怎么选？ - 小红书](https://www.xiaohongshu.com/explore/66518b9a0000000005007b1f?xsec_token=ABgsZawth9QaYeqFGFhvroUrQnp2-uJiFwrxEDnmydBiE=&xsec_source=pc_search&source=web_search_result_notes)
- 乔思伯(JONSBO) Z20， 主体尺寸长宽高 370*186*295 约20L, 
  - 支持240水冷 163mm双塔风冷
  - 支持限长363mm高性能显卡&全尺寸MATX主板
  - 支持背走线 电源支持ATX电源侧装 竖装
- 华硕AP 201, 主体尺寸长宽高460*205*350, 33L
  - 支持360水冷 17cm风冷
  - 显卡支持358mm 6面通风 4个硬盘位

- ## 🤔💡 [有没有适配较小机箱的高性能风冷推荐？ - 知乎](https://www.zhihu.com/question/491647469)
- 如果是小主机 可以考虑下压式的风冷
  - 下压式6管， 双风扇 还有RGB可以选 可以说是DIY玩家很合适了。
- 两种类型 一种可以随意伸缩 一种是PCIE的侧吹 可以吹 组风道。

- 🤔 与其自己研究风冷水冷风道，如何直接参考指定cpu/gpu的机箱案例

- [闪鳞g300，终于完全体，果然竖着才是最好看的  ](https://www.xiaohongshu.com/explore/68baad38000000001d028c54?xsec_token=ABltSBPxJJg2hpuRLt4eQMLjBIAoL5YNYMLjtqKfRi9_Q=&xsec_source=pc_search&source=web_search_result_notes)
  - 主板：精粤B760M-KD4 wifi 
  - 处理器：12490F 
  - 显卡：影驰4060大将白色 
  - 硬盘：爱国者P7000Z 1TB 
  - 内存：金百达银爵DDR4 3600 
  - 风冷：利民PA140WHITE+2个风扇TL-M12RW+2个显卡支架 
  - 内存风扇：乔思伯NF-1 
  - 电源：玄武650K白色 
  - 机箱风扇：底部1个15mm厚度薄扇利民TL-H12015W-S, 显卡右边9cm高度3个小风扇利民TL-P9W-S
- 显卡上边小风扇咋固定的呢？我G350也想搞个吹显卡
  - 扎带绑就好了，但我这个是吸风，把风从顶面排出去

- [可以带上飞机的风冷小钢炮，5070ti配置分享 - 小红书](https://www.xiaohongshu.com/explore/6809bcaf000000001c02e3f2?xsec_token=ABmERV5M2HyK5KDotkEXLkb7DeMGpfr5ndFUkECpUJvJQ=&xsec_source=pc_search&source=web_explore_feed)
  - 【机箱】闪鳞 G300 黑色: 188 x 345 x 260 mm, 16.8L 📌
  - 【散热】利民 PA140绝双刺客 6热管双塔
  - 【显卡】影驰 RTX5070Ti 金属大师黑金版 O16G
  - 关注了很久的风冷主机，博主搭配的配置合理，这一套基本也能满足2k高画质游戏需求了，还可以把主板显卡盒都寄来了，非常满意

- [【极鱼电脑】5K预算, 拿下RTX4060+i5 12400F - 小红书](https://www.xiaohongshu.com/explore/6707ad20000000001b023290?xsec_token=ABq7KokyP4l816ahZYziBBvJYC4Qak58_fLCWW_pIxj94=&xsec_source=pc_search&source=web_search_result_notes)
  - 【机箱】：闪鳞 G300 白色
  - 【散热】：乔思伯 CR1000EVO ARGB 白色
  - 【显卡】：影驰 RTX4060 8G 大将
  - 【CPU】：Intel i5 12400F

- [可以手提上飞机的5070ti迷你主机 - 小红书](https://www.xiaohongshu.com/explore/68148ed3000000002202ac13?xsec_token=AB-Uvho_Y0GpiJDp6zmaglVYdYNGTBg3XQwcDpwK8pUMA=&xsec_source=pc_user)
  - 【机箱】闪鳞 G300 手提桌面小机箱
  - 【散热】九州风神 阿萨辛4S 高端风冷
  - 【显卡】影驰 RTX5070Ti 金属大师 黑金OC 16G

- [有没有双塔风冷散热器适用于迫击炮主板不压内存，能压住9600X的 - 小红书](https://www.xiaohongshu.com/explore/67f87bc0000000001c001bd9?xsec_token=ABLUYzRFk1yrCyyZARJhGbzYFAAIgFouS3rVw01NAec2U=&xsec_source=pc_search&source=web_note_detail_r10)
  - 你不超频的话，单塔都行
  - 可以单塔+开侧板+电风扇
- 我准备上cmax，我看你电源下面装了一个进气吗？
  - CMAX可以的，显卡选择更多，前面板是个9cm风扇，前下进风，后上出风
- 利民rk120se
  - 你这个散热压的是啥u啊，这个和酷冷至尊612在纠结。。
  - C28，压的9700x，这个和酷冷也不是一个价位的啊，利民这个肯定够用

- 利民有一款，现在正在用, 双塔，6热管，不挡内存
- 我用的利民pa120 b650主板不压内存, 双塔就行 开pbo烤鸡145w 温度最高92
  - 我的b650e天选用不了双塔，左边一块铁右边是内存，但是装的时候没试过抬高
- 利民FC140
- 利民RK120
- 单塔双扇六热管就足够，利民制裁，我就在用，散热相当给力
- 我9700X用的利民PA120SE，目前温度正常
- 我9700X都只用利民X53下压，没那么焦虑

- 那些说四热管就随便压的是真没实际用过9系芯片，就单说9600X，睿频5.4的芯片，用4热管散热器只能让CPU跑到4.6赫兹，建议上双塔风冷或者240及以上水冷

- 单塔612apex足矣完全不档

- [9950x3d对itx机箱太友好了，既安静又高效，待机42度，玩游戏60多度还静音，双烤180w+450w ](https://www.xiaohongshu.com/explore/67ede5d3000000001202edf1?xsec_token=ABTemFrBXmKvmdgQwWJU9gnNkp5wlW6bL0Dk9uWTiJWBE=&xsec_source=pc_search&source=web_search_result_notes)
  - 配置是9950x3d+4090+利民120x67下压式风扇，
  - 利民120x67 和12015风扇，顶部一个12025一个12015
  - 下一步准备把4090换成5090d
  - 我利民90x47 压98x3d 玩个游戏轻松80度，早知道不上那么极限的itx了，想问问博主你的机箱便携性如何，上飞机方便吗，顺便想问问其他人的
  - 这就是我们 itx 玩家，盯着丐版买哈哈
  - 我用来生产力的虽然不是每时每分都在跑任务
    - 那建议上个好点的水冷或者双塔风冷

- [2024 伦敦自用手提4090 小主机 - 小红书](https://www.xiaohongshu.com/explore/6766dd2400000000130083af?xsec_token=ABboPWE_YbGyIsohkIYlHUozqKDRjLeUQCouU2QZY1chQ=&xsec_source=pc_search&source=web_search_result_notes)
  - 机箱：乔思伯 Z20, 180mm*298mm*370mm, 20L 📌
  - 风扇：乔思伯 ZK120W x3 + ZK120WR x3
  - 厂家应该出个带屏幕的风冷，把这个显示温度的换掉

- [闲置配件重组，小机箱里塞一张4090 - 小红书](https://www.xiaohongshu.com/explore/679000e6000000002900d92c?xsec_token=ABMD6mhQ-2BymdWDpwL5hvfM8OOubZz_p0mUadq6O7lw8=&xsec_source=pc_search&source=web_search_result_notes)
  - C+MAX的箱子，白猛禽4090 oc
  - 机械大师C34pro: 这都快中塔了，有些大了

- [打造最强ITX主机，仅12L的I9+RTX4090小钢炮 - 小红书](https://www.xiaohongshu.com/explore/65a92ab2000000002a033278?xsec_token=ABSBEXLTFElYGee4WjQ7FQrmAnS3M78y8zG87vi2iVIac=&xsec_source=pc_search&source=web_search_result_notes)
  - 分形工艺Ridge

- [能装4090的10L超迷你顶配 itx主机！ - 小红书](https://www.xiaohongshu.com/explore/66feab5b000000001902e426?xsec_token=ABlu4J6--wKooejNGxnEt8DN6ozr-DSEM4a2kEPukXYkI=&xsec_source=pc_search&source=web_search_result_notes)
  - 能在这么小的体积装下9950x➕4090，估计只有 联力 T1 了。

- [整个世界都安静了，装入5080显卡的风冷主机 - 小红书](https://www.xiaohongshu.com/explore/67d78dda000000000d017cfa?xsec_token=ABlo-M2mkEksa4R6fDVfvdseBp4z5h5kgTVaUZl9pOpYg=&xsec_source=pc_search&source=unknown)
  - 分型工艺 Torrent Nano 可以说是最强风冷ITX机箱，塞入旗舰CPU和5080/5090D显卡没有问题，硬件选择和走线方案经过多次迭代趋近完美
  - CPU：AMD 锐龙 7 9800X3D
  - 显卡：影驰 GeForce RTX 5080 金属大师白金版 OC
  - 机箱：分形工艺 Torrent Nano RGB 白色, 体积太大

- [高颜值135mm小塔，利民PA120 MINI开箱安装！_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1DH4y177Q2/?vd_source=deff4d2e2efa3273948dd6911a08fd39)
  - 塔体，总高度135mm

- 💡🌹 [除了难装，什么都挺好 - 小红书](https://www.xiaohongshu.com/explore/683a78d60000000023012638?xsec_token=AB_w2cD0Ez3Uraiu24sIOdVCNVjGP6ywl00WVNctWCHeY=&xsec_source=pc_search&source=unknown)
  - 如果需求的标签要满足大显卡、MATX、标规电源、双塔风冷、紧凑，闪鳞G350是不错的选择。
  - G350: 21 x 34 x 28 cm, 20L 📌
  - 底部风扇是 利民TL-C12PRO, 声音还能接受
  - 如果想用16cm电源来满足进风风扇在前面板左右居中位置（强迫症）、底部双进风扇，注意供电线、面板线都要预埋到主板下方
  - ROG B850小吹雪、九州风神阿萨辛4VC至尊版新品集合首秀，全白达成。
  - 98x3d，Rog b850-g，32g 8000阿斯加特，Rog 白金雷鹰1000w，阿萨辛4vc至尊版。
  - 显卡和固态没参考价值，5把机箱风扇看个人喜好
  - 这机箱自带显卡支架
- 你说g350的时候忘了它最大的特点，背插
  - 无论市场占比还是用户接受程度，背插这个点个人实在是不太关心。。何况amd平台的一线背插版好像是一个手数的出来
- G300顶部风扇位都没有，和这个比？加宽的部分在我眼里价值就能用大电源和好理线，而不是根本没几个型号可以选而且残值炸裂的背插配件。背插产品的销量能不能让这个方案撑过阵痛期都是问题

- 800多的风冷 疯了吗
  - 为颜值买单

- 我买的620，感觉到时候也把主板遮住了
  - 嗯前装挡内存后装挡主板

- 我装完了，但是玩游戏的时候拉满CPU80℃不到，gpu60多℃正常吗
  - 很低啊，75不高的

- 这个typc 接口你们的线能全插进去吗 我的苹果数据线不能完全插进去
  - 可以插，线跟机器没有完全贴合 是有缝隙的

- 电源前置还是不适合风冷吧，ch260/270要合适点吧
  - 能不撞墙降频就算能用，G350对标的是ch160/170，手提箱嘛肯定没法六边形
- 紧凑型直插机箱的电源一般都是这么装啊，前面冲网进风

- 这个的装机难度很高哦
  - 用16cm电源会稍有难度
- 这个双塔风冷造型很喜欢，侧看里面很像一块一块录音室的隔音棉
- 显卡限长多少啊，我335的卡好怕装不进去
  - 340

- io线跟底部风扇撞得一言难尽
  - 因为机箱出厂设计没给足够的弯折空间

- [利民最新性价比风冷之王？PA120后继有人？ - 小红书](https://www.xiaohongshu.com/explore/68a25df0000000001d008b69?xsec_token=AB6eEiv8Jga5YuqGHfqLGMKBS62nhuM7ceSyfjW0FC3fM=&xsec_source=pc_search&source=web_search_result_notes)
  - 两款：Royal Pretor 130 和 Royal Knight 120 （不是SE！！SE风扇是旧款！）

- [盘点利民随机十款产品，从夯到拉五个级别 - 小红书](https://www.xiaohongshu.com/explore/68b86a8a000000001d037f1d?xsec_token=ABTE1Axv-5o-X12XW6KbsOFY6VCPlbEhwKmVzBccY4g40=&xsec_source=pc_search&source=web_search_result_notes)
  - U120ex单塔风冷
  - 双塔风冷pa140，挑体质，换个配扇表现尚可，这里给到人上人

- ## [流言飞起的时代，4090真的值得入手吗？ - 哔哩哔哩](https://www.bilibili.com/opus/859701664964149281?from=search)
- 4090其实分为两个版本一款是专门用于游戏领域的显卡名为风扇卡，另一款则是用于深度学习领域名为涡轮卡。两者虽然都是4090实则内部结构、目标人群、运用场景其实都是不同的
- 风扇卡与涡轮卡的供电接口位置不同，涡轮卡的供电接口位置在接口尾部，供电线比风扇卡的线更短，这样是方便在机箱内部安装和理线，
  - 而风扇卡供电接口一般在显卡顶部，接线后线缆会高于机箱最高面，如果在服务器中使用风扇卡，服务器盖板盖不上。
- 在散热方向上面，涡轮卡散热方向是朝尾部散热，并于服务器风向是一致的，
  - 而风扇卡的散热是朝四面八方来散热的，平常的PC机箱放一张是可以适应的，但用作服务器上（很多时候是多卡）就不适合了，很容易因为散热不出机箱导致机箱内部温度过高出现宕机。

- 风扇卡的尺寸一般是2.5-3倍宽设计，而涡轮卡的尺寸大小是双宽设计，
  - 因为涡轮卡为了方便放入服务器里，所以涡轮卡的尺寸和高度都远远低于风扇卡，从而服务器可以支持4卡或者8卡，如果用风扇卡代替涡轮卡装在服务器里，那位置够不够还是一回事儿呢。

- ## [机箱小影响CPU散热吗？ - 知乎](https://www.zhihu.com/question/335865142)
- 机箱小肯定影响散热的。但是风道也很重要，有合理的风道，就可以使小机箱也能有不错的散热。
- 以我资深散热控的身份告诉你，机箱大小不是关键，所谓的风道也不是关键，关键是出风口，传统方式来说就是主板后挡板上面那个风扇口。
  - 现在大多是单风扇位，以前还有过双风扇位，这个位置一定要上一个大风量优质风扇，转速再适当调高点。
  - 进风口也一定要有，但是不敏感，大多数机箱进风口都在前部，你可以用低转速，至少要保证一个风扇吧，两个最好，没风扇，或者没有进风口也是不行的。

- 风道设计十分重要，如果你的机箱内风道没有设计，比如小机箱CPU散热直接吹到没有孔的机箱侧板或上板上，热全都锁在里面，那么自然待机50来度。
  - 对于上面无脑上水冷的，我只想说，本来机箱就已经很小了，你水箱和冷排放在哪里呢，更何况入门级水冷的散热效果比大部分风冷要差这是事实，小机箱还是设计好风道让空气在机箱里流动起来效果最好了。

- [我30来年diy才明白，最好的家用机箱就是开放式机架，风道什么都是骗人的 - 知乎](https://zhuanlan.zhihu.com/p/1931289802654855351)
  - 我吃惊的发现，开放机箱，只要机箱风扇轻转，就能形成均匀充沛的气流，根本无需机箱结构导风 ，开放机架最酷的是可以叠加，老电脑和新电脑做在一起，混合接几台显示器，键鼠弄几套，灵活性无以伦比
  - 只要家里有安全的空间（不被人泼水 宠物破坏等等），开放机箱就是diy最终归宿。
  - 板卡受力好，稳定寿命长，风扇避免了最不好的标签向下的摆放。维护管理也超级方便。灰尘也不严重，没什么怪异的乱流，就没什么顽固的灰尘积累。旁边放台空气净化器更佳。
- 家里没新风系统的别试了……灰尘和扬尘不是每个地理位置都控制得好的
- 灰太大了，不行的，我深圳离海边只有3公里30楼那灰大的。。

- ## [小机箱是怎么解决散热的？ - 小红书](https://www.xiaohongshu.com/explore/6710a2ab000000002100583d?xsec_token=ABM6aePzBDdvEmqW7AP7IWy2aQbrM8b-WUMg-puQ2H2DU=&xsec_source=pc_search&source=unknown)
  - 非常经典的机械大师C24，樱花粉配色，9.9L的体积
  - 小机箱散热到底行不行？只上风冷到底能不能压得住温度？
  - C24的机箱构造采用了菱形的开孔透气的侧板，也是非常有特色的一个设计，同时上下两面以及背面都有做开孔设计，保证了良好的透气性能；
  - 而小机箱的空间紧凑，能够实现最优风道方案的，其实就是机箱尾部的风扇，可以说是神来之笔，加装之后才能发挥小机箱的短风道优势
  - 显卡如果搭配一张双风扇的显卡温度则可以还要低上几度；

- [联力又一个有爆款潜质的机箱 - 小红书](https://www.xiaohongshu.com/explore/66e30ddd00000000120107c2?xsec_token=ABofaNF4fkho52iws-yq8ScYcAYtNeg2kdaya4XViqLIc=&xsec_source=pc_search&source=unknown)
  - 之前有人问过小型一点的MATX机箱推荐，最近刚好可以参考这台新上市的联力A3。
  - 它是联力和DAN Case联手设计的。
  - MATX结构，如果用ITX的话电源可以下置，虽然空间达到了26L多，但是毕竟它是MATX，在配件支持上也能够更加自由一些，风冷支持最高165mm，基本上市面常规风冷都能支持，水冷也能够支持到360mm的一体水冷。显卡支持则达到了比较夸张的415mm，并且支持竖装。整体模块化设计，装机难度不高。

- ## [你们的小机箱加不加散热风扇啊？感觉好闷，好烫 ](https://www.xiaohongshu.com/explore/68499022000000000f033b1b?xsec_token=ABEAH8XqEXMtphXMAXSQfa7ARcpcy6sEhThq_QezV-_NI=&xsec_source=pc_search&source=unknown)
- 看你功耗 整机150w以内随便加不加 超过建议加两把风道有进有出即可，我这日常用大概350w左右整机功耗用了4把 ，其实三把也够了。还有就如果不是很厚实的箱子，风扇越多越容易低频共振。

- 现在的电源好像都是500瓦及以上的，用不了那么多，想买小一点的都没有
  - 没必要买太小，现在650w以下都是白菜价200块钱不到这个钱就别省了。多点冗余的话不光是带设备不吃力，发热小了风扇转速低一点还更安静。

- 看了你的机箱布局，还是可以多加几把的，机箱尾部，顶部，以及显卡下加薄扇，肯定对散热有提升
  - 那基本就加满了，现在先加一个在机箱后面，试试温度
- 不想加多的话顶部正对着风冷最好也加一个

- 正常使用35度的GPU，40度的CPU

- ## [小机箱散热有问题吗？ - 小红书](https://www.xiaohongshu.com/explore/66cbb2f2000000001f016917?xsec_token=AB-BTjol0N_mwqMM3wA24XS93uB6WGXYwkAz6dvKjlB30=&xsec_source=pc_search&source=unknown)
- 不要看它小，这里有三个风扇把热量散出去，其实跟普通机箱没差的
- 不会，其实三面开网风道反而会更好。

- 小机箱，4个风扇够用了

- 我这纯闷罐都没问题
- 怕散热不行就打开侧盖呗

- 便携机箱就是因为要来回飞带着方便，水冷不能上飞机，便携还有什么意义

- 散热没问题的，我也是紧凑matx，公版3080满载也就70多度，贯穿风道

- 我7700+7900xt都随便压

- 本身atx电源前置就是不得不向空间靠拢，而舍弃了进风于妥协的一步，散热不好是先天缺陷。不过为了不影响进风，可以选择显卡cpu双240冷排，并将尾部的风扇作为进风使用，或者不追求侧透的话，直接开孔是最简单的

- 这么大已经不叫小机箱了 10-15升的大小我都嫌大

- ## [小机箱装机血泪教训——乔思伯d41 - 小红书](https://www.xiaohongshu.com/explore/67cd3148000000000e007149?xsec_token=ABQ2_O_qG8QQbUy7d9Jl_OsA4fk-MLl7s4Y__ZMDEuIUM=&xsec_source=pc_search&source=unknown)
- 提示：硬件茶谈的装机教程是个很好的小白教程，装机细节完全可以参考，但是小机箱装机顺序不能完全参照他的来！不然你就会跟我一样，拆了装装了拆装了拆拆了装
- 小机箱装机第一步：带上手套
- 小机箱装机第二步：拆开机箱，取出电源盒，准备一个小盒子（必备！！！）放机箱上拆下来的零散件：螺丝、挡板等等。
- 小机箱装机第三步：千万不要脑子一热就去装散热器！先给我拿出主板，装上cpu和ssd（NVMe），内存条这会可装可不装，
  - 然后把主板装到机箱里面，力道要恰到好处，既不要太重也不要太轻。
- 小机箱装机第四步：千万不要脑子一热就去装散热器！先给我把机箱连接线插到主板上！！！
  - 如果有显卡并且打算装ssd（sata）和机械硬盘的，请千万先给我把sata线也先插到主板上！
  - 重要补充：cpu电源一定要插好！不然后面你要么拆风扇要么在风扇和主板的夹缝中挣扎！
- 小机箱装机第五步：这会你可以装散热器了，只要记住：垂直风扇都朝下，水平风扇都朝前！
  - 先插风扇线，再固定散热器！
  - 如果用水冷散热器，现在水冷风扇一般已经固定在水冷上了，水冷和风扇应该分别插在pump_fan和cpu_fan上（
- 小机箱装机第六步：全模组电源，什么锅配什么盖，什么色插什么色的线，插好先把电源装进电源盒，把机箱电源线插入电源（别忘了打开电源开关！！！），再把电源盒固定到机箱上。
  - 有显卡先装显卡，没显卡直接插线理线。
- 小机箱装机即将完成：要装ssd（sata）和机械硬盘的，这会就可以把他们到固定位置或者硬盘架上，插上sata线和电源线。
  - 别忘了，之前要是没插内存条的要插上内存条，按照 2 → 4 → 1 → 3 顺序插满，确保双通道生效。

- ## [支持多指触控的开源触控板来啦！ - 小红书](https://www.xiaohongshu.com/explore/6711d2b30000000021006a5d?xsec_token=ABBAVIU5BmyEoOSC1_U-awDuJr92L8j26jKmwMyo1z8q4=&xsec_source=pc_search&source=web_search_result_notes)
  - Ploopy因其开源、3D打印的PC外设在小众市场中享有盛誉，其最新产品Ploopy Trackpad也延续了这一风格。这款“3D打印、开源的大型触控板”适用于桌面和笔记本电脑，尺寸为190 × 140 × 20毫米，比Apple的Magic Trackpad还大一些。
  - 它支持Windows和Linux上的最多五指多点触控手势，但由于macOS与QMK固件之间存在限制，苹果设备的手势支持受限
  - Ploopy Trackpad的外壳和触控表面都由PLA塑料3D打印而成，Ploopy声称这种材料比ABS更耐磨并且使用舒适。虽然最初计划使用玻璃表面，但最终选择了ABS，因为它在不同操作系统和设备上提供了更一致的跟踪效果。鉴于Ploopy的开源设计和活跃的社区，未来可能会有用户实现玻璃触控表面的改进。

- ## [妙控板有没有平替 Apple也太贵了 舍不得 #妙控板 #Apple #苹果#Macmini - 小红书](https://www.xiaohongshu.com/explore/6822f22b000000002202f95b?xsec_token=AB7ZPhxG4yCtuySva103gpkHVwVRQu1KHJQbB5APshtUc=&xsec_source=pc_search&source=web_search_result_notes)
- 没有，而且妙控有线>蓝牙连mac>蓝牙连ipad pro，很追求手感的话还要配一根长线，是连60hz的屏都能分辨出区别的程度
- 没有，尤其是破罗技，一生黑

- 买二手的吧，这个很耐用的，二手成色好和新的也没啥区别
- 买一代的二手也就一百多啊

- 是的，而且非常建议用有线连接，没有蓝牙的延迟，使用体验和mbp自带的基本一样

- [我的mac mini穷鬼套餐，不失体验 - 小红书](https://www.xiaohongshu.com/explore/68320649000000000c03825f?xsec_token=ABF0XhwOtoKDBQhKUo195eNCcCz6x5xwHH_366uEQDqSo=&xsec_source=pc_search&source=web_search_result_notes)
  - 一代妙控板可以配m4的，打开就直接连上了

- [苹果触控板理想平替 150块能实现80%功能？ - 小红书](https://www.xiaohongshu.com/explore/6728d5db000000001b01018c?xsec_token=ABXrx-jUlKOXDJg4dJl1XMWqHF5XtUK6_hozrpAJbVH9Q=&xsec_source=pc_search&source=web_search_result_notes)

- ## [最佳外接独立触摸板罗技casa touch体验 - 小红书](https://www.xiaohongshu.com/explore/6804c1f0000000001201c88a?xsec_token=ABZ7BagZ46pJM6c6btbNXwLHXUCDQ3iVwDsgN_a75SKZ8=&xsec_source=pc_search&source=web_search_result_notes)
  - 太长不看，最佳外置触摸板-妙控板替代品，可以无脑下手。
  - 罗技23年推出了一款pop-up desktop产品，里面包含了一个外置触摸板，上一次罗技推出独立的触摸板还是在十几年前。
  - 大小和笔记本上的差不多，有黑色/白色和粉红色。表面材质是玻璃，被塑料覆盖。重量157.6克，妙控板是230克，触摸面积比妙控板2小很多，
  - 可以用bolt和蓝牙进行连接，有三模，每一个都可以单独连接bolt和蓝牙，
  - Mac/Windows/ChromeOS（fydeOS）/iPadOS都支持所有功能和手势。
  - 可以连接三台设备，三个记忆点
- 没有国行
  - 闲鱼卖光了，可能要买套装了

- 比huke好用太多了

- ## [MacBook外接键盘使用分享 - 小红书](https://www.xiaohongshu.com/explore/66fbde7b000000002a034d62?xsec_token=ABuws2tsgaoDnK4grUvCbFCBwKv3VPGL8ZVJ0p8qGnHec=&xsec_source=pc_search&source=web_search_result_notes)
  - 一、苹果妙控键盘 - 桌板轴 给 Mac 配备的第一个键盘是妙控键盘。这款键盘在适配性方面堪称最佳。无论是颜值、重量、键位、多功能键还是续航，都与 Mac 实现了无缝衔接。唯一令人吐槽的地方在于按键手感，几乎没有任何反馈，打字的时候，你可以想象自己是在敲桌子，完全没有输入的乐趣。倘若你不在意手感，那么这款键盘是我的首要推荐。
  - 二、京造 K2- 红轴 受够了敲桌子般的无聊手感后，我换京东京造 K3 红轴键盘（与 Keychron 贴牌同款，当时众多自媒体都在安利的网红款，号称是 Mac 的第三方最佳适配键盘）。 这款键盘的颜值当时不错，黑橘配色，但是会打油，连接方式为蓝牙和有线双模式，轴体是佳达隆红轴，直上直下，比较轻盈。使用了一段时间后，体验尚可。槽点是键帽较高，使用时手部悬空，长时间使用会比较累。 内置电池待机时间短，充电较为频繁。对于强迫症患者来说，最难以忍受的是蓝牙断联和睡眠唤醒慢，还会吞字。
  - 三、罗技 MX Mechanical-红轴 恰好罗技新上市了一款 MX Mechanical 矮轴机械键盘，超长待机、三模连接，满足我的需求，于是入手了一把茶轴款。这款键盘的轴体与传统轴体有所不同，茶轴有点类似于黑轴，手感很重，不够轻盈，不太适应，换成了直上直下的红轴。这个矮轴相比高轴的红轴，回弹稍微重了一些。矮轴长期使用不会那么累了，连接速度也很快，没有断联和吞字的情况，整体来说还是比较满意的。这款键盘目前我也一直在使用中。说缺点的话，键帽比较小众，没有完美替换的，少了些乐趣。键轴也不支持热插拔。
  - 四、Nuphy Air75-越橘轴 给 Mac 主机配键盘，选择了 Nuphy 的 Air75 一代。键盘的包装和做工很精致，颜值是最高的，键帽好看，三模连接，采用佳达隆热插拔矮轴，连接速度快，不会出现断联和吞字的情况。后期换了Air75 二代，提高了连接速度和续航，增加新的轴体，新的轴体和垫棉带来了更好的手感。开始买的是芦荟轴，太轻，反馈弱，容易误触。换成了越橘轴，这个轴类似于 “HiFi 麻将音”，手感重一些，有更好的反馈力不会误触，长时间打字也不会累。

- 刚入手keychron K4 pro 香蕉轴，手感不错，适合办公打字。关键是可以via改键位，很香
  - 声音不大，挺柔软，适合在办公室用。

- [苹果电脑妙控键盘 最强平替 - 小红书](https://www.xiaohongshu.com/explore/67cc2e4f00000000290198e4?xsec_token=ABb9QlEEv4edez8zXqcCpR1avO8fI6BvZ09MtS_ONM62k=&xsec_source=pc_search&source=web_search_result_notes)

- [Mac book外接设备分享（穷鬼版） - 小红书](https://www.xiaohongshu.com/explore/67de16ad000000001d017c2e?xsec_token=ABYgeab4C5E3b9gpHBDeBL8QBz3tky0SCWCe9j92RoGQg=&xsec_source=pc_search&source=web_search_result_notes)
  - 键盘⌨️：京东京造k3 max（💰340r）
  - 鼠标🖱：蜻蜓VXE R1 SE+（💰70r）
  - 拓展坞：绿联五合一（💰158r）

- [可以支持Mac 的无线机械键盘，轻便静音，适合长期打字办公党的，宝子们有没有推荐呀？谁懂打字手指关节痛呀 - 小红书](https://www.xiaohongshu.com/explore/67a2ca1400000000290289d0?xsec_token=ABVfm3DcCUe0ysKmPGx8coQmEZCkPFcCZi28IS-M-_rUg=&xsec_source=pc_search&source=web_search_result_notes)
  - 可以支持Mac 的无线机械键盘，轻便静音，适合长期打字办公党的，宝子们有没有推荐呀？谁懂打字手指关节痛呀
- Nuphy Air系列或者Keychron K系列, Nuphy是静音的
- nuphy air系列，铝厂 MG65 都很好

- 宁芝静电容 mirco plum 84 35g版本

- 官方妙控键盘 轻薄 静音

- nuphy， 洛斐，渴创，都是支持苹果的成品键盘

- 京东k3max，keychron代工的，我用着还可以。矮轴不需要腕托。

- ## [Mac外接显示器使用的一些建议和优化方案 - 小红书](https://www.xiaohongshu.com/explore/67d823340000000007036b64?xsec_token=ABi4D7ISTN18_diLmfrK_nWQFIh4EdsPayzksbNRkvRgQ=&xsec_source=pc_search&source=web_search_result_notes)
- Mac外接低分辨率显示器出现的模糊问题
  - 当购买了Macbook或者Mac后 我们选择外接显示器 通常选择分辨率是 4k或者 5k的居多 但也有一些小伙伴用的是1080P或者是2k的显示器 导致Mac在此类显示器上会出现字体发虚、模糊等问题 
  - 因为Mac默认 4k以上分辨率的显示设备才会开启HiDPi  
  - 所以解决此类问题也很简单 利用软件强制开启HiDPi即可 这里推荐RDM 不仅开启HiDPi 还能开启显示器高刷 当然有条件的话 还是推荐换高分辨率的显示设备
- 外接显示器后无法利用外接键盘调节显示器亮度和音量
  - 第一次使用Mac外接显示器 用的外置键盘 却无法调节显示器的亮度 甚至音量也无法控制 这里推荐一款软件 即可解决此类问题 Monitor Control 是github上免费开源软件
- MacBook长时间外接显示器满电使用电池寿命问题
  - 这个问题也是大家很在意的 毕竟新买的设备都比较爱惜 这里推荐大家一款AIDente的软件 可以将设备电量🔋控制在 80%  还能控制MagSafe充电灯 充电的同时主动放电到 80% 的健康百分比等 还可以监控充电功率 计划充电等 网友评价很高 部分功能在Pro版本上需要付费 但一般基础功能免费开放
- Mac mini m4 本身是支持 最高 4k 240hz 的显示器 多台连接也有 4k 120 hz或者 4k 144hz 办公 60hz足够 但是高刷体验更佳 比如浏览网页更丝滑 拖动窗口也丝滑 有足够预算选是最优解

- ## [买了个超高清 4K 23.8 寸 便携显示器 - 小红书](https://www.xiaohongshu.com/explore/6810c50000000000090174b8?xsec_token=AB_QqkK7_ZF3_KbwyW1ZB1W-4U-ahovczyytBjBGhW3_A=&xsec_source=pc_search&source=web_search_result_notes)
  - 如果是长期自用办公和科研，至少要2K以上，搞艺术的最好是4K。
  - sculptor23.8寸 4K 显示屏

- ## 📌 [便携屏挑选建议 - 小红书](https://www.xiaohongshu.com/explore/678deac90000000018029a17?xsec_token=AByy4GMp8bWndvumxKkA0t2zCapY5R9quZuyaVsMR8Y_w=&xsec_source=pc_search&source=web_search_result_notes)
  - 首先明确需求: 办公还是游戏？Switch、主机或电脑？
  - 目前市场主流品牌均为非一线厂商，但不代表产品不达标，质量也有很不错的。
  - 主流品牌有，ARZOPA阿卓帕、雕塑家、EIMIO、CFORCE等，各有特色。其他品牌，若预算不足，要求不高，也可选择。 
  - 主要有两个参数需要注意
  - 帧率:60hz适合办公以及轻度游戏，包括switch、ps主机、电脑端。然后就是144hz及以上，适合FPS等竞技游戏，例如CS2
  - 色域: 主要标准有sRGB、Adobe RGB、DCI-P3、NTSC。基本上sRGB大于75%就有较好的颜色表现了，当然越高越好。
- 我相信1080p已经满足绝大多数人需求，更高分辨率不如直接上台显，对设备性能要求也会更高。
  - 这也是为啥只测试了1080p的版本，足够清晰了，屏幕又小，钱花在刀刃上。

- ## [mac mini 便携显示器难得找个完美的 - 小红书](https://www.xiaohongshu.com/explore/67be8dff00000000280368be?xsec_token=ABatyUb9Lx1XYXALQpMcvmTBAo775B3MvozPzD4EZe9RI=&xsec_source=pc_search&source=web_search_result_notes)
  - 经过多方对比，拿下arzopa 16寸的2.5k便携显示器，菜单方便，连接一根线即可，显示效果不错，不过这类显示器很难支持hidpi，目前就1280-800还行，不过感觉缩放有点大，原始分辨率字又很小

- 要么4k要么1080，m系列就是hidp有问题，得这两个分辨率，五年了都没解决很无语。intel版就不会
  - 理解为没法点对点就行

- 所以很多博主推荐苹果配4k显示器，就是要它默认1080p的hidpi。最重要的一点是mac的应用不能随意改变窗口大小，必须考虑这个问题（720p在用QQ音乐可能会与程序坞重叠）

- 一般来说2k显示器默认720p的hidpi

- [Mac外接便携屏 - 小红书](https://www.xiaohongshu.com/explore/68b697cf000000001d03b010?xsec_token=ABUJretia2o2VefwJarjPWkuB5UQVMIEKT9s7nuAqwVcs=&xsec_source=pc_search&source=web_search_result_notes)
  - 要不然 1080 要不然 4k，mac 用 2.5 是不行的
  - mac 的 hidpi 导致合并像素变成 720p，直接看 2.5k 的话小的没法看，其它分辨率会糊
  - 重新买了一个4k的 感觉不错

- [Mac mini + 2.5k 触摸便携双屏 - 小红书](https://www.xiaohongshu.com/explore/677691f1000000000902d752?xsec_token=AB-RIwo7Ptt2-VRNMWpcTa4IqKO9_fbiWVFPRQBKISJ7U=&xsec_source=pc_search&source=web_search_result_notes)
  - 2.5k 虽然开了 hidpi 只有1260*800分辨率， 但在16寸屏幕上字体显示比较舒服，不会吃力，缺点是双屏触摸屏很重（2.1kg），已经不能算便携了

- ## [mac怎么选便携外接屏？ - 知乎](https://www.zhihu.com/question/539704864)
- 便携的话就ipadpro很适合，平时可以当pad用，看看电影，记记笔记，干活的时候就可以作为外接屏，很方便。

- ## [英特尔怎么可能一年之间就没落了？ - 知乎 _202410](https://www.zhihu.com/question/2005473234)
- pc端只是表象，真正打断intel骨头的是服务器端。zen刚推出的时候，16~32核线程撕裂者，平均每核成本和售价比志强低，线程撕裂者一推出就收到市场好评，28核以下的志强市场立刻遭受线程撕裂者的瓜分。
  - 后续zen2架构下，推出最高64核心128线程的霄龙服务器，志强平台一个能打的都没有，论性能，拼不过，论价格拼不过，论能耗，拼不过，唯一可以拿出手说话的就是志强的内存延迟更低。
  - 到了zen3时期，霄龙服务器缓存进一步增大，延迟相比志强甚至略有优势，IPC也比志强好，功耗也更低，一颗64核心128线程的霄龙U，价格一万多美元，而Intel这边迟迟推不出新一代志强，还是28核心56线程的志强当道，价格也要1万多美元，你是商家，你选谁？
  - zen4时期，霄龙继续领先，志强还是被压着打，只能吃吃老本，掏不出什么让人满意的新U。
- 市场统计，志强份额还是领先于霄龙，但是那是历史，没什么卵用，新的大订单基本是霄龙吃下了，志强只能吃吃老本的更新换代，根本不解渴。
- Intel的困局本质上是性能更高的志强平台已经很久没更新技术，技术下放更是无从谈起。PC端的缝缝补补还能和amd打得有来有回，混个28开、46开、55开，服务器端真的是一败涂地。

- 不知道的以为英特尔还在挤牙膏，其实是英特尔肚里真没货。

- 服务端求稳，，路径依赖给Intel吃老本了两年，zen2的服务器我就测过了，真是性价比吊打Intel，而且为了好迁移程序AMD经常是单路对双路

- 旧的超算中心都是iU，稍微新的就是AU了；当然国产U也有，用过国产海光X86
  - 华为的前阶段国家还有补贴，对于小微企业还是有相当吸引力，毕竟云主机能用价格低就是王道。

- 就国内市场而言，我觉得是被海光打败的，我司已经2年没采购intel的服务器了，全是海光和鲲鹏。
- 海光就是zen1架构

- 性能强的的有amd，低功耗的有arm的。被挤了。

- intel技术出现了什么问题了？
  - 一切源自10nm难产……工艺上不去很难堆核…堆核了发热和功耗倍增对dc成本不划算……
  - 堆不了核，架构升级不了

- epyc 对于企业来说省的远不止是 cpu 差价的钱，整台服务器除了 cpu 外的其他硬件、机柜、电力等，用 epyc 一台能顶至强好几台，省下的钱可不是个小数字

- PC DIY市场对于厂家而言是最鸡肋的，利润不高但音量很大。商用、专业领域甚至游戏主机都是更优质的市场。但是对于一个产品导向的公司来说，他不会随意丢弃某个市场，只是支持力度的问题。PC DIY市场地位在目前基本就和讨饭的差不远

- 有没有可能，从十年前就开始没落了。intel7（10nm）这个工艺应该在2016年就出来的，结果到2024年了还在缩肛。

- intel岂止是在cpu上拉胯了，这几年砍了多少方向了？砍了，FPGA拆分了，放弃。说是要把工作重心放在cpu上，可是工艺早就被台积电甩开。最重要的服务器领域甚至拿不出和AMD旗舰型号对标的产品。要我说现在这个衰退速度还算慢的，要不是有使用惯性，intel应该更垮一些。

- ## [线程撕裂者这种核心超多的CPU主要能用来做什么，来使用掉绝大部分核心？ - 知乎](https://www.zhihu.com/question/500819181/answers/updated)
- 对于普通家用电脑来说，别说8核心，4核心基本就能满足许多需求了，这是因为大部分的常用软件并没有对多核心处理器进行优化，即使你的CPU核心数量再多也用不到，相当于浪费，但是像[线程撕裂者]这样的超多核心CPU可以在专业领域方面发挥很大的作用。
  - 3D图形渲染、数据中心和视频处理这些工作都可以非常好的利用多核心处理器的性能，更多的核心就能起到更多的作用
- 另外，CPU [X86架构] 经过多年的发展，如今想要大幅度提升单核性能已经很难了，所以英特尔和[AMD] 都在想方设法增加CPU核心数量，通过多核协同工作来提升CPU性能，未来还会有越来越多的软件和游戏对多核处理器进行优化，所谓的“一核有难，八核围观”的尴尬场景会越来越少

- 强计算生产力用途包括3D渲染，影视制作渲染，CG动画，有限元分析CFD，[Fluent]，SAS，Python，量化交易，[股票量化策略]，回测，高频交易，深度学习和机器学习，自动标记，训练，推理，平面美工，游戏程序开发，能赚钱的游戏直播主播，游戏工作室等等

- 工作站，服务器，多开虚拟机。

- 编译 ~ 比如chromium，虚幻引擎 100核也不多…

- ## [高端显卡买哪个 4090-48g vs 5090-32g - 小红书](https://www.xiaohongshu.com/explore/685fdabf000000002400c5fb?xsec_token=ABXvcJCtj5SgpnxaAuvQTSjGIPWgAdou5ozGSko5ifxSI=&xsec_source=pc_search&source=web_explore_feed)
  - 48GB版本4090是涡轮版魔改的，性能比4090主机版低20%性能。并且被改的卡不一定是一手卡，也没有官方质保。店家会保一年。
  - 5090目前的适配并不完善，要求新驱动。对少数深度学习项目有兼容性问题。

- 我其实不太理解为啥5090才给32g
  - 刀法，给多了影响算力卡出售。后续的卡可能会卖不动。
  - GTX定位不是跑模型，按最初的设想，算力卡才是跑模型的。这样就能通过Nvlink技术共享显存。
- 人家定位就是游戏显卡，什么游戏能用爆32G显存？跑推理训练去买pro6000啊

- 5090说不定能改64GB，说不定之后会有人改
  - 很难，没有bios

- 目前用 8 张 4090-48 挺稳定的
- 涡轮卡吗？
  - 我听说改48GB的基本上是涡轮版，因为这样利润最大。
  - 不但是利润，涡轮卡是双槽，一个机箱内可以塞更多的卡

- 48GB魔改版可以多张卡叠加显存嘛
  - 可以张量并行部署更大的模型
- 稳定的话应该没问题，组里有十几张都没啥问题
- 不能，多卡叠加显存必须去买老黄的专业ai计算卡

- 搞RTX pro 6000，一张就96G显存了
  - 8万多可以搞四张了
  - 现在价格6w5左右

- 还没买，我比较保守，怕48GB的有问题，倾向于5090。
  - 存在兼容性差的问题，同课题组有人买了5070，他说要求更版本的pytorch（因为支持的cuda版本必须高），所以低版本的pytorch项目跑不起来。
  - 12.8等高版本的pytorch不支持很多项目项目的

- 打游戏就老老实实买5090，ai都是伪需求，没几个用得上的，真正有需求的会去买ai卡
  - 跑模型为啥要买Geforce，买telsa的H200，H100不香么，再次点Quadro这种专业卡都有Pro 6000，跑ai完全吊打5090，真的有需求的人买不起也会去租的

- 5090配环境还是麻烦，干活的现在用4090一年后再考虑5090

- ## [8K配的工作站加4090 48G 成功跑起Deepseek - 小红书 _202504](https://www.xiaohongshu.com/explore/67fa372d000000001e00ba5a?xsec_token=ABufSAS9QghohUw1fzyX0iVOG5KQLPpvxE2fkT2okUTlk=&xsec_source=pc_search&source=web_explore_feed)
  - 运行的是ikawrakow/ik_llama.cpp/这个库，他用 AVX2 指令集做了许多优化
  - 运行的是 Q2 的版本的 deepseek v3，占用内存 222G，显存 20G，所以普通 4090 也能跑起来
  - 输入 41 t/s, 输出 8 t/s
  - 我用的是联想 P620准系统+256 内存 8K 多点的价格
  - 这个 P620 准系统也不差 5945ws 线程撕裂者 联想锁机 U 可以跑到 4.5G 十分便宜 5 6 百左右吧
  - PCIE 插槽多 128通道，每个插槽都可以拆分，以后扩展比较方便
  - 8 通道 DDR4 3200内存，不比 DDR5 慢，一条 32G 200 左右
  - 唯一缺点，主板的1000w 电源没法更大了，主板插槽也不支持两个高功率显卡，所以想在这个系统上插双 4090 是没戏，但可以插 2 个 A6000。
  - 同价位的 PC 是完全没法比的。虽然我这些都是二手，但几百块的 cpu 就是坏了换一个也没啥心疼
  - 我调查过其他的工作站服务器，epyc 志强，我觉得都不如线程撕裂者，平时当 AI 服务器，偶尔还可以剪剪片子性能也很好。

- 我买的带cpu的准系统，这个cpu锁联想的主板，所以便宜

- 问下这个机器的电源怎么接，机器本身电源只有两根显卡电源接口的线，4090至少要三根
  - 两根8pin转 4090 16pin的就可以了

- 这种魔改的显存官方驱动支持吗
  - 支持

- ## [我的RTX 4090 48G深度体验报告 - 小红书 _202503](https://www.xiaohongshu.com/explore/67ca95ca000000000d015047?xsec_token=ABqsyDIYHWnZyXfQUxpaLc8ZkBUCid6Z7dPfToCAZBfTA=&xsec_source=pc_search&source=web_explore_feed)
  - 用4K OLED电视测了《赛博朋克：往日之影》，路径光追全开+DLSS 3.5插帧，画面毛发反光真实到离谱…帧数居然稳在98！以前3080直接掉到40的酒吧霓虹灯场景，现在丝滑到想哭
  - 用Blender渲染公司新项目场景，32GB显存直接吃满！如果换成老显卡估计要崩…但4090 48G居然能边渲染边开着AE做特效预览，这才是真正的“生产力解放”
  - 偷偷试了本地部署70亿参数AI模型，48G显存跑图+训练同时进行
  - 电源建议1000W金牌起步！我旧电源带不动疯狂闪退
  - 机箱尺寸必须量好，这张卡比iPhone 15还长2cm
  - 非刚需慎入！除非你是8K剪辑/AI训练/富哥

- 适合笔记本吗？
  - 这是桌面级核弹！笔记本请认准4090移动版（完全不是一个东西）

- 跑游戏和普通4090无任何区别。甚至在没爆显存的基础上，跑AI也和普通4090速度是一样的，不要幻想有了48G就会更快。需要指出的是，跑70b模型确实能发挥48G的优势，不再是一个字一个字蹦。最后，涡轮风扇噪音极大，部分用户可能无法接受。

- ## [同预算，我发现了比4090 48g更优的卡 - 小红书](https://www.xiaohongshu.com/explore/68933df200000000250232c0?xsec_token=ABJcKpAiG7_wvbIBshcL_hTstuY3ZuJRr7DRybbzYqQno=&xsec_source=pc_search&source=web_explore_feed)
  - 公布结果：5880 ada 48g显卡，
  - 按照nvidia官方发布的datasheet，算力差距在20%。毕竟这个价位。考虑稳定性，和大显存，这卡还是比较好的选择
- a6000为啥比5880ada还贵？
  - a6000有两个版本，一个是a6000 一个是6000ada，你说的贵的，是ada版本，属于是4090一代产品，5880是6000ada的阉割版

- 4090的显存bandwidth太低了
  - 小作坊感觉也够用了，毕竟老师们的经费也不见得能买h100这类。

- 这个价格我只能想到是L20了，差不多的价格，显存比4090 48G大，关键是可以上NVLink联合显存，性价比真的高。

- ## [4090 48G真爽，给女儿的人工智能实验成功 - 小红书](https://www.xiaohongshu.com/explore/67de13d1000000000b0151d1?xsec_token=ABYgeab4C5E3b9gpHBDeBL8cszeX4XJpIaX3RvHVudVRc=&xsec_source=pc_search&source=web_explore_feed)
  - 很多朋友关心这块4090 48g。我的配置也说一下，我是联想P620 5945w的线程撕裂者加256内存，这一套系统大概8K，比起PC级的配置，经济实惠，主要是PCIE扩展可以有很多，内存通道也多，内存便宜。
  - 跑大模型，32b并发16能到400多t, 训练时，满精度只能跑1.5b, 如果是lora就无所谓了，32b也可以跑。

- 小模型参数不够推不出来怎么办？感觉有好多不确定性
  - 只要想办法让它通过思考输出和agent任务相关的token, 执行任务成功率就会很高，以前靠长cot是没法在小模型做的，通过RL训练解决了这个任务，如果只训练特定领域成本也不高
- 个人认为小模型推不出来反而是确定性太强，过于依赖提示词，用起来语言模型像文生图似的
  - 其实你这样想，学会从大量工具中挑选对的，以及写对参数，这些是不需要大量知识的，只需要输出符合逻辑的token作为上下文，它就能完成任务。RL+GRPO给了一种解决小模型提升干活能力的思路

- ## [如何评价 Framework 笔记本？ - 知乎](https://www.zhihu.com/question/475249794/answers/updated)
- 喜欢接口多惊鸿14就有3A2C，HKC，机械革命这些才是真的懂平民产品该什么样子的厂家。
  - framework? 产量和销量上不去，又没有日系的溢价的情况下根本没法做好品控，那就只能根据现有的模具改一改。

- 要模块化没问题，首先先把显卡可换，内存，网卡，硬盘，CPU，全部可换，你得去说服intel让它别改针脚，不然你这改个C口就模块化了？
  - 是啊，对于真的用笔记本的人来说，模块化应该是像00-10年代主流笔记本一样，起码能自行更换cpu吧。framework的伪模块化只是对外接口模块化罢了。

- framework要做的该是笔记本的pc架构标准，这不是一个公司可以做到的，而且还需要时间去拓展硬件生态

- cpu、显卡与主板绑定，更换直接换整个主板，所谓的模块化显卡就是接显卡坞之前还得再加个模块转换接口么？？大概了解了一下这个产品的现有信息，我只能说，眼前一黑。为了模块化而模块化的产品，请不要打diy的幌子噶韭菜了。

- 总结一下上述回答：轻薄和模块化是不可兼得的鱼和熊掌。目前很难在完全轻薄和最大程度模块化中间找到一个能让很多人满意的“度”。

- ## 🛝 [有没有前期将就用后期可以升级扩展到顶级的电脑配置？ - 知乎](https://www.zhihu.com/question/15227206505)
- 你想有钱了再搞扩展，最关键的就是主板一定要买好的，大板是必须的，而且是近期出的
  - 如果主板买的老、旧、丐版，那后期没法升级，只能全换
  - 其他CPU、显卡、内存、硬盘、电源都可以升级

- 不存在。我现在的配置是2070S，想升级，计划保留电源，硬盘和内存。结果发现ddr5的物理接口都改了，想要释放新显卡，就必须连内存一起换了。
  - 所以你要是用久了再升级，会发现当前零件全过时了。而短时间之后就升到顶配，那不如再等等直接入手顶配，毕竟电子产品，二手价格是腰斩的。
- 一般电脑升级，就是除了硬盘，电源可以保留，其他全换。

- 先买一个联想的P620准系统，它自带了机箱、80PLUS铂金电源、主板、CPU散热器以及内存散热器，这个是最贵的，大概是5000元左右，其他的我们都买丐中丐
  - CPU先用一颗最低端的线程撕裂者5945WX，联想锁的大概700-800元，区区12核24线程的低端CPU
  - 内存先简单来4条32G的 [三星DDR4 2R] x4 2666MHz的ECC内存，凑一个128G的内存容量，大概700元左右
  - 硬盘如果囊中羞涩，可以先来一根[西数的SN7100] 1T过渡一下，不过服务器主板有非常丰富的PCIE插槽，直接插PCIE固态或者用U2固态也是非常好的选择，如果需要素材比较多可以考虑上一个机械硬盘
  - 显卡我们可以挑一个亮机卡，比如[P1000]，这就只要200元了，UG、CAD并不是特别吃显卡的软件，买P1000这种低端的专业图形卡即可
  - 这样子一台简简单单的建模工作站就OK了
  - 日后想升级很简单，CPU换成5995WX，这是64核128线程的真 线程撕裂者，内存简简单单扩展到256G，硬盘往死里加就OK
- 这机箱真漂亮。玩装机，玩到最后就是机箱。

- 这种准系统工作站一个很容易忽略的问题是就是显卡无法安装游戏的显卡，机箱空间有限装不了，而且这种机箱设计本身就是面对工作站方向设计的，而且只能装那种工作站专用的涡轮显卡，那个声音很酸爽有够你受的。
  - 后来我用外接显卡方式解决了游戏显卡不能装工作站的问题，实现了能用大内存又能用高性能游戏显卡。
  - 现在游戏显卡基本上都是设计很高，而工作站对显卡高度是有限制的，里面空间也很小也不利于散热，所以如果可以重新选择我以后会倾向于用游戏主机来做设计。

- 这问题不是就给AM5定制的吗？
  - 主板：B850M带WiFi，1200左右，考虑后期升级一定要弄块好点的板子，要不然升级供电带不起来。
  - CPU：凑合用9600X或者7500F，前者1200后者800，未来可以升9800X3D或者9950X3D，游戏生产力都可以兼顾。或者搞块8500G用核显，连亮机卡都可以省。
  - 内存：16G×2 6000MHz或者6400Mhz，AMD不用上高频内存，6000频率同频用还省钱，后期ddr5普及之后可以升级32G×2 8000MHz分频用。
  - 显卡：凑合用随便淘张亮机卡就行，升级主板支持pcie5.0，别说5090，将来6090都能跑的满。
  - 硬盘：先买块4.0的固态用着，将来可以换pcie5.0的固态。
  - 电源：二手比较划算300块可以淘到海韵750W，大牌稳定可靠还安静，升级显卡也没什么瓶颈，不带90级别显卡都没什么压力。
  - 机箱：正常尺寸机箱，看个人喜好。

- ## [迄今为止，你用过的最好用的数码产品是什么？夸一夸? - 知乎](https://www.zhihu.com/question/14769217934)
- 联想P620工作站，最便宜的线撕工作站，同时各种拓展给够，有万兆、雷电、u2、nvme，还有四槽想咋拆分就咋拆分的PCIe4.0x16，也不会像t7960那样各种不认盘，继承老板们的锁平台线撕CPU还能再省一比马内，
  - 除了上不了机架和电源瓦数偏低之外没有任何缺点，当然你放到机柜最底下的托盘也不是不行
  - 想起来没得bmc用，这确实是最大的槽点，不过要是拿来当homelab的话那其实用不太到，不如小米智能插座

- [服务器里的基版管理控制器（BMC）是哪个？ - 知乎](https://www.zhihu.com/question/54716507)
  - BMC是一个独立于服务器系统的小型操作系统，作用是方便服务器远程管理、监控、安装、重启等操作。BMC接通电源即启动运行，由于独立于业务程序不受影响，避免了因死机或者重新安装系统而进入机房。
  - BMC只是一个集成在主板上的芯片（也有通过PCIE等各种形式插在主板上），对外表现形式只有一个标准RJ45网口，拥有独立IP。普通维护只需使用浏览器访问IP: PORT登录管理页面，服务器集群一般使用BMC指令进行大规模无人值守操作。
  - 一般服务器BMC网口是独立的，仔细看印有BMC字样。但是也有小型服务器BMC网口和通信网口是二合一的。
  - 当然也有不叫BMC的，只要遵守IPMI协议，都是类似的。

- ## [漫步者G1500BRA音响测评 - 小红书](https://www.xiaohongshu.com/explore/66d12cf8000000001f038e03?xsec_token=AB38kMIt8DJB5wbhI53QOtqHIYVOHf_3EnOAptaC3-TWM=&xsec_source=pc_search&source=web_search_result_notes)
  - 性价比高、7.1环绕的音效、有线输入和插电是USB二合一、内置声卡可调节等
  - 整体非常简约，包装里有音响本体、说明书、品牌贴纸、可插拔麦克风，线是一直在上面的
  - 虽然有两种连接方式，蓝牙和有线，但它还是比较倾向于有线，除非你手机或者其他不能插USB的设备使用，可以插在插座上，开启蓝牙模式，但我买回来主要是给电脑用
  - 如果是电脑有线使用，还可以去官网下载声卡软件，声卡软件里的功能非常多，虽然它默认的音效已经很完美了哈哈，但是可以自定义音效，适合不同人的需求
  - 7.1环绕音我一开始是不了解的，直到我用了这款音响以后，声音就像包裹了你的耳朵一样，非常有沉浸感
  - 音响有三种模式，音乐模式就是低音会强一些，游戏模式延迟低，声音大，电影模式声音要小一些 沉浸感强
  - 麦克风离远点就差劲了，放在显示器下面，离嘴巴太远，别买了

- 不适配ps5真的很无语
  - 但是他适配switch

- 感觉我的外放声音好小
  - 游戏模式会大一些

- 开关机的声音只需要按4 G 和5 音量加 两秒就能关掉了

- ## [为什么现在的音响大都是蓝牙音响？ - 知乎](https://www.zhihu.com/question/658012042)
- 一般好点的那些，都叫《音箱》，而叫《音响》的，都是只有玩得不多的人才会搜索的。

- 几乎所有的有源音箱都是有有线连接的，一般标注为：AUX接口，

- 一个新选择——Wi-Fi音箱。传输音频的方式，只能用蓝牙吗？是不是有其他方式？唉，那就对了！“Wi-Fi就是其一。”Wi-Fi音箱其实已经推出市场很久，只是在国内少有人知道。

- [为什么有线音箱少了？ - 知乎](https://www.zhihu.com/question/514579467)】
  - 没有少，只是有线音响（一般指有源音箱）。好点的都带蓝牙功能，方便手机平板无线连接

- ## [联想推出扬天 M4000q 台式机，i5-12400 + 16GB 内存，该款产品是否值得入手？ - 知乎](https://www.zhihu.com/question/518095107)
- DIY玩家来说肯定是看不上的，没有3080，没有16个PCIe通道。但是你把他看作一个办公主机，我觉得是非常香的。这个办公机不比那些采购用的臭鱼烂虾什么6代i5良心多了。
  - 配置上看起来和联想的家用天逸510s 比较类似，也都是7L小机箱

- ## [从笔记本转用台式机屏幕分辨率都是2k，但从16寸到27寸按理说会变糊，不会不适应吗？ - 知乎](https://www.zhihu.com/question/662562760)
- 不会，因为台式机和笔记本有个最大的区别，就是视距。
  - 笔记本是没办法的，你手是要放到键盘上的，视距几乎和臂长差不多，必须离近了看，所以ppi不够的话，颗粒感就会让人很难受
  - 台式机分辨率2k就已经够用了，只要你不是近视眼贴着屏幕看，4k和2k体验差距并不大

- 为了能让27寸显示器看起来能有一个舒适的视野范围，题主自己，不由自主地就会在更远些位置来使用
  - 无论是16寸还是27寸，为了得到更好地视觉体验，眼睛与屏幕的距离是不同的

- ## [惠普战99台式机值得买不？ - 知乎](https://www.zhihu.com/question/588726682)
- 这是有史以来性价比最高的品牌机了这个机器我也用了4个月了，给南桥、固态硬盘加了散热器和添加同型号内存条，改装电源并扩展了一张AMD W6400显卡，解决了主板过热问题，在2k环境下流畅运行。所以只能说，这个机器对得起这个价格。

- 我买的战99款式是14500集显款，也拆机研究了内部，我来说一下他几个缺点吧：
  - 1. 主机噪音过大
  - 2. 内存条是单通道 我因为是核显，想追求双通道加了600元从16g升级到32g, 其实这本身已经溢价了，但没想到这不是两根16g, 而是一根32g
  - 3. 开机按钮太细 因为按钮是窝在机器里，靠指腹根本按不下去，得用指甲掐。很不方便。

- DDR5 单根也是双通道

- ## [现在有政府20%补贴，同样配置买台式机整机是否比自己散装要更划算呢？ - 知乎](https://www.zhihu.com/question/666757762)
- 打八折后整机确实有一定优势，尤其是懒得动手或者不太懂DIY的用户，本来DIY还有价格优势，现在这波八折价格优势几乎被抹平了。
- 看了一圈电脑的，补贴主要针对的品牌整机笔记本一体机这类，单独配件不能单独补贴吧，也就是好像不能自己攒机器领补贴吧

- 即使是有20%政府补贴后依旧没看到性价比很高的整机，如果有，麻烦踢我一脚。

- 想买惠普暗影精灵10+显示器套装14代i5+4060ti+27寸2k显示器打折后6500这个价位，有点买主机送显示器的感觉

- 我也看了，确实没有。而且补贴的都是高价整机，他那个价格，你能自己挑配置攒一个更好的。

- 你如果去京东淘宝买那几个销量最高的主机应该是比自己组装更划算的，哪怕不需要补贴

- ## [为什么台式 PC 还处在组装（DIY）阶段？ - 知乎](https://www.zhihu.com/question/1899923881755678262)
- PC平台的DIY优势，使得它的每一个细节都专门化、专业化，因为它开放

- 我觉得所有消费电子产品都应该模块化, PC是唯一走在前面的

- 所谓的台式机DIY，说难听点，不过是一块大板各种插，再拧进一个足够大的箱子里。
  - 我的观点很简单，手机发展不出DIY市场，本质上是因为普通人都是手残，厂商能放心把一堆小东西扔给你自己装？

- 
- 
- 

# discuss-pc-amd/apu
- ## 

- ## 

- ## [AMD重启多显卡支持！最多四块、192GB显存 - 小红书 _202406](https://www.xiaohongshu.com/explore/6675185b000000001c025603?xsec_token=ABmlt9aXNo6GSeaJUrfgo6BtVy5GhYovAzw9hCuf6rrE0=&xsec_source=pc_search&source=unknown)
  - AMD最新发布的ROCm 6.1.3开发套件就支持在单个系统中配置多张GPU卡。
  - ROCm 6.1.3在特定RDNA显卡上支持TensorFlow，初步支持通过Windows WSL子系统运行ROCm。
  - AMD重启多显卡支持！最多四块、192GB显存, 目前支持多卡并行的型号仅限RDNA3 Navi31核心的高端系列，
  - 具体包括：RX 7900 XTX、RX 7900 XT、RX 7900 GRE、PRO W7900(双插槽)、PRO W7900、PRO W7800。
  - 其中，7900 XTX、PRO W7900可以双卡并行，首次正式支持的PRO W7900(双插槽)可以最多四卡并行。
  - 以上所有都仅限Ubuntu 22.04.3 HWE操作系统，需要搭配Linux 24.10.3版显卡驱动

- ## [how’s inference looking now in AMD GPUs? I don’t have one so that’s why asking here. : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1nw9tny/hi_hows_inference_looking_now_in_amd_gpus_i_dont/)
- if you can do with a single AMD AI 395 128GB do it, that's the dirty cheapest solution.
  - Also there is the option to run 2 different models on 2 x 395s and used a 3rd machine for AI Agent hooked to those local LLMs. Like a AMD 370. This whole setup consumes less energy than a single 3090.
  - The idea is extremely simple. You run the LLM/LLMs you want on the 395 and connect the Agent 0 from the third machine. Because A0 allows for multiple LLMs to be connected to, either local or remote eg ChatGPT, can even run a model for some of the quick menial work on the 780M of the second machine hooked to A0.

- How does R9700 compare to W7900? Gigabyte offers both as part of their AI TOP lineup, you're supposed to be able to slot those in your PC and do local training, but I see W7900 has 48G VRAM compared to R9700's 32G
  - R9700 is wayyyyy faster on Inference than the W7900.

- I only have AMD GPUs. They work great with llama.cpp/Vulkan.

- Other people have already pointed out the 8x MI50 approach, so I won't repeat what they have said, but will point out that that puts out 2, 400 watts of heat under full load.
  - If you don't mind paying a lot more up front, but cutting your power draw and heat output in half, and getting better performance, you could pick up 4x MI210 instead. These are going for about $4, 500 on eBay, have 64GB instead of 32GB, and support a wider variety of FP/BF/INT types than MI50.
  - The 8x MI50 seems like the more affordable option, but my own homelab is running up against power and cooling limitations, which complicates the math.

- 8 x Mi50 will get you to 256GB vram. They're not 3090s and you need to buy fan shrouds for them on eBay but they're fine for what they are and quite cheap.

- prompt processing will be a lot slower than your nvidia counterparts, but token-gen is pretty damn close to what you'd expect given memory bandwidth.

- 8 mi50 32gb or 8 v620 or 8 mi100, in order of cost and perf

- connecting more than two 3090 is tricky, you need motherboard with multiple PCIE slots

- [Whats your PC tech spec? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1nxny65/whats_your_pc_tech_spec/)
# discuss-os-linux/win/macos
- ## 

- ## 

- ## 

- ## 💡 [时至今日 win本阵营有能对标MacBook的产品嘛? - 知乎](https://www.zhihu.com/question/630359957)
- MacBook在换用了自家的M系列芯片之后，能耗比带来的续航问题完全不是一个量级的。
  - Windows本只能主打通用性，开放的系统环境和MacBook的封闭系统上不一样。
  - MacBook的工艺和品控也是最强的，CNC一体成型的工艺、苹果高成本转轴方案，大多数Windows本都无法匹敌。

- 幻 13，对标 MacBook air。屏幕和MacBook air同样尺寸同样画质，而且是视网膜高刷屏，强mac一手，续航大概有 Mac的80%，13性寸给了 75w大电池。
  - 插电后开启独显直连，性能强 mac 一大截，富哥们还可以插 4090 显卡坞，办公本瞬间变成台式机性能。
  - 槽点就是液金偏移和 16g 内存，不过不要保修的话 16g 内存可以 tb 升级到 32，或者买海外 4070款。
  - 性能，屏幕，续航三者都考虑的话，这款机子同尺寸没有对手，如果考虑触屏的话只有这一款可以考虑。

- 实质上是对标苹果的是小米，4000块的笔记本电脑都用CNC顶级工艺。。。只不过市场表现比较惨淡。
- 还有一些对标的，出师未捷身先死。例如ThinkPad X1 Nano，2024年就要没有了。戴尔XPS系列也一直试图对标，但是相去甚远，几乎没有对得上。

- 小米的还是做不到那么薄，毕竟是组装，cpu主板什么的不像mac的

- 华为有几款，粗看还可以，但是翻过来看到底部满是散热孔，感觉就不怎么舒服了。

- Microsoft Surface Laptop：这本来应该能成为像MacBook一样软硬结合的典范，无奈Windows Phone不给力早早被放弃，这款当初的Windows轻薄本标杆由于5年没换模具，现在已经相当落伍了。

- 因为利润在cpu和系统，做硬件的就那么多利润，外观好自然别的地方就得省

- 个人认为，相比于Windows笔记本，MacBook最好用的地方，在于触控板的体验。截至目前，应该还没有Windows笔记本的触控板能达到MacBook触控板的体验。

- 雷蛇啊，灵刃15，江湖人送措号黑苹果。

- 
- 
- 

- ## [国产系统大致比较和分析（优麒麟、开放麒麟、深度deepin、统信UOS、银河麒麟、中标麒麟） - 知乎](https://zhuanlan.zhihu.com/p/661508663)

### 桌面级系统

- 优麒麟是 Ubuntu 官方衍生版，得到来自Debian、Ubuntu、LUPA及各地Linux用户组等国内外众多社区爱好者的广泛参与和热情支持。
  - 针对中国市场加入大量本地优化功能。比如支持中文输入法、农历、天气插件
  - 优麒麟的桌面环境采用UKUI软件。UKUI 是一款轻量级的 Linux 桌面环境，基于 GTK 和 QT 进行开发。

- openKylin开放麒麟是中国首个桌面操作系统开发者平台由国家工业安全发展研究中心等单位联合成立，将打造具有自主创新技术的开源桌面操作系统，定位于桌面操作系统根社区 。

- 开放麒麟和银河麒麟桌面操作系统的关系类似于深度deepin和统信UOS。前者为根社区版本，后者是上游商业版本。社区版本更注重功能也更激进，商业版本则更注重稳定。

- deepin是由武汉深之度科技有限公司在Debian基础上开发的Linux操作系统，其前身是Hiweed Linux操作系统。
  - deepin操作系统内部集成了DDE（Deepin Desktop Environment）深度桌面环境），并支持deepin store、deepin Music、deepin Movie等第一方应用软件，定位于桌面操作系统根社区 。

- 统信UOS 由深度（deepin）为基础，经过定制而来的产品。
  - UOS 拥有 家庭版、专业版、服务器版 三个分支。

### 服务器级系统

- 银河麒麟高级服务器操作系统V10是针对企业级关键业务，适应虚拟化、云计算、大数据、工业互联网时代对主机系统可靠性、安全性、性能、扩展性和实时性等需求
  - 银河麒麟高级服务器操作系统汲取最新的云和容器开源技术，融合云计算、大数据、人工智能技术，助力企业上云
  - 产品同源支持飞腾、鲲鹏、龙芯、申威、海光、兆芯等自主平台，并针对不同平台在内核层优化增强。

- 基于RHEL系统开发

- 中标麒麟高级服务器操作系统软件V7.0是在多年Linux研制经验基础上，适应虚拟化、云计算、大数据，满足业务对性能、扩展性、安全等要求

### 👥

- 官方主要是麒麟。deepin原来是民间的，但是反而口碑最好，所以就转正，商业化成uos了。要是说开放性和前途，我站uos

- 还有连接打印机不好用

- 中标和银河还属于不同公司，中标一直是红帽系的，银河最早以FreeBSD为主的四核融合，后来难度太大，便倒向了红帽系。
  - 深度则是从一开始就是基于ubuntu，后来改成了基于Debian，再后来阿里、华为分庭抗礼，就又搞出来a版和e版，但主力开发的还是d版
- 其实deepin一开始就是基于Debian不稳定版，后来改为了基于ubuntu，再后来就改成了基于Debian稳定版，现在又改成了直接基于linux内核但是，由于过于激进，于是在玲珑生态起来之前，也兼容了Debian生态，类似于鸿蒙现在的路线。鸿蒙也是在自家生态起来之前先兼容安卓生态

- 真是扯淡，一个硬件厂家一套固件一套软件。你让开发者怎么做适配，

- ## [统信UOS何时被鸿蒙HarmonyOS替代？ - 知乎](https://www.zhihu.com/question/644762431)
- 统信UOS可以说是目前最好的信创操作系统，适配x86、arm、loongarch、+sw等指令集的国产CPU，包括海光、兆芯、鲲鹏、飞腾、龙芯、申威等 

- openHarmony和Harmony NEXT的生态是互通的吧；NEXT本身就基于openHarmony；
  - UOS已经发布了基于openHarmony的版本，你可以理解为UOS就是一套类似MIUI的东西，底层原来是Linux，现在换成openHarmony，

- 鸿蒙PC版和手机版生态是互通的，这意味着随着手机版的发行推广，未来推出的PC版一出生就有海量的应用，这种想象空间是巨大的

- 你们上次也是这么说openeuler的，现在快一统国内市场了

- 不需要替代，uos服务器版本来就有基于openeuler的版本，uos桌面版弄个openharmony版就行了。

- 本来谷歌华为是打算一块搞fuchsia的，目标是把chrome os和安卓全换这玩意，然后众所周知的原因fuchsia黄了，华为单干的就是现在的鸿蒙了

- ## [如何评价国产统信UOS系统？ - 知乎](https://www.zhihu.com/question/594635253)
- 统信这破系统完美地放弃了Linux类似物的所有优点，学习了封闭式系统的所有缺点：一台机器的一年授权费几百块，激活码过期后专门禁用微信和wps，不能更新下载软件，微信wps官网的linux版本安装包全部不能用，第三方安装包通通报错不兼容，只能从自带商店下载应用。要说卡脖子还是自己人最在行

- 从开发者的角度来看，这系统非常逆天。
  - 首先UOS专业版是收费系统，700元，不同架构的价格还不一样，不确定是否通用。如果不购买授权，试用期3个月，过了试用期就不能再更新和安装软件了。过了试用期不给root权限。

- 听说家庭版不用花钱，但是不开放下载，需要先提交申请。还有就是家庭版系统得绑定UOS账户，1个账户最多激活5台机器，多了就得解绑

- UOS把自己的软件生态搞得如此封闭，对于不太懂计算机的用户来说，基本上只能从他自家的应用商店安装应用软件，简直比苹果还霸道。

- 中标麒麟已经被银河麒麟合并了，现在主要是银河麒麟与统信两家。

- ## [银河麒麟和统信哪个好? - 知乎](https://www.zhihu.com/question/581675808)
- 银河麒麟不激活可以无限制使用全部功能，只是没有技术支持。
  - UOS不激活不能访问软件源，不能启用开发者模式。
- 银河麒麟跨SP的大版本升级需要收取额外的费用才能激活，UOS似乎不需要。

- 银河麒麟软件安装和权限管理和其他deb系Linux比较一致。
  - UOS权限过于封闭，不启用开发者模式不能获取root权限，不允许安装非商店包，且软件源的包使用deepin-elf-verify包阻止在其他Debian系系统上安装。而且对root权限的限制不仅针对用户也针对开发者。程序不能直接用sudo请求输入root密码提权而需要通过Dbus授权。

- 统信是给纯小白用的，怕给玩儿坏了，所以不给root权限，跟安卓一样。开发者大概也不会放着deepin不用去用统信。

- UOS对打印机驱动的支持比麒麟完善，因为它自己做了对Windows的打印驱动支持。

- 我司信创软件售后95%25以上问题的都是统信环境出的，甚至需要单独出包给统信签名，麒麟基本没啥售后除非sudoers文件被故意改过 

- 从开放性上来说，UOS统信更好一点，原因是UOS使用的DDE桌面开放性更好，提炼出了一套DTK套件，基于QT，这点感觉有发展前途，也是一个很好的思路，希望在这条开源的道路上更进一步，形成自己的特色。
  - 银河麒麟不知道怎么说，也有自己的桌面UKUI，但是个人感觉开放性不好，藏着掖着，只能让人觉得做的不好！

- 银河面向服务器，统信面向办公，面向家庭娱乐，各司其职，多好。

- ## 🆚 [带您了解星光麒麟和银河麒麟的差别 - 知乎](https://zhuanlan.zhihu.com/p/14101203674)
- 星光麒麟采用微内核设计，通过将非核心功能移到内核之外，大大提升了系统的安全性，在面临外部恶意攻击时，微内核的攻击面更小，能够有效抵御病毒和黑客的入侵。
  - 主要面向科研与超算领域，能够精准地处理复杂的数据模型，但其局限性在于对消费级硬件的适配性较差，普通电脑可能无法发挥其优势，且在软件生态支持上相对薄弱。

- 银河麒麟系统
  - 拥有强大的多任务处理能力，图形界面友好，操作便捷，对于普通用户和专业人员都能提供良好的使用体验，
  - 同时，对多种网络协议有着广泛的支持，保障了在不同网络环境下的稳定通信。
  - 多层次的安全访问控制机制，严格限制不同用户的权限，保障系统安全稳定运行。
  - 星光麒麟的软件生态相对小众，集中于科研等特定领域，银河麒麟则在民用和工业等多领域构建了较为丰富的软件生态，与众多第三方软件有良好的兼容性。
# discuss-os-arm
- ## 

- ## 

- ## 

- ## [2024年 arm windows 用来写代码能胜任吗? - 知乎](https://www.zhihu.com/question/725977971)
- NodeJS、Python、. NET 和 OpenJDK 都有 ARM 版，VS 之类 IDE 也有，但你要注意：
  - 很多插件可能没有 ARM 版。
  - 你的项目可能会有奇怪的 native 依赖，不过前端工具的话应该还好？
  - 如果你涉及到 Native 调试，那 ARM 处理器不少硬件调试功能是比不过 x86 的，比如内存断点就很残废。
  - ARM 的 CI（持续集成）产业还很原始，因为——地球上到现在就没几台 ARM 服务器。Github Actions 的 ARM runner 是今年才上线的，且似乎还收费。

- 跑不了, netty 他对x86有专门优化，在arm上运行一堆bug，只有毕生jdk才把arm的坑踩完了

- 难度比较大。python在winodws跑已经很麻烦了，arm环境更是雪上加霜。

- 这个问题可以关闭了，lunar lake的实机测试已经有了，XElite和Windows on ARM可以进棺材了。

- 2025年6月了，这个问题已经结束了。Lunar Lake就是一个市场的失败品。
  - lunar lake失败是因为被amd打死了，而不是输给了elite

- ARM用在笔记本上不具有任何优势。真想要的话去收个淘汰的飞腾信创笔记本，记得要AMD显卡的，然后自己装Gentoo. 东西都是原生的。

- 如果要写 C/C++ 配合 intrinsics 的话挺麻烦的。主要就是三套编译器 MSVC、GCC、Clang
# discuss
- ## 

- ## 

- ## 

- ## 

- ## 🤔 [国内品牌笔记本电脑近年来市场占有率越来越高，背后有哪些方面的因素？ - 知乎](https://www.zhihu.com/question/663854470)
- 笔记本早就已经是一国两岸的品牌统治全球市场了
  - 澳洲市场甚至都是国产品牌通吃，大陆的联想，对岸的微星华硕，基本把苹果以外的高端吃满了，惠普只能吃点低端市场

- 我想说内地销量排行里HP和Dell常年前五（联想稳定第一），ASUS虽然销量也不小但常年others，MSI这种只做游戏本还死撑溢价的那就是others中的others了
  - 惠普戴尔纯属被企业采购撑起来的，离开企业采购啥也不是

- 每年英特尔和 AMD 的新CPU发布都是周期性的，以前是1年为单位，现在差不多是三个季度为单位。
- 英伟达近来都是2年一个周期，30系，40系，50系显卡。
- 需要在一个发布周期里，尽快地卡在3月开学季，6月618大促，9月开学季，双11，这四个节点，完成研发，调试，制造，备货，渠道出货，同时营销团队也要给力做好宣传，最后赶在双12清库存，准备迎接下一波的新一代全新处理器。
  - 说白了，就一个字，卷。

- 如果是国内市场的话，国外的品牌主要就苹果、惠普和戴尔。国产厂商份额的提升，主要那就与戴尔这两年的操作有关
  - 前两年戴尔突然被自己人“卡脖子”了（不让用中国的元器件还是啥的），于是要逐渐退出中国市场

- 如果有一天，国产笔记本品牌不仅屏幕是京东方、华星光电，硬盘也都是长存，显卡也都是摩尔线程了，整个国产供应链“一条龙服务”，那戴尔、惠普之流还能有什么竞争力可言？

- 华为手机、小米手机在互联网时代的背书，大家对国产电脑的认知也在一点点发生变化，当然产品也确实做的越来越好

- 不光是国内整机品牌的占有率高，而且你看现在买电脑是不是大部分配件都是国产的，内存硬盘屏幕电池基本都是国产了

- 国产笔记本水平现在比国外某些老牌卷得多

- ## 万能的推，目前我的网络结构是这样：（光猫桥接模式）
- https://twitter.com/haozes/status/1728252250114715882
  - 1. 光猫->软路由->小米路由，手机电脑是连小米的 wifi 。但此时设备无法通过 IPV6 连接测试。
  - 2. 我试过，如果去除软路由。仅光猫-小米路由，用小米 PPOP 拨号，设备连小米WIFI 是能通过 ipv6 测试的。
  - 怎么解决方案1下的ipv6  ？
- 已经都解决。现在用软路由直接拨号，小米路由当AP有两个坑：
  1. openclash 插件会有点冲突，得先关掉，测试ipv6的连通性
  2. openclash  取消“ipv6”流量代理，“允许ipv6 dns 解析”两个设置，避免它影响。另外可以用国内站点测试，避免它直接走了clash：
- openwrt啥都有啊，翻墙都是辅助功能。
- 新一点的类似的openwrt无线路由器 接到猫上硬件一站式免折腾。折腾软件就可以了。

- ## 三年前我搬新家，配置了一套精妙无比的米家智能家居生态系统。 星罗棋布的隐藏传感器，让我体验丝滑无缝。 
- https://twitter.com/XDash/status/1690655450927112193
  - 三年过去了，Zigbee 和 Mesh 协议的传感器，也该更换纽扣电池了。 
  - 星罗棋布的隐藏传感器，害我找遍每个地缝。
- 我也差不多三年前弄的全屋智能系统，现在使用率也就20%
- 想想剩下的电改造的钱，三年一换真香。
- 所以要接线
