---
title: tool-iot-devices-gpu
tags: [amd, gpu, intel, nvidia]
created: 2026-01-15T15:32:49.462Z
modified: 2026-01-15T15:33:18.008Z
---

# tool-iot-devices-gpu

# guide

- tips-gpu
  - ä½¿ç”¨åœºæ™¯çš„è¦æ±‚: ğŸ¤” æ–‡å¤šè¿˜æ˜¯å›¾å¤š, æ˜¾å­˜å¤§(VRAM), é€Ÿåº¦å¿«(å¸¦å®½)
    - å¤§æ˜¾å­˜çš„æ–¹æ¡ˆ: mac, amd-ai-max, nvidia, intel-arc
    - Nå¡å‚è€ƒèµ„æ–™å¤šï¼Œèƒ½å‡å°‘åæœŸè°ƒå‚çš„å·¥ä½œé‡
    - 32GB/48GBå¤§æ˜¾å­˜å•å¡èƒ½å‡å°‘è¿ç»´çš„å·¥ä½œé‡, ğŸ‘€ comfyuiä½¿ç”¨å¤šgpuå˜å¤æ‚
  - æ˜¾å­˜ã€å¸¦å®½ã€ä½å®½, æ˜¾å­˜å¤Ÿå¤§æ‰èƒ½è¿è¡Œæ¨¡å‹ï¼Œè¿è¡Œæ¨¡å‹æ—¶çš„é€Ÿåº¦ä¸»è¦è€ƒè™‘å†…å­˜å¸¦å®½
  - ä¼°ç®—æ–‡æœ¬æ¨¡å‹é€Ÿåº¦ç”¨ `å†…å­˜å¸¦å®½å¦‚260GBpSec / æ¨¡å‹å®é™…ä½“ç§¯å¦‚13GB`, è¿˜è¦è€ƒè™‘contextçš„å½±å“
    - MoEæ¨¡å‹å¯¹å†…å­˜å¸¦å®½çš„è¦æ±‚ä¼šä½å¾ˆå¤š, ä¸»æµæ¨¡å‹å¦‚deepseek-v3/Qwen3-235B-A22B/glm-4.5-355b-a32béƒ½æ˜¯moeæ¶æ„
    - æ–‡æœ¬å¤§æ¨¡å‹çš„å…è´¹apiæ›´å®¹æ˜“è·å–
  - â“ æ–‡ç”Ÿå›¾çš„åœºæ™¯æ˜¯å¦ä¹Ÿç”¨æ­¤å…¬å¼è®¡ç®—, 
    - æ³¨æ„æ–‡ç”Ÿå›¾èƒ½é€šè¿‡loraåŠ é€Ÿï¼Œæ‰€ä»¥å†…å­˜å¸¦å®½é‡è¦æ€§é™ä½
    - æ–‡ç”Ÿå›¾æ—¶ç»å¸¸éœ€è¦VLMè§†è§‰æ¨¡å‹è¾…åŠ©ï¼Œæ‰€ä»¥VRAMè¶Šå¤§è¶Šå¥½
    - å¸¸è§æ–‡ç”Ÿå›¾æ¨¡å‹çš„å¤§å°åœ¨20GBå·¦å³ï¼Œå¤§æ˜¾å­˜çš„å•å¡ä¹Ÿå¯è¿è¡Œ
  - è€ƒè™‘è½¯ä»¶æ”¯æŒåº¦, comfyuiåœ¨arm/linuxå¹³å°çš„æ”¯æŒåº¦, dgx-sparké»˜è®¤linux
  - è®¡ç®—é›†ç¾¤: nvlink
  - ä¸»åŠ›å·¥å…·ä¸è¦ç”¨AMDçš„CPU/GPU, å› ä¸ºlinuxéœ€è¦ç‰¹æ®Šé…ç½®, éƒ¨åˆ†è½¯ä»¶ä¹Ÿéœ€è¦ç‰¹æ®Šé…ç½®å¦‚pytorch
  - æ¨¡å‹é€‰æ‹©: æ”¯æŒint4ã€fp8ã€fp4ï¼Œèƒ½å¦ç”¨nanchakuåŠ é€Ÿ, æ”¯æŒflash-attentionã€bf16ã€awqã€sglang

- å¤šæ˜¾å¡
  - å¤šæ˜¾å¡çš„æœºç®±å¤ªå¤§ï¼Œä¸ä¾¿æº, é…ç½®å’Œç»´æŠ¤æˆæœ¬é«˜ï¼Œä¸å¦‚ç”¨ä¸»æµå°å¼æœºç®±æˆ–æŸå®æˆå“å¯å®šåˆ¶å·¥ä½œç«™
  - å¤šæ˜¾å¡/å¤šç¡¬ç›˜éƒ½éœ€è¦å¤§ä½“ç§¯çš„æœºç®±ï¼Œitxä¸€èˆ¬ä¸èƒ½æ»¡è¶³ï¼Œéœ€è¦ä½¿ç”¨matx
  - é…ç½®nvlinkéœ€è¦å•ç‹¬çš„bridgeè¿æ¥çº¿
  - é€‰æ˜¾å¡è¦æ³¨æ„å°ºå¯¸ï¼Œå…¬ç‰ˆä¸€èˆ¬å°ºå¯¸é€‚ä¸­ï¼Œå…¶ä»–åœºæ™¯çš„å®šåˆ¶æ˜¾å¡æœ‰çš„é•¿å®½æ¯”è¾ƒå¤§ï¼Œå¦‚ä¸‰é£æ‰‡å¡ä¼šæ˜æ˜¾å¤§é›¨æ¶¡è½®å¡

- å¯è€ƒè™‘ç”¨å¤§å®¹é‡å•å¡å¦‚4090-48gbé…åˆå°æœºç®±, 
  - ğŸ‘€ å¤§æœºç®±ä¸æ–¹ä¾¿æºå¸¦æˆ–æ”¾è¿›èƒŒåŒ…(åŠŸèƒ½ vs æˆæœ¬), 
    - ğŸ’ ç‰¹å¤§å·çš„åŒè‚©åŒ…çš„é«˜åº¦å¯ä»¥æ»¡è¶³450mm, åº•éƒ¨é•¿å®½éš¾ä»¥æ»¡è¶³ 205x403
  - 4090å…¬ç‰ˆå°ºå¯¸304x137mm, æ¶¡è½®ç‰ˆ267x111x38mm, âš¡ï¸ ä¸‰é£æ‰‡ç‰ˆ350x140x53mm
  - å¯è€ƒè™‘itxæœºç®±åŒ…æ‹¬, æœºæ¢°å¤§å¸ˆc28(18)/cmax(20), é—ªé³g300(17)/g350(20), ä¹”æ€ä¼¯z20(20)
  - æœºç®±æ•£çƒ­è¦æ³¨æ„cpuåŠŸè€—å’Œæ˜¾å¡åŠŸè€—ï¼Œå¯é€‰ç”¨ä½åŠŸè€—cpu+é«˜åŠŸè€—æ˜¾å¡ï¼ŒåŒå¡”é£å†·, å°½é‡é€‰meshç½‘å­”ç‰ˆ
  - æš‚æ—¶é€‰æ‹©æœºæ¢°å¤§å¸ˆcmax(392*185*284mm, 20.5L), å› ä¸ºèƒ½æ”¯æŒè¾ƒé•¿çš„ä¸‰é£æ‰‡æ˜¾å¡ï¼Œæ˜¾å¡æ”¯æŒ 385*160mm ä»¥å†…
    - ä¹”æ€ä¼¯, å…¬å¼€èµ„æ–™æœ€å¤š, t6(13.6L), tk-o(16.45L),c6(18.4L), z20(20.2L)
  - æƒ³è¦128GBçš„å†…å­˜ï¼Œæœºç®±çš„ç©ºé—´å¤Ÿå¤§ä¸”æ»¡è¶³æ•£çƒ­éœ€æ±‚æ˜¯å‰æï¼Œè¿˜éœ€è¦ä¸»æ¿æä¾›4ä¸ªå†…å­˜æ’æ§½ï¼Œcpuçš„å’Œå†…å­˜çš„é¢‘ç‡æ˜¯èƒ½å’Œè°å·¥ä½œï¼Œé¢‘ç‡éƒ½ä¸èƒ½å¤ªé«˜
    - å†…å­˜æ¡çš„é¢‘ç‡è¦è€ƒè™‘cpuæ”¯æŒã€ä¸»æ¿æ”¯æŒ
    - å¤§å†…å­˜å¯¹è·‘MoEæ¨¡å‹æœ‰ç”¨

- mac
  - ultraçš„å†…å­˜å¸¦å®½æœ€é«˜è¾¾åˆ°800, ä½†ä¸è¦æ€¥, amd strix haloçš„ä¸‹ä¸€ä»£å’Œmac studioçš„ä¸‹ä¸€ä»£éƒ½ä¼šå‡çº§, é€‰æ‹©256GBç‰ˆæœ¬åˆé€‚çš„
  - [Performance of llama.cpp on Apple Silicon M-series Â· ggml-org/llama.cpp _202311](https://github.com/ggml-org/llama.cpp/discussions/4167)

- nvidiaæ€§èƒ½å¯¹æ¯”
  - [å¤§æ¨¡å‹GPUç®—åŠ›å¡æ±‡æ€» - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1904206218748236301)
  - [Sable Diffusion WebUI Benchmark Data: nvidia/amd/torch](https://vladmandic.github.io/sd-extension-system-info/pages/benchmark.html)
  - [Which GPU should I buy for ComfyUI Â· comfyanonymous/ComfyUI Wiki](https://github.com/comfyanonymous/ComfyUI/wiki/Which-GPU-should-I-buy-for-ComfyUI)
  - https://www.zhihu.com/question/615946801/answer/3156016610

- amd
  - [ROCm Compatibility matrix](https://rocm.docs.amd.com/en/latest/compatibility/compatibility-matrix.html)
  - [Linux support matrices by ROCm version](https://rocm.docs.amd.com/projects/radeon-ryzen/en/latest/docs/compatibility/compatibilityrad/native_linux/native_linux_compatibility.html)
  - [AMD Strix Halo â€” Backend Benchmarks (Grid View)](https://kyuz0.github.io/amd-strix-halo-toolboxes/)

- ğŸ†šğŸ”¥ [è‹±ä¼Ÿè¾¾çƒ­é—¨ GPU å¯¹æ¯”ï¼šH100ã€A6000ã€L40Sã€A100 - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/5041686924)
  - [Memory Bandwidth Comparisons - Planning Ahead : r/LocalLLaMA _202402](https://www.reddit.com/r/LocalLLaMA/comments/1amepgy/memory_bandwidth_comparisons_planning_ahead/)

- å‚æ•°å¯¹æ¯”
  - gpu-arch: 2020-ampere(a100/a6000), 2022-ada-Lovelace(L20/L40s/6000ada/4090/4090d), 2022-hopper(h100), 2024-blackwell(5090)
  - fp16/tflops
  - 3090æ˜¯æœ€åä¸€ä»£æ”¯æŒnvlinkçš„æ¶ˆè´¹çº§æ˜¾å¡ï¼Œä½ç«¯ä¸“ä¸šçº§æ˜¾å¡å¦‚5880/L20ä¹Ÿä¸æ”¯æŒnvlink

- gpu-specs

- Memory Bandwidth:
  - Nvidia DGX Spark: 273 GB/s
  - AMD AI Max 300: 256GB/s
  - M1(68)/M2/M3: 100 GB/s
  - M4: 120 GB/, 10-cpu, 10-gpu, 16-neural
  - M1/M2/M3 Pro: 150/200 GB/s
  - M1/M2/M3 Max: 300/400 GB/s
  - M4 Max: 546 GB/s
  - M1/M2/M3 Ultra: 819 GB/s
  - RTX 5090: ~1.8 TB/s
  - RTX PRO 6000 Blackwell: ~1.8 TB/s

```markdown
| GPU    | VRAM       | fp16 | v-bandwidth | v-bit | cu-core | power | note                          |
|--------|------------|------|-------------|-------|---------|-------|-------------------------------|
| A100   | 40GB HBM2  | 312  | 2039gb/s    | ?     | ?       | 400W  | 40g-9w                        |
| A6000  | 48GB GDDR6 | 77   | 768gb/s     | ?     | ?       | 300W  | 48g-3w3                       |
| 6000ad | 48GB GDDR? | ?    | 960/s       | ?     | ?       | ?     | 48g-4.8w                      |
| PR6000 | 96GB GDDR7 | ?    | 1792/s      | ?     | ?       | 600W  | 96g-6.6w                      |
| L20    | 48GB GDDR6 | 119  | 854gb/s     | 384   | 1.02w   | 350W  | no-nvlink                     |
| L40    | 48GB GDDR6 | 147  | ?           | ?     | ?       | 350W  | ee                            |
| L40s   | 48GB GDDR6 | 731  | 864gb/s     | ?     | ?       | 350W  | 48g-4.4w                      |
| 5880ad | 48GB GDDR6 | 69   | 960/s       | 384   | 1.28w   | 285w  | 48g-2.8w,no-nvlik,6000Adé˜‰å‰²  |
| 5000ad | 32GB GDDR6 | 65   | 576/s       | 256   | 1.41w   | 250w  | no-nvlink                     |
| 5090   | 32GB GDDR7 | 3352 | 1800gb/s    | 512   | 2.18w   | 450W  | 32g-2.3w, no-nvlink           |
| 4090   | 24GB GDDR6X| 330  | 1008gb/s    | 384   | 1.64w   | 450W  | 48g-2.4wğŸŒ¹, no-nvlink, 850wP  |
| 4090d  | 24GB GDDR6X| 330  | 1008gb/s    | 384   | 1.46w   | 425W  | 48g-1.9w,é¢‘ç‡é”ä¸”ä¸è¶…é¢‘         |
| 3090   | 24GB GDDR6X| ?    | 912gb/s     | 384   | 1.05w   | 350W  | 24g-8.3k, nvlk/VulkRT/OpenGL4 |
| 3090ti | 24GB GDDR6X| ?    | ?gb/s       | 384   | 1.08w   | 750W  | 24g-8.3k, nvlink              |
```

# discuss-stars
- ## 

- ## 

- ## 
# discuss-news
- ## 

- ## 

- ## 
# discuss-amd
- ## 

- ## 

- ## [8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906) : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/)
  - MiniMax-M2.1 AWQ 4bit @ 26.8 tok/s (output) // 3000 tok/s (input of 30k tok) on vllm-gfx906 with MAX context length (196608)
  - GLM 4.7 AWQ 4bit @ 15.6 tok/s (output) // 3000 tok/s (input of 30k tok) on vllm-gfx906 with context length 95000
  - GPUs cost: 880$ for 256GB VRAM (early 2025 prices)
  - Power draw: 280W (idle) / 1200W (inference)
- I'm using ASRock Rack ROMED8-2T (which has 7 PCIe 4.0 ports x16) but there are a lot of other possible solutions (using splitters / risers etc)
  - Yes, but that MOBO itself cost tons of money, near 1K$, while general consumer latest MOBOs cost 200$... So, if you'd summup , the setup is very far from being affordable..

- Have you used PCI-Splitter? If yes, which model exactly?
  - yes, it was random chinese hardware from aliexpress (no known brands):
  - 2x SlimSAS PCIe device adapters
  - 2x SlimSAS cables 8i
  - 1x SlimSAS PCIe host adapter (plugged on the motherboard in the PCIe 4.0 port)
  - (SFF-8654 8i)

- try the --enable-expert-parallel option for vllm, it can speed up output to moe.
  - every time I tried in the past (with glm 4.6 and other models), the speed was lower (-~5/10%) but the VRAM requirement was lower too, so the KV cache and max context length could be higher... I might give another shot for glm 4.7
# discuss-intel
- ## 

- ## 

- ## 
# discuss-nvidia
- ## 

- ## 

- ## 
# discuss
- ## 

- ## 

- ## 

- ## ğŸ’¡ [NVIDIA recently announced significant performance improvements for open-source models on Blackwell GPUs. : r/StableDiffusion _202601](https://www.reddit.com/r/StableDiffusion/comments/1qam2kr/nvidia_recently_announced_significant_performance/)
- NVIDIAâ€™s architecture codenames are deliberately drawn from historical figures associated with mathematics, physics, computing, or engineering. 
- Pascal Named after Blaise Pascal (1623â€“1662), a French mathematician, physicist, and early computer pioneer. Key associations: â€¢ Probability theory â€¢ Fluid mechanics â€¢ One of the first mechanical calculators (the Pascaline)
  - The name fit a generation focused on efficiency, numerical throughput, and a clean jump in performance-per-watt.
- 20xx: Turing Named after Alan Turing (1912â€“1954), foundational figure of computer science. Key associations: â€¢ Formal computation (Turing machine) â€¢ Cryptography (Enigma) â€¢ The conceptual basis of programmable machines
  - This generation introduced specialized hardware for new computational paradigms (RT cores, Tensor cores), which aligns very directly with Turingâ€™s legacy: redefining what â€œcomputationâ€ itself means.
- 30xx: Ampere Named after AndrÃ©-Marie AmpÃ¨re (1775â€“1836), physicist and founder of classical electromagnetism. Key associations: â€¢ Electrodynamics â€¢ Relationship between electricity and magnetism â€¢ The ampere as a unit of current
  - Ampere was about raw current: massive parallelism, high power envelopes, and brute-force throughput, especially for data centers and AI workloads.
- 40xx: Ada Lovelace Named after Ada Lovelace (1815â€“1852), often regarded as the first computer programmer.
  - This generation emphasized softwareâ€“hardware co-design, AI-assisted graphics (DLSS 3, frame generation), and abstraction layers where â€œrenderingâ€ is no longer purely geometric.
- 50xx: Blackwell Named after David Harold Blackwell (1919â€“2010), mathematician and statistician. Key associations: â€¢ Information theory â€¢ Game theory â€¢ Bayesian statistics â€¢ Decision theory
  - Blackwell architectures are framed around AI reasoning, inference efficiency, and statistical decision-making at scale, not just graphics. The name is far more â€œtheoreticalâ€ than previous ones, and that is intentional.
- So yes: these are real people, carefully chosen, and the progression mirrors NVIDIAâ€™s shift from graphics hardware toward general-purpose statistical machines that happen to render images.

- Yes, the nvfp4 model is definitely faster. It's about twice as fast as the fp8, but it's useless because it doesn't support LoRa.
  - It totally supports lora. Tested with Flux .2 and Zimage

- How is the quality of nvfp4 compared to fp8 or gguf?some people here saying nvfp4 quality is trash
  - Yes, there is a difference in quality. Flux2 has a lot of difference, but Zimage doesn't have much difference.
- LTX-2 nvfp4 is quite bad, quality-wise... Tested on a 5090 with torch2.9.1+cu130

- This only for Blackwell 5080 and 5090 : To make this work (nvfp4) you need : * Update comfyui to the latest version

- I have tested them most except Flux, the fact that NVFP4 for Z-image and Qwen don't put Character model lora into consideration, so no use. For LTX-2, It's just producing garbage. have not tested Flux yet but I'm sure the structure would be the same to ignore Character model lora. As for the speed, It's about the same. No improvement. RTX 5090 CUDA 13.0, Triton 2.9.1 Laptop.

- You can use FP8 and NVFP4 on other arch, it will fallback to BF16 computation. It just that FP8 is faster on Ada and Blackwell while FP4 is faster only on Blackwell

- Older GPUs (RTX 40/30-series): Use the INT4 setting; you will still see significant memory savings (3.6x) and speed improvements (up to 3x), though not the native FP4 benefit.

- ## [Do any comparison between 4x 3090 and a single RTX 6000 Blackwell gpu exist? : r/LocalLLM _202512](https://www.reddit.com/r/LocalLLM/comments/1pu62uz/do_any_comparison_between_4x_3090_and_a_single/)
- Just got here. 
  - Setup on Blackwell is still a bit of a pain, by comparison the 3090s are easy peasy
  - Prompt Processing performance - using GLM 4.6 quants in llama.cpp I get 3.5x the speed with the blackwell than I do with the 3090
  - Token Generation - essentially no difference (I think I can get a little better performance here from the Pro 6000 but between system differences and kernel differences the raw power of the Pro doesn't make up the difference in perf)
  - Power consumption - baseline consumption is very similar, with the pro sitting at around 3.5x one 3090. Under single query load the pro has been hitting 200 W
  - Sound - The pro is in my living room, I can barely hear it. The 3090s are in my office.... I can *really* hear it. Not horrible but I know when it's running.
  - I haven't played with vLLM much but for models that can be fully resident in 96 GB VRAM the Pro tentatively ran at 2x for both generation and prompt processing.
- My conclusion is:
  * For models fully resident in 96GB the Pro wins hands down (ignoring pricing)
  * For models partially resident in 96GB the Pro wins on processing but not prompt generation
  * When factoring in price, the 3090 is a great contender
  * When factoring in future improvements and the ability to easily go to 2 or 4 gpus I think the Blackwell wins.

- The 3090's will not give you 96gb of useable memory. There is VRAM overhead space being taken up to manage the communication between the cards, so you will only have about 88gb of actual useable VRAM. In addition it will run slower because it has to send information between cards.
  - Not true. I have yet to see a benchmark where the Pro 6000 wins. 4x3090 with TP and vLLM is faster, especially at long context, than anything on the Pro 6000.

- ## [Got me a 32GB RTX 4080 Super : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pstaoo/got_me_a_32gb_rtx_4080_super/)
  - I took a risk and bought a modified RTX 4080 Super from the Chinese market for around 1200 USD / 1000 EUR. Which for me because I live in Europe, the cheapest RTX 5090 I can find is around 2500 USD / 2100 EUR.
  - It's maybe not the best card for price per GB of VRAM considering the RTX 3090 is dropping a lot, but 32GB on one card for about half the price of a 5090 is nice. I do a lot of Diffusion model stuff, so it's great for that too.
  - It works with the stock Nvidia driver, no messing around, it was just literally plug and play. Card seems really good quality, metal back plate and metal case. Fan sounds like a small jet engine.
  - But running it around a month now and zero issues at all.
  - I got mine from this seller, price went up 1000 RMB though since i bought it though. I used Superbuy to handle forwarding.

- How is the temperature and what power limit. I am eyeing it for quite a while. It is about S$2400 in where i living.
  - It gets around 70C max under load, seems to hit around 300W under full load. Hasn't been an issue, the blower is pretty loud though.

- howâ€™s the speed? 
  - Same as a stock RTX 4080 super.

- Look like they doubled the NAND chip on that GPU. Curious about how you set up the driver to receive all the VRAM.
  - I didn't touch the driver, it's stock. I guess the RTX 4080 is special in that way, if you add bigger ~~NAND~~ GDDR VRAM chips, it gets recognized without much fuss.
- the reason for this is that there is a 4090 Mobile with less vram for the gaming laptop market, and it uses the same chip as the desktop 4080. The 4080 super is just a more performant gpu compared to the stock 4080. The idea is that the vram size is "changeable" , so technically NVIDIA could put more vram in such cards, and they actually did it in professional gpu like the rtx 6000 ada series (same core as rtx 4090 desktop gpu with more vram). And such that the driver must be able to recognize the vram to function properly.

- I hope NVIDIA doesn't disable it in the GPU mainboard in the future like Apple did in the mac studio ssd slot (mac studio has a ssd slot but it refuse to recognize any ssd even from other mac studio.)

- it's not nand... it's gddr dram

- ## [NVIDIA RTX PRO 5000 Blackwell GPU with 72GB GDDR7 memory is now released : r/comfyui _202512](https://www.reddit.com/r/comfyui/comments/1prz95a/nvidia_rtx_pro_5000_blackwell_gpu_with_72gb_gddr7/)
- Nvidia seems to have settled on $100/GB of vram. AMD will sell you a 7900xtx w/ 48gb of vram for $65/GB. But it's a triple slot and as far as rocm has come, the biggest improvements are kept back for the mi instinct series.

- That might be great news for the LLM community. But we image guys here are usually compute bound. I.e. a 5090 might be more worth to us than a VRAM extended 5000

- some of the WAN models go OOM even on a 6000 PRO for 1080p resolution and 1600+ frames, so this dosen't seem so interesting now.

- Only 14k cuda cores, guys, that's a solid meh.

- several hundred cheaper than the Max-Q RTX Pro with 24GB less RAM...

- Stupid question, if you just had a stupid amount of money could you use this card for gaming and blow away a 5090 or does it not work like that?
  - Yes you could, but not with the PRO 5000. PRO 6000 with 96GB is the one you want if you wanna beat the 5090. That one has the fully unlocked GB202 with 24064 cores, instead of "only" 21760 on the 5090, and it beats the 5090 in gaming by about 10%. But it starts at $7k.

- Memory bus on these GPUs is only 384-bit, not 512-bit as the Nvidia datasheet claims.
- I mean, the 5090 has 512-bit and GDDR7. So even with the higher memory capacity, itâ€™ll be slower, but just have more Ram to work on larger models?
  - Yes, bandwidth is severely reduced from 1792GB/s (512-bit) to 1344GB/s (384-bit).

- ## [AMD Radeon AI PRO R9700 benchmarks with ROCm and Vulkan and llama.cpp : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1prgi41/amd_radeon_ai_pro_r9700_benchmarks_with_rocm_and/)
  - Spec: AMD Ryzen 7 5800X (16) @ 5.363 GHz, 64 GiB DDR4 RAM @ 3600 MHz, AMD Radeon AI PRO R9700.
  - Software is running on Arch Linux with ROCm 7.1.1 (my Comfy install is still using a slightly older PyTorch nightly release with ROCm 7.0).
  - The LLM is instructed to summarise each chapter of a 120k-word novel individually, with a script parallelising calls to the local API to take advantage of batched inference. 
  - Mistral Small: batch=3; 479s total time; ~14k output words
  - gpt-oss 20B: batch=32; 113s; 18k output words (exluding reasoning)
  - TLDR is that ROCm usually has slightly faster prompt processing and takes less performance hit from long context, while Vulkan usually has slightly faster tg.
  - All using ComfyUI. 
  - Z-image, prompt cached, 9 steps, 1024Ã—1024: 7.5 s (6.3 s with torch compile), ~8.1 s with prompt processing.
  - SDXL, v-pred model, 1024Ã—1024, 50 steps, Euler ancestral cfg++, batch 4: 44.5 s (Comfy shows 1.18 it/s, so 4.72 it/s after normalising for batch size and without counting VAE decode). With torch compile I get 41.2 s and 5 it/s after normalising for batch count.
  - Flux 2 dev fp8. Keep in mind that Comfy is unoptimised regarding RAM usage, and 64 GiB is simply not enough for such a large model â€” without --no-cache it tried to load Flux weights for half an hour, using most of my swap, until I gave up.
  - I also successfully finished full LoRA training of Gemma 2 9B using Unsloth. It was surprisingly quick, but perhaps that should be expected given the small dataset (about 70 samples and 4 epochs). While I donâ€™t remember exactly how long it took, it was definitely measured in minutes rather than hours. 

- A lot of people considering 5080's/4090's for LLMs would probably be happier with this card. It's 32GB in a single slot with very reasonable prompt-processing speeds and good enough token-gen.
  - also it's not bad at all with diffusion models, even with ROCm in its early days. Quite promising.

- 4-bit quant/qlora support for Radeon cards just got merged into Unsloth a day or 2 ago. It's been added to Bitsandbytes for some time now, feel free to check it out 

- ## [They're finally here (Radeon 9700) : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pnd5uf/theyre_finally_here_radeon_9700/)
- Fuck, I'm old. Radeon 9700 was a top tier GPU back in the 2000's. They have used all the code names and started a new loop
  - Yeah search results are annoying. And AMD now has a 9700x CPU as well.

- I have one R9700 and planning to get another (or whatever comes next--PRO 36GB or 72GB sku). The benefits of the R9700 isn't really for performance/price. The benefits I see:
  - blower fan design lets you pack them side by side no no slot gaps needed
  - 32GB per 300W GPU means fewer GPUs/lesser PSU for given total VRAM
  - also fewer needed PCIe lanes to GPUs for given VRAM
  - ECC memory operation can be enabled on Linux for longer/reliable operation
  - PRO sku meant to be running continuously (with sufficient cooling)
  - RDNA4 should have better long-term support than older gens.

- ## ğŸ†š [Is it better to upgrade from 3080 to 3090 or 5080 for video generation? : r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/comments/1pjfmrw/is_it_better_to_upgrade_from_3080_to_3090_or_5080/)
- I have both the 3090 and 5080 hooked up to my computer as I am typing this. For majority of the task, including wan 720p 480p, qwen edit, z-image, and illustrious + 4k hires fix, 
  - I find myself only use 5080 and vram has not been an issue unless I'm running a very large batch size. 
  - The 5080 is twice as fast with support for fp8 and fp4. Since buying the 5080, the 3090 has only been used for llm when i need better prompt or gaming concurrently with running the workflow.

- Just a headsup 3090 doest support 8bit calculation. You would be using fp16 versions or anything lower than 16bit will be transformed to fp16
  - It does support INT8, just not FP8.

- My vote is for more vram. Best advantage of the 5080 is speed, but with vram it opens up so much more possibilities (and that speed won't matter if it wont fit into memory).

- ## [Does ComfyUI work flawlessly with AMD graphics cards too? Which card is more stable? Is there anything that can be done with Nvidia but not with AMD? : r/comfyui _202512](https://www.reddit.com/r/comfyui/comments/1pi94ix/does_comfyui_work_flawlessly_with_amd_graphics/)
- Using an AMD Graphics card is the difficult and treacherous road currently in the world of AI (I know, I primarily use a Radeon Pro w7800 32GB). Even installing basic Custom Nodes is fraught with dangers. One must examine each requirements.txt file to make sure it is not trying to install torch, torchaudio, and/or torchvision (which is rather unnecessary to put into the requirements file for a custom_node anyway.) Those lines will need to be commented out or erased after cloning the Git. If not done, then upon ComfyUI's next startup, it will uninstall the ROCm version of pytorch and install the Nvidia one instead breaking all inference functionality (this has happened to me a couple of times before. Blast my laziness!). 
  - Furthermore, some attention algorithms currently are not compatible with AMD cards. It's a cold, hard AI world for AMD GPU users. However, it is getting better. ROCm is getting better. 
  - Many AMD cards are great, especially for gaming. 
  - However, for the time being, if you have a choice, I recommend using a Nvidia card for AI.

- ## [Can M1/M2 Macbooks use ANY eGPU as a docking station? : r/eGPU _202212](https://www.reddit.com/r/eGPU/comments/zk8yzv/can_m1m2_macbooks_use_any_egpu_as_a_docking/)
- unfortunately. The M2 MacBook Air only supports one external display up to 6K resolution. The only option is to use DisplayLink adapters/docking stations if you want more than one external display.

- eGPUs work on Apple Silicon but the driver support on macOS is nonexistent because Apple no longer has any reason to listen to GPU manufacturers since they make their own now. You'd need to install Asahi Linux and wait for Thunderbolt support to be ready, but yes theoretically it should be doable

- ## ğŸ†š [V100 vs 5060ti vs 3090 - Some numbers : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1p4b6ti/v100_vs_5060ti_vs_3090_some_numbers/)
- Speed specs put the 3090 in first place in raw compute
  - 3090 - 35.6 TFlops FP16 (936Gb/s bandwidth)
  - V100 - 31.3 TFlops FP16 (897 Gb/s bandwidth)
  - 5060ti - 23.7 TFlops FP16 (448 Gb/s bandwidth)
- Machines:
  - 8x V100 SXM2 16G. This was the machine that I started on Vast with. Picked it up post ETH mining craze for dirt cheap. 2x E5-2690 v4 (56 threads) 512G RAM
  - 8x 5060ti 16G. Got the board and processors from a guy in the CPU mining community. Cards are running via MCIO cables and risers - Gen 5x8. 2x EPYC 9654 (384 threads) 384G RAM
  - 4x 3090, 2 NVLINK Pairs. Older processors 2x E5-2695 v3 (56 threads) 512G RAM
- Ran llama-bench with llama3.1 70B Instruct Q4 model with n_gen set to 256 (ran n_prompt numbers as well but they are just silly)

| Card    | T/s   | Relative | TFlops | Relative |
|---------|-------|----------|--------|----------|
| 3090    | 19.09 | 100%     | 36.6   | 100%     |
| V100    | 16.68 | 87.4%    | 31.3   | 87.9%    |
| 5060ti  | 9.66  | 50.6%    | 23.7   | 66.6%    |

- llm generation mostly scale with memory bandwidth rather than compute capability, for dense mode, if u use vllm and enable tensor parallel, u will get double that speed
- TG numbers do not depend much on compute capacity but almost completely on memory bandwidth

- Need prompt processing speed. Batch speeds. Tensor parallelism? Just post the raw llama bench tables and settings you used to run it. Also would be useful to have power numbers.

- ## ğŸ ğŸ¤” [1x 6000 pro 96gb or 3x 5090 32gb? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p2540n/1x_6000_pro_96gb_or_3x_5090_32gb/)
- 1x 6000 not 3x 5090. 
  - A single chunk of vram is always better than a divided one - unless your intention is to run multiple concurrent small models, then the 5090s give you more compute. 
  - And Epyc 9xxx over Threadripper for more ram channels, as long as you go for the high clockspeed versions (F). Go for 9575F, 64 cores, high clockspeed, 12 channels. Only disadvantage is having to use a server board.
- RTX 6000 Pro has the ability to split into (up to) 7 independent virtual graphics cards. There is really no advantage to 3x 5090.
  - There is. Since if you run TP, then those 2x 5090s can be faster. The 3rd 5090 will have to wait for a partner.
- That is absolutely wrong. Maybe because you havent used vLLM in tensor parallel and only have used llama.cpp or Ollama? 2x5090 or 4x5090 is way faster than 1x rtx 6000 if used in tensor parallel (which Ollamas and other hoobyist inference engines cant do)

- people dont realize that bandwidth between GPU and memory IS the bottleneck. a model can not load efficiently into separate cards. you end up with three tines only 32gb usable memory
  - No, thats not true. In tensor parallel 2 or 4 pcie 5.0 16x is enough for most of the tasks. So 2 or 4 5090 connected with pcie 5.0 16x is enough fast pcie link between gpus. I have tested this and was never able to fill the full bandwidth totally. The speed does not increase lineary but 2nd gpu gives 60% more inference speed when enough simultaneous requests.

- Canâ€™t -tp 3 with vllm
  - Pp3(Pipeline parallelism w 3 cards) works fine and is better with consumer gpus anyways since it requires less communication between cards. Data center card are connected with infinity band so tp is better

- I have 3x 5090 and 1x Pro 6000 as well as a Threadripper with 512GB RAM (and a second Threadripper and 3 3090), and my recommendation for running LLMs of that size locally is: Don't.
  - At least don't expect it to be good value or fast. 
  -  I use local for parallel workloads. 10k t/s throughput - now we are talking.

- A lot of people here are assuming the 5090 behaves like a normal single-GPU setup. It doesnâ€™t â€” not with the current software stack.
  - If your workload needs contiguous VRAM (anything 70B+, large context, MoE with large expert blocks, diffusion XL, video gen, etc.), a single 96 GB slab always beats 3Ã— 32 GB islands, regardless of raw TFLOPs.
  - Multi-GPU only wins if youâ€™re bulk-processing smaller jobs in parallel. 

- why I vote Epyc instead of Threadripper:
  - The higher singlethread perf of Threadripper isnt worth the extra cost.
  - EPYC DDR4 platform could save you a couple thousand over DDR5. 
- Why RtX 6000 Pro and not 3x5090:
  - The convenience and simplicity of a having 1 x GPU with less wiring and less grief with management and maintenance, PCIE risers and cases and PSU selections and power draw etc its worth the little bit extra spend. (Plus with 3x GPU you cant use tensor parallel in VLLM)

- I have both a 6000 Pro (typically using llama.cpp) and a 2x3090 setup (using vllm). 
  - 3 is an awkward number since I don't think you can run tensor parallel. Moving to 4 you'd definitely want the Epyc/TR/Xeon4+ type platform and may need things like PCIe extensions and a mining rig chassis, etc.
  - If you are just using naive layer splitting (llama.cpp) you only really get the speed of what a single 5090 bandwidth is limiting you to and poor GPU utilization, and you should think seriously about getting sufficient PCIe lanes/bandwidth to a power of 2 number of GPUs (2, 4, 8) to use tensor parallel in VLLM if you are messing with this as it unlocks utilizing all the GPUs more fully. I also tested llama.cpp on the 2x3090 setup but it was substantially slower than using vllm with tensor parallel and GPU utilization hovers around 50%, which is sort of expected, still taking full power according to nvtop. VLLM with tensor parallel I peg both GPUs and even setting them down all the way to ~200W costs very little performance. I'd probably want to confirm with power draw at the wall but I think this is ultimately very wasteful not to try to use tensor parallel plus the massive performance difference.
  - tldr: as a RTX 6000 enjoyer myself having also worked on multi-gpu setups, I'd probably push you more toward 3x5090, then you have a good upgrade path later if you want to Epyc/TR + 4x5090 and that will ultimately be far superior and at least roughly similar in total cost. It is more complicated, you need to move off llama.cpp to vllm, you need a giant mining chassis and maybe PCIe extensions, more PSUs (even if you set power down, you need to physically connect everything), etc. One RTX 6000 is "simpler."

- Also, come the future selling 5090s will be easier than a single 6000.

- 5090s will give ~3x the heat, ~3x the power, ~3x the memory bandwidth, ~3x the flops and will require ~3x the physical space.

- ## [Ollares one: miniPC with RTX 5090 mobile (24GB VRAM) + Intel 275HX (96GB RAM) : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1ov3x0m/ollares_one_minipc_with_rtx_5090_mobile_24gb_vram/)
  - Processor: IntelÂ® Ultra 9 275HX 24 Cores, 5.4GHz
  - GPU: NVIDIA GeForce RTX 5090 Mobile 24GB GDDR7
  - Memory: 96GB RAM (2Ã—48GB) DDR5 5600MHz
  - Initial price seems it would be around $4000

- the 5090 mobile is based on the 5080 desktop chip, is a bit less compute and bandwidth than a 5080 desktop owing to being a mobile part, but with 24GB instead of 16GB. The 5080 desktop is already about half the speed of a 5090. The system is still a dual channel DDR5 system after you run out of VRAM, just like a normal desktop. 
  - It feels to me like a very narrow market for the particular combination of very tiny form factor, good diffusion and smaller LLM performance (<32B), and low power usage. 
  - Otherwise, a better value is probably found for large (80-120B) MOE LLM in the 395/Spark, or a desktop 5090 which you can at least cram into a "smallish" mini ITX case like the Corsair 2000D.

- it cant compete with unified memory machines since its ram is slower by 4-8x compared to them. 

- ## ğŸ†š [Why Ampere Workstation/Datacenter/Server GPUs are still so expensive after 5+ years? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1ove1px/why_ampere_workstationdatacenterserver_gpus_are/)
- ada
  RTX 6000 Ada (48GB), on ebay for about 5000 USD.
  RTX 5000 Ada (32GB), on ebay for about 2800-3000 USD.
  RTX 4000 Ada (24GB), on ebay for about 1200 USD.
  NVIDIA L40 (48GB), on ebay for about 7000 USD.
  NVIDIA L40S (48GB), on ebay for about 7000USD.
  NVIDIA L4 (24 GB), on ebay for about 2200 to 2800 USD.

- ampere
  RTX A6000 (48GB), on ebay for about 4000-4500 USD.
  RTX A5000 (24GB), on ebay for about 1400 USD.
  RTX A4000 (16GB), on ebay for about 750 USD.
  NVIDIA A40 (48GB), on ebay for about 4000 USD.
  NVIDIA A100 (40GB) PCIe, on ebay for about 4000 USD.
  NVIDIA A100 (80GB) PCIe, on ebay for about 7000 USD.
  NVIDIA A10 (24GB), on ebat for about 1800 USD.

- ADA generation killed nvlink for pro level gpus. This means for multi-gpu training particularly bandwidth intensive tasks the ampere generation is superior.
  - Blackwell is also a big leap over ADA generation in both compute, but crucially VRAM as well.
  - As a result if you want multi-gpu ampere can often win, and if you want single GPU Blackwell is a no brainer vs Ada, so ADA doesn't have a market niche.

- Ampere cards are slower (about half perf compared to Ada), some less VRAM and don't support FP8.

- Their memory bandwidth is stil above what is available for the consumer market + their power consumption is even lower.

- What MOBO do you recommend for 5-6 cards? If possible AMD AM5.
  - Threadripper/threadripper pro/xeon workstation or server motherboard like amd epyc or intel xeon.
  - Only workstation/server motherboards have enough pcie lanes to satisfy your needs.
  - Consumer motherboards won't properly support that much gpus A z790-p supports 4 gpu, 1 with x16 lanes from the cpu and 3 x4 from the chipset (acting link a switch).
  - You could split the x16 (if bifurcation is supported). But still this is x4 pcie 4.0, could be a bottleneck.

- Ampere cards are still pretty good all-rounders that can handle AI workloads well, they support BF16 training and inference and that's not dead.

- ## [Does AMD AI Max 395+ have 8 channel memory like image says it does? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1os6t6w/does_amd_ai_max_395_have_8_channel_memory_like/)
- 4 channels * 64-bit interface * 8000 MT/s = 256 GB/s.
  - Itâ€™s LPDDR5x not DDR5 so itâ€™s technically 8 channels at 32 bit, but the effective bit width and bandwidth is the same.

- Probably a typo, but technically DDR5 has 2 32 bit channels per module x four dimm channels would make 8 *32-bit* channels, but it would be disingenuous to advertise it that way.

- My understanding is Halo Strix has quad channel DDR5.

- ## [AMD R9700: yea or nay? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1os2756/amd_r9700_yea_or_nay/)
- It's slower than a 3090 and doesn't offer fp4. 3090 can emulate fp8 and it's almost twice as fast. Also less of a headache...
- 3090 doesn't offer FP4 nor FP8, needs emulator and the perf tanks doing so. INT4, FP16, BF16, INT8, INT4.
  - Only Blackwell supports FP4, FP32, FP16, BF16, INT8, INT4, FP8.
- On R9700 FP8 and BF8 are fully supported, with improved perf.
  - FYI, FSR4 is FP8.

- ## [AMD Officially Prices Radeon AI PRO R9700 At $1299 - 32GB VRAM - Launch Date Oct 27 : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1oeg2g6/amd_officially_prices_radeon_ai_pro_r9700_at_1299/)
- 640GB/s
  - yes, for the same price, i can get 2x3090 for 900 gb/s and 48gb vram and cuda compatibility
- Specs sheet says int4 only, not Fp4...
- 640 GB/s Memory Bandwidth - slower than the 5 year old rtx 3090 and 1/3rd of a 5090. No FP4 or CUDA.

- They're Rx 9070's with blower fans and twice the VRAM.

- FYI R9700 is slim 300W card with ECC VRAM, can easily fit 2 on any motherboard.
  - Also make no sense to buy â‚¬750 RTX3090 when R9700 is â‚¬1000 if you can deduct the sales tax (normal price â‚¬1250-â‚¬1300) in Europe.

- Used 3090 are 500-550$. Even good ones from asus or msi.

- All the AMD pro series GPUs have official ROCm support.
  - Oh I know. I just mean Vulkan actually works.
- Full ROCm works and they have ECC VRAM.

- MI50s are good after fresh updates, although R9700 has lots of new things - FP8, matrix cores, etc.

- 70% slower than 5090
- Fp16 seems much closer
  - Only when sparse

- 4 of these in parallel is a compelling alternative to an RTX Pro 6000
  - Wins on VRAM Quantity 128GB VRAM vs. 96GB VRAM
  - Wins on Price ~5200 vs ~8200
  - Wins on Collective Bandwidth (if you user tensor parallel) 640x4=2.56TB/s vs 1.8TB/s
  - Only obvious loss is power usage 300x4 = 1200W vs 600x2 or 300x2 (600 or 1200W depending which model)
  - And upgradability. Few cases/motherboards are going to support more than 4 cards unless you start getting jank with it.

- ## [8å¹´äº†ï¼Œä¸ºä½•eGPUå¤–ç½®æ˜¾å¡ä¸èƒ½è¢«ç©å®¶æ¥å— - çŸ¥ä¹ _202507](https://zhuanlan.zhihu.com/p/1930776574258549666)
- åˆä»£å¤–ç½®æ˜¾å¡ï¼ˆeGPUï¼‰æ›¾è®¸ä¸‹è¯¸å¤šè±ªè¨€å£®è¯­ã€‚è¿™äº›çœ‹ä¼¼æ–°é¢–ç‚«é…·çš„æ˜¾å¡å¤–ç½®ç›’æœ¬åº”å½»åº•æ”¹å˜æ¸¸æˆæœ¬å¸‚åœº
  - å¦‚ä»Šï¼Œè·ç¦»é¢å‘æ™®é€šæ¶ˆè´¹è€…çš„ç¬¬ä¸€ä»£å¤–ç½®æ˜¾å¡é—®ä¸–ï¼Œå·²è¿‡å»äº†å…«å¹´ï¼Œå¯ç¬”è®°æœ¬ç”µè„‘å¸‚åœºå´æ²¡å¤šå¤§å˜åŒ–ã€‚
- é›·è›‡Coreå¹¶éé¦–æ¬¾å¤–ç½®æ˜¾å¡ï¼ˆç¬¬ä¸€æ¬¾çœŸæ­£æ„ä¹‰ä¸Šçš„ç¬”è®°æœ¬ä¸“ç”¨eGPUæ˜¯2008å¹´åç¡•æ¨å‡ºçš„XG Stationï¼‰ï¼Œä½†é›·è›‡Coreæ˜¯ç¬¬ä¸€æ¬¾å¼•èµ·è½°åŠ¨çš„äº§å“ã€‚
- å¤–ç½®æ˜¾å¡ç»ˆç©¶æ˜¯ä¸€ä»¶éœ€è¦é¢å¤–éšèº«æºå¸¦çš„è®¾å¤‡ã€‚èƒŒåŒ…é‡Œçš„ç©ºé—´æœ¬æ¥å°±å®è´µï¼Œå®Œå…¨å¯ä»¥ç”¨æ¥è£…å…¶ä»–é…ä»¶ã€‚æ›´ä½•å†µï¼Œè¿˜å¾—æŠ˜è…¾åŠå¤©è®¾ç½®é€‰é¡¹ï¼Œç¡®ä¿æ¸¸æˆèƒ½è¯†åˆ«å¤–ç½®æ˜¾å¡å¹¶è°ƒç”¨å®ƒæ¥æ¸²æŸ“ç”»é¢ï¼Œè¿™æ— ç–‘å¢åŠ äº†ä¸å°‘éº»çƒ¦ã€‚æ€»çš„æ¥è¯´ï¼Œè¿˜ä¸å¦‚é™ä½ç”»è´¨è¦æ±‚ï¼Œç”¨ç”µè„‘è‡ªå¸¦æ˜¾å¡æ¥å¾—ä¾¿æ·ã€‚
- 2025å¹´ï¼Œåç¡•ã€é›·è›‡å’ŒæŠ€å˜‰éƒ½æ¨å‡ºäº†æ–°æ¬¾å¤–ç½®æ˜¾å¡ã€‚ä»Šå¹´çš„å„ç±»ç§‘æŠ€å±•å’Œæ¸¸æˆå±•ä¸Šï¼Œä¸»æµå‚å•†ä¸€ä¸‹å­æ‹¿å‡ºäº†å››æ¬¾æ–°çš„å¤–ç½®æ˜¾å¡ã€‚
  - è¿™æ¬¡ç®—æ˜¯æ—¶éš”8å¹´åï¼Œå‚å•†ä»¬å¯¹å¤–ç½®æ˜¾å¡å¸‚åœºæœ€å¤§è§„æ¨¡çš„ä¸€æ¬¡å‘åŠ›ã€‚è¿™ä¸€åˆ‡éƒ½è¦å½’åŠŸäºé›·ç”µ5æ¥å£

- egpuçš„çœŸæ­£ä»·å€¼åœ¨äºä½ å’Œä½ çš„å®¶äººå¯ä»¥å…±äº«ä¸€å—æ˜¾å¡ï¼Œè¿™æ ·ä½ ä»¬å°±å¯ä»¥æŠŠé’±æ”’èµ·æ¥ä¹°ä¸€ä¸ª5070ä¸€èµ·ç”¨ï¼Œè€Œä¸æ˜¯æ¯ä¸ªäººéƒ½å»ä¹°ä¸€ä¸ª5090æ˜¾å¡ã€‚

- ç½‘ä¸Šé‚£äº›åšå¤–æ¥æ˜¾å¡è§†é¢‘çš„ï¼Œå‡ ä¹é€éœ²å‡ºå…¼å®¹å·®ã€ç¨³å®šæ€§å·®çš„é—®é¢˜ï¼Œç›¸æ¯”èµ·æ¥æ€§èƒ½æŸè€—éƒ½æ˜¯å°é—®é¢˜ã€‚ä¸èƒ½æ™®åŠæ˜¾è€Œæ˜“è§ã€‚

- ä½“éªŒè¿‡ä¸€æ¬¡ï¼Œå†ä¹Ÿæ²¡ä¹°è¿‡ã€‚å½“æ—¶xps15é…äº†ä¸ª1080çš„egpuï¼Œç»“æœæ€§èƒ½è¿˜æ¯”å¦‚å°å¼æœºç›´æ’çš„1060ç›´æ¥å¿ƒæ€å´©äº†
  - é›·ç”µ3æ€§èƒ½ç¼©æ°´30%æ˜¯å°æ„æ€

- ## [Dual GPU, AMD & Nvidia together? : r/losslessscaling _202504](https://www.reddit.com/r/losslessscaling/comments/1jpelqr/dual_gpu_amd_nvidia_together/)
- I tested a 4090 (4.0x16) with rx6600 (4.0x4) setup a while ago. While i was too lazy to find the perfect settings for it, i did not see any issues with drivers.
  - I just tested it for a day or two and couldn't overcome some issues with the performance. I think that it had something to do with using the chipset 4x4 line rather than cpu's.

- I rock a 4090 with 6600XT. I have both full drivers installed and run win 11. They donâ€™t clash.

- The main limitation is Nvidia's lower FP16 performance, but a 3080 TI is very good (30 TFLOPs), enough for 4k. Driver conflicts are a possibility, but nowadays everything conflicts with everything already. I've not heard of issues so far and it wouldn't do l so stop me from using the 3080. I would just get the minimal drivers for each GPU if possible.

- [Is it possible to use both and nVidia and AMD GPU? : r/LocalLLaMA _202407](https://www.reddit.com/r/LocalLLaMA/comments/1dt367v/is_it_possible_to_use_both_and_nvidia_and_amd_gpu/)
- Tested RX 7900 XTX and 4060 Ti (16GB) running together in LM Studio via Vulkan. Tried it with two models:
  - DS r1 70B Q5 â€” 10.05 tok/sec
  - QWQ 32b â€” 15.67 tok/sec
  - For comparison, RX 7900 XTX solo gets around 24.55 tok/sec in QWQ 32b.
- So you saying that dual is slower?
  - Exactly â€” a single 7900 XTX is better than a 7900 XTX + RTX 4060 Ti combo when the model requires less than 24GB of VRAM. In my configuration ofc, mb I did something wrong)

- [AMD + NVIDIA GPU : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1obgm8u/amd_nvidia_gpu/)
- I've got a RTX 5070 Ti (PCIe 5.0x16, CPU) and a RX 5500 XT (PCIe 4.0x4, CPU) in my AM5 PC. Is there a way to use both GPUs and the CPU to run the same gguf model?
  - You can compile multiple backends into Llamacpp and use them all at once.
  - Yup, the easiest way is to use Vulkan. Any llama.cpp-based software can do this (ie Lmstudio). Ollama not yet - experimental.

- [Is it possible to run my 9070 XT with my old RTX 3070? : r/losslessscaling _202511](https://www.reddit.com/r/losslessscaling/comments/1osolqu/is_it_possible_to_run_my_9070_xt_with_my_old_rtx/)
  - You can, but the top card will roast. And youâ€™ll lose the AMD specific 9070XT features.
  - when mixing Nvidia and AMD GPUs, since the monitor is always connected to the 2nd GPU(3070 in your case), on desktop you will only be able to use Nvidia App for stuff like color profiles, but in game it will be using FSR4 upscaling.
  - Personally I have used all combinations, AMD 1st GPU and Nvidia 2nd GPU, vise versa, and now im using 5070ti + 3060ti. I gotta say there is no real difference, all the small features like anti lag, RSR, make negligible differences.
  - Right now my setup is 3440x1440 HDR 165hz with 5070ti and 3060ti(PCIE4.0 x4). I can comfortably boost from 50-80fps to 160fps with 3060ti usage no more than 75%.

- ## ğŸ’¡ğŸ§© [AMD and Nvidia GPUs in the same machine. IT WORKS. : r/linuxhardware _202006](https://www.reddit.com/r/linuxhardware/comments/he9nhe/amd_and_nvidia_gpus_in_the_same_machine_it_works/)
  - TLDR: Nvidia Card in slot 2 with proprietary driver (v. 440xx) + AMD card in slot 1 open source driver (mesa v20.1), no configuration needed, just prime-run what you need to run with Nvidia card as the back-end renderer. Enjoy the smooth desktop and Nvidia/proprietary bond applications
  - The solution is a simple prime-run command. No messy xorg config files. In fact no manual configuration at all.

- What software requires proprietary Nvidia drivers?
  - In my case it's Davinci Resolve. A video editing software. It only runs with proprietary driver (AMD and Nvidia). I paid for the studio version.

- My laptop has an Intel iGPU and a GTX 1650, and I use an RX 5700 XT in an eGPU enclosure. I also use Mesa and the Nvidia proprietary driver simultaneously or alternating without issues (mostly).
  - It's a little different on a laptop trying to push external graphics but it still works
- So I suppose you use hybrid driver? iGPU as default OpenGL renderer and DRI_PRIME to use dGPU or eGPU? I'm curious how you access different GPUs.
  - I have pop OS, which includes a super handy GPU mode switcher tool (Integrated/Hybrid/Dedicated). You have to reboot each time (unless doing Hybrid and telling applications to run using dGPU).
  - I also installed the gswitch tool from egpu.io, and use it to switch between internal and external graphics when connected to my Thunderbolt 3 dock.
  - The only thing that really doesn't work is staying in "internal" graphics mode and trying to drive the external monitor (or vice versa; can't drive the internal display with external GPU). It "works" but the performance hit is so bad that even GNOME's desktop is unusable.
  - Other than that the whole setup is so easy that it takes me maybe 5 minutes to do on a fresh popOS install (or Ubuntu 20.04 with system76-power tool installed).
  - I highly recommend this setup if you ever plan on going the Nvidia laptop route and don't mind needing to use the prop drivers (even if your egpu is Nvidia it works perfectly).

- Iâ€™m trying to do the same - XPS 9500 with i7 CPU and GTX1650ti dGPUâ€¦ i have a razer core X enclosure and want to try an AMD RX 6600
  - basically plug and play, the drivers auto-installed and everything works smoothly!

- ## ğŸ¤” [1x4090 24GB or 3x3060 12Gb for Comfy? : r/comfyui _202409](https://www.reddit.com/r/comfyui/comments/1fql7if/1x4090_24gb_or_3x3060_12gb_for_comfy/)
- 3x3060 wonâ€™t give you 36GB. The model canâ€™t be split. There are variations of putting the T5 or clip into another card, but the multi GPU aspect will complicate it so much, it will drive you nuts. Especially since you say yourself you are a beginner. Donâ€™t do it. Go for 4090, comfy ui is complicated enough, you donâ€™t want to deal with anymore hardware setup issues on top of that. If you are tight on cash, go for a used 3090.

- In terms of AI a 4090 would still kill 3x 3060 and VRAM does not work as you think it does, you will not get 36GB VRAM with 3x 3060s

- 3x 3060 only if you're in production and your workflow can fit into 12GB VRAM and you're doing loads of parallel generations.

- 4090, speed is the key point, and there are many ways to reduce VRAM needs.

- ## [Sanity check: Using multiple GPUs in one PC via ComfyUI-MultiGPU. Will it be a benefit? : r/comfyui _202504](https://www.reddit.com/r/comfyui/comments/1k4xjxh/sanity_check_using_multiple_gpus_in_one_pc_via/)
- I own the ComfyUI-MultiGPU custom node. The two most common ways I see ComfyUI-MultiGPU are the following:
  - Moving parts of the process to a secondary GPU - typically CLIP/VAE
  - Using Distorch to offload the entirety of the UNet to another GPU or CPU (preferred).
  - In doing those two things, most people can get to a "naked" compute card, giving users the most latent space at the cost of speed. It is the way I use ComfyUI most of the time with a 2x3090 NVLink setup.
  - "Naked" in the sense that there is nothing taking up VRAM on the main compute card other than latent space. In a typical one-GPU setup, VAE, CLIP, and UNet are all resident on the card and take up VRAM that cant't be used for generations. Even a highly-quantized Q3 GGUF takes up (mostly) dead space in your card's VRAM as only one part of it is being dequantized at a time to be used actively during inference, then discarded until the next inference step.
  - In an optimal MultiGPU setup, all three componenets have been moved off the compute video card's VRAM entirely, allowing the card to be completely filled by latent spece. In this case, a 12G VRAM card can do video that fills that entire 12G of latent space, meaning either higher resolution or longer generations than if half (or more) of that VRAM was being used for component storage.

- ## [æœ‰å“ªäº›äºŒæ‰‹æ˜¾å¡å€¼å¾—ç©? - çŸ¥ä¹](https://www.zhihu.com/question/1955229739569619499)
- çº¯ç©æ¸¸æˆè¿˜å¯ä»¥åŠ ä¸ªmi50åˆ·é•­7biosï¼Œæ¯”vega64æ–°ä¸€ç‚¹

- 2080tiï¼Œåˆšåˆšè·Œäº†ä¸€å¤§æ³¢ï¼Œdyå åˆ¸æœ€ä¾¿å®œ1100-1200å°±èƒ½æ”¶ä¸€å¼ ï¼Œä½ èƒ½å¾—åˆ°ï¼šé»˜è®¤5060/3070ï¼Œè¶…é¢‘5060tiï¼Œæé™æ‘¸3080å±è‚¡çš„æ¸¸æˆæ€§èƒ½
  - ç›¸æ¯”æ›´æ—©çš„å¡å¦‚v100ï¼Œæœ‰DLSS4ï¼Œæœ‰å…‰è¿½ï¼Œå¯ä»¥è·‘æ¸²æŸ“å™¨ï¼Œç”šè‡³å¯ä»¥4kDLSSè¶…çº§æ€§èƒ½+æ’å¸§modï¼Œ60-70å¸§ç©ä¸€ç©2077è·¯å¾„è¿½è¸ª
  - å‡ ä¹æœ€å®Œç¾çš„æ˜¾å­˜æ‰©å®¹ä½“éªŒï¼Œ20ç³»æ˜¾å¡æ˜¾å­˜æ‰©å®¹åªéœ€è¦æŠŠæ˜¾å­˜é¢—ç²’ä»1G GDDR6æ¢æˆ2Gå°±å¯ä»¥
  - è¿™ä¸€ä»£çš„vbiosè¿˜æ²¡æœ‰ä¸Šé”ï¼Œæ‰©å®¹æ²¡æœ‰é©±åŠ¨bugï¼ˆ3070 16gæœ‰ï¼‰ï¼Œæ— éœ€æ‹†BGAï¼ˆ3080ã€4090éœ€è¦ï¼‰ï¼Œæ‰€æœ‰å“ç‰Œvbiosä»»æ„äº’åˆ·ï¼ˆ3080ã€4090éœ€è¦ç‰¹åˆ¶vbiosï¼‰ï¼Œå®Œå…¨ä¸å½±å“è¶…é¢‘ï¼ˆ3080ã€4090æ”¹åé”åŠŸè€—ï¼‰ï¼Œå®Œå…¨ä¸å½±å“æ•£çƒ­ï¼ˆ3080ã€4090æ”¹è£…åå˜æˆåŒé¢æ˜¾å­˜å¤§ç«ç‚‰ï¼‰
  - æ²¡æœ‰vegaç³»åˆ—HBMç¼©è‚›é—®é¢˜ï¼Œæ²¡æœ‰6800ç³»åˆ—ç‚¸æ ¸å¿ƒé—®é¢˜
  - å”¯ä¸€è¦æ‹…å¿ƒçš„å°±æ˜¯åˆ«ä¹°åˆ°æ—©æœŸé•å…‰æ˜¾å­˜å’Œé»„çš®è‚¡å¤§ä¿®å¡
- è¯´å®è¯ä¸å¦‚ç›´æ¥ä¹°5060ï¼Œè¿™ç©æ„åŠŸè€—å¿«åˆ°5060ä¸¤å€äº†ï¼ŒäºŒæ‰‹èƒ½ç”¨å¤šä¹…ä¹Ÿå¾ˆéš¾è¯´ã€‚

- ä¸ºä»€ä¹ˆ3060è¿˜èƒ½å–1300å—ï¼Ÿæˆ‘è§‰å¾—è¿™ä¸ªç ´å¡æ ¹æœ¬ä¸å€¼é’±ï¼Œç°åœ¨3070ä¹Ÿæ˜¯1300
  - 2gå¤§æ˜¾å­˜ï¼Œ2k3aæ–°æ¸¸æˆèƒ½åæ€4060æŒå¹³5060è¿™ç§8gçš„å¡
  - AIç»˜å›¾å…¥é—¨å¡ï¼Œå·²ç»æ²¡æœ‰æ¯”å®ƒæ€§ä»·æ¯”è¿˜é«˜çš„å¡äº†ï¼Œå¦å¤–3060ä¸å»ºè®®ä¹°äºŒæ‰‹ï¼ŒçŸ¿å¡å¤ªå¤šï¼Œå»ºè®®è¿˜æ˜¯ä¹°ä¸ªæ­£è§„ç‰Œå­çš„2000å·¦å³çš„æ–°å¡

- v100 32g ç°åœ¨çº¦2100 ï¼›æœŸç›¼é™åˆ°1500 ã€‚
  - v100 16gçº¦500ï¼›é…ä¸€å¥—æ°´å†· 900å—ã€‚
  - x99 e5ä¸€å¥—åŒæ˜¾å¡æ§½ä¸»æœºä¸è¿‡åƒå…ƒã€‚
  - 3500æ­ä¸€å¥—32gå•å¡ä¸»æœºï¼›
  - ä½†åŠŸè€—éš¾ç»·300-600wï¼Œæ‰€ä»¥æœ€åç”¨macç©apiäº†ï¼›

- åˆ«æŠ˜è…¾V100è¿™ç§åƒåœ¾äº†ï¼Œç°åœ¨è¶Šæ¥è¶Šå¤šçš„æ–°æ¨¡å‹ï¼Œéƒ½ä½¿ç”¨BF16ã€FP8ç”šè‡³NVFP4è®­ç»ƒæ¨¡å‹äº†ï¼Œå°±ç®—æ¨ç†ï¼Œç®—åŠ›è¦æ±‚ä¹Ÿæ˜¯8.0èµ·æ­¥ï¼Œæœ‰AIéœ€æ±‚ï¼Œè€è€å®å®ä¹°æ–°å¡ã€‚

- ## [AMD å‘å¸ƒ AI æ˜¾å¡ R9700ï¼Œæ€§èƒ½å·ç§°åŒçº§ 5 å€ï¼Œå®é™…è¡¨ç°å¦‚ä½•ï¼Ÿ - çŸ¥ä¹ _202509](https://www.zhihu.com/question/1946144571521234533)
  - R9700çš„AIæ€§èƒ½è¡¨ç°å´å¼•èµ·äº†å¤§å®¶çš„å…³æ³¨ï¼Œä½œä¸ºä¸€æ¬¾RDNA4æ¶æ„çš„æ˜¾å¡ï¼Œæ‹¥æœ‰32GB GDDR6ã€256-bitä½å®½ã€å³°å€¼å¸¦å®½çº¦640 GB/sã€300W TDPï¼Œä»¥åŠæœ€å¤šçº¦1531 TOPSï¼ˆINT4ï¼‰å’ŒFP16çº¦96 TFLOPSçš„AI/çŸ©é˜µè¿ç®—èƒ½åŠ›ã€‚

- è¿™ä¸ªæ˜¯RDNA4åº”è¯¥æ¯”395maxé‚£ä¸ªRDNA3.5çš„é­”æ”¹å¼ºä¸å°‘å§

- [AMDå‘å¸ƒAIç¥å¡R9700ï¼æ€§èƒ½æ˜¯åŒçº§5å€ï¼Œè‹±ä¼Ÿ - å°çº¢ä¹¦](https://www.xiaohongshu.com/explore/68c58dcb000000001c008551?xsec_token=ABxWJEP3Vpkb636xx2y79C0FRNAwvEq6parZO6uJSDPkM=&xsec_source=pc_search&source=web_search_result_notes)
- äººè¯ï¼š32gç‰ˆæœ¬9070
- æ²¡æœ‰ CUDA ç”Ÿæ€æ”¯æŒï¼Œä¹Ÿå°±ç©ç©ç°æˆçš„ toy model
  - ç”¨vulkanæˆ–è€…ZLUDAæˆ–è€…HIPè·‘å‘—ï¼Œåæ­£8kå—çš„å¡è‚¯å®šæ˜¯ä¸ªäººç”¨ï¼Œä¸éœ€è¦é›†ç¾¤ï¼Œä¸è€ƒè™‘å¤šå¹¶å‘æˆ–è€…äº¤ç«å•¥çš„ï¼Œèƒ½ç©

- åªèƒ½åœ¨Linuxå¹³å°ç”¨ï¼Œllmå¯èƒ½è¿˜è¡Œï¼ŒAIGCä¼°è®¡æ˜¯å„ç§ä¸å…¼å®¹æŠ¥é”™ã€‚å›¾ä¾¿å®œ6000çš„3090å°±æŒºå¥½çš„ã€‚r9700è·Œåˆ°6000ä¹Ÿè¿˜æ˜¯åªæ¨è3090ã€‚
  - è¡¥å……ä¸€ç‚¹ï¼Œr9700æ€§èƒ½åŒç­‰çº§çš„æ¸¸æˆæ˜¾å¡æ˜¯5070ï¼Œè·Ÿ5080ä¸æ˜¯ä¸€ä¸ªçº§åˆ«äº§å“ï¼Œæ˜¾å­˜å¤§ç¡®å®æ˜¯ä¸ªäº®ç‚¹ï¼Œä½†åˆä¸å¤Ÿå¤§åªæœ‰32GBï¼Œè®°å¾—ä¸é”™wançš„æƒé‡å¥½åƒæ˜¯35GBï¼ŒæŠ˜è…¾ä¸€åœˆåˆ°æœ€åè¿˜æ˜¯åªèƒ½é€‰48GBçš„4090ã€‚
  - ä¸è¦ä¸ºäº†æ¨ç†llmï¼Œé€‰æ¥é€‰å»åªèƒ½é€‰m4 maxï¼Œ128GB 400å¤šçš„å¸¦å®½ï¼Œè‹¹æœå¤´ä¸€æ¬¡æœ‰æ€§ä»·æ¯”ã€‚

- æ²¡æ„ä¹‰ï¼Œaå¡çš„aiå¡å¦‚æœæ€§ä»·æ¯”å¹²ä¸è¿‡2080ti 22gæ ¹æœ¬æ²¡å‡ºè·¯

- å¿…é¡»ç”¨HBMï¼Œè¯•é—®æŒ‰æ‘©åº—2019å¹´çš„HBMæ˜¾å­˜ä¸ºä»€ä¹ˆæ‚„å’ªå’ªçš„è·‘åˆ°è€é»„ä¸“ä¸šå¡ä¸Šäº†ï¼Ÿä¸ºä»€ä¹ˆä¹‹åæŒ‰æ‘©åº—ä¸ºä»€ä¹ˆä¸å‡ºHBMæ˜¾å­˜çš„æ˜¾å¡â€¦â€¦è¿™ç¾¤å¹•åäº¤æ˜“â€¦â€¦å‘µå‘µå‘µï¼è¿˜æ˜¯èµ„æœ¬ä¼šç©

- ## [RTX 4080 SUPER 32 GBé‡äº§ï¼Ÿæ˜¾å¡æ—¥æŠ¥10æœˆ6æ—¥ - çŸ¥ä¹ _202510](https://zhuanlan.zhihu.com/p/1958257251954452012)

- [è‹±ä¼Ÿè¾¾RTX4080S 32Gæ¶¡è½®æ˜¾å¡æ–°é²œå‡ºç‚‰ - å°çº¢ä¹¦ _202510](https://www.xiaohongshu.com/explore/68dc7ac30000000005031f93?xsec_token=ABF5bwSFkHhKX5sU6IgENqtkol2WzQxpTy_DJscw_OC8M=&xsec_source=pc_search&source=web_explore_feed)
- é‚£4080sæ˜¯ä¸æ˜¯è¦æ¶¨ä»·äº†
  - å¤šåŠä¸ä¼šï¼Œ4090æ¶¨ä»·è™½ç„¶ä¸€æ–¹é¢æœ‰æ˜¾å­˜é­”æ”¹çš„åŸå› ï¼Œä½†æœ€ä¸»è¦ç¦å”®ï¼Œè€Œä¸”ä¹Ÿä¸äº§äº†ï¼Œå­˜é‡æŸç§ç¨‹åº¦è‚¯å®šå°‘, ä½†4080å¸‚é¢ä¸Šè¿˜æ˜¯å¾ˆå¤šçš„
- è¦æ˜¯æœ‰ä¸‰é£æ‰‡ç‰ˆçš„å°±å¥½äº†ï¼Œæ¶¡è½®çš„å£°éŸ³å—ä¸äº†

- [ç°åœ¨ä¸Š4080superæ˜¯ä¸æ˜¯49å¹´å…¥å›½å†›ï¼Ÿ - çŸ¥ä¹ _202408](https://www.zhihu.com/question/664986798)
  - é™¤é4080s è·Œå€’6k ä¸ç„¶çœŸçš„ä¸å€¼å¾—è´­ä¹°äº†ï¼Œé¡¶å¤©ä¹Ÿå°±5070tiçš„æ€§èƒ½ï¼Œ é—®é¢˜äººå®¶æœ‰ å¤§åŠ›æ°´æ‰‹4å•Šã€‚

- [4080Superä¹°å“ªä¸ªå¥½ï¼Ÿä¸è¶…é¢‘ï¼Œä½†æ±‚çœå¿ƒä¸æŠ˜è…¾? - çŸ¥ä¹](https://www.zhihu.com/question/651409618)
- åŒºåˆ«ä¸»è¦å°±æ˜¯ç”¨æ–™ã€å¤–è§‚ã€å“ç‰Œå’Œå”®å

- ä¸»è¦è®¾è®¡å¤„åŒºåˆ«ï¼šä¾›ç”µã€æ•£çƒ­è§„æ ¼
  - åç¡• ROG çŒ›ç¦½ï¼š18+3 70Aä¾›ç”µï¼Œå•16PINä¾›ç”µï¼Œ4*8mm+3*6mmé“œç®¡ï¼Œå‡çƒ­æ¿
  - åç¡• TUF ç”µç«ç‰¹å·¥ï¼š12+2 50Aä¾›ç”µï¼Œå•16PINä¾›ç”µï¼Œ5*8mm+3*6mmé“œç®¡ï¼Œåˆ†ä½“å¼é•œé¢é“œåº•
- æ•´ä½“è€Œè¨€ï¼Œåç¡•æº¢ä»·ä¸¥é‡ï¼Œè§„æ ¼ä¸€èˆ¬ï¼Œè€Œå¾®æ˜Ÿåªæœ‰æ——èˆ°å‹å·èƒ½çœ‹ï¼Œä¸ƒå½©è™¹çš„å…¨ç³»ç”¨æ–™éƒ½ä¸é”™ï¼Œå½±é©°æ˜¾å¡çš„æ€§ä»·æ¯”é«˜ï¼Œè¿˜æœ‰å½±é©°HOFåäººå ‚æ˜¯RTX4080Superè§„æ ¼é‡Œé¢ä¾›ç”µæœ€é«˜çš„å‹å·ï¼Œè¿˜æœ‰ï¼Œä¸‡ä¸½ç›–æ‹‰å¤šè™½ç„¶æ˜¯ä»·æ ¼æœ€ä½çš„ï¼Œä½†ä¸æ˜¯ç”¨æ–™æœ€ä¸çš„ã€‚
  - ä¸‹é¢ä¸¤æ¬¾ä¸ºè‰¯å¿ƒå‹å·ï¼Œå¯æ— è„‘å…¥ï¼š 
  - **å½±é©°RTX4080 SUPER æ˜Ÿè€€OCï¼Œä¸ƒå½©è™¹RTX4080 SUPER AD OC**
  - æ——èˆ°å‹å·æ¨èï¼šä¸ƒå½©è™¹ç«ç¥å’Œæ°´ç¥ã€å¾®æ˜Ÿè¶…é¾™ï¼ˆé™ä»·é«˜å¯è€ƒè™‘ï¼‰ã€å½±é©°åäººå ‚

- æ¯ä¸ªå“ç‰Œä¹Ÿæœ‰æ——èˆ°å’Œæ¬¡æ——èˆ°ä¹‹åˆ†
  - æ¯”å¦‚å¾®æ˜Ÿå°±æœ‰ä¸‡å›¾å¸ˆï¼Œé­”é¾™ï¼Œè¶…é¾™ä¹‹åˆ†ï¼Œåˆ†åˆ«å¯¹åº”çš„å…¥é—¨æ¬¾ï¼Œè¿›é˜¶å’ŒæœŸé—´ï¼Œä»·æ ¼ç›¸å·®å‡ ç™¾ã€‚
  - å¦å¤–å°±æ˜¯æ”¯æŒä¸ªäººé€ä¿çš„ä¸ƒå½©è™¹ï¼Œå¯¹äºå”®åè¿™å—æ¥è¯´ç®€ç›´ä¸è¦å¤ªçœå¿ƒäº†ï¼Œ ä¸ƒå½©è™¹ä¸€æ¬¾Ultra W OCï¼Œå¦å¤–ä¸€æ¬¾AD OC åŒºåˆ«ä¸æ˜¯å¾ˆå¤§ï¼Œä¸»è¦æ˜¯å¤–è§‚åŒºåˆ«

- å¾ˆå¤šéƒ½æ˜¯æ¨èå¾®æ˜Ÿã€åç¡•ã€ä¸ƒå½©è™¹çš„ï¼ŒæŠŠå¾¡ä¸‰å®¶çš„æŠ€å˜‰ç»™æ¼äº†ï¼Ÿ
  - å› ä¸ºæŠ€å˜‰è¾±åå•Š

- ## [å¦‚ä½•è¯„ä»·RTX2080Ti 22Gé­”æ”¹ç‰ˆï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/8164944069)
  - è¿™å¼ å¡çš„ä»·æ ¼åœ¨2600ä¸Šä¸‹ã€‚åŒä»·ä½æˆ–é¢„ç®—å†…çš„Nå¡è¿˜æœ‰RTX 4060 8Gï¼ˆä»·æ ¼å·®ä¸å¤šï¼‰ï¼ŒTESLA P100ï¼ˆè¿™å¼ åªè¦1600å·¦å³ï¼‰

- [æˆ‘ä¹°è®¡ç®—å¡æ—¶åšçš„åŠŸè¯¾ å¯¹æ¯”è¡¨](https://www.zhihu.com/question/8164944069/answer/1940267056307077954)

- [What are your /r/LocalLLaMA "hot-takes"? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1obb4c4/what_are_your_rlocalllama_hottakes/)
  - Currently, no so-called AI PC or unified memory device can match the performance of a similarly priced combination of GPUs and server CPUs.
  - vLLM and CUDA 13 are dropping support for GPUs with SM7.5 and below, so buying any pre-Ampere GPU (e.g., 2080 Ti, V100) is not a sustainable long-term choice. In my opinion, only NVIDIA's Ampere (and newer) architectures and AMD's RDNA 4 are truly viable for AI workloads.

- 2080Ti~22gï¼šç›®å‰(2024å¹´12æœˆåº•)ï¼Œå¯ä»¥æ‰¾åˆ°2400-2500åº—ä¿ä¸€å¹´çš„ï¼Œå°½é‡é€‰å£ç¢‘å¥½çš„ã€‚
  - è¿™å¼ å¡èƒ½åŠ›å‡è¡¡ï¼Œæ˜¯å¹³æ°‘/åƒåœ¾ä½¬çš„Aiç¥å™¨ï¼ŒåŒæ ·ä¹Ÿæ˜¯æ¥è·¯ä¸æ˜nvéª‘å£«çš„å…¸èŒƒï¼Œç”Ÿäº§åŠ›è€æ‰‹ã€‚
  - å¤§æ˜¾å­˜ï¼Œé«˜ç®—åŠ›ï¼Œé«˜å¸¦å®½ï¼ŒåŠŸè€—åˆç†ï¼Œå™ªéŸ³æ•£çƒ­å¯æ¥å—ï¼Œè¿˜æ”¯æŒnvlinkæ‰©å±•ã€‚
  - å®ƒçš„å”¯ä¸€ç¼ºç‚¹å°±æ˜¯Turingæ¶æ„æœ‰ä¸€ä¸¢ä¸¢è€äº†ï¼Œä½†æ¯”voltaåˆè¦å¥½ä¸€ä¸¢ä¸¢ã€‚è‡³äºçŸ¿ä¸çŸ¿

- 2500é™„è¿‘çš„åŒä¸€ä»·ä½ï¼Œèƒ½ä¹°åˆ°ï¼š
  - V100-sxm2-16g~PCIe(è½¬æ¥å¡ï¼Œè®­ç»ƒå‘)
  - V100-pcie-16g-å®šåˆ¶(æ ¸å¿ƒæ¬æ¿ï¼Œè®­ç»ƒå‘)
  - 2080Ti~22g(æ˜¾å­˜æ‰©å®¹ï¼Œè®­ç»ƒæ¨ç†å…¼é¡¾)
  - 3070~16g(æ˜¾å­˜æ‰©å®¹ï¼Œæ¨ç†æ—¥å¸¸å…¼é¡¾)
  - 4060ti-8g(16gè¦æ¡æ¼ï¼Œæ¨ç†æ—¥å¸¸ç¯ä¿)
- 2025å¹´5æœˆåŠ å‡ å—å¡
  - A3000m~pcie(12Gæ˜¾å­˜ï¼Œç§»åŠ¨ç‰ˆé­”æ”¹ï¼ŒæŒ‘ä¸»æ¿ï¼Œ Â¥1400)
  - T10(16Gæ˜¾å­˜ï¼Œæ¨ç†æ€§ä»·æ¯”é«˜ï¼Œä¸€å®šè¦æå¥½æ•£çƒ­ï¼Œ Â¥1400)
  - 3080~20G(æ¬æ¿æ‰©å®¹ï¼Œåº”å¯¹3090æ¶¨ä»·ï¼Œæ³¨æ„æ•£çƒ­ï¼ŒÂ¥3500)
  - 5060Ti-16G(ä»£æ›¿4060tiï¼Œå…¼å®¹æ€§é€æ¸å¥½è½¬ï¼Œ Â¥3500)

- æˆ‘è®¤ä¸ºï¼Œæ·±åº¦å­¦ä¹ çš„ç”Ÿäº§åŠ›æå‡å…³é”®åœ¨äºæ··åˆç²¾åº¦è®­ç»ƒ(mixed-precision-training)å’Œä½ç²¾åº¦æ¨ç†
  - é™¤éä½ æœ‰éå¸¸æ˜ç¡®çš„ç›®æ ‡ï¼Œæ¯”å¦‚ä¾èµ–æ˜¾å­˜å¤§å°çš„ä¼ ç»Ÿå•ç²¾åº¦è®¡ç®—ï¼Œå¹¶ä¸”æœ‰ä¸¥æ ¼çš„é¢„ç®—ï¼Œæ‰è€ƒè™‘p100/p40
  - V100å¯ä»¥æä¾›è¶…è¶Š3090/4080çš„æ··åˆç²¾åº¦ç®—åŠ›ï¼Œé€‚åˆæ·±åº¦å­¦ä¹ è®­ç»ƒï¼Œä½†å› ä¸ºvoltaä½œä¸ºæœ€åˆä»£tensor coreï¼Œä¸æ”¯æŒint8/int4åŠ é€Ÿï¼Œé‡åŒ–æ¨ç†æ²¡æœ‰ä¼˜åŠ¿ã€‚ç›®å‰SXMç‰ˆæœ¬ä»·æ ¼ä¸é”™ï¼Œä½†è½¬æ¥pcieéœ€è¦æŠ˜è…¾ã€‚
  - 2080Ti~22gï¼Œç‰¹ç‚¹å¦‚å‰ï¼Œæ¨èã€‚ç²¾åº¦ä¸æ”¯æŒtf32/bf16/fp8/fp4ï¼Œåº“ä¸æ”¯æŒflash-attn:2+(å·²æœ‰ç§»æ¤ç‰ˆæœ¬å¯è§)ï¼Œå½±å“éœ€è¦æ ¹æ®å®é™…ç ”åˆ¤ã€‚
  - 3070~16gæˆ–è€…æ‹©æœºæ¡æ¼ä¸Š4060ti-16gï¼Œæ›´é€‚åˆå…¥é—¨é€‰æ‰‹ï¼Œä»€ä¹ˆéƒ½æƒ³è¯•è¯•ï¼Œæ¨èæ–°æ¶æ„ã€‚è¿™ä¸ªä»·ä½ä¸Šç›®å‰è¿˜èƒ½ä¹°åˆ°A2ï¼Œä½†åªæœ‰3050çš„æ€§èƒ½æ˜æ˜¾ä¸å¤ªå¤Ÿçœ‹
- Ampereå’ŒAdaæ¶æ„ç›¸æ¯”Turingçš„ä¼˜åŠ¿ï¼Œä¸€æ˜¯å·¥è‰ºæ”¹è¿›å¸¦æ¥çš„èƒ½è€—ä¼˜åŒ–ï¼ŒäºŒæ˜¯æ”¯æŒæ›´å¤šçš„æ•°æ®ç±»å‹åœ¨tensoråŠ é€Ÿï¼Œæ¯”å¦‚bf16ç”šè‡³fp8ï¼Œé™„å¸¦å°±æ˜¯flash-attention(2+)ä¾èµ–ç¡¬ä»¶æ¶æ„
- ä»CUDA **12.8**å¼€å§‹ï¼Œè‹±ä¼Ÿè¾¾å®˜æ–¹ä¸å†å¯¹maxwell(cc 5.2/5.3)ã€pascal(cc 6.0/6.1)ã€volta(cc 7.0) æä¾›æ›´æ–°ï¼Œ**é©±åŠ¨è¿˜å¯ä»¥æ­£å¸¸å®‰è£…ï¼ŒåŠ é€Ÿå¡ä¹Ÿå¯ä»¥æ­£å¸¸ä½¿ç”¨**ï¼Œä½†å®ƒä»¬è¢«æ ‡è®°ä¸ºè¿‡æ—¶æ¶æ„ï¼Œé€‰æ‹©æ—¶éœ€è¦è¯„ä¼°ã€‚

- é­”æ”¹æŠ€æœ¯å·²ç»éå¸¸æˆç†Ÿäº†ï¼Œç¨³çš„ä¸€æ‰¹ï¼Œæˆ‘ä¸Šäº†4å—æ¶¡è½®æ•£çƒ­çš„ï¼Œå–å®¶é€äº†NVlink, ä¸åˆ°1wå—é’±ï¼ŒæœåŠ¡å™¨å°±æœ‰äº†88Gæ˜¾å­˜

- åªèƒ½ç›¸é‚»ä¸¤å¼ å¡äº’è”ï¼Œè€Œä¸”ä¸¤æ¡nvlinké€Ÿç‡ä¸€èˆ¬~å½“ç„¶æ¯”PCIeè¿˜æ˜¯å¥½å¤šäº†
  - å®é™…è·‘ollamaæ¨ç†ï¼Œnvlinkæ²¡èµ·ä»€ä¹ˆä½œç”¨

- cuda12 ç”¨ä¸äº†è¿™ä¸ª2080ti
  - èƒ½ç”¨ï¼Œ20ç³»è¦è¯´æœ‰å•¥å¼Šç«¯é‚£å°±æ˜¯ä¸æ”¯æŒbf16ï¼Œè€Œä¸”è¿™å¡ç°åœ¨çœ‹æ€§èƒ½ä¹Ÿä¸è¡Œäº†ï¼Œä¹Ÿå°±5060çš„æ€§èƒ½
- p106è¿™ä¸ª10ç³»çš„éƒ½èƒ½è£…cuda12
- åˆ«è¯´pæ¶æ„äº†ï¼Œæˆ‘tesla m40éƒ½è£…ä¸Šäº†12

- 20ç³»ä¸æ”¯æŒbf16å’Œfp8ï¼Œæœ‰äº›æ–°æ¨¡å‹æ ¹æœ¬å°±è·‘ä¸äº†
  - æˆ‘ç”¨çš„2080åæ­£ç°åœ¨å•¥æ¨¡å‹éƒ½èƒ½è·‘ï¼Œä¸»è¦æ˜¯ç»˜ç”»

- æ¨èï¼Œæ€§ä»·æ¯”æ˜¯çœŸçš„é«˜ï¼Œä½œä¸ºæ¨ç†å¡ååˆ†ä¼˜ç§€ï¼Œä½†è®­ç»ƒçš„è¯è¦æ…é‡ã€‚å‰æ®µæ—¶é—´ä¹°äº†å…­å¼ åœ¨å®¶æ­äº†ä¸€ä¸ªå…­å¡æœºå™¨ï¼Œç”¨äº†å¿«ä¸€å¹´äº†æ²¡å‡ºè¿‡é—®é¢˜ï¼Œæ€»å…±åŠ èµ·æ¥132gæ˜¾å­˜æœ€ç»ˆèŠ±è´¹æ‰ä¸åˆ°2wã€‚
  - ç¼ºç‚¹ä¹Ÿæ˜¯æœ‰çš„ï¼Œæœ€å¤§çš„é—®é¢˜å°±æ˜¯æ¶æ„æ¯”è¾ƒè€ï¼Œä¸æ”¯æŒbf16ä»¥åŠä¸€äº›æ¯”è¾ƒæ–°çš„ç®—æ³•ã€‚ç›®å‰å‡†å¤‡æ¢æˆ3080æ”¹20gæˆ–è€…æä¸¤å¼ 4090æ”¹48gè·‘è·‘è®­ç»ƒï¼Œé¡ºä¾¿æŠŠä¹‹å‰2080tiçš„nvlinkæ­ä¸Š
  - æœ€åï¼Œå¦‚æœè¦æè®­ç»ƒï¼Œå»ºè®®è¿˜æ˜¯ä¸Š30ç³»ä»¥ä¸Šçš„ã€‚ä¸ç„¶ä¼šå› ä¸ºæ¶æ„å¤ªè€é‡åˆ°å¾ˆå¤šå‘ã€‚
  - å¾…æœºæ¯å¤©5åº¦ç”µï¼Œç«åŠ›å…¨å¼€æ¯å¼ å¡250wï¼Œä¸€å°æ—¶å°±1.5åº¦

- 2080tiæ˜¯äºŒä»£NVLinkï¼Œå’Œ3090ä¸‰ä»£nvlinkä¸é€šç”¨å§ã€‚

- 4090æ²¡æœ‰sliæˆ–è€…nvlink
  - lzçš„å…­å¡æœºä¹Ÿæ²¡æœ‰å¾€SLIä¸Šæ’ä¸œè¥¿å•Šã€‚

- Turingæ¶æ„çš„tensor coreèƒ½è·‘fp16å’Œint8ï¼Œä½†æ˜¯ç›®å‰çœ‹bf16æ˜¯è¶‹åŠ¿ï¼Œè¿˜æ˜¯å¾—æ…é‡
  - 30ç³»ä»¥ä¸Šçš„å¯ä»¥è·‘flash attention

- 4090 48GæœåŠ¡å™¨éƒ½æœ‰äº†ï¼Œæˆ‘ç°åœ¨å°±åœ¨ç”¨8å¡çš„
  - å¤§è‡´è®²ä¸‹ä½“éªŒï¼Œæ”¯æŒbf16çš„æ··å’Œç²¾åº¦æ¨¡å‹è®­ç»ƒåœ¨å‚ç…§80GA100çš„batch-sizeçš„æƒ…å†µä¸‹ï¼Œè™½ç„¶4090è¦å¼€grad_accumulation=2ï¼Œä½†æ˜¯é€Ÿåº¦æ¯”a100å¿«ä¸€äº›ï¼Œèƒ½åˆ°5.5:7.3 s/stepã€‚ ä¸æ”¯æŒæ··å’Œç²¾åº¦çš„æ¨¡å‹å°±è¦æ…¢a100ä¸€å€äº†
  - æ²¡nvlink, è·‘çš„æ˜¯VLAçš„è®­ç»ƒï¼Œä¸€ä¸ªæ˜¯RDTï¼Œä¸€ä¸ªpi0ï¼Œæ ¹æœ¬ä¸èƒ½å…¨å‚æ•°ï¼Œè™½ç„¶ioæ²¡å¡ä½ï¼Œä½†æ˜¯å¦‚æœæ¨¡å‹æ²¡å¼€bf16çš„æ··å’Œç²¾åº¦è®­ç»ƒçš„è¯æ…¢ä¸€å€å¤šäº†

- é­”æ”¹çš„2080tiä½ è®°å¾—ä¹°ä¸‰é£ä¸è¦ä¹°æ¶¡è½®å¡ï¼Œæ¸©åº¦é«˜ï¼Œå™ªéŸ³å¤§ï¼Œå®¹æ˜“å—ä¸äº†çš„ï¼Œæˆ‘å°±æ˜¯ä»æ¶¡è½®å¡æ¢æˆäº†ä¸‰é£ï¼Œå™ªéŸ³é—®é¢˜åŸºæœ¬è§£å†³

- 3000ä»¥å†…é­”æ”¹çš„2080tiç¡®å®å°±æ˜¯æœ€ä½³é€‰æ‹©ï¼Œ
  - è¿™å°ä¸€å¹´ç¾¤é‡Œå‡ ç™¾å·äººç¿»æ¥è¦†å»è®¨è®ºè¿‡æ— æ•°æ¬¡ï¼Œæœ€ç»ˆç»“è®ºéƒ½æ˜¯é­”æ”¹2080tiæ€§ä»·æ¯”æœ€é«˜
  - è¿™æ˜¯ä½ èƒ½å¾—åˆ°çš„æœ€ä¾¿å®œçš„20gä»¥ä¸Šæ˜¾å­˜ä¸”æ€§èƒ½è¿‡å¾—å»çš„å¡äº†ï¼Œæˆ‘è‡ªå·±ä¹Ÿç‚¼ä¸¹ç©å„¿ï¼Œå¡é’±æ˜¯èµšå›æ¥äº†çš„

- ä¸æ”¯æŒbf16æ˜¯ç¡¬ä¼¤ï¼Œå‰©ä¸‹çš„å°±æ˜¯ä½ ä¹°äº†å¤§æ¦‚ç‡å°±æ˜¯æ¥ç›˜ä¾ äº†ï¼Œè¿™å¡åœ¨æœªæ¥ä¸ä»…æ‰“æ¸¸æˆçš„çœ‹ä¸ä¸Šï¼Œç‚¼ä¸¹çš„ä¹Ÿçœ‹ä¸ä¸Šã€‚

- åœ¨2kå‡ºå¤´çš„ä»·ä½ä¸ç®—å·®ï¼Œä½†æ˜¯V100 16GBå’Œ3080 20GBçš„æ€§ä»·æ¯”æ›´é«˜

- 3080 20gè¦3åƒå¤šäº†
  - èƒ½è·‘BF16å°±å€¼å›å¤šçš„600-700äº†ã€‚å¦‚æœä¸è¦ä¿ä¿®çš„è¯é—²é±¼æœ‰2600å·¦å³çš„

- ## [2025å¹´AIæœ¬åœ°éƒ¨ç½²æ€§ä»·æ¯”ä¹‹ç‹ï¼åŒå¡V100ï¼32Gæ˜¾å­˜ï¼Œä½ä»·é«˜èƒ½ï¼Œç¢¾å‹2080Ti 22G - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1927666998030078159)
- åƒé—®3 32Bæ¨¡å‹åœ¨V100ä¸Šçš„tokenç”Ÿæˆé€Ÿåº¦ä¸ºæ¯ç§’20.34ä¸ªï¼Œ2080Tiåˆ™ä¸º13.43ä¸ªï¼›DeepSeek R1 32Bæ¨¡å‹åœ¨V100ä¸Šçš„é€Ÿåº¦ä¸ºæ¯ç§’21.28ä¸ªï¼Œ2080Tiä¸º18ä¸ªã€‚

- V100ä¸æ”¯æŒflash- attentionã€bf16ã€awqã€sglangæ–°ç‰ˆæœ¬ä¹Ÿä¸æ”¯æŒv100äº†ï¼Œæ˜¾å¡å¤ªè€äº†ï¼Œæ–°çš„åŠ é€Ÿç­–ç•¥ç”¨ä¸äº†ï¼Œé€Ÿåº¦è¿˜ä¸å¦‚æ¶ˆè´¹å¡å‘¢ã€‚
- å¯¹SDè·‘å›¾æ¥è¯´ï¼Œä¸æ”¯æŒint4ã€fp8ã€fp4ï¼Œä¸èƒ½ç”¨nanchakuåŠ é€Ÿã€‚SDä¹Ÿä¸èƒ½å¤šå¡æ˜¾å­˜å åŠ ã€‚
- stable Diffusionè·‘å›¾ï¼Œå¾ˆå¤šæ¨¡å‹éƒ½ç”¨ä¸äº†çš„ã€‚æ²¡å•¥æ„æ€

- å¾ˆå¤šæ¨ç†ç‰¹æ€§ä¸æ”¯æŒï¼Œè¿œè¿œæ²¡æœ‰2080tiå¥½

- V100ç¡®å®šä¸€åƒå°±èƒ½ä¹°åˆ°ï¼Ÿ
  - æ ¸å¿ƒæ¿å§ï¼Œæ•£çƒ­è½¬æ¥åŠ èµ·æ¥16gçš„3kï¼Œ32gçš„5kï¼Œæ¯•ç«Ÿæ˜¯6å¹´å‰çš„ä¸œè¥¿äº†
- å¤§æ¦‚1000å·¦å³å¯ä»¥æå®šçš„ã€‚æˆ‘éƒ½1000ä¸€å¥—å–çš„ï¼Œè¿˜æ˜¯æ”¹çš„æ°´å†·ã€‚nvlink 32gï¼Œä¹Ÿå°±2300å–äº†

- ## [èŠä¸€èŠMi50è¿™å¼ æ˜¾å¡ï¼ˆè‡ªè´¹è´­å…¥ï¼‰ - çŸ¥ä¹ _202508](https://zhuanlan.zhihu.com/p/1935428196653838934)
- åœ¨500å…ƒä»·ä½ï¼Œè¿™å¼ å¡æ‹¥æœ‰å…¶ç‹¬æœ‰çš„16Gå¤§æ˜¾å­˜ï¼Œæƒ³æœ¬åœ°éƒ¨ç½²LLMçš„å¯ä»¥å†²ä¸€å†²ã€‚è´´å¿ƒçš„å–å®¶å·²ç»å¸®æˆ‘åˆ·å¥½äº†RadeonProVllçš„vbiosï¼ˆæç¤ºï¼šè¿™å¼ å¡åªæœ‰åœ¨é•­7çš„vbiosä¸‹æ‰ä¼šä¸»åŠ¨è¿›è¡Œè§†é¢‘è¾“å‡ºï¼Œå¾ˆé‡è¦ï¼ï¼‰ï¼Œ
  - æœ¬åœ°éƒ¨ç½²DeepSeek32Bçš„æ¨¡å‹ç•¥æ˜¾åƒåŠ›ï¼Œæ¯•ç«Ÿæ¶æ„æŒºè€äº†è€Œä¸”æˆ‘åªæœ‰ä¸€å¼ å¡ï¼Œæœ‰æ¡ä»¶çš„å¯ä»¥å¤šå¡äº¤ç«ï¼ˆæ”¯æŒçš„ï¼Œæœ‰æ¥å£ï¼‰ï¼Œ
  - æ¸¸æˆæ–¹é¢æµç•…è¿è¡Œå€’æ˜¯ä¸èµ–ï¼Œ1080Pä¸‹ï¼Œæ­é…E5-2680V4ã€DDR4 2400çš„æƒ…å†µä¸‹ï¼ŒCS2é«˜ç”»è´¨å…¨å±€ç¨³å®š150å¸§ï¼Œæµç•…è¿è¡Œè‚¯å®šç§°å¾—ä¸Šï¼Œ
- å¯¹æˆ‘æœ€æœ‰å½±å“çš„åº”è¯¥å°±æ˜¯è¿™ä¸ªæ²ƒä¼¦å¡çš„ å™ªï¼éŸ³ï¼ çœŸçš„å¾ˆTMåµï¼Œè·Ÿä½“è‚²è€å¸ˆä¸åœå¹å£å“¨ä¸€æ ·
  - ä½†å¥½åœ¨å·²ç»æœ‰åŒé£æ‰‡ç‰ˆæœ¬ï¼Œå½“æ—¶å›¾ä¾¿å®œåŠ ä¸Šæ²¡ä¹°è¿‡æ¶¡è½®å¡ï¼Œè„‘å­ä¸€çƒ­ï¼Œä¸€å¤±è¶³æˆåƒå¤æ¨å•Š 
- æ˜¾å¡å…‰æœ‰ç¡¬ä»¶æ˜¯ä¸è¡Œçš„ï¼Œè½¯ä»¶å±‚é¢ä¹Ÿå¾—æ‰“ç£¨æ‰“ç£¨
  - è¿™å¼ å¡åœ¨è¿™ä¸ªä»·ä½æ®µï¼Œå°±ä½œä¸ºä¸€å¼ ä¸“ä¸šç”Ÿäº§åŠ›å¡æ¥è¯´ï¼Œæˆ‘å¯ä»¥æ‹ç€èƒ¸è„¯è¯´å‡ºAMD YESï¼æ¯•ç«Ÿè¿™ä¸ªä»·ä½çš„Nå¡ä¸æ˜¯1065ã€1063å°±æ˜¯40hxè¿™ç§è¦ä¹ˆæ˜¾å­˜å°ï¼Œè¦ä¹ˆè§†é¢‘ç¼–è§£ç èƒ½åŠ›è¢«ç å¾—æ¯”è·¯æ˜“åå…­è¿˜æƒ¨å†æˆ–è€…æ˜¯é­”æ”¹é”é©±ï¼Œæˆ‘æ˜¯å–œæ¬¢å°æœºç®±çš„äººï¼Œå°æ¿è¿™ç§åªæœ‰ä¸€ä¸‹x16æ§½çš„æ— å¤´éª‘å£«å¯ç”¨ä¸äº†

- è¿™å¡çš„minidpå¥½åƒä¸å¸¦éŸ³é¢‘è¾“å‡º
  - æˆ‘æ’è€³æœºç”¨çš„

- åŠŸè€—å¤ªé«˜äº†ï¼Œä¸é€‚åˆçœ‹è§†é¢‘ä¸Šç½‘ç”¨ã€‚

- https://www.zhihu.com/question/628771017/answer/1928750084457230617
  - å…³äºLLMæ¨¡å‹ç”Ÿæˆé€Ÿåº¦ï¼šNå¡CUDAå¼ºï¼ŒAå¡ä¸€èˆ¬ã€‚ä¸ªäººç”¨é€”ä¸€èˆ¬æ›´åœ¨ä¹æ¨¡å‹æ™ºå•†æ°´å¹³è€Œä¸åœ¨ä¹ç”Ÿæˆé€Ÿåº¦ï¼Œå› æ­¤æ˜¾å­˜å¤§å°å°±æ˜¯ç‹é“ï¼Œæ— è„‘é€‰å¤§æ˜¾å­˜/å†…å­˜é…ç½®ï¼ŒCPUæ¨ç†æ€§ä»·æ¯”æ›´é«˜ï¼Œå¯ä»¥è¿è¡Œè¶…é«˜å‚æ•°ï¼Œå¦‚deepseek 671Bï¼ŒCPUæ¨ç†åƒå†…å­˜å¸¦å®½ï¼Œé€šé“æ•°ä¸€å®šè¦æ»¡ã€‚å¦‚æœ‰ç”Ÿäº§åŠ›é€Ÿåº¦éœ€æ±‚ï¼Œæ— è„‘é€‰4090 48Gã€‚
  - AMD MI50 32Gä¸€ä»£ç¥å¡ï¼Œåœ¨windowsä¸‹åªèƒ½ç”¨Vulkanè·‘ï¼Œåœ¨linuxä¸‹å¯ä»¥ç”¨ROCmï¼Œ70Bq4ç”Ÿæˆé€Ÿåº¦10tk/sï¼Œæ€§ä»·æ¯”éå¸¸é«˜ï¼Œå¯æƒœè·‘ä¸äº†comfyUIï¼Œæƒ³è¦ç”»å›¾å¿…é¡»è‡ªå·±æ‰‹ç»„æ•£è£…webuiã€‚
  - SDXLæ¨¡å‹å ç”¨æ˜¾å­˜ä¸å¤§ï¼Œä»ç„¶æ˜¯8GBå¤Ÿè·‘ã€‚ä½†æ˜¯åªèƒ½åœ¨Nå¡ä¸Šè·‘çš„å¥½ï¼ŒAå¡ä¼˜åŒ–å¾ˆå·®ã€‚
  - æ–‡ç”Ÿè§†é¢‘æ¨¡å‹å ç”¨æ˜¾å­˜å·¨å¤§ï¼Œä¸€èˆ¬æ¶ˆè´¹çº§æ˜¾å¡åªèƒ½ç”Ÿæˆ480pæ¸…æ™°åº¦1åˆ†é’Ÿä»¥å†…æ—¶é•¿çš„è§†é¢‘ï¼Œ720påŠä»¥ä¸Šå¿…é¡»ç”¨å¤§äº48Gæ˜¾å­˜çš„Nå¡ã€‚

- [å¤§èˆ¹é å²¸ï¼ŒAMD MI50 16/32Gæ˜¾å¡æ™šæ¥ä¸€æ­¥ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1896388031436530191)

- [MI50 32g æˆ‘è¯´è¿™ä¸ªå°±æ˜¯å² - çŸ¥ä¹ _202504](https://zhuanlan.zhihu.com/p/1893227599272067712)
- ä»·æ ¼æ˜¯720åŒ…åœ†ï¼Œç°åœ¨è¢«jsqç‚’ä½œè€é«˜äº†
- è¿™ä¸ªå¡åˆ·ä¸äº†biosï¼Œä¸å¯èƒ½ æ˜¾ç¤ºè¾“å‡º
- amdé€šç—…ï¼Œä½ æƒ³ç©aiç”Ÿæ€æ”¯æŒç¨€çƒ‚
- æ€»ç»“ï¼šé¸¡è‚‹ä¸­çš„é¸¡è‚‹ï¼Œä¸å¦‚mi50ï¼Œ2080ti 22g
- è²Œä¼¼åè½¬äº†ï¼Œè¿™ä¸ªå¡å¬è¯´èƒ½åˆ·biosæœ‰è§†é¢‘è¾“å‡ºï¼Œè¿™ä¸ªç‰¹æ€§å¯¼è‡´ä»–å¯èƒ½ä¸ä¼šå¤ªfwï¼Œä½†æ˜¯æˆ‘è§‰å¾—32gçš„æ˜¾å­˜è¿˜æ˜¯å¤ªå¤§äº†æ²¡å¿…è¦ï¼Œä½œä¸ºä½å»‰ç®—åŠ›å¡æ¥è¯´ç¡®å®è¿˜å¯ä»¥
  - 2080Ti 22gåœ¨å‰é¢é¡¶ç€å‘¢ã€‚GCNæ¶æ„ç©æ¸¸æˆçœŸçš„ä¸é”™ä½†è®¡ç®—æ€§èƒ½å¤ªä½ï¼Œå³ä¾¿ä¸è€ƒè™‘åŠŸè€—æ•£çƒ­å™ªéŸ³ï¼Œç©æ¸¸æˆ16Gç‰ˆå¤Ÿå¤Ÿçš„

- å¦‚æœç”Ÿæ€å¥½çš„è¯ 32g æ˜¾å­˜æ€ä¹ˆå¯èƒ½æ‰ä¸åˆ°ä¸€åƒï¼Œ2080 ti 22g ç°åœ¨éƒ½è¦å¿«ä¸‰åƒäº†ã€‚ä¸€å¼  2080 ti 22gèƒ½ä¹°ä¸‰å¼  mi 50 32gã€‚

- ä½ å¿˜äº†æä¸€ä¸ªå¼±ç‚¹ï¼Œ300ç“¦çš„åŠŸè€—ï¼Œ100ç“¦çš„æ€§èƒ½
  - å€’ä¸åªæ˜¯ç”µè´¹çš„é—®é¢˜ï¼Œè¿™æ ·ç”µæºæ•£çƒ­å™ªéŸ³éƒ½ä¼šå¢åŠ ï¼Œå¤ªéº»çƒ¦äº†

- ROCmçš„æ”¯æŒè¯´ä¸å®šå•¥æ—¶å€™å°±æ²¡äº†ï¼Œåˆ°æ—¶å€™å¹²çªçœ¼

- æˆ‘çš„ç»“è®ºæ˜¯ç›®å‰å•å¡è¿è¡Œ gemma3 27B æˆ–è€… qwq 32Bï¼ŒMi 50 32G æ˜¾å¡å°±æ˜¯ç­”æ¡ˆã€‚1000 å…ƒæ˜¾å¡å¸¦æ¥ 20tokens/sï¼Œè¿˜æœ‰è°ï¼Ÿ
  - æŒ‰ AMD å®˜æ–¹æ•™ç¨‹å®‰è£…ä¸€ä¸‹ rocm å’Œé©±åŠ¨ï¼Œrocm ç‰ˆæœ¬ 6.3.3 å·²ç»éªŒè¯ï¼Œ ä¸€å®šæ˜¯å¯ä»¥çš„ã€‚
  - æŒ‰ ollama å®˜æ–¹æ•™ç¨‹ï¼Œæ‰‹åŠ¨å®‰è£… ollama å’Œ ollama-rocmã€‚
- ç”¨LMStudioçš„Vulkanåç«¯å³å¯ï¼Œå…æŠ˜è…¾ROCmï¼ŒQwen3å’ŒGemma3éƒ½èƒ½è·‘ï¼Œé€Ÿåº¦æ¯”ROCmæ‰“ä¸ƒæŠ˜

- ç°åœ¨ktransformeræ”¯æŒamdæ˜¾å¡äº†ï¼Œå¼„ä¸ª512çš„ddr4å†…å­˜+å››å¼ MI50åº”è¯¥å¯ä»¥è·‘é€šDeepseek671bï¼Œç­‰ä¸€æ‰‹æœ€å¼ºæ€§ä»·æ¯”çš„Deepseekæ»¡è¡€ç‰ˆæ–¹æ¡ˆï¼Œå¸Œæœ›å¤§ä½¬å¯ä»¥è¯•è¯•

- 32Gçš„ï¼ŒåŒå¡å¯ä»¥è¿è¡Œ70Bæ¨¡å‹ï¼Œ6-8tokens/Sï¼ŒåŸºæœ¬ä¸Šå±äºå¯ä»¥ç”¨çš„èŒƒç•´äº†ï¼Œå¦‚æœæ˜¯VLLMä¼°è®¡è¿˜èƒ½æ›´å¿«ï¼Œæˆ‘ç”¨çš„ollamaï¼Œå°±æ˜¯æ•£çƒ­è¦æå¥½ï¼Œæˆ‘é‡Œé¢æ’æ¶¡è½®ï¼Œå¤–é¢é£æ‰‡å¸ï¼Œå‹‰å¼ºå¯ä»¥æ§åˆ¶

- æˆ‘è§‰å¾—å…¶å®è›®å¥½çš„, å¤§å®¹é‡å¹¶ä¸”å¤§å¸¦å®½çš„æ˜¾å­˜, å¹¶ä¸”è¿˜æœ‰æœåŠ¡å™¨çš„ç¨³å®šæ€§, è¿™ä¸ªå¡çš„32 64ç®—åŠ›é«˜çš„å¤¸å¼ , å’ŒV100ä¸€æ ·, ä½œä¸ºä¸€å¼ ä¸“ä¸šå¡æ˜¯ç›¸å½“å¤Ÿç”¨çš„

- ## [32GB Mi50's were getting so expensive that I ended up buying a 32GB w6800 for about the same price instead : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pob44f/32gb_mi50s_were_getting_so_expensive_that_i_ended/)
  - Much better prompt-processing than the Vega iGPU's
  - 512GB/s vs 1TB/s memory bandwidth on the Mi50. Per benchmarks the mi50 doesn't actually get 2x performance on token-gen, but it's definitely something. If TG is your highest priority this is reason to consider the Vega cards
- What is the performance difference? Mi50 vs 6800. Mi50 is not very useful, outside of text inferencing, which is does great. RDNA actually has compute capability, and is supported going forward CDNA looks more and more like RDNA5 or 6.
- the w6800 does pretty good. Its much faster in prompt processing. 6800 has more compute and modern design supported by amd, and works in windows without hacks. Its the much better generalist card.
- where?
  - eBay. Follow a bunch of the bigger hardware reseller accounts and you'll eventually find short-lived deals on workstation cards.

- ## [NVLINK port support for RTX 3090 Ti, RTX 4080/4090 - Gaming and Visualization Technologies / Raytracing - NVIDIA Developer Forums _202210](https://forums.developer.nvidia.com/t/nvlink-port-support-for-rtx-3090-ti-rtx-4080-4090/231140)
- I just checked RTX 3090 Ti, that also does not have NVlink port. Am I right?
  - Yes, still there is no support but data transmission over PCIe board

- Until NVIDIA thinks about bringing NVlink bridge back, RTX 3090 is the last gpu of this tech.

- ## [3090 and 3090ti sli/nvlink : r/nvidia _202203](https://www.reddit.com/r/nvidia/comments/tslbxy/3090_and_3090ti_slinvlink/)
- Nope, SLI/ NVLink needs the exact same card to be paired with in order for it to work and pool the memory. I meant to say that you need the exact same series of card, cant mix and match a 3090 with a 3090ti. But yeah it doesnt matter which brand of a particular series you get. 

  - ## [AMD MI50 32GB better buy than MI100? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o2x0bv/amd_mi50_32gb_better_buy_than_mi100/)
  - While it's officially dropped from ROCm 7, we can still get it to work if we copy some files manually.. obviously this will sooner or later stop working but then we'll have Vulkan.. which (with llama.cpp at least) seems to be almost at a performance-parity with ROCm (or faster?).
- Mi100 is still too expensive. I thought they all have vulkan support.

- As Mi50 owner, I would say this: ROCm is a pain in the ass. I see no reasons to buy better AMD cards cause you'll will constantly search for solutions for some software bugs. Vulcan won't save you on Mi50, as performance is equal or worse - vulcan seem do be not as optimized for that chip. With Mi100, you'll be in ever rpugher shape because this card is not popular amongst the enthusiasts, so nobody will optimize the software for it. Given that Mi50 33GB costs $120, buying Mi100 over it is a useless waste of money; Mi100 should go down in price significantly before it becomes reasonable for a homelabber.

- ## [rtx 4090 vs rtx 5090 vs rtx 4090 48gb vram? : r/StableDiffusion _202505](https://www.reddit.com/r/StableDiffusion/comments/1km4snx/rtx_4090_vs_rtx_5090_vs_rtx_4090_48gb_vram/)
- across various benchmarks rtx5090 is around 30% faster than rtx4090 in terms of "raw power". It also has native FP4 support, which can increase speed by ~x2 and reduce memory usage ~x2 for use-cases where fp4 models available.
  - So, choice depends really on your use-case.
  - If you run repetitive tasks and it fits into rtx5090 32GB VRAM, especially in fp4 format, then, this is the way.
  - as an all rounder and for different experiments, rtx4090 with 48GB can be more interesting.

- If you have a workflow with multiples models, vram is king as it allows you to avoid loading / unloading models. It's the slowest part of the process in generation. With enough vram, you can keep all models loaded, gaining a tremendous amount of time each generation.
  - Example: generating with model A which has very good prompt understanding but shit styles, glossy skin etc...then inpainting / upscaling with model B which has better style and so on.

- ## ğŸ†š [RTX 5090 vs RTX 4090 48GB (or RTX 6000) : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mo92ou/rtx_5090_vs_rtx_4090_48gb_or_rtx_6000/)
- The jump from 32GB to 48GB is a big jump, but the question is whether it's actually relevant today. 
  - With 32GB, you can run 24B, 27B, and 30B MoE at 8 bit and blazing fast speeds. You can run 32B at Q6, and even 49B at Q4. You can run basically any diffusion model as fast as possible with no limits at 8 bit.
  - 48GB lets you run the same models with higher context length, or lets you run 70B at Q4KM. The problem with this however, is that the 70B class of models has been stagnant for a very long time, so it's questionable if there's actually any benefit of running models in that size class right now. Any model larger than that is generally an MoE and will need partial offloading to RAM anyway. For example, if running GLM Air 106B, there's not going to be a massive speed difference between 32-48GB, because the bottleneck is the RAM.
  - The RTX 6000 Pro 96GB is an amazing card, but it generally has the same problem as the 48GB. With 96GB, that opens up the path for 70B and 100B, but not really any larger. You still won't be able to run 200B+ without partial offloading, which means that again, RAM is the bottleneck.

- One question I have is the context length. Using this calculator, for a 32B model at Q8, I'd have less than 4K context even with the KV cache at Q8. So if I'm adding files of any significant length to the context, then that also skews for more VRAM? Or can the context also be offloaded to RAM?
  - Yes, context does take up VRAM, and VRAM consumption scales linearly in proportion to context length. The exact amount of VRAM for a certain amount of context varies based on the size of the model, and architectural choices like Sliding Window Attention and Grouped Query Attention. Hence, the exact memory usage footprint is unique to the exact model, so that website is likely not perfectly accurate. That said, the context length takes up VRAM whether you use it or not, and does not increase automatically if you exceed it.
  - You can offload context to RAM, but that will cause a massive slowdown overall, and I can't really recommend it except for very specific use cases.

- If you're running multiple AI applications simultaneously, then yeah probably.

- FLUX for image generation needs something like 12GB.
  - WAN2.2 for video generation can be ran on as little as 8GB.
  - Qwen3-30B at Q4 requires 18.6GB, the Q8 is 36GB.
  - Blender for 3D assets recommends >8GB.
  - Photoshop recommends >4GB.

- ## [å•å¡åŒèŠ¯48GB é“­ç‘„Arc Pro B60 Dual Turboä¸“ä¸šå¡è§„æ ¼è¯¦è§£ - çŸ¥ä¹ _202505](https://zhuanlan.zhihu.com/p/1908505488590636946)
- ç¬¬äºŒä»£é”ç‚«Proä¸“ä¸šå¡ï¼ŒåŒæ ·åŸºäºâ€œBattleMageæˆ˜æ–—æ³•å¸ˆâ€æ¶æ„ï¼Œæœ‰ä¸¤æ¬¾äº§å“
  - Pro B50 16GBï¼Œ16ç»„Xeæ ¸å¿ƒï¼Œ2048spï¼Œ128bitæ˜¾å­˜å¸¦å®½ï¼ŒTBPæ•´å¡åŠŸè€—70w
  - Pro B60 24GBï¼Œ20ç»„Xeæ ¸å¿ƒï¼Œ2560spï¼Œ192bitæ˜¾å­˜å¸¦å®½ï¼ŒTBPæ•´å¡åŠŸè€—120-200w

- B580 24GBçš„ä¼ é—»å·²ä¹…ï¼Œä¸ç®—æ–°é²œï¼›æœ€å‡ºäººæ„æ–™çš„æ˜¯é“­ç‘„ï¼Œæ‹¿å‡ºæ¥çš„Pro B60äº§å“æ˜¯å•å¡åŒèŠ¯
  - åŒDP 2.1 + åŒHDMI 2.1aæ»¡è¡€è¾“å‡ºï¼Œå‡å¯æ”¯æŒ8K 60hzæˆ–4K 240hzï¼›ç”±ä¸Šè‡³ä¸‹ï¼Œåˆ†å±äºä¸¤ä¸ªä¸åŒçš„GPU
  - è¿™å¼ æ˜¾å¡å¹¶æœªé›†æˆPCIeæ¡¥æ¥èŠ¯ç‰‡ï¼ŒPCIe Gen5 x8 + PCIe Gen5 x8ï¼Œéœ€è¦ä¸»æ¿æœ¬èº«æ”¯æŒPCIe x16é€šé“æ‹†åˆ†ã€‚
  - æ‰€ä»¥ç”µè„‘è®¾å¤‡ç®¡ç†å™¨å†…æ˜¾ç¤ºï¼Œåº”è¯¥æ˜¾ç¤ºä¸ºä¸¤å¼ Pro B60 24GBï¼Œè€Œéå•å¼ Pro B60 48GBï¼Œåªæœ‰é€šè¿‡ç›¸åº”ç¨‹åºï¼Œæ‰èƒ½å®ç°GPUã€æ˜¾å­˜ç»Ÿä¸€è°ƒç”¨ï¼Œæ–™æƒ³ä¸ä¼šæœ‰é©±åŠ¨ç¨‹åºå±‚é¢çš„ç‰¹æ®Šæ”¯æŒï¼Œå’Œå…¶ä»–å“ç‰Œçš„å•å¡å•èŠ¯æ— å¼‚ã€‚

- è¿™ä¹ˆè¯´å°±æ˜¯ä¸¤ä¸ªæ˜¾å¡ï¼Œæ’åœ¨ä¸€ä¸ªæ’æ§½ï¼Œå¹¶ä¸”ä¸¤ä¸ªå¡éƒ½æ˜¯pcie5.08 è¿™æ ·æ¯ä¸ªå¡çš„å¸¦å®½éƒ½ç æ‰äº†pcie 5.0 *16 çš„ä¸€åŠã€‚ è¿™ç§ä¸œè¥¿æœ‰é¬¼ç”¨å•Šã€‚è¿˜ä¸å¦‚æ’ä¸¤ä¸ªæ˜¾å¡
  - 1ï¼Œç°åœ¨çš„GPUæ€§èƒ½è·‘ä¸æ»¡å¸¦å®½ï¼ŒåŒ…æ‹¬5090ç”¨PCIE x8éƒ½æ²¡é—®é¢˜ï¼›
  - 2ï¼ŒAIæ¨¡å‹æ¨ç†å…¶å®å¯¹GPUæ€§èƒ½è¦æ±‚ä¸é«˜ï¼Œä¸»è¦æ˜¯å¡æ˜¾å­˜ã€‚
  - 3ï¼ŒB580åŸç”ŸGen4 x8ï¼ŒB60å‡åˆ°Gen5 x8ï¼Œä¸¤GPUé›†æˆåˆ°ä¸€å—å„¿å¹¶æ²¡é˜‰å‰²å¸¦å®½ã€‚

- åç»­æ‰€æœ‰çš„é”ç‚«ä¸­é«˜ç«¯å®¶ç”¨æ˜¾å¡éƒ½ä¼šå¤åˆ»ï¼ŒQ3å‘å¸ƒçš„B770 16GBå½“ç„¶ä¹Ÿä¼šæœ‰ç›¸åº”çš„32GBä¸“ä¸šå¡ï¼›è¿AMDä¹Ÿè¡¨ç¤ºä¼šè·Ÿè¿›äº§å“ã€‚

- ç‰™è†è¦æ˜¯æœ‰ç±»ä¼¼nvlinkçš„ä¸œè¥¿é‚£ä»·å€¼è¿˜èƒ½ä¸Šå‡ï¼Œæ¯•ç«Ÿç”­ç®¡è®­ç»ƒè¿˜æ˜¯æ¨ç†ï¼Œç“¶é¢ˆéƒ½åœ¨è¿™ä¸ªpcie 5.0x16çš„64GB/s ä¸Šã€‚

- 48Gæ˜¾å­˜ç©SDç”»å›¾ä¸å¾—çˆ½æ­»ã€‚è¯è¯´ç°åœ¨SDå¯¹intelçš„æ”¯æŒå’‹æ ·äº†ï¼Ÿèƒ½ç©ä¸ï¼Ÿ
  - ä¸æ€ä¹ˆè¡Œï¼Œåªèƒ½pf16å¦åˆ™åŠ loraå°±ä¼šé»‘å›¾ï¼Œè®­ç»ƒç›®å‰æˆ‘è¿˜æ²¡è·‘é€šè¿‡ã€‚è€Œä¸”åŒèŠ¯å¡å®é™…æ˜¯ä¸¤å¼ å¡ï¼Œå¸¸è§çš„éƒ¨ç½²æ¡†æ¶comfyUIä¸æ”¯æŒå¤šå¡

- ## [æ˜¾å¡æ—¥æŠ¥10æœˆ8æ—¥ï½œç¬”ç”µRTX5090æ˜¾å­˜å®¹é‡24Gï¼Ÿ - çŸ¥ä¹ _202410](https://zhuanlan.zhihu.com/p/853223258)

- æ•´ç†ä¸‹ç¬”è®°æœ¬RTX5090çš„å¯èƒ½å‚æ•°ï¼Œå¯ä»¥çœ‹åˆ°ï¼Œå’Œå°å¼æœºçš„RTX5090å·®è·è¿˜æ˜¯æ¯”è¾ƒè¿œçš„ï¼Œè¿å°å¼æœºæ˜¾å¡ä¸€åŠçš„è§„æ ¼éƒ½æ²¡æœ‰ï¼Œå¯èƒ½æ˜¯æœ‰å²ä»¥æ¥ç¬”è®°æœ¬å’Œå°å¼æœºå·®è·æœ€å¤§çš„ä¸€å¼ æ——èˆ°æ˜¾å¡
  - æ­¤å¤–ï¼Œç¬”è®°æœ¬RTX5090ç›¸æ¯”äºä¸Šä»£ç¬”è®°æœ¬RTX4090è¿›æ­¥ä¹Ÿä¸å¤§ï¼Œæœ€å¤§çš„è¿›æ­¥å¯èƒ½å°±æ˜¯æ¢äº†é•å…‰GDDR7æ˜¾å­˜ï¼Œæ˜¾å­˜å¸¦å®½æ˜æ˜¾æå‡

- CPUèƒ½åŠ ä¸ª12400ä¸å¸¦Kä¸ï¼Ÿ
  - å·²ç»ä¸å¸¦Käº†

- [ç¬”è®°æœ¬ç”µè„‘5090å’Œ5080æ¸¸æˆå·®è·è¾ƒå°ï½œæ˜¾å¡æ—¥æŠ¥4æœˆ3æ—¥ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1890877968840119676)
  - æ ¹æ®VideoCardZæŠ¥é“ï¼Œæœ€è¿‘è‘—åç¡¬ä»¶åª’ä½“Notebook Checkæµ‹è¯•äº†ç›¸åŒé…ç½®çš„RTX5090ç¬”è®°æœ¬å’ŒRTX5080ç¬”è®°æœ¬ï¼Œè¿™ä¸¤å°ç¬”è®°æœ¬ç”¨äº†ç›¸åŒçš„æ¨¡å…·ï¼Œæ˜¾å¡åŠŸè€—ä¸Šé™éƒ½æ˜¯175ç“¦ï¼Œéƒ½æ­é…é”é¾™9955HX CPUï¼Œæ®è¯´è¿™æ¬¾XMGæ˜¯æœºæ¢°é©å‘½çš„æµ·å¤–è´´ç‰Œå‚ã€‚
  - åœ¨ã€Šèµ›åšæœ‹å…‹2077ã€‹ä¸­ï¼Œ4Kåˆ†è¾¨ç‡ä¸‹ä¸¤æ¬¾ç¬”è®°æœ¬çš„å¹³å‡å¸§ç‡å·®è·ä»…ä¸º12%ï¼Œè¿œè¿œå°äºä¸¤è€…çš„ä»·æ ¼å·®è·
  - æ ¹æ®VideoCardZç»Ÿè®¡ï¼Œç›®å‰åŒ—ç¾å¸‚é¢ä¸Šæœ€ä¾¿å®œçš„RTX5090ç¬”è®°æœ¬æ¯”æœ€ä¾¿å®œçš„RTX5080ç¬”è®°æœ¬è¿˜è´µäº†72%ï¼Œæ‰€ä»¥åœ¨æ¸¸æˆå·®è·è¿™ä¹ˆå°çš„æƒ…å†µä¸‹ï¼Œæ¯«æ— ç–‘é—®RTX5080ç¬”è®°æœ¬æ›´å€¼å¾—å…¥æ‰‹ã€‚
- å¤š8gæ˜¾å­˜åˆ©å¥½ç”Ÿäº§åŠ›ï¼Œæ¸¸æˆæ€§èƒ½çš„è¯åŠŸç‡ç»™ä¸è¶³çš„æƒ…å†µä¹Ÿä¸å¤§å¥½æå‡

- ## [å½“å‰4090ç¬”è®°æœ¬æ˜¾å­˜ä¸ºä»€ä¹ˆæ˜¯16gï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/599743802)
- ç§»åŠ¨ç‰ˆ4090çš„æœ¬è´¨: ç”¨äº†æ¡Œé¢4090çš„åå­—ï¼Œä½¿ç”¨äº†æ¡Œé¢4080çš„è§„æ ¼ï¼Œè·‘å‡ºäº†æ¡Œé¢4070 Tiçš„æ€§èƒ½

- ç§»åŠ¨ç‰ˆçš„4090ç¡¬ä»¶å‚æ•°åŸºæœ¬ä¸Šå°±æ˜¯å°å¼æœº4080
  - å°å¼æœºçš„4080æœ€é«˜åŠŸè€—åœ¨400wå·¦å³ï¼ˆä¸åŒå“ç‰Œå‹å·ç•¥æœ‰å·®å¼‚ï¼‰ï¼Œç§»åŠ¨ç‰ˆçš„4090æœ€é«˜åŠŸè€—é™åˆ¶åœ¨175wå·¦å³ï¼ˆä¸åŒæ•£çƒ­è§„æ¨¡ç¬”è®°æœ¬å¯èƒ½ç•¥æœ‰å·®å¼‚ï¼‰

- 256Bitçš„æ˜¾å­˜ä½å®½å’‹ä¸Š24GBï¼Ÿ

- ## [ç¬”è®°æœ¬ç«¯RTX4090å’ŒRTX4080å·®è·åˆ°åº•æœ‰å¤šå¤§ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/581865855)
- 2024å¹´8æœˆæ›´æ–°ï¼Œä»¥åç¡•ROGæªç¥8 è¶…ç«ç‰ˆç³»åˆ—ä½œä¸ºå‚è€ƒ
  - ä»å„é¡¹è·‘åˆ†ä¸­å¯ä»¥çœ‹åˆ°RTX4090ç›¸æ¯”RTX4080ï¼Œé«˜å‡º10-16%
  - RTX 4090 Laptopæ‹¥æœ‰å®Œæ•´çš„256ä½å¸¦å®½ï¼ŒRTX 4080 Laptopå…¶æ˜¾å­˜ä½å®½é™ä½åˆ°192

- ## [ç¬”è®°æœ¬çš„4090æ˜¾å¡ç›¸å½“äºå°å¼æœºçš„ä»€ä¹ˆæ˜¾å¡ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/594215351)
- è¯¥æ˜¾å¡ç”¨äº†å°å¼æœºRTX 4090çš„åå­—ï¼Œç¡¬ä»¶è§„æ ¼ä¸Šå’Œå°å¼æœºRTX 4080ç›¸ä¼¼ï¼Œæ€§èƒ½ä¸Šå’Œå°å¼æœºRTX 4070 TIç›¸è¿‘ã€‚
  - ç¡¬ä»¶è§„æ ¼ä¸Šï¼Œç¬”è®°æœ¬RTX 4090 Mobileä½¿ç”¨AD103æ ¸å¿ƒï¼Œé‡‡ç”¨å°ç§¯ç”µN4åˆ¶ç¨‹å·¥è‰ºï¼Œæ­è½½9728ä¸ªCUDAå•å…ƒï¼Œ16 GB GDDR6æ˜¾å­˜ï¼Œæ˜¾å­˜ä½å®½ä¸º256 bitï¼›
  - åŒæ ·çš„ï¼Œå°å¼æœºRTX 4080ä¹Ÿä½¿ç”¨AD103æ ¸å¿ƒï¼Œé‡‡ç”¨å°ç§¯ç”µN4åˆ¶ç¨‹å·¥è‰ºï¼Œæ­è½½9728ä¸ªCUDAå•å…ƒï¼Œ16 GB GDDR6Xæ˜¾å­˜ï¼Œæ˜¾å­˜ä½å®½ä¸º256 bitã€‚ä¸ç¬”è®°æœ¬RTX 4090 Mobileåœ¨ç¡¬ä»¶è§„æ ¼ä¸Šä»£çš„åŒºåˆ«å°±æ˜¯ä¸€ä¸ªæ˜¯GDDR6ä¸€ä¸ªæ˜¯GDDR6Xæ˜¾å­˜ã€‚
- ä½†æ˜¯ï¼Œç¬”è®°æœ¬RTX 4090 Mobileçš„æ€§èƒ½è¿˜æ˜¯è¿œä¸å¦‚å°å¼æœºRTX 4080ï¼Œä¸»è¦å› ä¸ºåŠŸè€—å–‚ä¸é¥±ã€‚å°å¼æœºRTX 4080çš„TDPä¸º320 Wï¼Œå¯¹åº”çš„Boosté¢‘ç‡ä¸º2505 MHzï¼Œè€Œç¬”è®°æœ¬è¢«é”äº†åŠŸè€—å¢™ï¼Œä¸åŒåŠŸè€—é™åˆ¶ä¸‹RTX 4090çš„Boosté¢‘ç‡å¦‚ä¸‹ï¼š
  - RTX 4090 Mobile 120W = 1335-1695 MHz
  - RTX 4090 Mobile 130W = 1425-1815 MHz
  - RTX 4090 Mobile 140W = 1500-1950 MHz
  - RTX 4090 Mobile 150W = 1620-2040 MHz
  - RTX 4090 Mobile 175W = 2040-2040 MHz
- å› æ­¤ï¼Œæœ€åæ»¡è¡€çš„RTX 4090å•ç²¾åº¦æµ®ç‚¹ç®—åŠ›å¤§æ¦‚æ˜¯39.69 TFLOPSçš„æ°´å¹³ï¼Œå’Œå°å¼æœºRTX 4070 TIçš„40.09 TFLOPSå·®ä¸å¤šï¼Œæœªæ¥è¦æ˜¯RTX 4090 Mobileè§£é”200 WåŠŸè€—å¢™å¯èƒ½æœ‰å¸Œæœ›è¶…è¿‡å°å¼æœºRTX 4070 TI

- ## [èŠ¯åŠ¨ç§‘æŠ€å‘å¸ƒã€Œé£å 3 å·ã€å…¨åŠŸèƒ½ GPUï¼Œè¯¥äº§å“éƒ½æœ‰å“ªäº›äº®çœ¼æ€§èƒ½ï¼Ÿ - çŸ¥ä¹ _20250923](https://www.zhihu.com/question/1953762444674565105)
  - èŠ¯åŠ¨ç§‘æŠ€â€œé£å 3 å·â€å…¨åŠŸèƒ½ GPU æ˜¨æ—¥åœ¨ç æµ·é¦™å±±ä¼šè®®ä¸­å¿ƒæ­£å¼å‘å¸ƒï¼Œå…¶é‡‡ç”¨å…¨å›½äº§åº•å±‚è®¾è®¡ï¼ŒåŒæ—¶æ‹¥æœ‰ AI æ™ºç®—ç®—åŠ›å’Œ 8K é‡åº¦æ¸²æŸ“ç®—åŠ›ï¼Œå…¼å®¹ DirectX12ã€Vulkan1.2 ç­‰å›¾å½¢æ¥å£ï¼Œå¹¶é€‚é…ç»Ÿä¿¡ã€Windows ç­‰ç³»ç»Ÿã€‚
  - èŠ¯åŠ¨ç§‘æŠ€â€œé£å 3 å·â€å…¨åŠŸèƒ½ GPU åœ¨è¡Œä¸šå†…ç‡å…ˆå®ç°å›½äº§å¼€æº RISC-V CPU ä¸ CUDA å…¼å®¹ GPU çš„æ·±åº¦èåˆï¼Œå¯ä¸€ç«™å¼è¦†ç›–å¤§æ¨¡å‹è®­æ¨ã€å‚ç±»å¤šæ¨¡æ€åº”ç”¨ã€ç§‘å­¦è®¡ç®—ä¸é‡åº¦å›¾å½¢æ¸²æŸ“
  - åœ¨ç”Ÿæ€æ–¹é¢ï¼Œâ€œé£å 3 å·â€åœ¨è½¯ä»¶ç«¯å…¼å®¹ PyTorchã€CUDAã€Tritonã€OpenCL ç­‰ä¸»æµ AI å’Œè®¡ç®—ç”Ÿæ€ã€å’Œ DirectXã€OpenGLã€VulKan ç­‰æ¸²æŸ“ç”Ÿæ€ï¼Œé€‚é…ç»Ÿä¿¡ã€éº’éºŸã€Windowsã€Android ç­‰æ“ä½œç³»ç»Ÿã€‚
  - åœ¨å¤§æ¨¡å‹æ–¹é¢ï¼Œâ€œé£å 3 å·â€æ˜¯å›½å†…é¦–æ¬¾å•å¡é…å¤‡ 112GB + å¤§å®¹é‡é«˜å¸¦å®½æ˜¾å­˜å’Œè‡ªç ” IP çš„å…¨åŠŸèƒ½ GPUï¼Œçªç ´ç›®å‰å›½äº§ GPU æ˜¾å­˜å’Œå¤šå¡æ¬è¿çš„ä¸Šé™ï¼Œå•å¡å¯æ”¯æŒå¤šç”¨æˆ· 32B / 72B å¤§æ¨¡å‹ï¼›å•æœºå…«å¡èƒ½ç›´é©± DeepSeek 671B / 685B æ»¡è¡€ç‰ˆå¤§æ¨¡å‹ã€‚

- [GPU Fenghua No.3, 112GB HBM, DX12, Vulcan 1.2, Claims to Support CUDA : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1noru3p/gpu_fenghua_no3_112gb_hbm_dx12_vulcan_12_claims/)

- ## [å¦‚ä½•ä½¿ç”¨intelçš„gpuå’Œnpuè·‘å¤§æ¨¡å‹ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/5293002844)
- å»ä¸‹è½½ollama intelç‰ˆï¼Œè¿è¡Œï¼Œç„¶åå°±èƒ½è°ƒç”¨æ ¸æ˜¾è·‘æ¨¡å‹äº†ï¼Œä½†32gæœ¬å­æ ¸æ˜¾åªèƒ½è°ƒç”¨20G

- ç›®å‰æ¯”è¾ƒé€‚åˆåŠå°ç™½çš„åŠæ³•ï¼Œå°±æ˜¯ä¸‹è½½intelçš„ollama portable zip
  - ä»¥ä¸Šè¿™ä¿©åº”è¯¥éƒ½æ˜¯åŸºäºllama.cppçš„syclåç«¯ä¼˜åŒ–çš„ã€‚
  - å…¶å®åœ¨NPUæˆ–GPUä¸Šè¿è¡Œå¤§è¯­è¨€æ¨¡å‹è¿˜å¯ä»¥ç”¨OpenVINOã€‚ä½†ä¸æ˜¯å¾ˆé€‚åˆå°ç™½ï¼ŒOpenVINOæ›´é€‚åˆæœ‰ä¸€å®šå¼€å‘èƒ½åŠ›çš„ç”¨æˆ·ã€‚

- ä½ éœ€è¦ç”¨openvinoï¼Œæ¨ç†çš„æ—¶å€™æŒ‡å®šæ¨ç†çš„è®¾å¤‡npuï¼Œgpuè¿˜æ˜¯cpuï¼Œå°±å¯ä»¥æŠŠæ¨ç†è¿è¡Œåˆ°ä½ æƒ³è¦çš„è®¾å¤‡ä¸Šã€‚ä½ å¯ä»¥åˆ°openvinoçš„notebookæ‰¾åˆ°å¾ˆå¤šä¾‹å­ã€‚

- intelè‡ªå¸¦çš„gpuä¸æ˜¯æ™®é€šæ˜¾å¡ï¼Œä¸èƒ½ç”¨gpuè¿™ä¸ªé€‰é¡¹ï¼Œ

- ## [3090 ç›¸å½“äº 40 ç³»çš„ä»€ä¹ˆæ˜¾å¡å‘¢ï¼Ÿè¿™æ¬¾äº§å“çš„ä¼˜ç¼ºç‚¹åˆ†åˆ«æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/681021828)
- 3090èµ„æ·±ç”¨æˆ·ï¼Œé¦–å‘è´­å…¥ä¸€ç›´ç”¨åˆ°4090é¦–å‘, 
  - 3090ç†è®ºè·‘åˆ†æ€§èƒ½ç­‰äº4070tis superã€‚
  - ä½†æ˜¯3090æ˜¾å­˜æ›´å¤§ï¼Œåœ¨aiæ–¹é¢æ€§èƒ½æ›´å¼ºï¼Œå¯ä»¥è·‘ä¸€äº›é‡åŒ–å¤§æ¨¡å‹ï¼Œ4090èƒ½è·‘çš„3090éƒ½èƒ½è·‘ã€‚
  - 3090çš„<èƒ½è€—æ¯”>æ¯”4090å·®å¾ˆå¤šï¼Œå› ä¸º3090æ˜¯ä¸‰æ˜Ÿ8nmå·¥è‰ºï¼Œç›¸å¯¹å°ç§¯ç”µ5nmå·®è·éå¸¸å¤§ã€‚ç›¸åŒåŠŸè€—ä¸‹4090æ€§èƒ½è¶…3090 60%ã€‚
  - 3090åŠŸè€—ä¸Šé™éå¸¸é«˜ï¼Œæœ€é«˜åŠŸè€—äº§å“ä¸ºevga 1000ç“¦æ°´å†·æ¬¾ï¼Œä»400ç“¦åˆ°1000ç“¦æ€§èƒ½æå‡10%ã€‚ç”±æ­¤å¯è§èƒ½è€—æ¯”ä¹‹å·®ã€‚

- AIç®—åŠ›æ¥è¯´ï¼Œcudeç®—åŠ›è¡¨ç°ï¼Œè·ŸRTX4070å·®ä¸å¤šï¼›

- ## [3090 24gå’Œ3090ti 24gä»å“ªèƒ½ä¹°åˆ°æ–°çš„? - çŸ¥ä¹](https://www.zhihu.com/question/585256727)
- 3090/tiåŸºæœ¬ä¸Šä¸ä¼šæœ‰æ–°çš„äº†ï¼Œtiè¿˜å¥½ç‚¹ï¼Œä½†æ˜¯è´§æœ¬æ¥ä¹Ÿä¸å¤šï¼Œæ ‡æ–°çš„åº—ä»·æ ¼éƒ½é«˜ï¼Œç”šè‡³é«˜è¿‡4080é’™ä¸­é’™
  - ä½ çš„æƒ…å†µï¼Œè¦ä¹ˆ4080ï¼Œè¦ä¹ˆ4090ï¼Œè¦ä¹ˆäºŒæ‰‹3090/tiå°†å°±ç‚¹ï¼Œ4070tiçš„å¸¦å®½å¤ªä½äº†ï¼Œæ˜¾å­˜12Gå…¶å®ä¹Ÿèƒ½ç”¨ã€‚
  - å…¶å®3070ã€3080/tiä¹Ÿéƒ½æ˜¯å¯ä»¥ç”Ÿäº§åŠ›ã€æ¸²æŸ“çš„ã€‚éç‰¹æ®Šåœºæ™¯ä¸ä¼šéšéšä¾¿ä¾¿çˆ†æ˜¾å­˜çš„ã€‚åªæ˜¯4070tiçš„æ˜¾å­˜ä½ã€å¸¦å®½ä½äº†ä¸è¯´ï¼Œcudaæ ¸å¿ƒè¾ƒ3090å°‘äº†30%

- ## [2x4090 vs 6000 ada vs L20 vs L40s: what is the bottleneck for llm inference/finetuning? : r/LocalLLaMA _202408](https://www.reddit.com/r/LocalLLaMA/comments/1exwc04/2x4090_vs_6000_ada_vs_l20_vs_l40s_what_is_the/)
- a 6000 > 4090, less power, no moving data between PCI bus/CPU/cache to GPUs. 1 48gb will often beat 2 24gb for fine tuning.

- The l40s is essentially the 4090 with more vram. It is exactly the same chip. Nvidia released the l40s because the 4090 chips were available and demand on H100 was too high / H100 production volume too low. The 4090 has transformer engine which boosts fp8 performance. If you want inference speed on fp8 and don't need advance nccl support I would go for the 4090s

- ## [ç½‘ä¸Šä¼ çš„æ²¸æ²¸æ‰¬æ‰¬çš„96GBæ˜¾å­˜çš„4090é­”æ”¹ç‰ˆæ˜¯çœŸå®å­˜åœ¨çš„ä¹ˆï¼Ÿæ˜¯æ€ä¹ˆåšåˆ°çš„å‘¢ï¼Œæœ‰äººç”¨è¿‡ä¹ˆï¼Ÿ - çŸ¥ä¹ _202502](https://www.zhihu.com/question/13164350111)
- å› ä¸ºæœ‰å…¨æ–°L20 48Gï¼ˆå¯è¿‘ä¼¼å½“ä½œ4090D 48Gï¼‰åœ¨2.5ä¸‡çš„ä»·ä½å‹ç€ï¼Œå®é™…ä¸Šç°åœ¨4090çš„ä»·æ ¼å·²ç»æ¶¨æ— å¯æ¶¨ã€‚
  - ğŸ‘€ L20ä¸æ”¯æŒnvlink

- ä»¥ä¸Šè¿™äº›ä¸œè¥¿ä½ éƒ½è§£å†³äº†ï¼Œé‚£ä½ å‡†å¤‡æ€ä¹ˆè¿‡VBIOSæ£€æµ‹ï¼Ÿ

- ## [2wå·¦å³çš„é¢„ç®—ç‚¼ä¸¹ï¼Œ2å¼ 3090çŸ¿å¡è¿˜æ˜¯ä¸€å¼ 4090? - çŸ¥ä¹](https://www.zhihu.com/question/592038342)
- ä¸¤å¹´å‰å¸®åŒå­¦è£…è¿‡åŒè·¯RTX 3090ç‚¼ä¸¹ç‚‰ï¼Œä¸ªäººå»ºè®®è¿˜æ˜¯ç”¨å•å¡RTX 4090ã€‚
  - åŒå¡RTX 3090çŸ¿æ¸£ä¸å®¹æ˜“å®‰è£…ï¼Œå¸‚é¢ä¸Šå¼€æ”¾å¼æ•£çƒ­çš„RTX 3090çš„åšåº¦æ™®éæ¥è¿‘3-slotï¼Œè‡³å°‘è¦é¢„ç•™1-slotçš„æ•£çƒ­ç©ºé—´ï¼Œä¹Ÿå°±æ˜¯è¯´æ¯å¼ æ˜¾å¡è¦å ç”¨4-slotï¼Œç›®å‰æ¶ˆè´¹çº§ä¸»æ¿å¸¸è§çš„PCIeåŒºåŸŸå®½åº¦æ˜¯7-slotï¼Œå®‰è£…ä¸¤å¼ RTX 3090æœ‰ç‚¹å›°éš¾ã€‚è€Œä¸”æ¡¥æ¥å™¨NVLink Bridgeçš„å¸¸è§è§„æ ¼æ˜¯2-slotæˆ–è€…3-slotï¼Œå’Œä¸Šé¢è¯´çš„4-slotå ç”¨ç©ºé—´æœ‰çŸ›ç›¾
  - æ‰€ä»¥ï¼Œè¦ç»„åŒå¡RTX 3090ä¼˜å…ˆè€ƒè™‘æ¶¡è½®ç‰ˆï¼Œä¸€èˆ¬åšåº¦åªæœ‰2-slotï¼Œä½“ç§¯æ›´å°ï¼Œä½†æ˜¯RTX 3090æ¶¡è½®ç‰ˆçŸ¿æ¸£çš„ä»·æ ¼æ¯”éå…¬ç‰ˆè´µäº†1000å…ƒï¼Œå†åŠ ä¸ŠNVLink Bridgeçš„ä»·æ ¼ï¼Œå³ä¾¿ä½ æ¡çŸ¿æ¸£ï¼Œä¸¤å¼ æ¶¡è½®RTX 3090çš„æˆæœ¬ä¹Ÿè¶…è¿‡å•å¼ å…¨æ–°RTX 4090äº†ã€‚
  - å¦å¤–ï¼ŒåŒå¡RTX 3090çŸ¿æ¸£éœ€è¦çš„é¢å®šåŠŸç‡æ›´å¤§ã€‚å•å¡RTX 4090çš„TDPæ˜¯450 Wï¼ŒåŒå¡RTX 3090çš„TDPæ˜¯700 Wï¼Œç”µæºæˆæœ¬ä¹Ÿæ˜¯è¦è€ƒè™‘çš„ï¼Œè€Œä¸”å¤§åŠŸç‡ç”µæºæ»¡è½½çš„æ—¶å€™æŒºåµã€‚

- ä¸¤å¼ 3090çŸ¿å¡+nvlink (ç›¸å¯¹äºpcieï¼Œå¤§æ¨¡å‹DP TP 2-3å€é€Ÿåº¦æå‡)
  - æ€§èƒ½å¯¹æ¯”ï¼ˆgpt2 1.3B 8batch 2TPï¼Œæµ‹è¯•ç¯å¢ƒalpa/jaxï¼‰
  - TPæ¨¡å¼ï¼šç»“è®ºå¸¦nvlinkåœ¨148ä¸ªall reduceçš„æ¡ä»¶ä¸‹ï¼Œå¯ä»¥æå‡200%çš„é€Ÿåº¦ 12.95s vs 38.53
  - DPæ¨¡å¼ï¼šç»“è®ºå¸¦nvlinkçš„æ¡ä»¶ä¸‹ï¼Œå¯ä»¥æå‡150%çš„é€Ÿåº¦ 12.03s vs 31.15s

- 2å¼ 3090è¦ç»„å»ºå¸¦nvlinkçš„å¹³å°æ¯”ä½ æƒ³åƒçš„è¿˜è¦éº»çƒ¦ï¼Œå’Œå•å¡4090æ¯”è¾ƒä¸€ä¸‹
  - ä¸»æ¿: åŒå¡3090éœ€è¦ä¸€ä¸ªè‡³å°‘atxçš„å¸¦åŒPCIEx16æ’æ§½çš„ä¸»æ¿ï¼Œè€Œä¸”ä¸¤ä¸ªæ˜¾å¡æ’æ§½è·ç¦»è¦æ˜¯æ ‡å‡†çš„3æ§½é—´è·æ‰è¡Œï¼Œè€Œæ˜¾å¡å‚å•†ä¸ä¼šç»™ä½ è¯´è¿™äº›æ•°æ®å¾ˆéº»çƒ¦ï¼Œå¾ˆå®¹æ˜“ä¹°é”™ï¼Œ4090å°±ä¸€èˆ¬çš„matxå°±è¡Œï¼Œä¸»æ¿ä»·æ ¼å°±èƒ½ä¾¿å®œå‡ ç™¾
  - ç”µæº: åŒå¡3090ä¸€èˆ¬è¦ä¹°1400wå·¦å³çš„ç”µæºï¼Œ4090ä¸€èˆ¬850wå°±å¾ˆå¤Ÿäº†ï¼Œä»·æ ¼ä¼°è®¡å·®ä»·300åˆ°500å·¦å³
  - æ˜¾å¡: è¦ç»„åŒå¡è€ƒè™‘åˆ°nvlinkä¸€èˆ¬æœ€å¤§3æ§½ï¼Œåªèƒ½é€‰æ¶¡è½®ç‰ˆæˆ–æ°´å†·ç‰ˆï¼Œæ¯”ä¸€èˆ¬3é£æ‰‡é£å†·å•å¡è´µ2000ä»¥ä¸Š
  - æœºç®±: åŒå¡3090ä¸€èˆ¬è¦æ”¯æŒatxçš„å¤§æœºç®±æ‰èƒ½ä¿è¯æ‰‡çƒ­, 4090ä¸€èˆ¬çš„matxæœºç®±å°±è¡Œï¼Œå·®ä»·ä¸€èˆ¬ä¸ä¼šç‰¹åˆ«å¤§
- å¦å¤–nvlinkä¹Ÿè¦1500å·¦å³
  - ä»·æ ¼: å®é™…ä¸Šè¦æ¯”å•å¡4090çš„ä»·æ ¼è´µå¤§æ¦‚4000ä»¥ä¸Š
  - åŠŸè€—: åŒå¡3090çƒ¤é¸¡åŠŸè€—ä¼°è®¡è¦è¶…è¿‡800wè¦è¿œè¿œå¤§äºå•å¡4090çš„450wï¼Œæ•´æœºåŒçƒ¤ä¼°è®¡è¦çªç ´1000w
  - è´¨ä¿: åŒå¡3090éƒ½æ˜¯çŸ¿å¡æ— è´¨ä¿ï¼Œèƒ½ç”¨å¤šä¹…çœ‹è„¸ï¼Œè€Œ4090å…¨æ–°å¸¦è´¨ä¿
  - ä¿å€¼:30ç³»é»˜è®¤çŸ¿ï¼Œ30ç³»å·²ç»è‡­å¤§è¡—äº†ï¼Œä»¥åå¦‚æœç”¨ä¸ä¸Šäº†è¦å‡ºæ‰ï¼Œ30ç³»ä¸å¥½å‡ºæ‰‹ï¼Œè€Œ40ç³»åå£°å¥½è¦æ¯”30ç³»ä¿å€¼å¤šäº†

- ä¸¤å¼ 3090éƒ½æ²¡æœ‰ä¸€å¼ 4090è´µï¼Œä¸€å¼ 4090æ˜¯ä¸€å¼ 3090çš„ä¸‰å€æœ‰å¤šã€‚

- 4090æ²¡æœ‰nvlinkï¼Œå¤§æ˜¾å­˜è¿˜æ˜¯é€‰3090tiå¤šå¡

- æœ‰æ— nvlinkåˆ°åº•æœ‰å¤šå¤§åŒºåˆ«
  - æ¨¡å‹å¤§å°ä¸è¶…è¿‡24g åŒºåˆ«ä¸å¤§ã€‚ 2å—4090ä¸€èˆ¬ç›¸å½“äº2å€çš„batch size, Nvlinkè§£å†³çš„ä¸»è¦æ˜¯â€”â€”èƒ½ä¸èƒ½çš„é—®é¢˜ã€‚ æ¯”å¦‚æ¨¡å‹å¤§å°è¶…24gæ—¶
- nvlinkåªæ˜¯å¢å¤§å¸¦å®½ï¼Œå¹¶ä¸ä¼šå¢å¤§æ˜¾å­˜ï¼Œå¦‚æœä½ ä¸äººä¸ºæŠŠæ¨¡å‹å†™åˆ°ä¸¤å¼ å¡ä¸Šï¼Œæ¯ä¸ªiterationä¸¤å¼ å¡å„è·‘ä¸ªçš„ï¼Œç„¶åæ¢¯åº¦å›ä¼ çš„æ—¶å€™ä¸¤å¼ å¡è¦åŒæ­¥æ¢¯åº¦ï¼Œæœ‰äº†nvlinkè¿™ä¸ªè¿‡ç¨‹ä¼šæ›´å¿«ã€‚å¦‚æœ24gä¸å¤Ÿï¼Œä½ ä¸äººä¸ºå†™åˆ°ä¸¤å¼ å¡ä¸Šï¼Œä¸¤å¼ å¡ä¹Ÿè·‘ä¸äº†ã€‚

- 3090åªèƒ½åŒå¡ä¸‹nvlinkï¼Œæ²¡æœ‰nvlinkçš„å¡æˆ–è·¨ä¸¤ç»„ç”¨nvlinkè¿æ¥çš„å¡ä¼šèµ°pcieåˆ°cpuè¿›è¡Œæ•°æ®äº¤äº’ã€‚ä½ ç”¨çš„pytorchä¼šç”¨ncclè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ–¹å¼äº¤äº’ã€‚

- å…³äºâ€œ3090åªèƒ½åŒå¡ä¸‹nvlinkâ€ï¼Œè¿™ä¸ªèµ„æ–™æ‚¨æ˜¯ä»æ¥çœ‹åˆ°çš„ï¼Ÿ
  - å½“ç„¶æ‰€æœ‰PCIE GPUåº”è¯¥éƒ½å—è¿™ä¸ªé™åˆ¶ï¼Œå”¯ä¸€ä¾‹å¤–å¯èƒ½æ˜¯DGX stationä¸Šçš„å››å—A100
  - 3090åªæœ‰åŒå¡çš„bridge

- ## [NVIDIA Tesla V100 SXM2 - 2025å¹´AIæœ¬åœ°éƒ¨ç½²æ€§ä»·æ¯”ä¹‹ç‹ï¼åŒå¡V100ï¼32Gæ˜¾å­˜ï¼Œä½ä»·é«˜èƒ½ï¼Œç¢¾å‹2080Ti 22G - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1927666998030078159)
- å¹´åˆ16GBæ˜¾å­˜çš„V100å”®ä»·1000å…ƒï¼Œå¦‚ä»Šå·²é™è‡³600å…ƒï¼Œè¿œè¶…Mai50æˆä¸ºæ€§ä»·æ¯”ä¹‹ç‹ã€‚å…¶ç®—åŠ›å¯ä¸1500å…ƒå·¦å³çš„RTX 2080 Ti 22Gç›¸åª²ç¾ã€‚
  - å”¯ä¸€çš„é—¨æ§›åœ¨äºå…¶æ¥å£è®¾è®¡ï¼šV100é‡‡ç”¨æœåŠ¡å™¨ä¸“ç”¨çš„SXM2æ¥å£ï¼Œè€Œéå¸¸è§çš„PCIe x16é‡‘æ‰‹æŒ‡ã€‚SXM2æ¥å£ä¸€ä¾§ä¸ºPCIeï¼Œå¦ä¸€ä¾§ä¸ºNVLinkï¼Œæ”¯æŒå¤šå¡äº’è”ã€‚
  - é€šè¿‡200å…ƒå·¦å³çš„è½¬æ¥æ¿å’Œ80å…ƒçš„æ•£çƒ­å™¨æ”¹è£…ï¼Œä»…éœ€900å…ƒå³å¯è·å¾—2080 Tiçº§åˆ«çš„æ€§èƒ½ã€‚
- å•å¼ V100æ˜¾å¡çš„æ˜¾å­˜ä¸º16GBï¼ŒåŒå¡é…ç½®å¯è¾¾32GBã€‚ç›®å‰æ¶ˆè´¹çº§æ˜¾å¡ä¸­ä»…æœ‰RTX 5090å…·å¤‡32GBæ˜¾å­˜ï¼Œä½†ä»·æ ¼é«˜è¾¾2ä¸‡å…ƒèµ·ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŒV100æ–¹æ¡ˆä»…éœ€2000ä½™å…ƒï¼Œä½¿å¾—22Gæ˜¾å­˜çš„2080Tiç¬é—´å¤±å»æ€§ä»·æ¯”ä¼˜åŠ¿ã€‚

- æµ‹è¯•å¹³å°é‡‡ç”¨åæ“B650Mä¸»æ¿ï¼Œå› å…¶ä»…æœ‰ä¸€ä¸ªPCIeæ’æ§½ï¼Œéœ€åœ¨BIOSä¸­å¯ç”¨é€šé“æ‹†åˆ†åŠŸèƒ½ä»¥æ”¯æŒåŒæ˜¾å¡ã€‚å®‰è£…å®Œæˆåï¼Œæˆ‘ä»¬å°†å¯¹åŒV100ç³»ç»Ÿè¿›è¡Œæ€§èƒ½æµ‹è¯•ã€‚
  - é€šè¿‡NVIDIA SMIå·¥å…·æ£€æµ‹NVLINKçŠ¶æ€ï¼Œå¯è§GPU0ä¸GPU1çš„é“¾æ¥çŠ¶æ€å‡æ˜¾ç¤ºç›¸åŒå¸¦å®½å€¼ï¼Œè¡¨æ˜ä¸¤ç‰‡GPUå·²æˆåŠŸäº’è”ä¸”NVLINKåŠŸèƒ½æ­£å¸¸å¯ç”¨ã€‚
  - åœ¨å‚æ•°ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œ2080Tiçš„æ¯æ¬¡è¿­ä»£è€—æ—¶çº¦ä¸º2.17ç§’ï¼Œè€ŒV100çš„æ¯æ¬¡è¿­ä»£è€—æ—¶çº¦ä¸º1.39ç§’ã€‚

- V100ä¸æ”¯æŒflash- attentionã€bf16ã€awqã€sglangæ–°ç‰ˆæœ¬ä¹Ÿä¸æ”¯æŒv100äº†ï¼Œæ˜¾å¡å¤ªè€äº†ï¼Œæ–°çš„åŠ é€Ÿç­–ç•¥ç”¨ä¸äº†ï¼Œé€Ÿåº¦è¿˜ä¸å¦‚æ¶ˆè´¹å¡å‘¢ã€‚
  - å¯¹SDè·‘å›¾æ¥è¯´ï¼Œä¸æ”¯æŒint4ã€fp8ã€fp4ï¼Œä¸èƒ½ç”¨nanchakuåŠ é€Ÿã€‚SDä¹Ÿä¸èƒ½å¤šå¡æ˜¾å­˜å åŠ ã€‚

- æœ¬è´¨ä¸Šè¿˜æ˜¯å•å¡16Gï¼Œæ²¡ä»€ä¹ˆåµç”¨ï¼Œè€Œä¸”å¾ˆå¤šæ¨ç†ç‰¹æ€§ä¸æ”¯æŒï¼Œè¿œè¿œæ²¡æœ‰2080tiå¥½

- ## [ä¸»è¦åšç”Ÿæˆæ¨¡å‹ï¼Œè‡ªç”¨æ·±åº¦å­¦ä¹ æœåŠ¡å™¨ä¹°åŒå¡3090è¿˜æ˜¯ä¹°å•å¡4090ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/9062530414)

- å¯¹äºä½ç®—åŠ›åœºæ™¯ï¼Œæœ€å¤§çš„æ€§èƒ½ç“¶é¢ˆæ°¸è¿œæ˜¯æµ®ç‚¹æ€§èƒ½ï¼Œå› ä¸ºè¿™ä¼šå†³å®šä½ çš„è®­ç»ƒæ—¶é•¿ã€‚
  - RTX3090çš„æµ®ç‚¹æ€§èƒ½ï¼ˆFP32/FP16ï¼‰35.6TFLOPSï¼ŒRTX4090åˆ™é«˜è¾¾82.6TFLOPSï¼Œå‡ ä¹é«˜å‡ºäº†3090ä¸€å€ï¼Œæ˜¾å­˜å¸¦å®½ä¹Ÿç•¥é«˜ï¼ŒåŒæ—¶TensorCoreçš„ç‰ˆæœ¬ä¹Ÿæ›´æ–°æ”¯æŒæ›´å¤šæ•°æ®æ ¼å¼ã€‚
  - RTX3090çœ‹èµ·æ¥æœ‰NV LINKï¼Œä½†åªæ˜¯ä¸ªé˜‰å‰²ç‰ˆï¼Œç”¨æˆ·ç¡®å®å¯ä»¥åœ¨ä»£ç ä¸­å°†æ•´ä¸ªæ¨¡å‹åˆ†é…åœ¨ä¸¤å¼ å¡ä¸Šï¼ˆéœ€è¦å®Œå…¨æ‰‹åŠ¨å®ç°ï¼‰ï¼Œä½†æ˜¯å…¶å¸¦å®½åªæœ‰112.5Gbpsï¼Œä¸åˆ°æ˜¾å­˜å¸¦å®½çš„1/8ï¼Œè®­ç»ƒæ—¶ä¼šå‡ºç°ä¸¥é‡çš„æ€§èƒ½ç“¶é¢ˆã€‚ä¹°2å¼ RTX3090 24GBå¹¶ä½¿ç”¨NVLINKæ¡¥è¿æ¥å¹¶ä¸æ„å‘³ç€ä½ èƒ½å°†å…¶å½“ä½œä¸€å¼ 48GBæ˜¾å­˜çš„å¡ä½¿ç”¨ã€‚
  - å¦å¤–RTX3090æ˜¯æŒ–çŸ¿é‡ç¾åŒºï¼Œç”šè‡³æ›¾ç»æœ‰ä¸€æ®µæ—¶é—´å•å¡æ—¥æ”¶å…¥èƒ½åˆ°120RMBï¼Œå¾ˆå®¹æ˜“ä¹°åˆ°çŸ¿å¡ã€‚

- åŒå¡ç®—åŠ›æ˜¯é€¼è¿‘å•å¡çš„ï¼Œè™½ç„¶æœ‰ä»£å·®ï¼Œç†Ÿæ‚‰åŒå¡åï¼Œå¯¹å¤šå¡ä¹Ÿæœ‰äº†ç»éªŒï¼Œå¾ˆå®¹æ˜“æ‰©å±•åˆ°å¤šæœºå¤šå¡ã€‚ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒç¦»ä¸å¼€å¤šæœºå¤šå¡ã€‚

- 3090tiæ²¡æœ‰æ€§ä»·æ¯”ï¼ŒåŒå¡3090åŠ nvlinkä¹Ÿä¸é”™çš„è®°å¾—å†…å­˜é…å°½é‡å¤§ã€‚4090ä¸æ”¯æŒnvlinkçš„ï¼Œè®­ç»ƒä¸æ¨èã€‚

- 2080ti 22gbä¹Ÿæ˜¯é¦™çš„ä¸è¡Œã€‚è¿½æ±‚ä»·æ ¼ï¼Œå°±ä¹°4å¼ 2080ti 22gbçš„æ›´çˆ½ã€‚ä¸è¿‡ä¸æ”¯æŒflash attentionå’Œbf16æ•°æ®æ ¼å¼ã€‚

- å¦‚æœæ˜¯è®­ç»ƒä¸ºä¸»ï¼Œæ¯«æ— ç–‘é—®é€‰3090x2ã€‚
  - å¦‚æœæ˜¯æ¨ç†ä¸ºä¸»ï¼Œä¸”æ˜¾å­˜å ç”¨è¶…è¿‡24Gï¼Œé€‰3090x2ï¼›æ˜¾å­˜å ç”¨å°äº24Gï¼Œé€‰4090ã€‚

- åŒå¡å§ï¼Œæ€ä¹ˆä¹Ÿå¾—è®©è‡ªå·±çš„ä»£ç æ”¯æŒä¸€ä¸‹ddpï¼Œä¸ç„¶ä»¥åå¡å¤šäº†è¿˜å¾—é‡æ–°è¸©å‘ã€‚

- å»å¹´æˆ‘ç»™æŸé«˜æ ¡å®éªŒå®¤æ”’æœºé‚£ä¼šå„¿ï¼Œæ°å·§æŠŠä¸¤å¥—é…ç½®éƒ½æŠ˜è…¾è¿‡ã€‚
- å…ˆè¯´åŒ3090è¿™èŒ¬å„¿ï¼Œä¹çœ‹æ˜¾å­˜æ€¼åˆ°48GæŒºå”¬äººï¼Œä½†æ‚¨çŸ¥é“è·‘Stable Diffusionæ—¶æ˜¾å­˜å¸¦å®½è¢«PCIEé€šé“å¡è„–å­çš„æ»‹å‘³å—ï¼Ÿ
  - ä¸Šä¸ªæœˆæ‹¿åŒå¡è·‘1280x720å›¾ç”Ÿè§†é¢‘ï¼Œæ˜¾å­˜å€’æ˜¯å¯Œè£•å¾—èƒ½å…»é±¼ï¼Œå¯å®é™…ååé‡æ¯”å•å¡å°±å¤šå‡º23%ï¼ŒGPUåˆ©ç”¨ç‡æ›²çº¿è·Ÿè¿‡å±±è½¦ä¼¼çš„â€¦
- ä¸è¿‡æ‚¨è¦æ˜¯æå¤§è¯­è¨€æ¨¡å‹å¾®è°ƒï¼Œå•å¼ 4090çš„24Gç¡®å®å®¹æ˜“æŠŠè£¤å­å¡åˆ°è„šè„–å­ã€‚å¯åˆ«å¬è®ºå›é‚£å¸®æ•²é”£è¾¹å„¿çš„çå¿½æ‚ ï¼Œå®æµ‹ç”¨QLoRAæŠ€æœ¯èƒ½æŠŠ70äº¿å‚æ•°æ¨¡å‹å‹è¿›18Gæ˜¾å­˜ï¼Œè¿™èŠ‚éª¨çœ¼å„¿ä¸Š4090çš„ç¬¬ä¸‰ä»£Tensor Coreç›´æ¥è®©è®­ç»ƒé€Ÿåº¦é£èµ·ï½
- å‰äº›å¤©ç»™æŸç¾é™¢åŠ¨ç”»ç³»é‚£å¸®å“¥ä»¬è£…æœºå™¨ï¼Œä»–ä»¬æ‹¿åŒ3090è·‘æ¸²æŸ“ä»¥ä¸ºæ¡ç€å®ï¼Œç»“æœç”µæºä¸‰å¤©ä¸¤å¤´è·³é—¸â€”â€”æ•´æœº850Wç”µæºæ»¡è½½æ—¶æ»‹å•¦ä½œå“çš„åŠ¨é™ï¼Œè·ŸäºŒè¸¢è„šåœ¨æœºç®±é‡Œå¼€partyä¼¼çš„
- è¦æ˜¯é“äº†å¿ƒç©åˆ†å¸ƒå¼è®­ç»ƒä¸”é¢„ç®—ç»·å¾—ä½ç”µè´¹ï¼ŒåŒå¡èƒ½çœä¸‹17%çš„è¿­ä»£æ—¶é—´ï¼›å¯è¦æ˜¯å°±å›¾ä¸ªç—›å¿«è·‘å•å¡å¤§æ¨¡å‹ï¼Œ4090é‚£9%çš„FP32æ€§èƒ½æå‡é…ä¸ŠDlss3æŠ€æœ¯ï¼Œæ¸²æŸ“è¾“å‡ºæ—¶ç»å¯¹èƒ½è®©æ‚¨ä½“éªŒä»€ä¹ˆå« åŸåœ°èµ·é£

- NVLinkæ¡¥æ¥å™¨å¯å®ç°ç›¸é‚»ä¸¤å¼ 3090çš„å¹¶è”ï¼ŒåŒå‘å¸¦å®½æœ€é«˜600 Gbit/sï¼ˆçº¦75 GB/sï¼‰ï¼Œä½†ä»…æ”¯æŒä¸¤å¡é—´ç‚¹å¯¹ç‚¹é€šä¿¡ï¼Œæ— æ³•å®ç°å¤šå¡å…¨äº’è”ï¼ˆå¦‚GPU0ä¸GPU1äº’è”åï¼ŒGPU1ä¸GPU2ä»éœ€é€šè¿‡PCIeé€šä¿¡ï¼‰ï¼›
  - å¯ä¹°ä¸“ç”¨NVLinkæ¡¥æ¥å™¨ï¼ˆå¦‚åç¡•ROG NVLink Bridgeï¼‰å¹¶ç¡®ä¿ä¸»æ¿æ”¯æŒï¼›åŒæ—¶éœ€åœ¨Linuxç³»ç»Ÿä¸­å¯ç”¨TCCæ¨¡å¼ï¼ˆå…³é—­æ˜¾ç¤ºè¾“å‡ºï¼‰ï¼Œå¹¶ä¾èµ–ç¬¬ä¸‰æ–¹æ˜¾å­˜ç®¡ç†å·¥å…·ï¼ˆå¦‚NVIDIA MPSï¼‰ã€‚

- ## [å·²æœ‰3090Tiä¸€å¼ ï¼Œå†å¢åŠ ä¸€å¼ å¡ï¼Œæ˜¯é€‰æ‹©å¢åŠ 3090Ti+NVLINKï¼Œè¿˜æ˜¯å¢åŠ 4090ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/623563385)
- 3090.nvlink, æå‰å­¦ä¹ ä¸€ä¸‹åŒå¡æ€ä¹ˆç”¨ï¼Œæœªæ¥å¤§é¡¹ç›®éƒ½æ˜¯å¤šå¡ï¼Œçº¯ç²¹çš„å•å¡åšä¸äº†ä»€ä¹ˆç‰¹åˆ«å¤§çš„é¡¹ç›®ï¼Œçºµä½¿H100ä¹Ÿæ˜¯å¤§æŠŠäººç”¨8å¡ã€‚

- ç‚¼ä¸¹æ˜¯è‡ªå¨±è‡ªä¹è¿˜æ˜¯å°†æ¥æƒ³æ‹¿æ¥è°‹ç”Ÿï¼Ÿè°‹ç”Ÿçš„è¯å¤§æ˜¾å­˜å¤šå¡ä»»åŠ¡ååŒæ˜¯å¿…ä¿®è¯¾ï¼Œä¸€å¼ å¡æ€ä¹ˆå­¦ï¼Ÿ

- 3090Tiçš„Nvlinkå’Œä¸“ä¸šç‰ˆçš„ä¸ä¸€æ ·ï¼Œå¸¦å®½å‡åŠ, ä¹Ÿæ²¡æœ‰æ˜¾å­˜çœŸæ­£æ± åŒ–

- ## [4090ä¸æ”¯æŒnvlinkï¼Œåœ¨è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶å…·ä½“çš„è¡¨ç°æ˜¯ä»€ä¹ˆï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/603522002)
- æ²¡æœ‰[NVLink] åŠŸèƒ½åªæ˜¯ä¸èƒ½é€šè¿‡æ¡¥æ¥å™¨è¿›è¡Œä¸¤ä¸¤äº’è”(å³ä¸èƒ½é€šè¿‡NVLinkä¸¤ä¸¤è¿›è¡Œp2p)ï¼Œä½†æ˜¯å¯¹äºå¤šå¡å¹¶è¡Œè®­ç»ƒæ˜¯æ”¯æŒçš„ï¼Œå¯ä»¥é€šè¿‡device to host to deviceè¿™ç§èµ°[PCIeé“¾è·¯] è¿›è¡Œæ•°æ®å¹¶è¡Œå¤„ç†.
  - nvlinkæœ€é«˜å¯ä»¥åšåˆ°800GB/Sï¼ŒPCIEåªæœ‰128GB/S
- pcieæ…¢å¾ˆå¤šè€Œä¸”å—åˆ¶äºä¸»æ¿

- ## ğŸ‘· [4090 48Gæ¶¡è½®ç‰ˆæ·±åº¦ä½“éªŒ - çŸ¥ä¹ _202502](https://zhuanlan.zhihu.com/p/22691935175)
  - ComfyUIè·‘flux fp16æ¨¡å‹ï¼Œå›¾åƒå®½é«˜1000*1000ï¼Œstep20ï¼Œè·‘å®Œ22ç§’å·¦å³ï¼Œæ˜¾å­˜å ç”¨36Gï¼Œå¤§æ˜¾å­˜å°±æ˜¯å¥½ã€‚

- èƒ½ä¸èƒ½æµ‹ä¸€ä¸‹ä¸åŒ3090 NVlinkçš„å·®è·
  - çœ‹æ‹¼å‡ å—3090å§ï¼Œä½ è¦2å—è‚¯å®šæ¯”ä¸è¿‡4090 48Gï¼ŒNVLINkæ‹¼æ˜¾å¡è¿˜æ˜¯æœ‰æŸè€—çš„

- ç°åœ¨æœ‰4090 48gä¸‰é£æ‰‡ç‰ˆä¸åµäº†
- ä¸€ç›´æ­£å¸¸ç”¨ï¼Œå°±æ˜¯æ˜¾å¡æ¸©åº¦ä¸€ç›´ä¸Š90åº¦ï¼Œæš´åŠ›é£æ‰‡è½¬é€Ÿæ²¡æ³•è°ƒå°

- æ”¹å®Œè¿˜èƒ½æ‰“æ¸¸æˆå—
  - å¯ä»¥æ­£å¸¸æ‰“æ¸¸æˆ

- å“ªæœ‰é è°±é”€å”®æ¸ é“å•Šï¼Ÿ
  - æƒ³ç©é­”æ”¹ å“ªä¼šæœ‰å”®åå•Š ä¸éƒ½æ˜¯è‡ªå·±ç©å—

- æ€§ä»·æ¯”æœ€é«˜çš„æ˜¯2080ti 22gï¼Œä¸è¿‡è¦æ¢ç”µæº

- è¿™ç§å”®åæ€ä¹ˆæå‘¢
  - æˆ‘æ˜¯æ‹¿è‡ªå·±çš„4090å»æ”¹çš„ï¼ŒèŠ±äº†5åƒ5ï¼Œ åº—å®¶è¯´æœ‰ä¸‰å¹´è´¨ä¿å“

- bç«™ä¿®ç”µè„‘çš„å¼ å“¥èƒ½ä¿®ï¼Œåªè¦ä¸æ˜¯æ ¸å¿ƒåäº†éƒ½èƒ½ä¿®

- [é­”æ”¹çš„RTX 4090 48Gå¡å€¼å¾—é€‰å—ï¼Ÿ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1888965700464406937)
  - è¿™å¡æˆ‘ä¹°äº†ä¸€å¼ ç”¨äº†ä¸€ä¸ªæœˆäº†ï¼Œåˆ†äº†540gddr5å†…å­˜è·‘äº†æ»¡è¡€ç‰ˆq2åŠä¸ªæœˆäº†ï¼Œæ²¡å•¥é—®é¢˜ï¼ŒåŸºæœ¬åœ¨15åˆ°20tokens

- [æŠ€å˜‰RTX4090æ¶¡è½®å¡å¤ªåµæ€ä¹ˆåŠï¼Ÿæ”¹è¿™ä¸ªæ°´å†· - å°çº¢ä¹¦](https://www.xiaohongshu.com/explore/6713b9e70000000026037edd?xsec_token=AB3p2aSFwZpNJgLEWUwjbY4ZRQfalBGT1nX8ghdKLP2ls=&xsec_source=pc_search&source=unknown)
  - æ¶¡è½®å¡ï¼Œéƒ½æ”¾åœ¨æœåŠ¡å™¨é‡Œé¢ç”¨çš„ï¼Œå¦‚è¿‡ä½ æƒ³æ”¾åˆ°PCæœºç®±é‡Œé¢ä½¿ç”¨ï¼Œé‚£è¦åšçš„ç”Ÿç†å…‹æœé¦–å…ˆå°±æ˜¯å™ªéŸ³
- é‚£ä½ æŠŠè½¬é€Ÿè°ƒä½å‘—
  - æ¶¡è½®ç‰ˆæœ¬èº«æ•£çƒ­å°±ä¸å¦‚é£æ‰‡çš„ï¼Œè°ƒä½æœ‰é£é™©å•Š

- ## [ä¸å¡å€¼ä¸å€¼å¾—ä¹°? - çŸ¥ä¹](https://www.zhihu.com/question/596939079)
- è¦çœ‹å…·ä½“å‹å·ã€‚ ä¸¾ä¸ªä¾‹å­ï¼ŒRTX 40ç³»é¦–å‘RTX 4090ã€RTX 4080ã€RTX 4070 TIä¸‰æ¬¾å‹å·ï¼Œæ•£çƒ­æ¨¡å…·éƒ½è¶…è§„æ ¼äº†ï¼Œå³ä¾¿æ˜¯ä¸ç‰ˆå‹å·éƒ½ç”¨æ–™åè¶³ï¼Œæ‰€ä»¥ä¸€äºŒçº¿å“ç‰Œçš„ä¸å¡æ˜¯æœ€å€¼å¾—ä¹°çš„
  - ä½†æ˜¯ï¼Œä»RTX 4070å¼€å§‹æƒ…å†µå‘ç”Ÿäº†å˜åŒ–ï¼Œâ€œRTX 40ç³»æ˜¾å¡æ•£çƒ­è¿‡å‰©â€çš„è§„å¾‹ä¹Ÿè¢«æ‰“ç ´äº†ã€‚å¸‚é¢ä¸Šçš„ä¸ç‰ˆRTX 4070æ™®éé‡‡ç”¨ä½å»‰çš„æ‚¬è‡‚å¼æ•£çƒ­æ–¹æ¡ˆï¼Œè¿™ç§æ•£çƒ­ä¸èƒ½å®Œå…¨è¦†ç›–ä¾›ç”µåŒºåŸŸï¼Œå®¹æ˜“å¯¼è‡´VRMçš„ä¸ªåˆ«åœ°æ–¹è¿‡çƒ­ã€‚

- å½“æ—¶åˆè¡·ä¹Ÿæ˜¯æƒ³çœ‹çœ‹ä¸ç‰ˆç©¶ç«Ÿå¤Ÿä¸å¤Ÿç”¨ï¼Œé«˜é˜¶ç‰ˆæœ¬æ‹¥æœ‰æ›´å¸…æ°”çš„å¤–è§‚ï¼ˆæè´¨ï¼‰ï¼Œæ›´å¥½çš„æ•£çƒ­ï¼ˆé£æ‰‡ã€çƒ­ç®¡çš„æ•°é‡ç­‰ï¼‰ã€æ›´å¥½çš„è¶…é¢‘æ€§èƒ½ï¼ˆbenchmarkå¯èƒ½è¿˜èƒ½çœ‹ï¼Œå…·ä½“åˆ°æ¸¸æˆå¸§æ•°åªèƒ½è¯´ï¼šä½ æ‡‚çš„ï¼‰ï¼Œä½†çœŸæ­£å½±å“æ˜¾å¡æ€§èƒ½çš„ï¼Œæ˜¯GPUå’Œæ˜¾å­˜ã€‚ä¸è¿‡ï¼Œå¦‚æœé«˜é˜¶ç‰ˆæœ¬æº¢ä»·å¯ä»¥æ¥å—ï¼Œå¯ä»¥é€‰ï¼Œæ¯”å¦‚æ›´å¥½çš„æ•£çƒ­æ€§èƒ½å¸¦æ¥çš„æ˜¯æ¸©åº¦çš„é™ä½ï¼Œæ¸©åº¦çš„é™ä½æ›´æœ‰åˆ©äºæ•´æœºçš„ç¨³å®šæ€§ã€‚

- å¾®æ˜Ÿä¹°ä¸‡å›¾å¸ˆã€‚ç°åœ¨ä¸‡å›¾å¸ˆæ¯”è¶…é¾™æ€§èƒ½ä½ä¸åˆ°3%ä»·æ ¼æ•´æ•´å¤šä¸€åƒã€‚å¾®æ˜Ÿä¸‡å›¾å¸ˆå’Œ[å¾®æ˜Ÿè¶…é¾™] å·®è·å°±æ˜¯æ¸¸æˆæ»¡è½½æ¸©åº¦é«˜13â„ƒã€‚

- ä¸€å®šè¦ç›¸ä¿¡è€é»„å’Œè‹å¦ˆçš„åˆ€æ³•ï¼Œ4070ä½ å°±æ˜¯èŠ¯ç‰‡ä½“è´¨å†å¥½ã€ä¾›ç”µå†å¼ºã€æ•£çƒ­å†ç¨³ï¼Œç»™å®ƒè¶…å†’çƒŸäº†æœ€å¤šèƒ½åˆšåˆšå¥½èµ¶ä¸Šé»˜é¢‘çš„4070Ti
  - æ˜¾å¡æœ€é‡è¦çš„æ˜¯é‡Œé¢çš„PCBé‚£ä¸€å°éƒ¨åˆ†ï¼Œæ ¸å¿ƒã€æ˜¾å­˜ç­‰ç­‰ï¼Œå¤–é¢é‚£3På¤§ç©ºè°ƒä¹‹ç±»çš„ï¼Œå¯¹äºçº¯ç²¹çš„æ€§ä»·æ¯”çˆ±å¥½è€…æ¥è¯´ï¼Œåº”è¯¥æ”¾åˆ°æœ€æ¬¡ä¸€çº§æ¥è€ƒè™‘

- æˆ‘åªçŸ¥é“ç¬¬ä¸€æ¬¡ä¹°æ˜¾å¡ä¹°æœ€ä¾¿å®œçš„ä¸å›¾å¸ˆï¼Œç›´æ¥è®©æˆ‘æ¡Œå­é™„è¿‘æ¸©åº¦ä»æ˜¥å¤©å˜æˆå¤å¤©ã€‚è‡ªæ­¤ä¸‹å¼ æ˜¾å¡çš„æ¸©åº¦æ˜¯æˆ‘è€ƒè™‘çš„å› ç´ ä¹‹ä¸€ã€‚

- ä¸å¡å†ä¸ï¼Œç†è®ºæ€§èƒ½éƒ½è¶…è¶Šä¸‹çº§[æ˜¾å¡]ï¼Œæ— éæ˜¯ä¾›ç”µå’Œæ•£çƒ­åšçš„å·®ç‚¹ï¼Œè¾¾ä¸åˆ°[åŠŸè€—å¢™]æ²¡æ³•è¶…é¢‘ï¼Œä½†æ˜¯èŠ¯ç‰‡æ€»å½’è¿˜æ˜¯é‚£é¢—èŠ¯ç‰‡ã€‚
  - åè¿‡æ¥è¯´4070åšçš„å†å¥½ï¼Œå†å †ç”¨æ–™éƒ½ä¸å¯èƒ½è¶…è¶Š[4070ti]ã€‚åªè¦å”®åè´¨ä¿èƒ½ä¿è¯ï¼Œåœ¨é‡‘é¢é¢„ç®—é‡Œé¢ï¼Œä¹°ä¸€å—é«˜çº§çš„ä¸å¡ä¹Ÿæ˜¯å¯ä»¥è€ƒè™‘çš„ã€‚

- å¤Ÿç”¨å°±å¥½å§ï¼Œæˆ‘ä¹°çš„4070sä¸‡å›¾å¸ˆçš„å¡ï¼Œæ„Ÿè§‰ä»·ä½æŒºåˆé€‚çš„ï¼Œç”¨ç€ä¹Ÿç¨³å®šã€‚

- ## ğŸ’¡ [ä¸ºä»€ä¹ˆ50ç³»åˆ—æ˜¾å¡å‡ºæ¥ï¼Œæ€§ä»·æ¯”æ²¡æœ‰æå‡åè€Œ40ç³»åˆ—æ˜¾å¡æ¶¨ä»·äº†ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/1902322217175463161)
- - 40ç³»æ¶¨ä»·æ˜¯å› ä¸ºå•†å®¶éƒ½åœ¨æ¸…åº“å­˜ï¼Œåº“å­˜å‡å°‘ã€‚ä»…å‰©çš„40ç³»æ˜¾å¡é›†ä¸­åˆ°å°‘æ•°å‡ ä¸ªå•†å®¶æ‰‹ä¸­ã€‚ä½†æ˜¯å¸‚åœºçš„éœ€æ±‚æ²¡æœ‰å‡å°‘ã€‚éœ€æ±‚ä¸å˜ï¼Œä¾›åº”å‡å°‘ï¼Œä»·æ ¼å¿…ç„¶æé«˜ã€‚è€Œä¸”è¿™å‡ ä¸ªå•†å®¶çš„æœ€ä½³ç­–ç•¥å°±æ˜¯æ¶¨ä»·ï¼Œå› ä¸ºä»¥åè‚¯å®šæ˜¯è¦è·Œçš„ï¼Œè¶ç°åœ¨èƒ½å–é«˜ä»·å¿…é¡»å–é«˜ä»·ã€‚
  - 40ç³»æ˜¯ä¸å¯èƒ½å†å¤§é™ä»·äº†ã€‚å› ä¸ºæ²¡æœ‰å·¨é‡çš„æ˜¾å¡ä¸‹æ¥å‡»ç©¿å•†å®¶çš„åº“å­˜ã€‚ç°åœ¨åè€Œæ˜¯å•†å®¶é‚£é‡Œæ²¡æœ‰åº“å­˜ï¼Œå‰©ä¸‹çš„æ˜¾å¡åªèƒ½æƒœå”®ã€‚

- ä»¥å‰è€é»„çš„äº§èƒ½å…¨éƒ¨ç”±æ¶ˆè´¹çº§å¸‚åœºæ‰¿æ‹…ï¼Œæ‰€ä»¥æ¯æ¬¡æ–°å¡ä¸€å‘å¸ƒï¼Œå°±ä¼šæœ‰æµ·é‡çš„æ–°å¡ç ¸ç›˜ï¼Œæ—§å¡å¿…é¡»é™ä»·æ¸…ä»“å¦åˆ™ä¼šçƒ‚æ‰‹é‡Œã€‚
  - ç°åœ¨å¯ä¸ä¸€æ ·äº†ï¼Œè€é»„8æˆçš„äº§èƒ½è¢«AIè®¡ç®—å¡åƒæ‰ï¼ŒæŠ•æ”¾ç»™æ¶ˆè´¹çº§å¸‚åœºçš„äº§èƒ½å¯¥å¯¥æ— å‡ ï¼Œæ–°å¡åˆ«è¯´ç ¸ç›˜äº†ï¼Œå¡«è¡¥å¸‚åœºç¼ºå£éƒ½ä¸å¤Ÿï¼Œæ‰€ä»¥è€å¡å°±æœ‰äº†å¥‡è´§å¯å±…çš„ä»·å€¼ï¼Œæ¶¨ä»·ä¹Ÿæ˜¯ç†æ‰€å½“ç„¶çš„äº†ã€‚
- ä¸ºä»€ä¹ˆä¸å¢åŠ äº§èƒ½ï¼Ÿ
  - è€é»„æ˜¯fablessï¼Œè‡ªå·±æ²¡æœ‰å·¥å‚ï¼Œå®Œå…¨ä¾èµ–ä»£å·¥ï¼Œä¸»è¦æ˜¯æ‰¾å°ç§¯ç”µï¼Œç„¶è€Œå°ç§¯ç”µæ¯å¹´çš„äº§èƒ½æ˜¯å›ºå®šçš„ï¼Œå…¨çƒå¤§å®¢æˆ·é‚£ä¹ˆå¤šï¼Œå°±ç®—æ˜¯è€é»„ä¹Ÿåšä¸åˆ°äº§èƒ½åŒ…åœ†ï¼Œæ‰€ä»¥è€é»„èƒ½åˆ†åˆ°çš„äº§èƒ½æ˜¯æœ‰é™çš„ï¼Œä»–æƒ³å¢äº§ä¹Ÿåšä¸åˆ°ã€‚
  - è¿™ä¹Ÿèƒ½è§£é‡Šä¸ºä»€ä¹ˆæœ‰æ—¶å€™è€é»„ä¼šæ‰¾ä»£å·¥ï¼Œæœ€ä¸»è¦çš„åŸå› æ˜¯å°ç§¯ç”µé‚£è¾¹åˆ†åˆ°çš„äº§èƒ½ä¸è¶³ä»¥è¦†ç›–äº§å“çº¿ã€‚
- æ‹¿ç€è®¾è®¡å›¾çº¸å¤šæ‰¾å‡ å®¶ä»£å·¥å‚ä¸å°±æœ‰äº§èƒ½äº†å—ï¼Ÿ
  - èŠ¯ç‰‡è¿™ä¸œè¥¿ï¼Œæ¯”è¾ƒç‰¹æ®Šï¼Œä»–å¿…é¡»åœ¨è®¾è®¡ç«‹é¡¹çš„æ—¶å€™å°±æ•²å®šä»£å·¥å‚ã€‚
  - æƒ³è¦æ‰¾ä»£å·¥å‚ä»£å·¥ï¼Œè‡ªå·±æ˜¯å¿…é¡»è®¾è®¡åˆ°é—¨ç”µè·¯ä¸€çº§çš„å¸ƒçº¿ï¼Œç„¶è€Œä¸åŒçš„ä»£å·¥å‚ï¼Œä»–çš„å…ƒä»¶å¸ƒå±€æ˜¯ä¸ä¸€æ ·çš„ï¼Œæ¯”å¦‚Aå‚çš„ä¸éé—¨æ˜¯2x2å¸ƒå±€ï¼Œé‚£Bå‚çš„ä¸éé—¨å¸ƒå±€å¯èƒ½å°±æ˜¯1x4äº†ï¼Œæ¢å‚å°±æ²¡æ³•åšï¼Œå› æ­¤ä¸€æ¬¾èŠ¯ç‰‡å¿…é¡»åœ¨ç«‹é¡¹çš„æ—¶å€™å°±å†³å®šå¥½æ‰¾å“ªå®¶ä»£å·¥ï¼Œè®¾è®¡å¥½åŸç†å›¾ï¼Œç„¶åä½¿ç”¨ä»£å·¥å‚æä¾›çš„EDAè½¯ä»¶è¿›è¡Œå¸ƒçº¿ã€‚
  - å¤§å®¶æƒ³è±¡ä¸­çš„åƒæœºåŠ å·¥ä¸€æ ·ï¼Œæœ‰äº†è®¾è®¡å›¾å°±èƒ½éšä¾¿æ‰¾ä¸ªå·¥å‚åŠ å·¥ï¼Œåœ¨èŠ¯ç‰‡é¢†åŸŸæ˜¯ä¸å­˜åœ¨çš„ï¼Œå“ªæ€•ä½ æ‹¿åˆ°äº†å®Œæ•´çš„4090å¸ƒçº¿å›¾ï¼Œæ‰¾ä¸­èŠ¯ä»£å·¥ï¼Œä¹Ÿæ˜¯åšä¸äº†çš„ï¼Œé™¤éä½ æ„¿æ„é‡æ–°è®¾è®¡å¸ƒçº¿ï¼Œé‚£å°±è¦å…ˆæå‡ åƒä¸‡ç¾å…ƒçš„è®¾è®¡è´¹ï¼Œå†æä¸Šäº¿ç¾å…ƒçš„æµç‰‡è´¹ç”¨ï¼Œå¹¶ä¸”æœ‰æµç‰‡å¤±è´¥å…¨éƒ¨è´¹ç”¨æ‰“æ°´æ¼‚çš„é£é™©ï¼Œéå¸¸åˆ’ä¸æ¥ã€‚

- åŸåˆ™ä¸Šæ˜¯è¿™æ ·çš„ï¼Œä¸è¿‡æ¢å‚ä¹Ÿæœªå¿…å°±ä»£ä»·å¾ˆå¤§ã€‚8Gen1å’Œ8+å°±æ˜¯ä¸€å¹´å†…ä»ä¸‰æ˜Ÿæ¢æˆäº†å°ç§¯ç”µã€‚
  - æ—©å°±æ›å…‰äº†ï¼ŒæŒ‰å°ç§¯ç”µçš„ç”Ÿäº§å‘¨æœŸï¼Œ8gen1æ‰¾å°ç§¯ç”µåšï¼Œé«˜é€šä¼šæœ‰å¤§åŠå¹´çš„äº§å“çœŸç©ºæœŸï¼Œæ‰€ä»¥ä»–ä¸€å¼€å§‹å°±ç«‹é¡¹åšä¸¤æ¬¾ï¼Œä¸‰æ˜Ÿå·¥è‰ºçš„8gen1å…ˆé¡¶ä¸Šï¼ŒåŠå¹´åå†æ¨å‡º8+ã€‚å°ç§¯ç”µçš„äº§èƒ½é¢„å®šèµ·ç æå‰2å¹´ï¼Œå‘ç°8gen1ä¸è¡Œå†æ”¹8+ä½ è¿™äº§èƒ½éƒ½æ’åˆ°2å¹´ä»¥åå»äº†ã€‚
  - å¥½åƒæ˜¯ç»†èŠ‚è®°ä¸æ¸…æ¥šäº†ï¼Œåˆšå¼€å§‹é«˜é€šå°±æ˜¯æ‰¾äº†å°ç§¯ç”µå’Œä¸‰æ˜Ÿåˆ†åˆ«åšä¸€éƒ¨åˆ†ï¼Œå“ªçŸ¥é“ä¸‰æ˜Ÿåšçš„å‘çƒ­é‡ä¸¥é‡ï¼Œæœ€åä¸å¾—å·²è®¢å•å…¨ç»™å°ç§¯ç”µäº†.
- è‹¹æœ6Sä¸Šçš„A9å¤„ç†å™¨ç›´æ¥ä¿©ç‰ˆæœ¬ï¼Œä¸‰æ˜Ÿå’Œå°ç§¯ç”µçš„éƒ½æœ‰

- å›½å†…è¿˜ç®—æ˜¯æœ‰è‰¯å¿ƒçš„ï¼Œ50ç³»å‡ºæ¥ä»¥åï¼Œ40ç³»å¯ä»¥ä¹°åˆ°ï¼Œç¨æ¶¨ä¸€ç‚¹ä»·ä½†è¿˜ç®—æœ‰æ€§ä»·æ¯”ã€‚
  - ç¾å›½å¸‚åœºè‡ªä»50ç³»ä¸Šå¸‚ä»¥åï¼Œæ­£ä»·çš„40ç³»ä¸€å¤œä¹‹é—´å…¨ä¸‹æ¶äº†ï¼Œåªæœ‰é»„ç‰›çš„é«˜ä»·å¡åœ¨å–ã€‚
  - åœ¨ç¾å›½ï¼ŒNV 50ç³»å’ŒAMD 9070/9070xtä¸Šå¸‚ä»¥æ¥ä¸€ç›´å¤„äºç¼ºè´§çŠ¶æ€ï¼Œéƒ½è¢«é»„ç‰›ç”¨botä¹°èµ°äº†ï¼Œæ™®é€šäººå¾ˆéš¾ä¹°åˆ°ã€‚æ¯”å¦‚Amazonä¸Šé¢æ­£å¸¸ä»·æ ¼çš„ä¸€ç›´ç¼ºè´§ï¼Œä¸€å †éè‡ªè¥é»„ç‰›æŒ‚é«˜ä»·ã€‚
- æ™®é€šå•†å®¶ä¹Ÿä¸å…è®¸å–å—
  - æ™®é€šå•†å®¶çš„éƒ½è¢«é»„ç‰›ä¹°å®Œäº†

- 40ç³»å‡ºæ¥çš„æ—¶å€™AIçƒ­åˆšåˆšå…´èµ·ï¼Œ[4090] ä¹‹æ‰€ä»¥ä»·æ ¼çˆ†ç‚¸ï¼Œä¹Ÿæ˜¯å› ä¸ºå¾ˆå¤šäººéƒ½æ‹¿4090å½“ç”Ÿäº§åŠ›å¡ç”¨ï¼Œç‚¼AI
  - è€é»„æ˜¯çœ‹åˆ°äº†40ç³»ï¼Œå°¤å…¶æ˜¯4090å–çˆ†äº†çš„æƒ…å†µä¸‹ï¼Œæ‰å†³å®š50ç³»æŒ¤ç‰™è†ï¼Œå…¨åŠ›æŠ¼å®è®¡ç®—å¡ä¸Š
  - AIå¡å–çš„æ¯”æ¸¸æˆå¡è´µå¤šäº†ï¼Œä»·æ ¼å¯ä»¥è¯´çªç ´å¤©é™…
  - äººå®¶èµ„æœ¬ä¹Ÿä¸æ˜¯å‚»å­ï¼Œæ¯”èµ·èŠ±çœŸé‡‘ç™½é“¶ä¹°ç®—åŠ›å¡ï¼ˆè¿˜å¯èƒ½ç”¨ä¸äº†å‡ å¹´å°±è¢«æ·˜æ±°ï¼‰ï¼Œè¿˜ä¸å¦‚å·ä¸€å·ç®—åŠ›ï¼Œç”¨ä¾¿å®œå¡æå¤§æ¨¡å‹ã€‚

- ## [5090Dæ˜¾å¡æ¯”4090å¼ºçš„è¯ä¸ºä½•æ²¡æœ‰è¢«ç¦ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/9802980796)
- 5090dæ˜¯nvidiaåˆ›ç«‹ä»¥æ¥æ€§ä»·æ¯”æœ€å·®çš„æ˜¾å¡ã€‚
  - 5090då’Œ4090dç®—åŠ›ä¸€æ ·ï¼Œä½†æ¯”4090dè¿˜å·®ï¼Œå› ä¸º4090då¯ä»¥é­”æ”¹48gæ˜¾å­˜ï¼Œ5090dä¸ä»…ä¸èƒ½æ‰©æ˜¾å­˜è¿˜ç¦æ­¢å¤šå¡äº’è”ï¼Œè·‘aiè½¯ä»¶3ç§’é”æ­»ï¼Œè¦é‡å¯æ‰èƒ½ç»§ç»­ç”¨ï¼Œå¯ä»¥è¯´åªèƒ½ç©æ¸¸æˆ

- ä¸­å›½ç‰¹ä¾›ç‰ˆæ——èˆ°æ˜¾å¡RTX 5090Dï¼Œè™½ç„¶å…¶AIç®—åŠ›å‰Šå‡äº†çº¦29%ï¼Œä½†æ˜¯æ¸¸æˆæ€§èƒ½å´å‡ ä¹å¹¶æœªç¼©æ°´ï¼Œä»·æ ¼ä¸º16, 499å…ƒèµ·ï¼ˆç•¥é«˜äºRTX 5090çš„1999ç¾å…ƒï¼Œå³çº¦14648å…ƒçš„ä»·æ ¼ï¼‰ï¼Œå°†äº1æœˆ30æ—¥ä¸Šå¸‚ã€‚ï¼Ÿæ€§èƒ½å·®äº›ï¼Œä»·æ ¼è¿˜è´µäº›

- ## ğŸš€ [å¦‚ä½•çœ‹å¾…æ–°æ¨å‡ºçš„Nvidiaæ¨å‡ºçš„4090dï¼Ÿ - çŸ¥ä¹ _202312](https://www.zhihu.com/question/637218617)
  - 12999å’Œ4090ä»·æ ¼ä¸€æ ·ã€‚çœ‹äº†cudaè§„æ ¼æ˜¯å®Œæ•´çš„ad102çš„ç™¾åˆ†80

- è€é»„çš„åˆ€æ³•è¿˜æ˜¯å¾ˆç²¾å‡†çš„ï¼Œè¿™æ¬¡RTX 4090Dåˆšå¥½å¡åœ¨3A090å‡ºå£ç®¡åˆ¶æ¡ä¾‹çš„ç•Œé™ä¸Šã€‚
  - â€œæ•°æ®ä¸­å¿ƒèŠ¯ç‰‡â€ç”Ÿæ•ˆèŒƒå›´æ¯”è¾ƒå¤æ‚ï¼Œä½†æ˜¯â€œéæ•°æ®ä¸­å¿ƒèŠ¯ç‰‡â€ä¸€åˆ€åˆ‡åœ¨TPP = 4800ä¸Šï¼Œä¹Ÿå°±æ˜¯è¯´å¯¹æ¶ˆè´¹çº§RTX 40æ˜¾å¡ï¼ŒTPPé™åˆ¶æ˜¯4800ä¸Šã€‚
  - è€Œè¿™å¼ RTX 4090 Dï¼Œè¯¥æ˜¾å¡æ­è½½14592ä¸ªCUDA æ ¸å¿ƒï¼ŒåŠ é€Ÿé¢‘ç‡ 2.52GHzï¼Œè¯¥æ˜¾å¡æ­è½½14592ä¸ªCUDA æ ¸å¿ƒï¼ŒåŠ é€Ÿé¢‘ç‡ 2.52GHzï¼Œæ˜¾å­˜ä¸º 24GB 384bit GDDR6Xï¼Œæ˜¾å¡æ€»åŠŸè€— 425Wï¼Œå¸¸è§„æ¸¸æˆåŠŸè€— 302Wã€‚

- è¿™ç©æ„æœ‰4090ä¹æˆè§„æ ¼ï¼Œåº”è¯¥è¿˜æ˜¯åœ¨ç¦å”®èŒƒå›´å†…çš„ï¼ˆæ‰€ä»¥nvè¿˜é™åˆ¶äº†è¶…é¢‘ï¼‰

- 4090Dè¿™å¼ å¡æœ¬èº«ï¼Œæµå¤„ç†å™¨è§„æ¨¡æ˜¯4090çš„89%ï¼ŒTensor Coreå’ŒRT Coreä¹Ÿæ˜¯ç­‰æ¯”ä½å‰Šå‡ï¼Œå¹¶æœªä¸“é—¨ç¼©æ‰äº†Tensor Coreçš„è§„æ¨¡ï¼Œæ˜¾å­˜ä¹Ÿæœªé˜‰å‰²ï¼ŒL2ç”šè‡³å¯èƒ½ä¸å˜ï¼Œä¼°è®¡ç»¼åˆæ¸¸æˆæ€§èƒ½å¯ä»¥è¾¾åˆ°4090çš„90%-95%å§

- éƒ½2023å¹´äº†ï¼Œè¿˜èƒ½åœ¨æ¶ˆè´¹å¸‚åœºä¸Šè§åˆ°å¤§å…¬å¸å‡é‡ä¸å‡ä»·çš„æ“ä½œï¼Œåªèƒ½è¯´å„æ–­è¿˜æ˜¯å‰å®³
  - æŠ€æœ¯æ€§å„æ–­æ€»æ¯”æ”¿ç­–æ€§å„æ–­è¦å¥½ï¼Œæœ€èµ·ç äººå®¶æ˜¯å‡­çœŸæœ¬äº‹å„æ–­çš„

- è¿™å°ç ä¸€åˆ€æ— ä¼¤å¤§é›…ï¼Œä¾ç„¶æœ‰73.54TFLOPsâ•CUDAâ•24GB GDDR6Xï¼ŒåŠæ‰“7900XTXè·Ÿ4080ä»¥åŠæ–°çš„4080S

- å…¨çƒèŠ¯ç‰‡å¼€å§‹è½¬å‘Riscç»“æ„ï¼Œç¾å›½çš„å¤§éƒ¨åˆ†ä¼ä¸šé€‰æ‹©armï¼Œ åªæœ‰NVDIAç›®å‰çš„GPUå’Œè°·æ­Œçš„TPUä¸æ˜¯ï¼Œä½†è°·æ­Œä¼¼ä¹ä¹Ÿæ‹¥æŠ±armï¼Œ è‡³å°‘æ˜¯ç”¨åœ¨å…¶æ‰‹æœºéƒ¨åˆ†ã€‚ ä¸­å›½å¤§ä¼ä¸šå´æ˜¯æ‹¥æŠ±RISC-Vï¼Œè¿™ç§å¼€æºçš„ç¡¬ä½“æ¶æ„è§„æ ¼ï¼Œå› ä¸ºä¸éœ€è¦ç»™armä¸“åˆ©ã€‚è‹±ç‰¹å°”ä¹Ÿæ‹¥æŠ±RISC-Vï¼Œ è¡¨ç¤ºä¸armç«äº‰ã€‚

- ## [é­”æ”¹ç‰ˆ4090 48Gæ˜¾å¡æ€§ä»·æ¯”å¤§æ¢è®¨ï¼šçœŸçš„å€¼å¾—å…¥æ‰‹å—ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/14343647195)
- 40ç³»æ˜¾å¡çš„ä¸»è¦æ€§èƒ½ç“¶é¢ˆæ˜¯[æ˜¾å­˜ä½å®½] ï¼Œå’Œæ˜¾å­˜å®¹é‡å…³ç³»ä¸å¤§ã€‚è¿™ä¸€ç‚¹æˆ‘ä»¬å¯ä»¥ä»4090å’Œrtx6000adaä¸¤å¼ å¡å¾—åˆ°éªŒè¯ã€‚
  - è¿™ä¸€ç‚¹æˆ‘ä»¬å¯ä»¥ä»4090å’Œrtx6000adaä¸¤å¼ å¡å¾—åˆ°éªŒè¯ã€‚
  - 4090å’Œrtx6000adaæ‹¥æœ‰ç›¸åŒçš„æ¶æ„çš„gpuæ ¸å¿ƒï¼ŒåŸºæœ¬ç›¸ç­‰çš„cudaæ ¸å¿ƒæ•°é‡ï¼ˆä¸€ä¸ªæ˜¯16384çš„cudaæ ¸å¿ƒï¼Œä¸€ä¸ªæ˜¯18176çš„cudaæ ¸å¿ƒï¼‰ï¼ŒåŸºæœ¬ç›¸åŒçš„æ˜¾å­˜ä½å®½ï¼ˆ4090æ˜¯1008GB/Sï¼Œrtx6000adaæ˜¯960GB/Sï¼‰ã€‚
  - ä¸¤å¼ å¡åªæœ‰æ˜¾å­˜å®¹é‡ä¸åŒï¼ˆ4090çš„æ˜¾å­˜æœ‰24gï¼Œrtx6000adaæ˜¾å­˜48gï¼‰ã€‚ç»è¿‡æµ‹è¯•ï¼Œä¸¤å¼ å¡æ— è®ºæ˜¯æ¸¸æˆæ€§èƒ½ï¼Œaiæ¨ç†ï¼Œç»“æœéƒ½æ˜¯å·®ä¸å¤šçš„ã€‚rtx6000adaå¹¶æ²¡æœ‰å› ä¸º48gçš„æ˜¾å­˜å®¹é‡è€Œæœ‰æ›´åŠ ä¼˜ç§€çš„æ€§èƒ½ã€‚
  - åŒæ ·çš„ç»“è®ºè¿˜å¯ä»¥åˆ†æ4090å’Œ4070å¾—åˆ°ã€‚4090æ‹¥æœ‰16384çš„cudaæ ¸å¿ƒï¼Œ4070æ‹¥æœ‰5880ä¸ªcudaæ ¸å¿ƒã€‚4090çš„cudaæ ¸å¿ƒæ•°é‡æ˜¯4070çš„å°†è¿‘ä¸‰å€ã€‚æŒ‰ç†è¯´ï¼Œå°†è¿‘ä¸‰å€çš„æ˜¾å­˜å®¹é‡åº”è¯¥å¸¦æ¥ä¸‰å€çš„æ€§èƒ½ï¼Œä½†æ˜¯4090çš„å®æµ‹æ€§èƒ½åªæœ‰4070çš„å°†è¿‘2å€ã€‚4090çš„ä½å®½384bitï¼Œ4070çš„ä½å®½192bitã€‚4090åˆšå¥½æ˜¯4070çš„ä¸¤å€ã€‚

- ç²—ç•¥è®¡ç®—ï¼Œæ¯10äº¿ä¸ªå‚æ•°å¤§çº¦éœ€è¦4Gæ˜¾å­˜æ¥åŠ è½½ï¼Œæ‰€ä»¥48Gæ˜¾å­˜èƒ½è·‘120äº¿å‚æ•°çš„å¤§æ¨¡å‹ï¼Œå¦‚æœæ˜¯åŠç²¾åº¦çš„æœ€å¤šå¯ä»¥è·‘240äº¿å‚æ•°çš„æ¨¡å‹ï¼Œæ˜¾å­˜ä¸å¤Ÿå¤§ï¼Œæ¨¡å‹éƒ½åŠ è½½ä¸è¿›å»ï¼Œ48Gé­”æ”¹å‡ºæ¥çš„éƒ½æ˜¯æ¶¡è½®å¡ï¼Œæ’æœºæ¶æœåŠ¡å™¨ç”¨çš„ï¼Œå°±ä¸æ˜¯ç»™æ¸¸æˆç©å®¶æŠ˜è…¾å‡ºæ¥çš„

- æˆ‘ä¸‹å‘¨å»æ‰¾ä¸ªå·¥å‚åšä¸€ä¸‹æµ‹è¯•ï¼Œé­”æ”¹çš„åŸç†å°±æ˜¯åœ¨[4090ä¸»æ¿] ä¸ŠåŠ [æ˜¾å­˜ç²’] ï¼Œè¿™ä¸ªåˆ†ä¸¤ç§ä¸€ç§æ˜¯ä»–ä»¬ä¹°çš„[3090ä¸»æ¿]ï¼Œæ¢æˆ4090èŠ¯ç‰‡ï¼Œç„¶ååŠ ç„Šæ¥24Gæ˜¾å­˜ç²’ï¼ŒåŠ ä¸¤ä¸ªã€‚è¿™ç§ä¿—ç§°æ¶ˆè´¹çº§ï¼Œå°±æ˜¯éœ€è¦æ¯å¤©å…³æœºä¸€æ¬¡ï¼Œè¦ä¸ç„¶ä¼°è®¡ä¸»æ¿é¡¶ä¸ä½ï¼Œæ¯•ç«Ÿæ˜¯ç‰¹ä¹ˆçš„3090æ”¹
  - å¦å¤–ä¸€ç§å°±æ˜¯åŸºäº4090ä¸»æ¿ï¼ŒåŠ ç„Šæ˜¾å­˜ç²’ï¼Œè¿™ç§ç†è®ºä¸Šæ¯”è¾ƒOKï¼Œä½†æ˜¯è¿˜æ²¡æœ‰å®æµ‹ï¼Œå‡†å¤‡å¸¦ä¸Šæ˜¾å¡å»æ·±åœ³æ‰¾äººé­”æ”¹ä¸€ä¸‹ï¼Œè¦æ˜¯èƒ½æˆäº†ï¼Œé‚£å°±æˆ‘å°±å¼€è¾Ÿä¸€ä¸ªé­”æ”¹ä¸šåŠ¡ï¼Œæœ¬æ¥å—äº¬ä¹Ÿæœ‰ï¼Œä½†æ˜¯å—äº¬æ²¡æœ‰å®¶é‡Œåˆ›å•Šï¼Œäº§ä¸šé“¾æœ‰ç‚¹æ‹‰ã€‚åªèƒ½æ‹›è˜é‚£äº›ä¿®æ‰‹æœºæ¥å¹²

- ## [å¯¼å¸ˆç»™30wé¢„ç®—è£…4-6å¡æœåŠ¡å™¨ï¼Œç›®å‰æ‰“ç®—ä¸Š5880adaï¼Œè¦å™ªéŸ³è¾ƒä½ã€ä¸è¦æ¶²å†·ï¼Œæ±‚åˆé€‚æ–¹æ¡ˆï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/1939707476724389292)
- 5880æ»¡è½½åŠŸè€—ä»…285wï¼Œæ¯”3090éƒ½ä½ï¼Œæ¶¡è½®ä¹Ÿå¾ˆé™éŸ³ã€‚

- https://zhuanlan.zhihu.com/p/1939741761053364343
  - ä¹‹å‰æˆ‘åˆ†äº«è¿‡ï¼Œåœ¨RTX Pro 6000 ä¸Šæ‰§è¡Œollamaä¸gpt-oss-120bï¼Œæ•ˆç‡éº»éº»å“‹ï¼Œä¸è¿‡ï¼Œç”¨å®ƒæ¥è¿è¡ŒvLLMå´æ˜¯ä¸€æŠŠå¥½æ‰‹ï¼Œç‰¹åˆ«æ˜¯ä¸»æµçš„32Bæ¨¡å‹: Qwen3-32Bï¼Œèƒ½è¾¾åˆ°22 token/sçš„é€Ÿåº¦ï¼Œ
  - å¯¹æ¯”ä¸Šä¸€ä»£å¡çš‡5880(6000 adaçš„å°å…„å¼Ÿ)æ€ä¹ˆæ ·å‘¢ï¼Ÿå®æµ‹è¿‡ï¼Œ2x5880ä¹Ÿå°±æ‰24 token/sã€‚

- [è‹±ä¼Ÿè¾¾ä¸­å›½ç‰¹ä¾›ç‰ˆRTX 5880å‘å¸ƒï¼æ€§èƒ½æ¯”æ——èˆ°å¤§ç è¿‘25%ï¼Œæ¯”RTX 5000åªé«˜6% - çŸ¥ä¹ _202401](https://zhuanlan.zhihu.com/p/676491377)
  - ç›¸æ¯”äºæ——èˆ°çº§RTX 6000ï¼Œå®šåˆ¶ç‰ˆ5880åœ¨æ€§èƒ½æ–¹é¢å¯è°“æ˜¯å¤§å¹…é™çº§â€”â€”CUDAæ ¸å¿ƒå°‘äº†23%ï¼Œå•ç²¾åº¦æµ®ç‚¹æ€§èƒ½ä½äº†24%ã€‚
  - å®é™…ä¸Šï¼Œå®ƒçš„è¡¨ç°æ›´åŠ æ¥è¿‘RTX 5000â€”â€”ä¸¤é¡¹å‚æ•°åˆ†åˆ«æå‡äº†10%å’Œ6%ã€‚

- ## [åŒRTX A6000æ˜¾å¡åšæ·±åº¦å­¦ä¹ ï¼Œä½¿ç”¨nvlinkæ¡¥æ¥å™¨èƒ½å®ç°æ˜¾å­˜å…±äº«æˆ96Gå—ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/455953236)
  - åŒRTX A6000ï¼ˆæ˜¾å­˜æ˜¯48Gï¼‰æ˜¾å¡åšæ·±åº¦å­¦ä¹ ï¼Œä½¿ç”¨nvlinkæ¡¥æ¥å™¨èƒ½å®ç°æ˜¾å­˜å…±äº«æˆ96Gå—ï¼Ÿæ’ä¸Šäº†A6000çš„å®‰åŸ¹æ¶æ„çš„æ¡¥æ¥å™¨ï¼Œè¾“å…¥nvidia-smi nvlink -i 0 -s æ˜¾ç¤ºçš„æ˜¯é€Ÿåº¦14GB/Sï¼Œä¸æ˜¯æ˜¾ç¤ºçš„activeï¼Œç„¶åè·‘è®­ç»ƒï¼Œè¿˜æ˜¯åªèƒ½ä½¿ç”¨48Gçš„æ˜¾å­˜ã€‚
- NVçš„æ¡¥æ¥å™¨åŠŸèƒ½å…¶å®åœ¨å®£ä¼ ä¸Šæœ‰äº›é—®é¢˜ï¼ŒåŠ äº†æ¡¥æ¥å™¨å…¶å®ä¹Ÿå®Œå…¨ä¸å¯èƒ½äºŒå¡åˆä¸€ã€æ˜¾å­˜å€å¢ï¼Œè¿˜æ˜¯ä¸¤å—å®Œå…¨ç‹¬ç«‹çš„æ˜¾å¡ï¼Œæ¡¥æ¥å™¨åªæ˜¯è®©ä¸¤å—æ˜¾å¡ä¹‹é—´å¯ä»¥å¿«é€Ÿäº¤æ¢æ•°æ®ï¼Œä¸ç”¨å†ä»CPUé‚£è¾¹ç»•ä¸€å¤§åœˆï¼›è‡³äºç”¨äº†åŒå¡ä»¥åæå‡äº†å¤šå°‘æ€§èƒ½ï¼Œä¸»è¦çœ‹åº”ç”¨è½¯ä»¶æœ¬èº«å¯¹å¤šGPUä¼˜åŒ–çš„æ€æ ·ï¼Œä¸åŒçš„åº”ç”¨è½¯ä»¶çš„è¡¨ç°å®Œå…¨ä¸ä¸€æ ·ï¼Œå¯èƒ½1+1æ¥è¿‘2ï¼Œä¹Ÿå¯èƒ½1+1â‰ˆ1.5, ä¹Ÿå¯èƒ½1+1=1ï¼Œç”šè‡³å¯èƒ½1+1<1ã€‚
  - æœ€æ–°ä¸€ä»£çš„Ada Lovelaceæ¶æ„çš„æ——èˆ°å¡RTX4090, RTX6000Adaå’ŒL40å¹²è„†æŠŠå¯¹NVLinkçš„æ”¯æŒå½»åº•å–æ¶ˆäº†ï¼Œå¯è§ç›®å‰è¿™ä¸ªåŠŸèƒ½åœ¨å›¾å½¢ç±»åº”ç”¨çš„é¢†åŸŸæœ‰å¤šæ‹‰èƒ¯ï¼Œè¿™ä¸ªåŠŸèƒ½ç°åœ¨æˆäº†aiè®¡ç®—é¢†åŸŸçš„ä¸“æœ‰åŠŸèƒ½äº†

- å¯ä»¥ç›´æ¥åœ¨æ˜¾å­˜é—´ä¼ è¾“æ•°æ®ï¼Œä¸éœ€è¦å†ç»è¿‡å†…å­˜äº†æ˜¯å—
  - ç†è®ºä¸Šæ˜¯ï¼Œä½†æ˜¯ä¹Ÿéœ€è¦åº”ç”¨è½¯ä»¶æ”¯æŒæ­¤åŠŸèƒ½æ‰è¡Œï¼Œè¿™ä¸ªè¦çœ‹ç®—æ³•çš„ï¼Œå®é™…ä¸Šå¤šå¡çš„ç®—æ³•æ²¡é‚£ä¹ˆç®€å•

- NVLinkä¸æ˜¯å•çº¯çš„æ˜¾å­˜å åŠ ï¼Œ48Gå˜96Gï¼Œè€Œæ˜¯ä¼šå¢åŠ ä¸¤å¼ æ˜¾å¡ä¹‹é—´çš„æ˜¾å­˜äº¤äº’å¸¦å®½ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¤šå¡è·‘æ·±åº¦å­¦ä¹ ï¼Œä¸€èˆ¬æ˜¯æŒ‡æ•°æ®å¹¶è¡Œï¼Œå³æ¯ä¸ªæ˜¾å¡å¤„ç†ä¸€éƒ¨åˆ†æ•°æ®ã€‚

- ## ğŸš€ [å¦‚ä½•è¯„ä»·Nvidia A6000æ˜¾å¡ï¼Ÿ - çŸ¥ä¹ _202010](https://www.zhihu.com/question/424306404)
  - å–æ¶ˆQuadroå‘½åï¼Œé‡‡ç”¨10752 CUDAæ»¡è¡€GA102æ ¸å¿ƒï¼Œ48GBçš„GDDR6ä¸å¸¦Xæ˜¾å­˜ï¼Œä¾›ç”µæ¥å£ä¸ºæ–°8Pinæ¥å£ï¼ˆEPS-12Vä¸ä¼ ç»ŸPcie-8Pinæ˜¾å¡ä¾›ç”µå£ä¸å…¼å®¹ï¼‰

- è™½ç„¶è¯´å–æ¶ˆäº†Quadroå¡çš„å‘½åï¼Œä½†æ˜¯è¿™ä¸œè¥¿çœ‹èµ·æ¥ä¾æ—§æ˜¯å’ŒQuadroä¸€è„‰ç›¸ä¼ ï¼Œå½“ç„¶å«ä»–â€œä¸“ä¸šå¡â€æˆ–è®¸æ›´ç›´è§‚ï¼Œ
  - ä¸€ç›´ä»¥æ¥ï¼Œæ¸¸æˆå¡å’Œä¸“ä¸šå¡éƒ½æœ‰ç€ä¸€é“éå¸¸æ˜æ˜¾çš„åŒºåˆ«ï¼Œé‚£å°±æ˜¯OpenGLé©±åŠ¨ï¼Œä¸“ä¸šå¡èƒ½æ‰“ï¼Œä½†æ˜¯æ¸¸æˆå¡ä¸èƒ½æ‰“ï¼Œè™½ç„¶è€é»„æœ‰æ”¾å‡ºè¿‡Studioé©±åŠ¨å‡ºæ¥ï¼Œä½†æ˜¯ä¾æ—§æ˜¯æ— æ³•å–ä»£OpenGLé©±åŠ¨çš„åœ°ä½

- A6000é‡‡ç”¨çš„è¿˜æ˜¯Nå¡30ç³»çš„å®‰åŸ¹æ¶æ„ï¼Œæ‰€ä»¥æ³¨æ„å®‰è£…cudaè¿˜æ˜¯éœ€è¦cuda11.1åŠä»¥ä¸Šç‰ˆæœ¬ã€‚
  - ä¸€ä¸ªæœ‰æ„æ€çš„ç‚¹æ˜¯åŠŸç‡300wï¼Œæ¯”3090çš„350wè¦ä½ï¼Œå¯è°“æ˜¯ä½åŠŸè€—é«˜æ•ˆèƒ½äº†ã€‚
  - A6000çš„48Gå¤§æ˜¾å­˜ä¸¤å€äº3090è¿˜æ˜¯æ¯”è¾ƒé€‚åˆä¸Šå¤§æ¨¡å‹çš„ï¼Œçœ‹è¿‡ç½‘ä¸Šè¯„ä»·A6000çš„æ€§èƒ½ä¼˜åŠ¿ä¸»è¦ä½“ç°åœ¨transformeråŒç²¾åº¦æ¨ç†å’Œåˆ†å¸ƒå¹¶å¡è®­ç»ƒï¼Œç”±äºç›®å‰ç‚¼ä¸¹çš„æ¨¡å‹è¦æ±‚real-timeæ¯”è¾ƒå°ï¼Œå•å¡è¶³ä»¥

- ## [RTX A6000å­˜åœ¨çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ åŒæ ·çš„ä»·é’±ä¸ºä»€ä¹ˆä¸ä¹°ä¸¤å—3090äº¤ç«å‘¢ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/483799457)
- è‹±ä¼Ÿè¾¾æŠŠæ¶ˆè´¹çº§æ˜¾å¡å’Œä¸“ä¸šçº§æ˜¾å¡åŒºåˆ†å¾—è¿˜æ˜¯å¾ˆå¼€çš„ï¼Œæ¸¸æˆå¡ä¸æ”¯æŒå¤šè·¯NVENCæµã€ä¸æ”¯æŒvGPUï¼Œä¸æ”¯æŒECCè‡ªåŠ¨çº é”™ï¼ŒopenGLæ€§èƒ½è¾ƒå·®ã€‚
  - å¦‚æœä½ æœ‰è·‘ç§‘å­¦è®¡ç®—çš„éœ€æ±‚çš„è¯ï¼ŒECCæ˜¾å­˜æ˜¯éå¸¸é‡è¦çš„ï¼Œå¯ä»¥è¯´å¿…å¤‡ï¼Œè¿™ç§æƒ…å†µä¸‹ä½ åªèƒ½ç”¨ä¸“ä¸šå›¾å½¢å¡æˆ–è€…è®¡ç®—å¡æ¥è·‘ã€‚
  - å¤šè·¯NVENCä¹Ÿå¯ä»¥å¾ˆå¥½åœ°ç”¨åœ¨å¤§å‹å¹¿æ’­æ§åˆ¶å°å†…ï¼Œç”¨äºå½•åˆ¶æ¨æµã€‚è¿™ä¹Ÿæ˜¯æ¸¸æˆå¡åº”ç”¨ä¸åˆ°çš„é¢†åŸŸã€‚

- RTX A6000æ˜¯åŸºäºNVIDIA Ampereæ¶æ„çš„è¶…é«˜ç«¯ä¸“ä¸šæ˜¾å¡ï¼Œæ­è½½æœ€æ–°ä¸€ä»£çš„ RT Coreã€Tensor Core å’Œ CUDA Core
  - å…¼å…· ECCæ ¡éªŒã€GPUDirect for Videoã€å¤šGPU æ”¯æŒã€å››é‡ç«‹ä½“ç¼“å†²ã€Mosaicå¤šæ˜¾ç¤ºå™¨ã€Quadro Syncç­‰åŠŸèƒ½ã€‚

- å› ä¸ºè€é»„ç¦æ­¢æ•°æ®ä¸­å¿ƒä½¿ç”¨æ¸¸æˆå¡

- ## [8wå·¦å³çš„åŒå¡4090æˆ–å•å¡A6000æœåŠ¡å™¨ï¼Œæœ‰ä»€ä¹ˆå¥½çš„æ¨èï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/646105848)
- è‡³å¼ºWçš„T7960å¡”å¼å·¥ä½œç«™ï¼Œä¿å®ˆæœ€å¤§4*A6000çš„æ”¯æŒã€‚
  - ä¸Šé™ä¸º56Cçš„W9-3495Xï¼›DDR5-4800å†…å­˜ï¼›PCIE 5.0ï¼›åº”è¯¥å¤Ÿç”¨çš„å­˜å‚¨ç©ºé—´ï¼ˆå¯é€‰RAID/NVMEç­‰é«˜çº§é€‰é¡¹ï¼‰
  - W7-3465X+128GBå†…å­˜+RTX4090*2/ä¸€ä¸ªA6000ï¼Œå’¬å’¬ç‰™ä¹Ÿä¸æ˜¯æä¸å®šã€‚
- æ­è½½è‡³å¼ºå¯æ‰©å±•ä¸‰ä»£çš„åŒè·¯å¡”å¼T550æœåŠ¡å™¨ï¼Œæœ€å¤§ä¸¤ä¸ªA6000ã€‚
  - ä¸Šé™å°±ä¸è¯´äº†ï¼ŒDDR4-3200; PCIE 4.0ï¼›æ ‡é…ç‹¬ç«‹é˜µåˆ—å¡ä»¥åŠ150TB+å­˜å‚¨ç©ºé—´ä¹Ÿç›¸å½“ä¸é”™ã€‚
  - è¿™ä¸ªæ€§ä»·æ¯”è¾ƒä½ï¼Œä¸æ˜¯å¾ˆæ¨èã€‚
- ä¸»æµçš„æœºæ¶å¼è§£å†³æ–¹æ¡ˆï¼Œä¹Ÿå°±æ˜¯PE R750æœåŠ¡å™¨ï¼Œæ”¯æŒæœ€å¤§3*A6000ï¼Œä½†æ˜¯å»ºè®®å°½é‡æ§åˆ¶åœ¨ä¸¤å¼ ï¼Œ
  - å®ƒæœ‰ä¸ªå§Šå¦¹å‹å·å«åš750XAï¼Œå€’æ˜¯å¯ä»¥æ”¯æŒåˆ°4*A6000ï¼Œç›¸åº”çš„ä¹Ÿè¦ç¨å¾®è´µé‚£ä¹ˆä¸€ç‚¹ç‚¹ã€‚ã€‚ã€‚
- A6000ä¼˜åŠ¿åœ¨äºæ¡¥æ¥æ‰©å®¹æ˜¾å­˜ï¼Œæ‰€ä»¥è¿˜æ˜¯å°½é‡é€‰æ‹©å¡”å¼ï¼Œå¯èƒ½ç”¨ä¸ä¸Šï¼Œä½†æ˜¯å¿…é¡»è¦æ”¯æŒå¯¹ä¸å•¦ã€‚
  - æ‰€ä»¥æˆ‘è®¤ä¸ºæœ€ä¼˜é€‰æ‹©è¿˜æ˜¯å¡”å¼å·¥ä½œç«™T7960, 8é€šé“DDR5-4800+PCIE5.0å•Šï¼Œä½•å…¶å…ˆè¿›ã€‚

- 7960èƒ½åªä¹°æœºæ¶å’Œç”µæºå—, å…¶å®ƒçš„æƒ³è‡ªå·±é…
  - æœ€ä½æœ€ä½ï¼Œå¾—å¸¦ä¸Šå¤„ç†å™¨ã€‚å†…å­˜ç¡¬ç›˜æ˜¾å¡è‡ªå·±é…å»å§ã€‚

- [å¯¼å¸ˆç»™äº†10wä¹°æ·±åº¦å­¦ä¹ çš„æœåŠ¡å™¨ï¼Œåªè¦æ±‚2å¼ a6000çš„å¡ï¼Œå…¶ä»–é…ç½®æœ‰ä»€ä¹ˆæ¨èä¹ˆï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/628269514)
  - æ¨èç”¨å„ä¸ªå‚å®¶çš„å•è·¯æ——èˆ°è§£å†³æ–¹æ¡ˆç›´æ¥é€‚é…ã€‚ ä¹Ÿå°±æ˜¯åŸºäºAMD Threadripper PROæˆ–è€…å…¨æ–°è‡³å¼ºWç›¸å…³çš„æœ€æ–°å¹³å°ã€‚
  - ä¾‹å¦‚ä¸šå†…éå¸¸ä¼˜ç§€çš„æˆ´å°”Precision 7960 å¡”å¼ã€‚ æ€§èƒ½ä¸Šé™åº”è¯¥æ˜¯ 56C112Tï¼›4TB DDR5å†…å­˜ï¼ˆ16ä¸ªå†…å­˜æ’æ§½ï¼‰ï¼›æ»¡é…10ä¸ª3.5ç¡¬ç›˜æ§½ä½ï¼›æ»¡é…æœ€å¤§å››å¼ A6000, å±å®çš„ä¼˜ç§€
  - æˆ–è€…è¯´7960çš„å°å¼Ÿ5860ä¹Ÿä¸æ˜¯ä¸è¡Œ ï¼Œæ‰©å±•èƒ½åŠ›ç¨å·®ï¼Œä½†æ˜¯æ”¯æŒä¸¤å¼ A6000è¿è¡Œï¼Œåº”è¯¥ä¹Ÿæ˜¯æ²¡å•¥é—®é¢˜çš„ã€‚
  - æˆ–è€…ä¸Šä¸€ä»£Precision 7920ä¹Ÿè¡Œï¼Œè™½è¯´æ˜¯æœ«æœŸç¨æœ‰æ¶¨å¹…ï¼Œä½†æ˜¯æ€§ä»·æ¯”ä¾æ—§æ˜¯å¼ºæ— æ•Œï¼Œæ——èˆ°æ¯•ç«Ÿæ˜¯æ——èˆ°ï¼Œè™½ç„¶è¯´æ˜¯ä¸Šä¸€ä»£ã€‚ä¸è¿‡ä¸¤å¼ A6000ã€‚ã€‚ã€‚ã€‚ è‚¯å®šè¡Œã€‚

- ## [è£…æœºé…ç½®è®¨è®ºï¼Œå•å¡A6000oråŒå¡4090ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/599204652)
- 6000ADAå¤ªè´µï¼Œ6000æ„Ÿè§‰å¤ªè€ï¼Œ4090æ˜¾å­˜æ¯”è¾ƒç´§å¼ ï¼Œé©¬ä¸Š5090å‡ºæ¥ä¹‹åï¼Œè‚¯å®š6000å’Œ6000ADAä¼šé™ä»·ï¼Œç”šè‡³å±Šæ—¶5090è¯´ä¸å®šæœ‰32Gæ˜¾å­˜ï¼Œå¯èƒ½æ˜¯æ›´åˆé€‚çš„é€‰æ‹©ã€‚å¾ˆçº ç»“ï¼Œäºæ˜¯ç›®å‰æ‹¿ä¹‹å‰å…¥é—¨æ—¶å€™è´­ä¹°çš„4060Ti16Gåœ¨é¡¶ç€ç”¨ã€‚

- æ®æˆ‘æ‰€çŸ¥ A6000å”¯ä¸€çš„ä¼˜åŠ¿å°±æ˜¯æ˜¾å­˜å¤§ï¼Œå®é™…ä¸Šè€ƒè™‘å¤§æ˜¾å­˜éƒ½æ˜¯è€ƒè™‘é™ä½å¤šå¡å¹¶è¡Œé€šä¿¡å¼€é”€çš„ï¼Œä¸€å¼ ä¹Ÿä½“ç°ä¸äº†å¤§æ˜¾å­˜ä¼˜åŠ¿...

- ## [å®éªŒå®¤é…ç½®æœåŠ¡å™¨ï¼Œ4090ï¼Œa100å’Œa800é€‰å“ªä¸ªï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/595107162/answer/4153401353)
- æµ‹è¯•å‘ç°ï¼ŒL40é€Ÿåº¦ä¸4090Dç›¸å½“ï¼Œè¡¨ç°ä»¤äººæ»¡æ„ã€‚
  - A100, 40GB, 312 TFLOPS, Â¥90, 000+, 400W, 
  - A6000, 48GB GDDR6, 79 TFLOPS, 300W, Â¥33, 000, 300W
  - 4090, 24GB GDDR6X, 330 TFLOPS, Â¥12, 000, 450W
  - L40, 48GB GDDR6, 147 TFLOPS, Â¥44, 000+, 350W
  - å‚å•†è§£é‡Šï¼Œç”±äºä¸­ç¾åšå¼ˆçš„åŸå› ï¼ŒL40å®£ä¼ ä¸Šæœªå®šä½ä¸ºè®­ç»ƒå¡ï¼Œä½†å®é™…ç”¨äºè®­ç»ƒå®Œå…¨æ²¡é—®é¢˜ã€‚
- è™½ç„¶L40 çš„ FP16 æ€§èƒ½ï¼ˆçº¦ 147 TFLOPSï¼‰åœ¨ç†è®ºä¸Šä¼˜äº A6000ï¼ˆçº¦ 78.75 TFLOPSï¼‰ï¼Œä½†åœ¨å®é™…è®­ç»ƒå¤§å‹æ¨¡å‹æ—¶ï¼Œå®ƒå¹¶ä¸è¢«å¹¿æ³›æ¨èï¼ŒåŸå› å¦‚ä¸‹:
  - ä¼˜åŒ–ä¸è½¯ä»¶æ”¯æŒï¼šæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚ TensorFlow å’Œ PyTorchï¼‰é€šå¸¸ä¼šé’ˆå¯¹ç‰¹å®šçš„ GPU è¿›è¡Œæ·±åº¦ä¼˜åŒ–ï¼Œç‰¹åˆ«æ˜¯ A100 å’Œ A6000ã€‚NVIDIA çš„ Ampere æ¶æ„ï¼ˆå¦‚ A100 å’Œ A6000ï¼‰åœ¨è®­ç»ƒæ€§èƒ½ä¸Šå¾—åˆ°äº†å¹¿æ³›è®¤å¯ï¼Œè®¸å¤šè®­ç»ƒç®—æ³•å’Œä¼˜åŒ–å™¨éƒ½é’ˆå¯¹è¿™äº›æ˜¾å¡è¿›è¡Œäº†è°ƒæ•´ã€‚è€Œ L40 çš„æ¨ç†ä¼˜åŒ–å¯èƒ½ä¸é€‚åˆè®­ç»ƒä»»åŠ¡ã€‚
  - è®¡ç®—æ¶æ„ï¼šå°½ç®¡ L40 çš„ FP16 æ€§èƒ½è¾ƒé«˜ï¼Œå…¶è®¡ç®—æ¶æ„å’Œç¡¬ä»¶è®¾è®¡å¯èƒ½ä¸å…·å¤‡ A6000 çš„ç‰¹æ€§ï¼Œä¾‹å¦‚ Tensor Cores çš„ä¼˜åŒ–ä½¿ç”¨ã€‚Tensor Cores å¯ä»¥å¤§å¹…æé«˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—æ•ˆç‡ï¼Œå°¤å…¶åœ¨å¤§å‹æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ã€‚

- L40å•å¡è®­ç»ƒæ˜¯æ²¡é—®é¢˜ï¼Œæœ€å¤§é—®é¢˜æ˜¯ä¸æ”¯æŒNvlink
- a6000ä¸æ¯”4090é«˜å¤šå°‘ï¼Œä»·æ ¼é«˜è¿™ä¹ˆå¤š
  - æ˜¾å­˜å¤§

- 4090ä¸æ”¯æŒå¤šå¡å¹¶è”ï¼Œä¹Ÿå°±æ˜¯å¤§æ¨¡å‹æ˜¯æ ¹æœ¬æ²¡æ³•è®­å¾—ï¼Œå®ƒæ˜¯å•å¡æ€§èƒ½å¼ºï¼ŒåŒæ—¶æ€§ä»·æ¯”é«˜(é˜‰å‰²NVLinkäº†ï¼Œå¤šå¡å¸¦å®½ä¸¥é‡ä¸è¶³)ã€‚
  - a800æ˜¯a100çš„é˜‰å‰²ç‰ˆï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæœ‰a100é€‰ï¼Œè‚¯å®šä¼˜å…ˆa100ã€‚å› ä¸ºæ˜¯è¦è®­ç»ƒå¤§æ¨¡å‹ï¼Œå»ºè®®è´­ä¹°sxmç‰ˆæœ¬çš„a100ã€‚

- è¦ç©stable diffusionï¼Œç›®å‰çš„ç‰ˆæœ¬4090è¶³å¤Ÿç”¨äº†ï¼Œå…¶ä»–å¤§æ¨¡å‹å…ˆæ˜ç™½æä»€ä¹ˆï¼Œè‡ªå·±è®­ç»ƒè¿˜æ˜¯è·‘è·‘lammaï¼Œglm6bä¹‹ç±»çš„ï¼Œè‡ªå·±è®­ç»ƒç¨å¾®å¤§ç‚¹çš„æ¨¡å‹åŸºæœ¬éƒ½ä¸å¤Ÿï¼Œå…ˆç®—æ¸…æ¥šå¯¹æ˜¾å­˜çš„éœ€æ±‚ä¸€èˆ¬å°±èƒ½æœ‰å†³å®šäº†ã€‚æ‰€ä»¥ä¸ªäººå»ºè®®ï¼Œä¸çŸ¥é“è·‘ä»€ä¹ˆæˆ–è€…åªè·‘stable diffusionå…ˆä¸Šä¸ª4090ï¼Œåæ­£ä¾¿å®œã€‚æ˜ç¡®çŸ¥é“æä»€ä¹ˆï¼Œæ ¹æ®éœ€è¦å¯ä»¥æµ‹ç®—å‡ºæ¥
- 4090ä»å¤´è®­ç»ƒå›¾ç‰‡ç”Ÿæˆå¤Ÿç”¨å—ï¼Ÿæœ‰äººå®è·µè¿‡å—ã€‚è¿™äº›ã€‚
  - æŒ‰åŸç‰ˆæ¨¡å‹åŸç‰ˆè®­ç»ƒæ–¹å¼ä»0è®­ä¸å¤ªå¤Ÿï¼ŒåŸç‰ˆæ˜¯A100X8X32è®­äº†15ä¸‡GPU Hoursï¼Œæ¯å¼ å¡batch size=8ï¼Œæ€»æ‰¹æ¬¡8X256ï¼Œç²—æš´ç‚¹ç®—24gçš„4090ä¸€ä¸ªæ‰¹æ¬¡åªèƒ½2~3ï¼Œå°±ç®—ä¸è€ƒè™‘æ…¢çš„é—®é¢˜ï¼Œæ‰¹æ¬¡ä¾èµ–ä¹Ÿéœ€è¦å¦å¤–è§£å†³
- é—®é¢˜çš„å…³é”®æ˜¯ä¸ºä»€ä¹ˆè¦ä»å¤´è®­ç»ƒï¼ŒåŸºç¡€æ¨¡å‹ä¸Šå¾®è°ƒè€—è´¹å¾ˆå°çš„èµ„æº
  - æœ‰åˆ›æ–°çš„è¯‰æ±‚ï¼Œç„¶åå¾®è°ƒä¸å¤Ÿç”¨(å…·ä½“é¢ä¸´çš„é—®é¢˜å¯èƒ½å’Œå¤§ç¥ä»¬çš„ä¸åŒ)

- ## [åŒa6000å’ŒåŒ4090å“ªä¸ªè®¡ç®—æ›´å¿«å‘¢ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/664827604)
- å°è§„æ¨¡æ•°æ®ï¼Œå•å¡èƒ½è·‘çš„æƒ…å†µï¼Œæ˜¾ç„¶4090æ¯”A6000å¿«çš„å¤šã€‚
  - æ•°æ®è§„æ¨¡å†å¤§ç‚¹ï¼Œè¶…å‡ºäº†åŒå¡çš„å­˜å‚¨ç©ºé—´ï¼Œ4090å°±è·‘ä¸äº†äº†ï¼Œæ¯•ç«ŸA6000ä¸€å—å°±èƒ½é¡¶2/4å—4090å•Šã€‚

- [æ¸²æŸ“å»ºæ¨¡ï¼Œæ˜¯ä¸€ä¸ªA6000ç‰›é€¼è¿˜æ˜¯ä¸¤ä¸ª4090ç‰›é€¼? - çŸ¥ä¹](https://www.zhihu.com/question/658932305)
  - åœ¨ä¸æº¢å‡ºæ˜¾å­˜çš„æƒ…å†µä¸‹æ¸²æŸ“å•4090æ¯”A6000èµ·ç ç‰›é€¼2å€ï¼ŒåŒ4090å°±æ˜¯4å€ï¼Œ
  - gpu-zä¸Šé¢æœ‰æ•°æ®ï¼Œä¸€ä¸ª4090æ˜¯ä¸€ä¸ªa6000çš„140%

- ## [æ·±åº¦å­¦ä¹ ç”¨ä»€ä¹ˆå¡æ¯”è¾ƒç»™åŠ›ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/612568623)
- Nvidiaçš„PCæœºæ˜¾å¡ï¼Œåˆ†ä¸ºä¸‰ä¸ªç³»åˆ—ï¼ŒTeslaç³»åˆ—ï¼ŒQuadroç³»åˆ—ï¼ŒGeforceç³»åˆ—ã€‚ã€‚
  - åŸå…ˆåˆ†åˆ«å¯¹åº”ä¸“ä¸šç§‘å­¦è®¡ç®—ï¼Œä¸“ä¸šå›¾å½¢æ˜¾å¡ï¼Œä¸ªäººæ¶ˆè´¹çº§æ˜¾å¡é¢†åŸŸç­‰ã€‚
  - ä»ä»·æ ¼ä¸Šçœ‹ï¼ŒåŒç­‰ç®—åŠ›çš„ä»·æ ¼å·®ä¸å¤šæ˜¯4ï¼š2ï¼š1ã€‚
- åœ¨ç®—åŠ›å·®ä¸å¤šçš„æƒ…å†µä¸‹ï¼ŒTeslaå’ŒQuadroç³»åˆ—çš„æº¢ä»·æº¢åœ¨å“ªé‡Œäº†å‘¢ï¼Ÿ
  - ä¸»è¦æ˜¯ç¨³å®šæ€§ï¼ˆåŠ äº†ECCçº é”™æœºåˆ¶ï¼‰ï¼Œæ˜¾å­˜æ•ˆç‡æ›´é«˜ï¼ˆHBM2 vs GDDRï¼‰, æ˜¾å­˜å®¹é‡æ›´å¤§ï¼ˆä¸€èˆ¬Geforceå¡æ˜¾å­˜åœ¨10Gå·¦å³ï¼Œä½†Quadroå’ŒTeslaä¸€èˆ¬éƒ½æ˜¯24Gï¼Œç”šè‡³48Gï¼Œè¿˜æœ‰å¡é—´äº’è”æŠ€æœ¯NVlinkç­‰ï¼‰ã€‚
  - ä¸ªäººç”¨çš„è¯ï¼ŒGeforceå°±å·®ä¸å¤šäº†ï¼Œå¦‚ä¸Šä¸€ä»£çš„3090ï¼Œä»·æ ¼ï½13000Â¥ï¼Œç®—åŠ›ä¹Ÿè¶³
  - å¯ä»¥ä¸ŠA6000ï¼Œï½30000Â¥ï¼Œä¹Ÿè¿˜ä¸é”™
- åœ¨CES 2025å±•è§ˆä¼šä¸Šï¼Œè‹±ä¼Ÿè¾¾å‘å¸ƒçš„é‡‡ç”¨Blackwellæ¶æ„çš„æ˜¾å¡æ˜¯GeForce RTX 50ç³»åˆ—ï¼ŒåŒ…æ‹¬RTX 5090ã€RTX 5080ã€RTX 5070Tiå’ŒRTX 5070ç­‰å‹å·ï¼Œå¯¹æ ‡40xxçš„æ˜¾å¡
  - Teslaç³»åˆ—ï¼Œæ¨å‡ºäº†B200ï¼Œç»™æ•°æ®ä¸­å¿ƒç”¨çš„ï¼Œæ­»è´µæ­»è´µã€‚
  - ç›®å‰æš‚æœªæœ‰Quadroç³»åˆ—çš„æœ€æ–°æ¶æ„æ˜¾å¡å‘å¸ƒï¼Œè¿™ä¸ªç³»åˆ—å›½å†…å¯ç”¨çš„æœ€é«˜é…ç½®æ˜¯Ada5880, å¤§æ¦‚5ä¸‡å—ä¸€å¼ å§ï¼Œæ¬¡ä¸€ç‚¹å¯ç”¨Ada 5000, 3ä¸‡å¤šä¸€å¼ å§ã€‚ã€‚

- ç°åœ¨è¡Œæƒ…ä¸å¥½ï¼Œä»·æ ¼ä¸‘é™‹ã€‚åˆ«è¯´4090äº†ï¼Œå°±è¿3090ä¹Ÿæ¶¨ä»·äº†ä¸å°‘ã€‚å¹²è„†æé­”æ”¹2080tiå§22Gçš„ã€‚æ”¹å¥½çš„ä¸€å¼ 2500å—é’±ï¼Œä¹°3å¼ ä¹Ÿå°±ç°åœ¨ï¼ˆ2023.11.4å·ï¼‰ä¸€å¼ 3090çš„é’±ã€‚ç„¶åä¸Šæ´‹åƒåœ¾è‡³å¼ºx99å¹³å°ï¼ŒæœåŠ¡å™¨å†…å­˜éšä¾¿æ’æ»¡128gã€‚ç”µæºç”¨çŸ¿ç”µæº1600wï¼Œåæ­£æ”¾å®éªŒå®¤ï¼Œä¸å¿ƒç–¼ã€‚
  - æˆ‘åŠä¸ªæœˆå‰æƒ³ä¹°4090ï¼Œå•†å®¶ä¸å‘è´§ã€‚åªèƒ½é€€æ¬¾ï¼Œå½“å¤©æ™šä¸Šç«‹é©¬å»é—®rtx a6000èƒ½ä¸èƒ½å‘è´§ï¼Œå½“æ—¶2w6çš„ä»·æ ¼ã€‚é‚£ä¼šå•†å®¶è¿˜æ²¡ååº”è¿‡æ¥ã€‚åªæ˜¯æ™šä¸Šä¸å‘è´§ï¼Œå‡‰å‡‰ã€‚ç¬¬äºŒå¤©å•†å®¶å·²ç»æ”¹ä»·æˆ3wäº†ã€‚

- H100 ï¿¥23.5W, A100 ï¿¥13Wï¼Œä½†ç°åœ¨ä¸€èˆ¬äººæˆ–å…¬å¸ä¸å¥½ä¹°äº†ï¼Œå·²ç»è¢«ç¦è´­äº†

- ## [æœ‰æ²¡æœ‰ä¾¿å®œç‚¹çš„AIç®—åŠ›æ˜¾å¡? - çŸ¥ä¹](https://www.zhihu.com/question/634145498)
- 2080ti 22Gï¼Œä»…éœ€2000å‡ºå¤´
- 3080 20Gï¼Œ3090 24G ä¾¿å®œä¸”æ”¯æŒbf16

- è¦ä¾¿å®œï¼Œå¤§æ˜¾å­˜ï¼ŒL40ç³»åˆ—çœ‹çœ‹ã€‚è‹±ä¼Ÿè¾¾Ada Lovelace æ¶æ„ï¼Œ48GBæ”¯æŒECCçš„GDDR6æ˜¾å­˜ï¼Œä¸¤è€…çš„æ˜¾å­˜å¸¦å®½éƒ½æ˜¯864GB/S, 
  - L40Sä½œä¸ºL40çš„å‡çº§ç‰ˆæœ¬ï¼Œä¸»è¦åœ¨FP32è¿ç®—èƒ½åŠ›æç¤ºå¹…åº¦ä¸º1.1TFLOPSï¼Œåœ¨TF32 Tensor Core TFLOPSã€FP16 Tensor Coreã€FP8 Tensor Coreã€INT8 Tensor Coreè¿ç®—èƒ½åŠ›å‡æå‡ ä¸€å€å·¦å³ã€‚

- è¦ä¾¿å®œï¼Œå¤§æ˜¾å­˜ï¼Œp40æœ€åˆé€‚ã€‚24gæ˜¾å­˜ï¼Œä»·æ ¼800å·¦å³å°±å¯ä»¥å…¥æ‰‹ã€‚è®°å¾—ä¸€èµ·ä¹°é£æ‰‡å’Œç”µæºè½¬æ¥çº¿ã€‚
  - 900å—æ‹¿ä¸‹è¿‡p100ï¼Œæ„Ÿè§‰å·²ç»æ˜¯å¹³æ°‘ä»·é‡Œé¢æœ€å¥½æ€§èƒ½çš„äº†ï¼Œè¿˜æœ‰hmbæ˜¾å­˜ã€‚

- ç§Ÿäº‘æœåŠ¡å‘€ï¼Œèƒ½é€‰çš„ä¸“ä¸šå¡å¤šè¿˜èƒ½å¼€ç¥¨è®©è€å¸ˆèµ°ç»è´¹

- ## [4090 é­”æ”¹ 48g æ˜¾å­˜æ˜¯æ€ä¹ˆåšåˆ°çš„ï¼Ÿ - çŸ¥ä¹ _202502](https://www.zhihu.com/question/11803840385)
- 4090æ˜¾å¡æœ‰ä¸¤ä¸ªå…„å¼Ÿï¼Œå«åšl40så’Œa6000 adaï¼Œéƒ½ç”¨çš„ad102å†…æ ¸ï¼Œè¿™ä¸¤ä¸ªå…„å¼Ÿæ˜¾å­˜éƒ½æ˜¯48gçš„ã€‚
  - æ²¡æœ‰a6000 adaï¼Œè¦ä¹ˆæ˜¯30ç³»çš„a6000ï¼Œè¦ä¹ˆæ˜¯40ç³»çš„6000 adaã€‚æˆ‘ä¹‹å‰ä¹Ÿæé”™è¿‡ã€‚

- é­”æ”¹çš„4090 48Gä»·æ ¼æ˜¯2ä¸‡2å·¦å³ï¼ˆç»™æ–™çš„åŠ å·¥è´¹æ˜¯5.5kå·¦å³ï¼‰ï¼ŒåŒä¸€ä»£ï¼ˆ40ç³»æ˜¾å¡ï¼‰æ ¸å¿ƒçš„6000 ada 48Gè¦5ä¸‡2å·¦å³ï¼Œä¸Šä¸€ä»£ï¼ˆ30ç³»æ˜¾å¡ï¼‰æ ¸å¿ƒçš„A6000 48Gæ˜¯3ä¸‡2å·¦å³

- 3090èŠ¯ç‰‡å‘å”®æ—¶ï¼Œæ˜¾å­˜é¢—ç²’æœ€å¤§1GBï¼Œ24GBæ˜¾å­˜éœ€è¦24é¢—ï¼ŒPCBæ¿æ­£åé¢éƒ½æœ‰ã€‚
  - 4090èŠ¯ç‰‡å‘å”®æ—¶ï¼Œæ˜¾å­˜é¢—ç²’è¾¾åˆ°2GBï¼Œ24GBæ˜¾å­˜åªéœ€è¦12é¢—ï¼ŒPCBæ¿åªæœ‰ä¸€é¢æœ‰ç„Šç›˜ã€‚
  - æµå‡ºçš„4090 48GBæ”¹ç‰ˆæ˜¾å¡biosï¼Œæ­£å¥½å‘ç°4090é’ˆè„šå®šä¹‰å’Œ3090ä¸€æ ·ï¼Œå¯ä»¥ç„Šåœ¨3090PCBä¸Š
  - è¿™æ ·4090èŠ¯ç‰‡+3090PCB+24é¢—2GBæ˜¾å­˜+æµå‡ºé­”æ”¹æ˜¾å¡bios=4090 48GBæ˜¾å¡ã€‚
  - æ˜¾å¡bioså¤§æ¦‚ç‡ä¸æ˜¯x86æŒ‡ä»¤ï¼Œå¦åˆ™æ—©å°±æœ‰ä¸ªäººé­”æ”¹ç‰ˆï¼Œè¯±æƒ‘å¤ªå¤§äº†ã€‚
- æœ‰èƒ½åŠ›æ”¹çš„ï¼Œå¤§æ¦‚ç‡ä¸æƒ³å¾—ç½ªè‹±ä¼Ÿè¾¾ã€‚æ‰€ä»¥è¿™ç§åªèƒ½å·å·æï¼Œæ²¡ä¿éšœ

- å®ƒå°±æ˜¯ä¸ªæ··åˆä½“ï¼š4090æ ¸å¿ƒ+3090çš„PCBæ¿+24é¢—2GBæ˜¾å­˜é¢—ç²’ï¼Œè€Œä¸”è¿˜æœ‰ä¸‹é¢å‡ ä¸ªå·§åˆï¼Œä»»ä½•ä¸€ä¸ªå¤±æ•ˆéƒ½ä¸æˆç«‹ï¼š
  - 4090çš„æ ¸å¿ƒå’Œ3090çš„æ ¸å¿ƒé’ˆè„šä¸€æ ·ï¼Œæ‰€ä»¥å®ƒå¯ä»¥ç„Šåˆ°3090çš„PCBæ¿å­ä¸Šå¹¶è¢«è¯†åˆ«ï¼›
  - 3090ç”¨çš„æ˜¯24*1GBçš„æ˜¾å­˜é¢—ç²’ï¼Œæ¿å­æ²¡æœ‰é™åˆ¶æ˜¾å­˜é¢—ç²’å®¹é‡ï¼Œå¯ä»¥æ¢æˆ2GBçš„ï¼›
  - Nå‚å†…éƒ¨æµå‡ºäº†æ˜¾å­˜é©±åŠ¨ã€æ˜¾å¡å›ºä»¶å±…ç„¶èƒ½å®Œç¾æ”¯æŒã€‚

- ç½‘ä¸Šæµ‹è¯„æ˜¾ç¤º4090 48G æ˜¾å¡è¿˜å¯ä»¥æ”¯æŒ FP8ï¼Œç”šè‡³è¿™æ¬¾æ˜¾å¡ä¹Ÿå·²ç»å‡ºèµ°æµ·å¤–ï¼Œæ¥è‡ªåŠ æ‹¿å¤§çš„å°å“¥åœ¨å¹³å°ä¸Šæ™’å‡ºäº†è‡ªå·±åœ¨ eBay ä¸Šä¹°çš„ RTX 4090 48Gï¼Œå”®ä»·è¦ 3 ä¸‡äººæ°‘å¸èµ·æ­¥ã€‚

- ä¸å¾—ä¸ä½©æœçš®è¡£åˆ€å®¢çš„åˆ€æ³•äº†ã€‚ç§‘æŠ€æ˜¯ç¬¬ä¸€ç”Ÿäº§åŠ›ï¼Œåˆ€å®¢åœ¨é€†å¤©è€Œè¡Œã€‚
  - çš®åˆ€å®¢ç›®æ ‡åœ¨äºèµšé’±ï¼Œ å‘å±•ç”Ÿäº§åŠ›åªæ˜¯ä½ æƒ³æ³•è€Œå·²ï¼Œäººå®¶æ‰ä¸ç®¡ä½ å‘å±•ç‹—å±ç”Ÿäº§åŠ›å‘¢

- å’Œ3090åŒä¸€çº§åˆ«çš„ä¸“ä¸šå¡æ˜¯RTX A6000ï¼Œ48Gæ˜¾å­˜ï¼Œå…¶å®ƒå‚æ•°å’Œ3090å·®ä¸å¤šçš„ï¼Œç°åœ¨çš„å”®ä»·ä»ç„¶æ˜¯3ä¸‡å…ƒ
  - å’Œ4090åŒä¸€çº§åˆ«çš„ä¸“ä¸šå¡æ˜¯RTX 6000ADAï¼Œ48Gæ˜¾å­˜ï¼ŒCudaæ ¸å¿ƒæ¯”4090å¤šçº¦2000ä¸ªï¼Œå”®ä»·ç°åœ¨6ä¸‡å…ƒï¼›
  - ç•¥ä½ä¸€ä¸ªçº§åˆ«çš„RTX 5880ADAï¼ŒCudaæ ¸å¿ƒæ¯”4090å°‘çº¦2000ä¸ªï¼Œå”®ä»·åœ¨5ä¸‡å·¦å³ï¼› 
  - æ‰€ä»¥4090 48G 2.3Wçš„å”®ä»·ï¼Œæ€§ä»·æ¯”æé«˜

- a100æ€§èƒ½è¿œä¸å¦‚4090
  - 4090æ¨ç†ç‹è€…

- é©±åŠ¨ï¼Ÿå¡çš„biosæ‰æ›´é‡è¦
  - ä½ è¯´å¾—å¯¹

- å…¶å®å°±æ˜¯ä¼ è¨€æµå‡ºæ¥é‚£ç‰ˆvbiosï¼Œæ²¡æœ‰é‚£ç‰ˆvbiosï¼Œå°±æ²¡æœ‰åç»­48Gã€‚
  - vbiosæœ‰æ•°å­—ç­¾åä¼šå’ŒèŠ¯ç‰‡å†…çš„å®‰å…¨èŠ¯ç‰‡ä½œç›¸äº’æ ¡éªŒï¼Œå› æ­¤ç»•ä¸è¿‡å»ï¼Œè€Œåœ¨2023å¹´æµå‡ºæ¥äº†ä¸€ä¸ªå·¥å…·ï¼Œå¯ä»¥æŠŠä¸åŒå“ç‰Œçš„vbiosï¼ˆæœ‰æ•°å­—ç­¾åç‰ˆï¼‰äº’åˆ·ï¼Œæ‰€ä»¥æ‹¿åˆ°48Gçš„vbioså°±ç­‰äºæœ‰äº†48Gçš„4090ï¼Œæ— éæ˜¯å¦‚ä½•æ¬æ¿ï¼Œç”šè‡³æœ‰èƒ½åŠ›å¯ä»¥é‡æ–°è®¾è®¡ä¸€å¼ pcbæ¥æ‰©å¼ ï¼Œæ¢å¥è¯è¯´ï¼Œå¦‚æœæœªæ¥æœ‰æ›´å¤§æ˜¾å­˜å®¹é‡çš„biosæµå‡ºï¼ŒåŸåˆ™ä¸Šä¹Ÿå¯ä»¥åšæ›´å¤§æ˜¾å­˜çš„å¡ã€‚
  - vbiosç­¾åæ˜¯ä¸GPUèŠ¯ç‰‡å†…å®‰å…¨ç»„ä»¶è¿›è¡Œæ ¡éªŒï¼Œæ ¡éªŒé€šè¿‡æ—¶ï¼ŒGPUæ‰ä¼šå®Œå…¨åˆå§‹åŒ–ã€‚

- æ—¢ç„¶4090 48Gæ˜¯ç”¨3090çš„PCBæ¿é­”æ”¹çš„ é‚£ä¸ºä»€ä¹ˆå¸‚é¢ä¸Šéƒ½æ²¡è§åˆ°é­”æ”¹3090 48Gçš„ï¼ŸæŒ‰ç†æ¥è¯´3090æˆæœ¬æ¯”4090ä½å¾—å¤šå•Š
  - 30ç³»æ²¡æœ‰å¯¹ä½çš„48gè®¡ç®—å¡ï¼Œbiosä¼šå¡ä½ç‚¹ä¸äº®ã€‚é—²é±¼ä¸Šæœ‰å¾ˆå¤š3090èŠ¯ç‰‡è½¬ç§»åˆ°4090pcbä¸Šçš„å¡ï¼Œä»·æ ¼è¿˜æŒºå®æƒ ï¼Œæ•¢ä¹°çš„äººä¸å¤šã€‚
- å› ä¸º3090æ²¡æœ‰48Gçš„BIOSæµå‡ºã€‚4090çš„AD102æ ¸å¿ƒè¿˜ç”¨äºada6000ç­‰ä¸“ä¸šæ˜¾å¡ï¼Œå‡ºå‚æ˜¾å­˜å°±æ˜¯48Gï¼Œè¯´æ˜AD102æ˜¯å¯ä»¥å…¼å®¹48Gæ˜¾å­˜çš„ï¼Œ

- ## [ä¸ºä»€ä¹ˆNå¡ä¸€å®šè¦å¸¦cuda? - çŸ¥ä¹](https://www.zhihu.com/question/592464568)
- cudaåˆä¸æ˜¯ç¡¬ä»¶â€¦â€¦toolkitæƒ³è£…å°±è£…ï¼Œä¸å¸¦ä¹Ÿä¸ä¼šå½±å“æ‰“æ¸¸æˆ
  - å¯¹äºnå¡æ¥è¯´ï¼Œcudaæœ¬æ¥å°±ç®—æ˜¯å¢å€¼æœåŠ¡ï¼Œå¯¹äºå¾ˆå¤šå¼€å‘è€…æ¥è¯´æä¾›äº†å¥½ç”¨åˆé«˜æ•ˆçš„å¼€å‘æ¥å£ã€‚
  - æˆ‘æ˜¯ä¸è®¤åŒæŠŠnå¡å’Œcudaåˆ’ç­‰å·ã€‚ä½†æ˜¯cudaæ˜æ˜¾å·²ç»å½¢æˆäº†å¼€å‘è€…å¥½å¼€å‘-ä½¿ç”¨è€…ä½“éªŒé«˜-Nå¡å æœ‰ç‡é«˜çš„è‰¯æ€§å¾ªç¯ã€‚

- å› ä¸ºç°åœ¨çš„æ˜¾å¡çš„å¯ç¼–ç¨‹ç®¡çº¿éƒ½æ˜¯é€šç”¨çš„ã€‚é™¤å»å…‰æ …å™¨ç­‰å°‘æ•°éƒ¨åˆ†ï¼Œåƒé¡¶ç‚¹ç€è‰²å™¨ç­‰ç­‰ï¼Œé©±åŠ¨åº•å±‚å’Œç¡¬ä»¶è®¡ç®—å•å…ƒå’ŒCUDAå…¶å®éƒ½æ˜¯å…±ç”¨ä¸€å¥—ä¸œè¥¿ã€‚

- è‡³äºè¯´AMDæ²¡æœ‰CUDAï¼Œä½†æ˜¯AMDæœ‰ROCmå’ŒOpenCLå•Šã€‚åªä¸è¿‡å› ä¸ºè½¯ä»¶æ”¯æŒå·®ï¼Œä¸å¥½ç”¨ï¼Œç”¨çš„äººå°‘è€Œå·²ã€‚

- ## [æ˜¯å¦å­˜åœ¨æ”¯æŒcudaçš„æ ¸æ˜¾è½»è–„æœ¬ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/663409299)
- ä¸å­˜åœ¨ã€‚cudaæ˜¯è‹±ä¼Ÿè¾¾ç‹¬æ˜¾çš„ï¼ŒNå¡ç‹¬å ï¼Œæ˜¯ç‹¬æ˜¾çš„åŠŸèƒ½ã€‚æ ¸æ˜¾æ˜¯åŸºäºCPUçš„ï¼Œè€ŒCPUæ˜¯å› ç‰¹å°”å’ŒAMDè¿™ä¸¤å®¶çš„ã€‚
- ä¸å­˜åœ¨ã€‚Cudaæ˜¯è‹±ä¼Ÿè¾¾è‡ªç ”çš„åªæ”¯æŒè‡ªå®¶æ˜¾å¡çš„ç®—æ³•ã€‚æ ¸æ˜¾æ˜¯Intelå’ŒAMD CPUè‡ªå¸¦çš„ã€‚è‹±ä¼Ÿè¾¾å¹¶ä¸ç”Ÿäº§ç¬”è®°æœ¬çš„CPUã€‚

- CUDAæ˜¯è‹±ä¼Ÿè¾¾ç‹¬å®¶ï¼Œç›®å‰æ ¸æ˜¾è½»è–„æœ¬æ˜¯Intelå’ŒAMDï¼Œè‡ªç„¶ç°åœ¨å¸‚é¢ä¸Šæ‰¾ä¸åˆ°æ”¯æŒCUDAçš„æ ¸æ˜¾è½»è–„æœ¬ã€‚
  - ç°åœ¨çš„Intelã€AMDç”šè‡³é«˜é€šä¸»æ¨çš„AI PCå…¨é NPUï¼Œä½†æ˜¯ç›®å‰åº”ç”¨è½¯ä»¶å±‚é¢èƒ½å¤Ÿè°ƒç”¨NPUçš„å¯¥å¯¥æ— å‡ ã€‚
  - è€ŒCUDAæ”¯æŒå„ä¸ªè½¯ä»¶åº”ç”¨çš„æ”¯æŒæ˜¾ç„¶æ›´åŠ å¹¿æ³›ã€‚åˆ°æ—¶å€™å¤§æ¦‚ç‡ä¼šå‡ºç°è‹±ä¼Ÿè¾¾çš„æ ¸æ˜¾æœ¬æ‰æ˜¯çœŸæ­£æ„ä¹‰ä¸Šçš„AI PCã€‚
  - ç›®å‰è‹±ä¼Ÿè¾¾çš„GPUä¸€å¤§é—®é¢˜æ˜¯æ˜¾å­˜å®¹é‡è¾ƒå°ï¼Œæ¯”å¦‚ä¸»æµçš„ç”œç‚¹æ¸¸æˆå¡RTX 4060å°±æ˜¯8GBæ˜¾å­˜ã€‚ç°é˜¶æ®µè·‘æœ¬åœ°çš„LLMã€Stable diffusionç“¶é¢ˆå¹¶ä¸åœ¨å¤šå°‘å¤šå°‘TOPSçš„ç®—åŠ›ï¼Œè€Œåœ¨äºå¾ˆå®¹æ˜“çˆ†æ˜¾å­˜ï¼Œä½ è¿è·‘éƒ½è·‘ä¸èµ·æ¥ã€‚
  - æœªæ¥è‹±ä¼Ÿè¾¾è‡ªå·±çš„æ ¸æ˜¾æœ¬ï¼Œå¯ä»¥å…±äº«å†…å­˜ï¼Œæœ¬åœ°çš„å¤§æ¨¡å‹å’ŒAIåº”ç”¨å¯èƒ½æ‰ä¼šçœŸæ­£å‘å±•èµ·æ¥ã€‚

- ## [ä¸ºä»€ä¹ˆè¯´CUDAæ˜¯NVIDIAçš„æŠ¤åŸæ²³? - çŸ¥ä¹](https://www.zhihu.com/question/564812763)
- è‹±ä¼Ÿè¾¾ä»cudaé‡Œå­¦åˆ°çš„æœ€é‡è¦çš„ä¸€è¯¾ï¼Œå°±æ˜¯è½¯ç¡¬ä»¶æ†ç»‘ã€‚
  - è®¡ç®—ç•Œcudaä¹‹æ‰€ä»¥å‰å®³ï¼Œä¸ä»…ä»…æ˜¯å› ä¸ºå®ƒå¯ä»¥è°ƒç”¨GPUè®¡ç®—ï¼Œè€Œæ˜¯å®ƒå¯ä»¥è°ƒç”¨GPUç¡¬ä»¶åŠ é€Ÿã€‚
  - physXä¹Ÿæ˜¯ï¼ŒNå¡é™å®šã€‚ ç”šè‡³äºè¯´ï¼Œå¦‚æœéœ€æ±‚é‡å¤Ÿå¤§ï¼Œè‹±ä¼Ÿè¾¾æŠŠä¸‰ç»´ä½“ç§¯çš„æœ‰é™å·®åˆ†æ“ä½œï¼Œæœ‰é™å…ƒçš„æ£€æµ‹å‡½æ•°ç§¯åˆ†æ“ä½œï¼Œå…¨åšæˆâ€œç”µè·¯æ¿è®¡ç®—â€ä¹Ÿä¸æ˜¯ä¸å¯ä»¥ã€‚ åŒæ—¶é…åˆç€è‡ªå·±çš„è½¯ä»¶ä½“ç³»ä¸€èµ·å¾€å¤–æ¨ã€‚
  - è¿™æ‰æ˜¯è‹±ä¼Ÿè¾¾çœŸæ­£çš„ç»„åˆæ‹³ã€‚ åœ¨è¿™å¥—ç»„åˆæ‹³ä½“ç³»ä¸‹ï¼Œcudaæ‰®æ¼”ç€èƒ¶æ°´çº§æ ¸å¿ƒèˆªæ¯çš„è§’è‰²ã€‚è¿˜æœ‰å…¶å®ƒæŠ¤å«èˆ°ï¼Œè€Œè¿™äº›æŠ¤å«èˆ°éƒ½ç»•ä¸å¼€ã€‚

- 2007å¹´6æœˆï¼ŒCUDAå‘å¸ƒã€‚åœ¨Nvidiaçš„æŒç»­ç²¾è€•ç»†ä½œä¸‹ï¼ŒCUDAå·²ç»æˆäº†ç§‘å­¦è®¡ç®—é¢†åŸŸçš„äº‹å®æ ‡å‡†ã€‚
  - ç­‰ç¥ç»ç½‘ç»œç®—æ³•ç«äº†ï¼ŒNvidiaåˆå¤§åŠ›æ”¯æŒï¼Œå„å¤§AIæ¡†æ¶å› æ­¤ä¼˜å…ˆæ”¯æŒCUDAï¼Œå½¢æˆäº†æ­£å¾ªç¯ã€‚
  - ä¹‹åï¼ŒNvidiaåˆå‘åŠ›ä¸AIå…³ç³»ç´§å¯†çš„è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººé¢†åŸŸï¼Œæ¨å‡ºäº†é’ˆå¯¹æ±½è½¦çš„Driveç³»åˆ—èŠ¯ç‰‡å’Œé’ˆå¯¹å·¥ä¸šæœºå™¨äººçš„Jetsonç³»åˆ—ã€‚
  - è¿™ä¸€åˆ‡ï¼Œéƒ½æ˜¯ä»¥CUDAä½œä¸ºè½¯ä»¶åˆ‡å…¥ç‚¹ï¼Œæœ€ç»ˆï¼ŒCUDAå°±æˆäº†ä»Šå¤©çš„æ ·å­ï¼Œå˜æˆäº†åˆæ·±åˆå®½çš„æŠ¤åŸæ²³ã€‚

- amdæŠ›å¼ƒopencläº†ï¼Œæ¨ä»–è‡ªå®¶çš„rocm

- ç”¨fpgaã€ç”šè‡³æ˜¯æ›´å®šåˆ¶åŒ–çš„asicæ¥åŠ é€Ÿï¼Œæ—©å°±æœ‰äº†ï¼Œæ€§èƒ½è¶…è¿‡nvidiaçš„åŒç±»äº§å“æ¯”æ¯”çš†æ˜¯ï¼Œé—®é¢˜è¿˜æ˜¯ç”Ÿæ€å’Œæ€»æ‹¥æœ‰æˆæœ¬ã€‚googleä¹Ÿåšäº†TPUï¼Œåœ¨å†…éƒ¨ç”¨å¾—ä¹Ÿä¸å°‘ï¼Œç”Ÿæ€è¿˜æ˜¯å¹²ä¸è¿‡CUDAã€‚æ€§èƒ½æœ‰æ—¶å€™å¹¶ä¸æ˜¯å†³å®šæ€§å› ç´ ã€‚
