---
title: lib-db-app-community-perf-optimization
tags: [database, perf]
created: 2023-10-26T18:59:45.214Z
modified: 2023-10-26T19:00:02.186Z
---

# lib-db-app-community-perf-optimization

# guide

# discuss-stars
- ## 

- ## 

- ## To optimize performance, We( @DatabendLabs )â€™ve done: 
- https://x.com/BohuTANG/status/1862364484100399594
  - 1. Adopted efficient compression like zstd. 
  - 2. Optimized warehouse processor scheduling to hit S3 max bandwidth. 
  - 3. Merged S3 requests to minimize API call overhead.

- ## æ­£åœ¨åšä¸€ä¸ªwebåº”ç”¨ï¼Œæ•°æ®åº“æŸä¸ªè¡¨å¯èƒ½ä¼šæœ‰100wé‡çº§çš„æ•°æ®ï¼Œå‡å¦‚å•çº¯ä½¿ç”¨nextjs + supabase æ–¹æ¡ˆï¼Œé—®é¢˜å¤§å—ï¼Ÿ
- https://twitter.com/wsygc/status/1775093291165634871
- å½“è¯´åˆ°æ•°æ®åº“ä½¿ç”¨çš„æ—¶å€™ï¼Œè¦åˆ†æ˜¯ä¸æ˜¯å­˜å¤šæŸ¥å°‘ï¼Ÿå¦‚æœæŸ¥å¤šä¸”å¤šç»´åº¦æŸ¥è¯¢ï¼Œå¯èƒ½å¾—å†è·‘ä¸€ä¸ªESï¼Œå¦‚æœåªæ˜¯ç®€å•æŸ¥è¯¢ï¼Œå•è¡¨å¯ä»¥æ’‘å¾ˆä¹…
- pg 100w è‚¯å®šæ²¡é—®é¢˜ï¼Œsupabase æ˜¯å®Œæ•´çš„ pg 100w è‚¯å®šä¹Ÿæ²¡é—®é¢˜ï¼Œä½†æ˜¯ supabase çš„å®šä»·çœŸçš„åˆ°äº† 100w é‚£ä¹ˆé’±åŒ…å¯èƒ½ä¼šæœ‰é—®é¢˜

- pq ç™¾ä¸‡æ•°æ®æŸ¥è¯¢å¾ˆå¿«ï¼Œæˆ‘æŠŠæŸå…¬é“¾çš„ç´¢å¼•å­˜åœ¨ supabase, å…¶ä¸­ä¸€å¼ è¡¨çš„æ•°æ®æœ‰ 6000W+ï¼Œä¾ç„¶å¯ä»¥åœ¨ä¸€ç§’å†…å“åº”ï¼Œä¸è¿‡æˆ‘æ˜¯æœ¬åœ°éƒ¨ç½²çš„ supabaseï¼Œè´­ä¹°æœåŠ¡çš„è¯ï¼Œä¼šæ…¢ä¸€äº›

- ç™¾ä¸‡åœ¨æ•°æ®åº“é¢†åŸŸæ˜¯ä¸ªå°æ•°ç›®ï¼Œåšå¥½ç´¢å¼•å³å¯
- 100wæ•°æ®é‡ï¼Œéƒ½ä¸å«æ•°æ®ï¼Œéšä¾¿æŠ˜è…¾ï¼Œmysqlä¹Ÿç®—æ˜¯åƒä¸‡çº§çš„ï¼Œæ›´é«˜å°±ä¸Šclickhouse
- 100G ä¹Ÿä¸ç®—å¤šï¼ŒMySQL ä¸€æŠŠæ¢­ï¼ŒæŸ¥è¯¢å¾ˆè½»æ¾ï¼Œåªè¦åšå¥½ç´¢å¼•å°±å¥½äº†, å®åœ¨å¤šäº†å°±åˆ†åŒºåˆ†è¡¨ã€‚

- æ²¡ç”¨è¿‡supabaseï¼Œä»¥mysqlå¥½ä¹…ä¹‹å‰çš„ç‰ˆæœ¬ä¸ºåŸºå‡†ï¼Œæ²¡ç´¢å¼•çš„æƒ…å†µä¸‹ï¼Œå…¨è¡¨æ‰«æ1såœ¨100w~200wæ•°æ®ï¼Œé­”æ”¹ç‰ˆåº”è¯¥ä¼šå¿«ï¼Œç´¢å¼•åšå¥½ï¼Œå°±æ²¡å•¥äº‹
# discuss-vendors
- ## 

- ## 

- ## ğŸ”¥ [Techniques for Microsoft SQL database design and optimization | Hacker News_201612](https://news.ycombinator.com/item?id=13277061)
- 
- 
- 

# discuss
- ## 

- ## 

- ## Database Caching is a million-dollar technique you canâ€™t ignore.
- https://twitter.com/ProgressiveCod2/status/1765333907522916465
- Cache-Aside Strategy
- Read-Through Strategy
- Write-Around Strategy
- Write-Through Strategy
- Write-Back Strategy

- ## ğŸŒ° Many tutorials give you tips to improve your Database performance using only single-column Indexes.
- https://twitter.com/deisbel/status/1761384992775500055
  - Using 4 examples, Iâ€™ll teach you 3 new tips to create great Composite Indexes.

- ## ğŸ’¡ Data skipping is one of the common techniques used with large volumes of data to achieve better query performances.
- https://twitter.com/Dipankartnt/status/1759054983029203240
  - The whole idea is simple - read as less data as possible
  - What that means is that as a compute engine paired with a data lake table format like @apachehudi , it should read only the data files that are needed to satisfy the query from the storage. Data skipping reduces the volume of data that needs to be scanned and processed.
  - Of course this is made possible because of the metadata information provided by file formats like Parquet. Basically, each Parquet file contains the min/max values of each column along with other useful info: num of NULL values. These min/max values are 'column statistics'.
  - Now, although, we could directly leverage these stats for data skipping, this could affect the query performance because the engine still has to go through each file to read the footer. 

- What's Hudi's approach?
  - it adds a next level of pruning. basically it takes all these stats & collates the info in the form of an INDEX.
  - Indexes such as this are incorporated into Hudi's internal metadata table so engines can just get the files where the data is stored.
  - Therefore, instead of reading individual Parquet footers, compute engines can directly go to the metadata table's index & fetch the required files. Way faster.
  - [Hudiâ€™s Column Stats Index and Data Skipping feature help speed up queries by an orders of magnitude _202206](https://www.onehouse.ai/blog/hudis-column-stats-index-and-data-skipping-feature-help-speed-up-queries-by-an-orders-of-magnitude)

- ## [Understanding Connections and Pools | Hacker News _202101](https://news.ycombinator.com/item?id=25644656)
- 
- 
- 

- ## you can delete old data very efficiently if you partition your table by time ranges and drop one of them.
- https://twitter.com/tobias_petry/status/1748340608849072602
  - [Delete Old Rows with Partitions - Database Tip](https://sqlfordevs.com/partition-delete-old-rows)
- Can this be applied to previously created tables?  A clients MySQL database has grown so large there isn't enough disk space to run OPTIMIZE TABLE after deleting table records.  This seems like a good trick to free up disk space.
  - You would have to create a new table and insert the rows there again.
- Have any tricks to free up disk space on a MySQL 5.6.3 database when the host doesn't have enough disk space to run OPTIMIZE TABLE?

- What would you use to maintain the partitions, events, or external cron?
  - I would use a cron

- ## âš¡ï¸ æˆ‘å¸ææ•°æ®åº“ä¼˜åŒ–ï¼Œæˆ‘æ²¡å¿ä½ï¼Œè¯´æˆ‘ä»¬å…ˆæŠŠè¯»å†™åˆ†ç¦»æäº†ã€‚å› ä¸ºè‡³å°‘èƒ½æå‡ºæ¥ 10x çš„æ€§èƒ½ï¼Œè¿˜å¥½æ“ä½œï¼Œä¸ç”¨è€ƒè™‘ç»†èŠ‚ã€‚
- https://twitter.com/yfractal/status/1757276270457770225
  - ä½†åˆ°æ‰§è¡Œä¸Šï¼Œåˆæ˜¯å„ç§ç»†èŠ‚ä¼˜åŒ–ã€‚æ¯”å¦‚æˆ‘è¯´æŠŠåå°ä»»åŠ¡éƒ½ä¸¢åˆ° read replica ä¸Šï¼Œå˜æˆäº†æŒ‘æµé‡å¤§çš„ã€‚ç„¶åè¿˜æœ‰å¿™ç€åŠ  cache çš„ã€‚

- æœ€è›‹ç–¼çš„æ˜¯æä¸ªåŠå¤©ï¼Œè´¹å°½å£èˆŒï¼Œæœ€å PM è¿‡æ¥è¯´ï¼šä½ ç»™çš„æ–¹æ¡ˆæ€ä¹ˆåšäº†è¿™ä¹ˆä¹…ï¼Ÿæ€ä¹ˆåˆ°ç°åœ¨é—®é¢˜è¿˜å¥½å¤šï¼Ÿ

- ## âš¡ï¸ å…¬å¸æä¼˜åŒ–ï¼Œè®¨è®ºå¦‚ä½•ä¼˜åŒ– SQL å’Œä¸šåŠ¡ã€‚æˆ‘å®åœ¨å¿ä¸ä½ï¼Œè·Ÿä»–ä»¬è¯´ï¼Œå…ˆåšè¯»å†™åˆ†ç¦»ï¼Œä¹‹åæŠŠèƒ½è¿èµ°çš„è¡¨éƒ½è¿èµ°ã€‚
- https://twitter.com/yfractal/status/1745979462615982578
  - æˆ‘å¸è™½ç„¶é‡å¤§ï¼Œä½†æ²¡å¤šå¤§éš¾åº¦ã€‚åç«¯éš¾åº¦å¤§çš„ä¸€ä¸ªç°è±¡æ˜¯ç”¨æŠ˜è…¾å­˜å‚¨ï¼Œæ¯”å¦‚ä¸Šå®¶å…¬å¸æŠ˜è…¾äº† 5ã€6 ç§å­˜å‚¨ã€‚
  - æ‰€ä»¥ç°åœ¨è¯»å†™åˆ†ç¦»ï¼ŒåŠ è¿è¡¨å¯ä»¥è‹Ÿæ´»å¾ˆä¹…äº†ã€‚è¿ç§»çš„å¦ä¸€ä¸ªå¥½å¤„æ˜¯ï¼Œè´£ä»»æ¸…æ™°ï¼Œå¥½ä¼˜åŒ–ã€‚

- è¯»å†™åˆ†ç¦»ï¼Œæ•°æ®å½’æ¡£ï¼Œæ…¢SQLå’Œç´¢å¼•ä¼˜åŒ–åŸºæœ¬å¯ä»¥è§£å†³å¤§éƒ¨åˆ†é—®é¢˜ã€‚å¾ˆå¤šæ—¶å€™è¿ç§»å’Œæ‹†åˆ†çš„éš¾ç‚¹ï¼Œä¸åœ¨æŠ€æœ¯è€Œåœ¨äººï¼Œé…åˆåº¦ä¸é«˜ï¼Œæ¨è´£
  - è¯»å†™åˆ†ç¦»ä¹Ÿè¦çœ‹ä¸šåŠ¡è§„æ¨¡å’ŒäººåŠ›åˆ†é…ï¼Œå¤§éƒ¨åˆ†æ—¶å€™ï¼Œæ•°æ®å½’æ¡£ï¼Œæ…¢æŸ¥è¯¢ä¼˜åŒ–ï¼Œåº”ç”¨ç¼“å­˜å¯ä»¥è§£å†³é—®é¢˜ã€‚

- è¿è¡¨æ˜¯ä»€ä¹ˆï¼ŸæŠŠå…¶ä»–æœåŠ¡çš„è¡¨è¿ç§»åˆ°å•ç‹¬çš„æ•°æ®åº“ï¼Ÿ
  - æ©ï¼Œæ˜¯çš„ã€‚

- è¯·é—®å•¥æ˜¯è¯»å†™åˆ†ç¦»ï¼Ÿ
  - ä¸»åº“å†åŠ å‡ ä¸ªä»åº“ï¼Œç„¶åè¯»æµé‡å»ä»åº“ã€‚ç‰ºç‰²ä¸€ç‚¹ç‚¹ä¸€è‡´æ€§ï¼Œæ¢è¯»æµé‡å¯æ‹“å±•ã€‚

- ä¹‹å‰æˆ‘å°±åªæ¨è¿™ä¸ªï¼Œæ•ˆæœå¾ˆå¥½ã€‚æˆ‘é€‰å‹çš„å€¾å‘æ˜¯ç”¨æœ€ç³™çŒ›ä¸” proved çš„åŠæ³•æ¥è§£å†³è¡Œä¸šé‡Œå·²ç»è§£å†³è¿‡çš„é—®é¢˜ï¼ŒæŠŠæœ€æ— èŠçš„è½åœ°å·¥ä½œæ¨ä¸‹å»ï¼Œå¾ˆå¤šèŠ±æ‹³ç»£è…¿çš„åŠæ³•çƒ‚å°¾é£é™©éƒ½å¾ˆé«˜
  - well-defined/resolvedçš„é—®é¢˜å…¶å®æœ€é€‚åˆç”¨é‚£ç§çƒ‚å¤§è¡—ä¸”boringçš„æ–¹æ¡ˆã€‚æå·¥ç¨‹ä¸æ˜¯è‰ºæœ¯è¡¨æ¼”ï¼Œç”¨å¯æ§çš„æˆæœ¬èµ¶ç´§æŠŠäº‹å„¿åšå¥½æ˜¯æœ€é‡è¦çš„ã€‚å·¥ä½œè¶Šä¹…ç»å†çš„æƒ…å†µè¶Šå¤šè¶Šè§‰å¾—æ²‰è¿·/é™·å…¥çº¯ç²¹æŠ€æœ¯å¯¹engineeræ¥è¯´æ˜¯ä¸€ç§ç—…

- 
- 
- 

- ## Parquet Row Group Skipping with Bloom Filter in Apache Hudi 
- https://twitter.com/Dipankartnt/status/1735715073409106056
  - Parquet filter pushdown is a performance optimization method that prunes irrelevant data from a parquet file to reduce the amount of data scanned by a query engine

- You can skip row groups using 3 ways:
  - column min/max statistics. Column min/max stats are simple. Based on the min/max stats, you can filter out values that are not in the range.
  - dictionary filter. With dictionary, you can filter out values that are between min & max but not in the dictionary. Problem with dictionary is that it can consume more space with higher cardinality columns.
  - bloom filter. 
- A bloom filter is a probabilistic data structure that allows you to identify whether an item belong to a data set or not. 
  - It either outputs: "definitely not present" or "maybe present" for every data search.
  - By using Bloom filters, you can efficiently skip over large portions of the Parquet file that are irrelevant to your query, reducing the amount of data that needs to be read and processed.
- With the 0.14 release of @apachehudi , you can use this bloom filter to skip non-relevant row groups. This can be very valuable for improving query performance & reducing I/O operations when dealing with large-scale data.

- ## ğŸ”¥ [Demystifying Database Performance for Developers | Hacker News_202205](https://news.ycombinator.com/item?id=31287442)
- 
- 
- 

- ## ğŸ”¥ [Database Performance at Scale â€“ A free book | Hacker News_202310](https://news.ycombinator.com/item?id=37778069)
- 
- 
- 
