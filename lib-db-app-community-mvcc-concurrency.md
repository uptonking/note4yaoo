---
title: lib-db-app-community-mvcc-concurrency
tags: [community, concurrency, database-design, mvcc]
created: 2023-11-01T10:14:25.622Z
modified: 2023-11-01T10:15:06.245Z
---

# lib-db-app-community-mvcc-concurrency

# guide

- [é«˜å¹¶å‘ç³»ç»Ÿè®¾è®¡ 40 é—® | JAVA æ¶æ„å¸ˆç¬”è®°](https://zq99299.github.io/note-architect/hc/)
# discuss-stars
- ## 

- ## Optimistic vs Pessimistic concurrency control
- https://x.com/sunbains/status/1864854818214338821
  - TiDB used to be OCC only but had to change to PCC and OCC is now optional.
  - [Pessimistic or Optimistic Concurrency Control? Lessons Learned from Real-World Customer Scenarios | by siddontang _202412](https://medium.com/@siddontang/pessimistic-or-optimistic-concurrency-control-lessons-learned-from-real-world-customer-scenarios-a4f0b8dd6e49)

- I assume that CockroachDB started with naive OCC and now has AOCC (Adaptive OCC). And this lesson will be learned many times.
  - [Navigating CockroachDB Traffic Jams with a Smile: Adaptive Optimistic Concurrency Control | by Byte Blog | Medium _202409](https://byteblog.medium.com/navigating-traffic-jams-with-a-smile-adaptive-optimistic-concurrency-control-in-cockroachdb-da03c747bb69)

- ## Reading this paper: 'Why Events Are A Bad Idea (for high-concurrency servers) (2003)'
- https://x.com/iavins/status/1863933376627097644
- Event based programming is bad says academic authors who never wrote large scale software
- they didnâ€™t know about Java Virtual Threads at that time

- ## ğŸ†šï¸ğŸ¤¼ğŸ» To thread, or not to thread: How to build high concurrency systems?
- https://x.com/penberg/status/1860248258808852900
  - There are two main concurrency models: threads and events, but which one should you choose for your system?
- A thread is a sequence of instructions that represents an independent flow of control within a program. A thread has its own program counter and a thread, but typically shares other resources such as memory with other threads.
  - However, sharing resources has a downside too: you must synchronize access to shared resources (for example, with locks); otherwise you may end up corrupting data or having race conditions. Writing correct multi-threaded code can be extremely challenging.
  - Synchronization a major drawback for high concurrency: it can be hard to get right (risking data corruption and deadlocks) but it is also often inefficient because locking can prevent concurrency.
- Event-based concurrency departs from the sequential programming model to asynchronous one. 
  - Instead of having independent sequences of instructions (threads), you express concurrency via event handlers.
  - An event-based system has an event loop (sometimes referred to as I/O dispatcher) and bunch of event handlers, as illustrated by Ousterhout.
  - Today, event loops use OS interfaces such as io_uring, epoll, or kqueue, to poll for events such as received network messages or I/O completion, and then call the application specific event handlers to perform work.
  - However, the asynchronous nature of event-based systems can make the control flow of the system hard to understand, an argument made by Behren et al some years after Ousterhout's critique on threads.
- Behren et al also make the point that much of the validity of Ousterhout's claim is because of the way threads are implemented as kernel threads. However, implementing threads in user space solves the task switching overheads.
- If there is no fundamental difference between events and threads, how do you choose between the two? Behren claims that the decision should be based on what is more appropriate for your type of application. But is this still true two decades later?
  - I think not: thread implementations have consolidated on the inefficient kernel-thread model, using shared data is expensive, and I/O is much faster than the CPU. 
  - Therefore, lot of high concurrency systems have adopted a thread-per-core approach with event-based concurrency.
  - tl; dr; For high concurrency systems, an event-based concurrency model seems to be a good default to go for because you're able to take better advantage of hardware capabilities because of practical reasons.
  - Of course, the core argument remains: you can likely implement thread-based concurrency as efficiently as event-based. In fact, in GPUs, threading model works fine because scheduling and context switching is implemented in hardware itself.

- The third option is virtual threads is cooperative multi tasking  in userland and managed by a virtual machine. Probably want normal threads for CPU bound tasks. Ideally 1 per core. And virtual threads or events based on the architecture.

- The type of workload is important, to be implemented in either ways.
  - Lightweight threads like Goroutines are cheaper, smaller in size and context switch within the same process is also cheap.
  - The scheduling algorithms differ, Go is a round robin algorithmâ€¦ equal but not fair
- To continue how Go works, as an example of combining this two modelsâ€¦
  - Go uses different goroutines for different work
  - Go uses the epoll/kqueue/IOPC for network calls (aka network poller)
  - Go forks a new OS thread for blocking IO like files.
  - This approach maximizes throughput 
  - The chances of forking so many threads is unlikely due to Ulimit descriptors.

- Erlang VM uses managed processes and message passing with the Actor model, where does it fall in your description? It doesnâ€™t seem to fall under events as I understood your description. And it avoids data sharing becomes of the inherent problems.
  - Runtimes like V8/NodeJS using the event model with async I/O, but achieving concurrency with its single-thread model is quite a hassle Iâ€™d say.

- Events then would get handled one after another or in parallel? if parallel, what is running them? I guess threads?
  - You have parallel execution only if there are multiple cores. Even threads run one after another (although pre-empted) on a single CPU core. Typically, I/O dispatchers are per core so event handling is concurrent, but not parallel.

- You can get much of the benefit of both models using coroutines (fibers, green threads). Your local model is like you know, but the code can stall and do something else useful when a wait is required. FWIW, very high concurrency programming dispenses(å»æ‰, è±å…) with shared memory entirely.

- https://x.com/iavins/status/1860320145383821745
  - the duality of a man
# discuss-transactions
- ## 

- ## ğŸ’¡ How do atomic transactions work in a database?
- https://twitter.com/Franc0Fernand0/status/1759563991540494663
- Atomicity makes it possible for a transaction to have only 2 results:
  - it successfully executes all the operations and commit
  - it aborts because of a failure and undoes all the changes
- In a single database, atomicity is possible by saving all the changes to the data in a write-ahead log (WAL).
  - Every time changes are made, the WAL saves them on the disk so that they will last.
  - Each entry of the WAL contains all the information to undo all the changes.
- However, the WAL approach is insufficient if the transaction involves more than one database. 
  - In these situations, the two-phase commit protocol (2PC) is often used. 
  - The protocol says that one process should be the coordinator and the others should be participants.
  - Most of the time, the coordinator is the client process that requires the transaction.
- 2PC protocol has 2 steps:
- 1. Prepare
  - The coordinator requests everyone to get ready to complete the transaction. Each participant answers if they can commit or not. 
  - After answering, the participants wait for further instructions.
- 2. Commit
  - If all participants are ready, the coordinator sends them a commit request. Otherwise, it will ask to abort.
  - This step is a point of no return: the transaction has to be committed or aborted.
- The 2PC protocol works but has some drawbacks: 
  - it's slow since it requires multiple round trips between the processes
  - it gets blocked if the coordinator fails or a participant fails in the commit step
- Replicating the involved processes makes 2PC more robust but adds complexity.

- ## Relies on a global logical clock and claims that an MVCC visibility check can be as simple as an integer comparison. 
- https://twitter.com/sunbains/status/1757617183218442588
  - Order transaction on their â€œrecentnessâ€ which speeds up GC.
- Would love to see a comparison to LMDB, as its the best one(currently) in terms of random read perf.

- ## What is a Transaction
- https://twitter.com/DominikTornow/status/1759623957291147331
  - A transaction is a sequence of a read or a write of a data object that ends in exactly one commit or exactly one abort.

- ## What are transactions in a database?
- https://twitter.com/Franc0Fernand0/status/1754422259869974682
  - Transactions are a way for databases to group a set of operations into one.
  - This is helpful in many ways and simplifies the programming model at the application level.
  - In a single relational database, transactions make sure that the following ACID conditions are met

# discuss
- ## 

- ## 

- ## 

- ## 

- ## ğŸŒµ Are there databases which let you continue the execution of a transaction on connection failures or machine restarts? 
- https://x.com/iavins/status/1801450141993628092
  - Why donâ€™t databases support such a feature?
- This is doable once your client-commit is entirely optimistic. My stuff does this, so it supports what you're asking for without actively doing it.
- WAL integrity checks might be able to do this fr

- Such a design would imply in-memory transaction state actually gets persisted somewhere. Git is an example of a â€œdatabaseâ€ that does this, in the form of branches. @DoltHub extends the idea to a proper structured database. A branch is just a long-lived transaction.

- Technically such databases exist: Git or really any DVCS.
  - The idea requires 2 assumptions to make them feasible:
  1. Outdated reads are acceptable.
  2. Transactions don't rollback on write conflicts.
  - Apply to CRDTs and local-first systems - but few of them support transactions.

- FWIW, Iâ€™ve seen this implemented in a proprietary database that I worked on. It used an optimistic commit protocol. It worked just fine, there is technical barrier.
  - There is â€œno technical barrierâ€, itâ€™s just that people are more used to pessimistic commit and therefore see everything through that lens.
  - The problem is on the db side, not about whether the clientâ€™s operation completed on the db or not. The latter is easy to resolve by rollback by default. The real problem is that the db canâ€™t tell if the client will retry or resume. The db canâ€™t keep holding resources on behalf of the client forever. There is usually timeout mechanism before the db will release any resources that were held on behalf of the client.
- A distributed database can do this partially, if a storage node goes down. A query that needs data from that node will retry from the new leader. Not the same thing exactly but at least in spirit. If a client fails over to another node because of a SQL node crash, XA IIRC does support this for transactions that are in XA Prepared state.

- Oracle has this feature - called Application Continuity. Client-side logic adds a lot here.
- Oracle RAC has transparent application failover since 2017 and application continuity but it seems too much money for avoiding optimizing application code making it re-run friendly.

- Very interesting question. The whole Distributed SQL was invented because we didn't want to solve this problem. But if we go back to on Prem database days, Tandem used to sell "bulletproof" DBs back in the day. Let me check if their architecture supported such failures

- One argument against building out such a feature in the distributed database or as some proxy layer on top of it is that its as or more likely that the code running the transaction dies. The system needs to do something reasonable in that case too.
  - If thereâ€™s way more db nodes, then the complexity might be worth it. But how do you even recover? Ideally the client recover, but getting db client drivers to be more sophisticated is hard work. Low risk tolerance, legacy code and many langs.
  - If an http-based, request-response oriented protocol sat underneath your sql drivers, then maybe youâ€™d see more such recovery protocols built into transaction protocols. Tl; dr to do it right itâ€™s probably gotta be in the driver, and thatâ€™s a hard place to innovate
- For many distributed databases, you wonâ€™t see a failure unless the transaction coordinator node dies. Itâ€™d be unreasonable if any node dying lead to all transactions failing, but itâ€™s hard to do better than that and itâ€™s arguably not so bad given the app can die too.

- To get good performance, transactions (both read-only and read-write) must "flow".  They need to be "short" (time-wise) and "small" (data-wise).  With this in mind, failing-fast is a better option than allowing "resumable" transactions. 
  - There might be use-case for "resumable" transactions, but to me, and with the information I have, this is not a feature I would invest much in

- Not the full support but Redshift with scheduling queries does something similar. You can schedule query have a event bridge and then use that to get the query execution status.

- ## Here's a new post walking through an implementation of MVCC and major SQL transaction isolation levels, in 400 lines of Go code.
- https://x.com/eatonphil/status/1791225675287867742
  - These ideas might sound esoteric, but they impact almost every developer using any database.
  - [Implementing MVCC and major SQL transaction isolation levels | notes.eatonphil.com](https://notes.eatonphil.com/2024-05-16-mvcc.html)
- Why do we set all the visible versions as deleted when using â€œsetâ€ and â€œdeleteâ€? The mvcc implementations I saw usually only modify the most recent value.
  - It's a good question. I'm not sure! I decided until I knew for sure it was best to mark them all.

- ## Are there any databases which do MVCC at page level granularity and have multi writer concurrency? Or any research papers on the same?
- https://twitter.com/iavins/status/1775532839179633076
  - Only one I know is SQLite's `BEGIN CONCURRENT` (alpha) feature
- SQLite also does single writer MVCC with pages.. but I am interested in learning more about multiple writers.

- I donâ€™t understand the question. MVCC is usually associated with transactions and the versions are usually of rows (donâ€™t have to be). A page can be made up of many rows and versions of these rows from many different transactions. Some active and some completed or rolled back. Note rollback doesnâ€™t imply that the changes are undone. The changes are â€œinvisibleâ€, thatâ€™s the job of the MVCC.
- AFAIK, Oracle uses row versioning for MVCC, InnoDB copies that design.
- Itâ€™s still row version based. The head of the row version list is in the block. Thatâ€™s all. It makes no sense to log both blocks and row versions. 
- Postgres stores the versions in the table and thatâ€™s why vacuum is a pain.

- ## ğŸ†šï¸ Trying to get a sense for thread architectures:
- https://twitter.com/eatonphil/status/1773518000043299309
- Single-threaded: Cache-friendly and scales decently if you use some form of async io. Good for tasks with little-to-medium CPU-work since cache won't get busted. i.e. works fine with shared data between tasks. Too much CPU-work though limits your ability to scale.
- Work-stealing & work-sharing: Cache-unfriendly, works fine with sync or async io, since cache is core-local and work-switching threads (to another core) thus busts the cache/makes it ineffective. Fine for tasks with little CPU-work where cache doesn't matter as much. Scales well for little CPU-work tasks.
- Thread-per-core: Cache-friendly so long as work is independent, works fine with sync or async io. Scales best among all options for small or large CPU-work. Even if there is shared data, still probably scales better than work-stealing/sharing and single-thread.
- Multi-process: Basically the same as thread-per-core, if processes are 1:1 with cores?

- cache is not important compared to blocking IO(disk>network) wait, which is the origins async-io is used for. But concurrent programming includes the threading/cache thing, while the `concurrent` concept covers the syntax sugar async/await.

- Thread-per-core with blocking I/O is going to end up leaving lots of cores idle so you pretty much have to use async. I wonder if the cache friendliness/unfriendliness has ever been measured or if it's propaganda. Sadly, results are going to be different between Intel & AMD.

- If you want to build intuition, think about how you are mapping hardware resources (CPU and memory) to your application logic and state.

- ## If you are interested in exploring Race Conditions & Data Races, I recommend Testing for Race Conditions via SMT Solving
- https://twitter.com/DominikTornow/status/1758921755807437258

- ## The diagram below explains what ACID means in the context of a database transaction.
- https://twitter.com/alexxubyte/status/1732783270855876654

- ## ğŸ˜ï¸ When Uber created their in-house database (known as Schemaless), they wanted high-availability writes.
- https://twitter.com/ProgressiveCod2/status/1726136613468852376
  - This was difficult to achieve because all writes in their setup went to the Leader node.
  - And if the Leader node goes down, HA becomes difficult to achieve.
  - To get around this issue, Uber used a technique known as Buffered Writes.
  - Buffered Writes meant that every write request was stored on at least two nodes: - The Primary Leader - The Secondary Leader
  - But there's a lot that goes behind the scenes to make the whole thing work
  - [PC#16 - The Secret Trick to High-Availability_202310](https://progressivecoder.beehiiv.com/p/the-secret-trick-to-high-availability)

- ## Cool thread-safe triple buffering approach made lock-free by atomically swapping the pointers to the buffers
- https://twitter.com/zack_overflow/status/1722275195594133817
  - Thinking about using this for my code editor when I add LSP support
  - The main render thread needs to get data from the second thread which manages the LSP server process

- ## ä»Šå¹´ gopher å¤§ä¼šä¸Šçš„åˆ†äº«ï¼šBuilding a Highly Concurrent Cache in Go: A Hitchhiker's Guide
- https://twitter.com/kscooooo/status/1719976363166498889
- LRUã€LFU éƒ½ç®€å•è¿‡äº†ä¸€ä¸‹ï¼Œè¿˜æœ‰ä¿©è€…çš„ç»“åˆ LRFU (Least Recently/Least Frequently) ï¼Œå·¥ä½œåŸç†ç±»ä¼¼äº LFUï¼Œæ¯ä¸ªé¡¹éƒ½æŒæœ‰ä¸€ä¸ªå€¼ï¼Œå« CRF(Combined Recency and Frequency)ï¼Œé€šè¿‡ä¸€ä¸ªå‚æ•° Î» æ¥å†³å®šå¯¹æœ€è¿‘ entry çš„æƒé‡ã€‚
  - ç„¶åç€é‡ä»‹ç»äº† DLFU (Decaying Least Frequently Used)ï¼Œæ˜¯ LFU çš„å˜ä½“ï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œentry çš„è®¿é—®è®¡æ•°ä¼šè¡°å‡ã€‚å¯¹æ¯” LFU ä¼šæŠŠé•¿æœŸä¸ç”¨çš„çƒ­é—¨ entry å¹²æ‰ã€‚
- å…¶æ¬¡å°±æ˜¯è€ç”Ÿå¸¸è°ˆçš„ä¼ªå…±äº«ã€‚ä¼ªå…±äº«å°±æ˜¯å¤šä¸ª cpu è®¿é—®äº†ä¸€ä¸ª cache line ä¸Šä¸åŒçš„å˜é‡ï¼Œä¸€ä¸ª cpu ä¿®æ”¹äº†é‚£ä¹ˆå…¶ä»–çš„ cpu çš„ç¼“å­˜å°±å¤±æ•ˆäº†ï¼Œå°±å¾—èµ° cpu åŒæ­¥çš„åè®®(mesi)ï¼Œè¿™ä¸ªå°±æ…¢äº†ã€‚
  - å¦‚æœè®¾è®¡çš„æ•°æ®ç»“æ„æ¯”å¦‚åƒ struct çš„ä¿©æˆå‘˜éƒ½ä¼šä¿®æ”¹çš„è¯ï¼Œå°±ç©ºé—´æ¢æ—¶é—´ï¼Œç»™è¿™ä¿©æˆå‘˜å˜é‡ä¸­é—´å¡æ— ç”¨å­—èŠ‚æŠŠä½ç½®ç»™å äº†ï¼Œè®©è¿™ä¿©æˆå‘˜å˜é‡åœ¨ä¸åŒçš„ cache line ä¸Šã€‚è¿™æ ·å¤šæ ¸ cpu ä¿®æ”¹å„è‡ªçš„æˆå‘˜å˜é‡æœ€å¤§ç¨‹åº¦å°±é¿å…äº† mesi è¿™ç§åŒæ­¥ã€‚
- xsync. Map æˆ‘ä¹‹å‰çœ‹åˆ°è¿‡ï¼Œæ˜¯ java å†™çš„é‚£ä¸ªæ—¶åºæ•°æ®åº“ QuestDb ä¸»ç¨‹å†™çš„å¹¶å‘åº“ï¼Œæ ¸å¿ƒæ€æƒ³å°±æ˜¯æ•°æ®ç»“æ„åŸºäº CLHT(Cache-Line Hash Table) è®¾è®¡çš„ï¼Œè·ŸåŸç”Ÿçš„ map å’Œ sync. Map ç›¸æ¯”ï¼Œå®ƒçš„æ¡¶æ˜¯ç‹¬å  cache line çš„ï¼Œç©ºé—´æ¢æ—¶é—´ã€‚
  - sync. Map æˆ‘è®°å¾—ä¹Ÿæ˜¯ç©ºé—´æ¢æ—¶é—´ï¼Œä½†æ˜¯æ€è·¯ä¸åŒï¼Œå¼„äº†ä¸€ä¸ª read map ä¸“é—¨å¹¶å‘è¯»ï¼Œå†™çš„è¯å°±æ˜¯å¾€ dirty map å†™ï¼Œç„¶ååŒæ­¥ï¼Œåœ¨ read map è¯»ä¸åˆ°å°±å¾€ dirty map é‡Œè¯»ï¼Œkey åˆ çš„è¯å¤šä¸ªä¼šæ”’åœ¨ä¸€èµ·åˆ ï¼Œç»†èŠ‚è¦å¤šå¾ˆå¤šã€‚ 
  - xsync. Map å°±ä¾§é‡äºå¹¶å‘ç¯å¢ƒä¸‹å†™å¤šçš„æƒ…å†µã€‚
- ç„¶åå°±æ˜¯å•ç‹¬å¼€ goroutine å¼„ç¼“å­˜æ¸…ç†ï¼Œé¿å…é˜»å¡ä¸»é€»è¾‘ã€‚é©±é€ç¼“å­˜çš„è¿‡ç¨‹æ¢äº†æ’åºç®—æ³•ï¼Œä¸æ˜¯ç”¨çš„æ ‡å‡†åº“çš„ pdqsortï¼Œç”¨çš„ quickselect ç®—æ³•ï¼Œæ˜¯çº¿æ€§ç®—æ³•ï¼Œåªå¯¹ N ä¸ªæœ€å°å…ƒç´ è¿›è¡Œæ’åºï¼Œä¸å¯¹æ•´ä¸ªæ•°ç»„è¿›è¡Œæ’åºã€‚
  - ç›¸å½“äºå‰ªæäº†ã€‚è¿™ä¸ªå°è±¡è¿˜æ˜¯è›®æ·±åˆ»çš„ï¼Œå½“æ—¶åœ¨é¢‘é“é‡Œä½œè€…è®¨è®ºçš„æ—¶å€™ï¼Œç¤¾åŒºé‡Œå°±æœ‰äººç«‹é©¬å›ç­”äº†è¿™ä¸ªï¼Œå½“æ—¶çœ‹åˆ°è¿˜æ˜¯è§‰å¾—å¦™å•Šï¼Œè¿˜æ˜¯ç³Š crud ä¹…äº†åŸºæœ¬éƒ½æ˜¯ sort æ ‡å‡†åº“ä¸€æŠŠæ¢­ã€‚å½“æ—¶ go runtime çš„ä¸»ç¨‹ michael è¯´éšæœºé©±é€è›®å¥½çš„ï¼Œéƒ½ä¸ç”¨æ’åºäº†ï¼Œä½œè€…å›å¤è¯´éšæœºé©±é€æ˜¯è›®å¥½çš„ï¼Œè¿˜èƒ½ç®€åŒ– 80% çš„å®ç°
- è¿˜æœ‰å¸¸è§åŸå­æ“ä½œæ›¿æ¢é”ï¼Œè¿™é‡Œ go å¹¶å‘åº“ä¸»ç¨‹ bryan å½“æ—¶ç‰¹æ„åœ¨é¢‘é“é‡Œå¼ºè°ƒäº†ä¸€ä¸ªç‚¹ï¼Œå¦‚æœä½¿ç”¨åŸå­æ“ä½œåœ¨æ¯æ¬¡è®¿é—®æ—¶æ›´æ–°å…±äº«å˜é‡ï¼Œå…¶å®ä¹Ÿä¸ä¼šä»åŸå­æ“ä½œä¸­æåˆ°è›®å¤§çš„æå‡ã€‚åŸå­å†™å…¥åªæ˜¯å°†é”å®šä»æ˜¾å¼ï¼ˆåœ¨ä»£ç ä¸­)ç§»åŠ¨åˆ°éšå¼(CPU ç¼“å­˜ä¸€è‡´æ€§åè®® mesi)ï¼Œè¿˜æ˜¯æœ€åä¼šç¢°åˆ°é”å®š cache line äº§ç”Ÿç«äº‰ã€‚
- æ‰€ä»¥å¤§éƒ¨åˆ†ç¯‡å¹…éƒ½æ˜¯å¼ºè°ƒä¼ªå…±äº«ã€cpu ç¼“å­˜ä¸€è‡´æ€§åè®®ã€‚
  - å…¶ä»–çš„æŠ€å·§æœ‰æ¯æ¬¡æ“ä½œè®°å¾—æœ‰è¶…æ—¶å…œåº•ï¼Œç¼“å­˜æ“ä½œæ—¶è®°å¾—è¦åŠæ—¶æ£€æŸ¥ ctxï¼Œè¶…æ—¶äº†å°±ç«‹é©¬é‡Šæ”¾é”è¿”å›ã€‚
  - å†™å¥½ go bench(RunParallel) ã€å–„ç”¨ pprofã€benchstat å¯¹æ¯”æ–°æ—§ bench çš„å®ç°ç­‰ã€‚ç„¶åå·®ä¸å¤šå°±æ²¡äº†
- ç»“åˆäº†ç¤¾åŒºé¢‘é“çš„è®¨è®ºå’Œ ppt çš„æ€»ç»“ï¼Œè§†é¢‘ä»Šå¹´ gopher å¤§ä¼šè¿˜æ²¡æ”¾å‡ºæ¥ï¼Œæ€»ä½“æ¯”è¾ƒç®€å•ï¼Œéƒ½æ˜¯ç¤¾åŒºè€ç”Ÿå¸¸è°ˆçš„ã€‚åšä¸ªç¬”è®°æ–¹ä¾¿è‡ªå·±
