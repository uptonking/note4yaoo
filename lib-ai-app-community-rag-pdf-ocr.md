---
title: lib-ai-app-community-rag-pdf-ocr
tags: [community, image, ocr, pdf, rag]
created: 2026-01-25T17:22:38.106Z
modified: 2026-01-25T17:23:01.510Z
---

# lib-ai-app-community-rag-pdf-ocr

# guide

# discuss-stars
- ## 

- ## 

- ## [Architecture breakdown: Processing 2GB+ of docs for RAG without OOM errors (Python + Generators) : r/Rag _202602](https://www.reddit.com/r/Rag/comments/1qv2ks2/architecture_breakdown_processing_2gb_of_docs_for/)
  - Most RAG tutorials teach you to load a PDF into a list. That works for 5MB, but it crashes when you have 2GB of manuals or logs.
  - I built a pipeline to handle large-scale ingestion efficiently on a consumer laptop. 
  - I made a full code-along video explaining the implementation line-by-line using Python and LangChain concepts.
  - Here is the architecture I used to solve RAM bottlenecks and API rate limits:
  - Lazy Loading with Generators: Instead of docs = loader.load(), I implemented a Python Generator (yield). This processes one file at a time, keeping RAM usage flat regardless of total dataset size.
  - Persistent Storage: Using ChromaDB in persistent mode (on disk), not in-memory. Index once, query forever.
  - Smart Batching: Sending embeddings in batches of 100 to the API with tqdm for monitoring, handling rate limits gracefully.
  - Recursive Chunking with Overlap: Critical for maintaining semantic context across cuts.

- ## [AIå¼€å‘è€…å·¥å…·(2)â€”â€”2024 å¹´ 12 ä¸ªå¼€æºæ–‡æ¡£è§£æé¡¹ç›®çš„é€‰å‹å¯¹æ¯”è¯„æµ‹ï¼šPDFè§£æã€OCRè¯†åˆ«åŠŸèƒ½è§£è¯»ã€åº”ç”¨åœºæ™¯åˆ†æåŠä¼˜ç¼ºç‚¹æ¯”è¾ƒ - è«å°”ç´¢éšç¬”](https://liduos.com/ai-develope-tools-series-2-open-source-doucment-parsing.html)

# discuss-YOLO/traditional
- ## 

- ## 

- ## 

- ## 

- ## 

- ## [From YOLO to VLMs: Advancing Zero-Shot and Few-Shot Detection of Wastewater Treatment Plants Using Satellite Imagery in MENA Region | Abstract _202512](https://arxiv.org/abs/2512.14312)

# discuss-pdf-elements
- ## 

- ## 

- ## 

- ## [Best way to handle pdfs containing huge tables in RAG : r/Rag](https://www.reddit.com/r/Rag/comments/1reezcu/best_way_to_handle_pdfs_containing_huge_tables_in/)
- Docling and detect and prechunk tables

- Parse into csv and add into a SQLite, have an agent to do that, don't do programmatically. Or have the rag trigger an agent with vision capabilities to answer
  - One approach, that can be expensive, is to have a vision model to parse the content and on a second step, the data is cleaned by another LLM call and validated against the original table. If you're running models at your end, it could be worth it.

- Sounds more suitable to store them in database tables and create/use a tool that query them.
# discuss-vlm
- ## 

- ## 

- ## 

- ## 
# discuss-tips
- ## 

- ## 

- ## ğŸ†š [RAGçŸ¥è¯†åº“è¿˜æœ‰æå¤´å—ï¼Ÿ _202602](https://linux.do/t/topic/1576669)
- ç”±äºä¸šåŠ¡éœ€æ±‚ï¼Œå›¢é˜Ÿéœ€å¤„ç†æµ·é‡çš„ç”µå­æ–‡æ¡£ã€‚å•ä¸ªè¡¨å•æ¶‰åŠçš„ PDF æ•°é‡é€šå¸¸åœ¨ 1000 è‡³ 5000 ä»½ä¸ç­‰ã€‚è‡ª 2025 å¹´ 12 æœˆå¯åŠ¨çŸ¥è¯†åº“é¡¹ç›®ä»¥æ¥ï¼Œæˆ‘ä»¬åœ¨å‡ ä¸ªæœˆçš„å®æˆ˜ä¸­é‡åˆ°äº†ä»¥ä¸‹ç“¶é¢ˆï¼š
  - è¶…é•¿æ–‡æ¡£è§£æéš¾ï¼šéƒ¨åˆ†æ–‡æ¡£é¡µæ•°è¿‡å¤šï¼Œè§£æé€Ÿåº¦ææ…¢ï¼Œä¸”ææ˜“è§¦å‘å†…å­˜æº¢å‡ºï¼ˆOOMï¼‰ã€‚
  - å·¥ç¨‹å›¾çº¸è¯†åˆ«ç²¾åº¦ä½ï¼šé¡¹ç›®ä¸­åŒ…å«å¤§é‡å·¥ç¨‹å›¾çº¸ï¼Œé€šç”¨è§£æå·¥å…·å¸¸å°†å…¶è¯¯è¯†åˆ«ä¸ºæ™®é€šå›¾ç‰‡ï¼Œå¯¼è‡´è¦ç´ æå–ç¼ºå¤±æˆ–ä¸¥é‡å¤±çœŸã€‚
  - å¤šæ ¼å¼å…¼å®¹æ€§å·®ï¼šç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶æ¶µç›– PDFã€å›¾ç‰‡ã€CAD åŠ Office åŠå…¬æ–‡æ¡£ã€‚è™½ç„¶ PDF è§£æç›¸å¯¹æˆç†Ÿï¼Œä½†å¯¹äºè¶…å¤šåˆ—çš„å¤æ‚ Excel è¡¨å•ï¼Œè§£ææ•ˆæœå ªç§° â€œç¾éš¾â€ã€‚
  - æ·±åº¦é—®ç­”å¯é æ€§ä¸è¶³ï¼šå¯¹äºé«˜éš¾åº¦çš„é€»è¾‘æ¨ç†é—®ç­”ï¼Œå•çº¯ä¾èµ–é€šç”¨ LLM çš„è¯­ä¹‰æ£€ç´¢å¾€å¾€éš¾ä»¥è¾¾åˆ°ä¸šåŠ¡ç²¾åº¦è¦æ±‚ã€‚
  - å…¶ä»–é•¿å°¾é—®é¢˜ï¼šå¦‚ç¦»çº¿éƒ¨ç½²ç¯å¢ƒä¸‹çš„èµ„æºåˆ†é…ã€å“åº”å»¶è¿Ÿç­‰ã€‚
- æˆ‘ä»¬ä¼˜å…ˆå¯¹å¸‚é¢ä¸»æµçš„å¼€æº RAG æ¡†æ¶è¿›è¡Œäº†è¯„æµ‹ï¼Œä»¥ä¸‹æ˜¯å„äº§å“çš„å®æµ‹åé¦ˆï¼š

1. RAGFlow
åœ°ä½ï¼šRAG é¢†åŸŸçš„é‡é‡çº§æ¡†æ¶ã€‚

æ ¸å¿ƒä¼˜åŠ¿ï¼š
å¤šç§Ÿæˆ·æ¶æ„ï¼šå…·å¤‡å®Œå–„çš„å›¢é˜Ÿ / ç§Ÿæˆ·æƒé™ç®¡ç†ã€‚
å·¥ä½œæµç¼–æ’ï¼šæ”¯æŒçµæ´»çš„é—®ç­”æµç¨‹å®šåˆ¶ã€‚
DeepDocï¼šå…¶è‡ªç ”çš„è§£æç¥å™¨ï¼Œæ˜¯ç›®å‰å¯¹å¤æ‚æ–‡æ¡£é€‚é…åº¦è¾ƒé«˜çš„æ–¹æ¡ˆä¹‹ä¸€ã€‚
å®æµ‹ç—›ç‚¹ï¼š
ç²¾åº¦ä¸æ€§èƒ½å¹³è¡¡ï¼šé¢å¯¹å¤æ‚å›¾çº¸æ—¶è¦ç´ é—æ¼è¾ƒå¤šã€‚CPU æ¨¡å¼è§£æææ…¢ï¼ŒGPU æ¨¡å¼è™½æœ‰æå‡ä½†ä»éå®æ—¶ï¼›ä¸”ç”±äºæ˜¯å†…éƒ¨æ¡†æ¶ï¼ŒäºŒæ¬¡å¼€å‘é—¨æ§›è¾ƒé«˜ã€‚
OCR é€‚é…ï¼šç¬¬ä¸‰æ–¹ OCR å¼•æ“æ¥å…¥å°šå¤„äºå®éªŒé˜¶æ®µï¼Œåº•å±‚ä»é«˜åº¦ä¾èµ–å†…ç½®å¼•æ“ã€‚
å¹»è§‰é—®é¢˜ï¼šçº¯å‘é‡æ£€ç´¢éš¾ä»¥åº”å¯¹ç»“æ„åŒ–æŸ¥è¯¢ï¼Œéœ€å¼•å…¥ MCPï¼ˆModel Context Protocolï¼‰ç»“åˆ SQL æŸ¥è¯¢æ¥å¢å¼ºã€‚
å¼€æºåè®®ï¼šApache License 2.0ã€‚

2. Dify
åœ°ä½ï¼šç”Ÿæ€æœ€ä¸°å¯Œã€ä¸Šæ‰‹é—¨æ§›æœ€ä½çš„ RAG å¹³å°ã€‚

æ ¸å¿ƒä¼˜åŠ¿ï¼šæå…¶å‡ºè‰²çš„æµç¨‹ç¼–æ’ä¸æ’ä»¶åŠŸèƒ½ï¼Œæ”¯æŒå¤–æŒ‚ RAGFlow çŸ¥è¯†åº“ã€‚
å®æµ‹ç—›ç‚¹ï¼š
çŸ¥è¯†åº“ â€œåç§‘â€ï¼šDify çš„æ ¸å¿ƒåœ¨äº Agent ç¼–æ’ï¼Œå…¶è‡ªå¸¦çš„æ–‡æ¡£åˆ‡å‰²ä¸ç´¢å¼•åŠŸèƒ½ç›¸å¯¹åŸºç¡€ã€‚è™½å¯é€šè¿‡æ’ä»¶å¼ºåŒ–ï¼Œä½†éœ€æŠ•å…¥é¢å¤–å¼€å‘ç²¾åŠ›ã€‚
å•†ä¸šé™åˆ¶ï¼ˆéœ€ç•™æ„ï¼‰ï¼š
å…¶ä¿®æ”¹ç‰ˆçš„ Apache 2.0 åè®®è§„å®šï¼šç¦æ­¢æœªç»æˆæƒè¿è¥å¤šç§Ÿæˆ· SaaSï¼›å¿…é¡»ä¿ç•™ UI æ ‡è¯†ã€‚
ç»“è®ºï¼šè‹¥éœ€å¤šç§Ÿæˆ·æˆ–æ·±åº¦å»æ ‡è¯†åŒ–å•†ç”¨ï¼Œå¿…é¡»é€šè¿‡ API æ¥å…¥è‡ªç ”å‰ç«¯ã€‚

3. Yuxi-Knowï¼ˆè¯­æï¼‰
ç‰¹ç‚¹ï¼šç•Œé¢å‹å¥½ï¼Œç¬¦åˆå›½äººé€»è¾‘ã€‚
å®æµ‹ç—›ç‚¹ï¼šOCR å¼•æ“å¯¹æ¥æ–¹æ¡ˆè¾ƒä¸ºç”Ÿæ¶©ï¼Œè™½ç„¶é€‰é¡¹å¤šï¼Œä½†å®é™…è½åœ°æ•ˆæœä¸ä½³ï¼Œæ›´é€‚åˆå¤„ç†ä¸­çŸ­ç¯‡å¹…çš„ç®€å•æ–‡æ¡£ã€‚
å¼€æºåè®®ï¼šMITï¼ˆæåº¦å‹å¥½ï¼‰ã€‚

4. Weknora (è…¾è®¯)
ç‰¹ç‚¹ï¼šæ¨¡å—åŒ–æ¶æ„ï¼Œèåˆå¤šæ¨¡æ€é¢„å¤„ç†ï¼Œå¤§å‚å‡ºå“ã€‚
å®æµ‹ç—›ç‚¹ï¼šOCR å¼•æ“å†…èšç¨‹åº¦è¿‡é«˜ï¼Œå‡ ä¹æ— æ³•çµæ´»åˆ‡æ¢æˆ–è‡ªå®šä¹‰è¯†åˆ«é€»è¾‘ï¼Œæ”¹é€ éš¾åº¦å¤§ã€‚
å¼€æºåè®®ï¼šMITã€‚

5. AnythingLLM
ç‰¹ç‚¹ï¼šåŠŸèƒ½å…¨å®¶æ¡¶ï¼Œå†…ç½® TTS/STTï¼Œå¼€ç®±å³ç”¨ä½“éªŒå¥½ã€‚
å®æµ‹ç—›ç‚¹ï¼šOCR ä¾æ—§æ˜¯çŸ­æ¿ï¼Œé¢å¯¹ä¸“ä¸šé¢†åŸŸçš„é‡åº¦æ–‡æ¡£æ˜¾å¾—åŠ›ä¸ä»å¿ƒã€‚
å¼€æºåè®®ï¼šMITã€‚

- æ ¸å¿ƒç—›ç‚¹æ±‡æ€»
  - OCR è§£æèƒ½åŠ›çš„ â€œèµ„æºä¸ç²¾åº¦â€ æ‚–è®ºï¼šåœ¨ç¦»çº¿éƒ¨ç½²çš„æœ‰é™ç¡¬ä»¶è³‡æºä¸‹ï¼Œéš¾ä»¥åŒæ—¶æ»¡è¶³ â€œé«˜é€Ÿåº¦â€ ä¸ â€œé«˜ç²¾åº¦â€ çš„è§£æéœ€æ±‚ã€‚
  - æ··åˆæ£€ç´¢ä¸ç»Ÿè®¡åˆ†æç¼ºå¤±ï¼šä¸šåŠ¡é—®ç­”ä¸åªæ˜¯è¯­ä¹‰åŒ¹é…ï¼Œå¾€å¾€æ¶‰åŠå¤§é‡æ•°æ®åº“ç»Ÿè®¡ï¼ˆChat2SQLï¼‰ã€‚çº¯ RAG æ–¹æ¡ˆåœ¨å¤„ç†è¿™ç±»é—®é¢˜æ—¶å®¹æ˜“ â€œæŠ“çâ€ã€‚

- æ€»ç»“ä¸åæ€ ç›®å‰é¡¹ç›®å¤„äº â€œæ·±æ°´åŒºâ€ï¼šé€šç”¨æ¡†æ¶åœ¨é¢å¯¹å·¥ä¸šçº§ã€è¶…é•¿ã€å¤æ‚ç‰ˆé¢æ–‡æ¡£æ—¶ï¼Œå‡å‡ºç°äº†æ˜æ˜¾çš„è¾¹é™…æ•ˆåº”ã€‚
- ä¸‹ä¸€æ­¥æˆ‘ä»¬å¯èƒ½éœ€è¦è·³å‡º â€œå…¨ç›˜ä¾èµ–å¼€æºæ¡†æ¶â€ çš„æ€ç»´ï¼š
  - è§£è€¦è§£æå±‚ï¼šå»ºç«‹ç‹¬ç«‹çš„ OCR è§£æé›†ç¾¤ï¼Œå°†é•¿æ–‡æ¡£åˆ‡ç‰‡å¹¶è¡Œå¤„ç†ï¼Œå¹¶å¼•å…¥ VLM è¿›è¡Œå…³é”®è¦ç´ æ ¡å‡†ã€‚
  - å¼ºåŒ– MCP æ¶æ„ï¼šä¸å†å¼ºæ±‚ LLM è¯»æ‡‚æ•´ä¸ª Excelï¼Œè€Œæ˜¯é€šè¿‡ Agent è°ƒç”¨ SQL æ’ä»¶è¿›è¡Œæ•°æ®åˆ†æã€‚

- ğŸ‘¥

- rag å·²ç»è¢«æ•°å­¦è¯æ˜äº†ä¸Šé™ï¼Œå› æ­¤å¯ä»¥è¯´å·²ç»æ˜¯å¯ä»¥çœ‹åˆ°å¤´çš„è·¯äº†ï¼Œç°åœ¨æ›´å¥½çš„æ–¹æ³•åº”è¯¥æ˜¯å­¦ä¹  openclawï¼Œåšä¸€ä¸ªæ–‡ä»¶ç›®å½•ï¼Œåˆ¶ä½œä¸€ä¸ªåªè¯» agent æŸ¥è¯¢æ–‡ä»¶å¤§çº²è¿›è¡Œå…·ä½“å†…å®¹æŸ¥è¯¢

- æˆ‘ä¹‹å‰åœ¨å°ç±³å·¥ä½œçš„æ—¶å€™çœ‹åˆ«çš„éƒ¨é—¨çš„åŒäº‹ä¹Ÿæœ‰é‡åˆ°ä½ ä»¬è¿™ä¸ªé—®é¢˜çš„ï¼Œæœ€åçš„è§£å†³åŠæ³•æ˜¯é­”æ”¹ difyï¼Œè‡ªå·±é€ å‰ç«¯ï¼Œå¼€å‘é¢å¤–æ’ä»¶çš„å½¢å¼ï¼Œè¿™ä¸ªæˆ‘æ„Ÿè§‰æ˜¯åšçš„æœ€å¿«çš„æ–¹å¼äº†ï¼Œå…¼å®¹æ¨¡å¼éƒ½å¯ä»¥ä¾é æ’ä»¶è§£å†³ï¼Œç„¶åé˜…è¯»è¶…é•¿æ–‡æœ¬è¿™ä¸ªå…·ä½“å¾—çœ‹ä½ çš„éœ€æ±‚çš„äº†ï¼Œæˆ‘ä¹‹å‰åšéƒ¨é—¨è‡ªå·±çš„ RAG çš„æ—¶å€™æ˜¯æŒ‰ç…§ dify åšçš„ï¼Œç”¨äºä¸€äº›è¯Šæ–­å·¥ä½œï¼Œå»¶è¿Ÿå“åº”è¿˜æ˜¯å¾ˆå¿«çš„

- RAG è¿˜æ˜¯å¾—çœ‹äººçš„è®¾è®¡ï¼Œå› ä¸ºä¹‹å‰éš”å£éƒ¨é—¨èƒ½å‹ç¼©åˆ° 100msï¼Œæ­£ç¡®ç‡ 100%ï¼Œæˆ‘ç¬¬ä¸€æ¬¡åšåªèƒ½ 300msï¼Œè¿™çœŸçš„æ¯”ä¸äº†ï¼Œåˆ‡ç‰‡å¤ªåƒç»éªŒäº†ï¼Œæ­å»ºç®€å•ï¼Œä¼˜åŒ–éå¸¸éš¾

- 
- 
- 
- 
- 

- ## [Information extraction with long document (images included) : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1nuvwak/information_extraction_with_long_document_images/)
- The document parsing step you outlined is solid but you're overcomplicating the image-text mapping part.
  - Your approach of converting pages to images first then using VLM is actually the right direction, but I'd simplify step 1. Instead of doing separate OCR for bounding boxes then trying to map cropped images back to text tags, just let the VLM handle both text extraction AND image positioning in one pass. Most decent VLMs can already tell you "there's an image here between paragraph X and Y" without needing separate OCR tools. For the actual implementation, yeah OpenAI's vision models work great for this but if you want to keep it local, try running something like LLaVA or the newer Qwen2-VL models. The key thing is being really specific in your prompts about maintaining spatial relationships and hierarchical structure. 
  - For step 2, instead of trying to get the LLM to cite page numbers (which honestly isn't super reliable), consider keeping a metadata mapping where each extracted section links back to the original page ranges from step 1. 
  - The RAG approach you mentioned as a backup is actually pretty smart because it gives you flexibility when section titles are vague or when content spans weird boundaries. We've seen similar challenges with educational content processing using Docstrange and the hybrid approach tends to work better than trying to make one method handle all edge cases perfectly.

# discuss-solutions
- ## 

- ## 

- ## 

- ## 

- ## [å¦‚ä½•æé«˜mineruè§£æPDFæ–‡ä»¶ä¸­å›¾ç‰‡çš„åˆ†è¾¨ç‡ï¼Ÿ ](https://linux.do/t/topic/1498862)
- æœ€å¥½å…ˆçœ‹çœ‹åŸç‰ˆ pdf å¦å­˜å‡ºæ¥æ˜¯ä¸æ˜¯å°±æ˜¯é«˜è´¨é‡çš„

- ## [DeepSeek-OCR - Lives up to the hype : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1ocrocy/deepseekocr_lives_up_to_the_hype/)
  - Hardware - 1 x A6000 ADA on a Ryzen 1700 /w 32gb ram
  - Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 3.29it/s, est. speed input: 3000.81 toks/s, output: 220.20 toks/s]
- Here is a more technical analysis. The result being the model response. Going to update the OP with this data.
  * **Content and Metadata:** The `result` field within each object in the `results` array contains the core document information as a string. This string is a mix of:
  * **Markdown-like Syntax:** It uses Markdown conventions for formatting, such as `#` for headings and `**` for bold text.
  * **HTML Tags:** It directly embeds HTML `<table>` tags to structure tabular data.
  * **Custom Tags:** The format uses a set of unique tags to provide additional metadata:
  * `<|ref|>` and `<|/ref|>` : These tags appear to act as "reference" or "type" markers. They enclose a word that categorizes the succeeding content, such as `title` , `text` , or `table` .
  * `<|det|>` and `<|/det|>` : These tags likely stand for "details" or "detection" and enclose what appear to be coordinates `[[45, 90, 380, 114]]` . These represent the bounding box or location of the corresponding element on an original document page.

- Howâ€™s the quality of markdown files after processing?
  - Honestly its insane. I run qwen 3 vl 30ba3b instruct and this kind of detail that includes bbox coordinates can take 30+ seconds a page and still doesn't get it right

- The problem with this kind of OCR is, when classic OCR can't recognize a word, it writes gibberish. When Deepseek OCR can't recognize a word, it writes a word that fits the context the most. Gibberish you can pinpoint with a spellchecker. A made up but grammatically correct word you have to proofread manually.
  - But, it's also what a human would try to achieve as well.

- Does it handle forms with checkboxes ?
  - Not from my limited experience. There are multiple layers of pre and post processing im working through. The api has most of them enabled but a few are lacking.

- I would say, in terms of precision: Dots.ocr > Deepseek ocr > Docling
  - What I like about dots.ocr is that they return an array of layout elements (text, category, bbox); which you can serialize to any formatâ€”especially markdown.

- ## ğŸ§©ğŸ’¡ [FinePDFs: Liberating 3T of the finest tokens from PDFs _202601](https://huggingface.co/spaces/HuggingFaceFW/FinePDFsBlog)
- At the beginning of 2025, our team faced a question that looms over the entire field of AI: Have we run out of data?
  - In creating both iterations of the FineWeb datasets (Penedo et al., 2024, 2025), we had effectively exhausted CommonCrawlâ€™s (Common Crawl Foundation, 2024) HTML archives. At that point, self-hosted crawls or synthetic data (Maini et al., 2025) seemed to be our only options forward.
  - But wasnâ€™t there a large amount of high-quality data still hidden in CommonCrawl for the curious to find? Indeed there was: PDFs.
  - While this format has been largely ignored by the open-source AI community for years, we found it to be a source of high-quality data for pretraining, and importantly a source of long-context documents so often missing from open data.
- Pretraining corpora have historically been derived almost entirely from processed HTML snapshots of CommonCrawl (e.g., Dolma (Soldaini et al., 2024), Nemotron-CC (Su et al., 2024), and RedStone (Chang et al., 2024)). Alternative datasets such as Harvardâ€™s Institutional Books corpus (Institutional Data Initiative, 2025) and the PleIAs Common Corpus (Langlais et al., 2025) exist, but they are smaller in scale and less systematically analyzed. By contrast, the rich non-HTML content embedded within CommonCrawlâ€”especially PDFsâ€”has received comparatively little attention, leaving a large, underexplored opportunity for corpus construction.
- PDFs make up only about 0.6% of the crawl (Common Crawl Foundation, 2025). Why spend time on such a small fraction of the web?
  - The answer lies in information density. Unlike HTML pages, which are often lightweight and boilerplate-heavy, PDFs are typically long dense documentsâ€”reports, government papers, and manuals. Content that requires significant effort to create typically correlates with higher information density.
- PDFs were built to preserve appearance, not to expose structure. Where HTML gives you a clean tree of tags, a PDF gives you a set of drawing commands: put this glyph here, draw that line there, paint an image now. The result is visually faithful, but missing any sort of semantic structure.
- there are three broad strategies:
- Pure parsing (PyMuPDF (PyMuPDF contributors, 2024), pdfminer (pdfminer.six contributors, 2024))
  - Strategy: read the raw PDF text objects and infer structure with heuristics (font size â†’ headers, proximity â†’ columns).
  - Quality: fails on scanned PDFs and struggles with complex layouts, as well as tables and math.
- Pipeline methods (MinerU (Wang et al., 2024), Docling (Livathinos et al., 2025))
  - Strategy: detect layout blocks with a small ML model, align embedded text to those blocks, then apply specialized extractors (tables, figures, captions) before stitching output together.
  - Quality: better structure than pure parsing while staying lighter than full OCR. However, reading order and fragmented words can still be a challenge.
  - Cost: batching and good resource utilization are hard to implement (many moving parts and GPU/CPU interop). <1 page/sec on 1 core with minimal ML inference.
- End-to-end OCR / VLM (Nougat (Blecher et al., 2023), Got-OCR (Wei et al., 2024), RolmOCR (Reducto AI, 2025))
  - Strategy: render each page to an image and let a vision-language model transcribe the page directly.
  - Quality: best reading order and parsing of certain structures (tables, math, etc.). However, it can randomly hallucinate names/tables or full pages in ways that are hard to detect.
- Early qualitative checks, along with OlmOCR results (Poznanski et al., 2025), suggested that end-to-end OCR was the best-quality option. But running it on every PDF was far beyond our GPU budget. 
- Inspired by CCpdf (Turski et al., 2023), we therefore devised a hybrid approach: use GPU parsing only when needed (scanned documents) and keep everything else on the cheap CPU path. 
  - To do so, we trained a lightweight classifier that predicts whether a PDF is extractable (CPU parsing) or needs OCR (GPU parsing).
  - we trained a lightweight XGBoost classifier (Chen & Guestrin, 2016) on features aggregated from 8 random pages plus doc-level signals (inspired by MinerU (Wang et al., 2024) and Docling (Livathinos et al., 2025)). We intentionally excluded Doclingâ€™s bitmap-coverage feature because it was too expensive to compute at scale.

- We ultimately selected `Docling` as our CPU extractor, as it consistently ranks first across our eval tasks. 
  - However, if one is more CPU constrained, we strongly recommend `PyMuPDF4LLM` since itâ€™s almost 8x faster than Docling (even after our optimizations), while staying close in quality.

- To pick an OCR model, we benchmarked model-based extractors on three suites: OlmOCR (Poznanski et al., 2025), OmniDocBench (Ouyang et al., 2024), and the multilingual slice of CC-OCR (Yang et al., 2024). 
  - Evaluations ran in May 2025, so newer OCR models released after that date are not included.
  - Finally, based on the results, we chose RolmOCR (Reducto AI, 2025) for production at the time of extraction because it delivered reasonable speed. We used a min/max resolution range of 1280â€“2048 to balance quality and speed.
- Since we needed the pipeline to be as fast as possible, we tested inference with the 3 most popular engines: SGLang (3.04 pages/sec), vLLM (3.64 pages/sec), and LMDeploy (4.1 pages/sec). Throughput results made the choice of engine a no-brainer: we used `LMDeploy`, the fastest by far.
- We also noticed the OCR model would sometimes go into repetition loops until it exhausted the context length (8192 tokens in our case). This hurt both quality and speed as useless tokens were being generated at long sequence lengths. To fix this, we implemented a simple repetition checker (line, character, and word-level) directly inside LMDeploy, which further increased throughput to 5.17 pages/sec.

- After extraction, we refined the raw text to steadily improve quality. 
  - The postprocessing steps focused on three things: cleaning Docling tags, removing boilerplate, and filtering out hallucinated content.
- Because Docling uses a layout model, itâ€™s able to deduce and label many common structures (tables, figures, captions, etc.). 
  - We only cared about two: tables (`<docling_table/>`) and picture annotations (`<docling_picture_annotations/>`) as others were empty and not used due to disabling extractors for these structures.
  - For tables, we used heuristic extraction via `pymupdf4llm`, which caused many of them to be frequently broken (misaligned columns, missing headers). We therefore experimented with removing problematic tables, removing all tables, or cleaning them up.
  - Picture annotations had a similar issue. We initially kept them, but they often contained sequences of letters, tick labels, or isolated numbers disconnected from any surrounding text. Such issues are caused by charts
  - We therefore tested filtering annotations by alpha ratio (number of alphabetic characters / total characters); finding the best setting to be a threshold of 0.8, altough all the results were rather close.
- remove boilerplate: headers, footers, watermarks, company names. 
- We noticed a recurring hallucination pattern in RolmOCR outputs: on blank pages and pages that were mostly graphics/images, the model would hallucinate fluent text, always starting with "The"
  - To fix this, we did not have time to train a dedicated image classifier, and running a heavy vision model across the whole corpus was too expensive.

- The result is two SoTA datasets: FinePDFs and FinePDFs-EDU. While we tried to preserve as many documents as possible, we still removed more than 66% of our initial dataset to create FinePDFs, and more than 96% to create FinePDFs-EDU. This massive reduction was mostly due to the deduplication steps and heavy model based filtering (for the EDU variant).
# discuss
- ## 

- ## 

- ## [Structure-first RAG with metadata enrichment (stop chunking PDFs into text blocks) : r/Rag](https://www.reddit.com/r/Rag/comments/1r9updr/structurefirst_rag_with_metadata_enrichment_stop/)
  - [Metadata-Enriched RAG: Document Structure Beats Text Chunking](https://kudra.ai/metadata-enriched-rag-agent-why-document-structure-beats-text-chunking/)
  - The problem is research papers aren't random text. They're hierarchically organized (Abstract, Introduction, Methodology, Results, Discussion). Each section answers different question types. Destroying this structure makes precise retrieval impossible.
  - I've been using structure-first extraction where documents get converted to JSON objects (sections, tables, figures) enriched with metadata like section names, content types, and semantic tags. The JSON gets flattened to natural language only for embedding while metadata stays available for filtering.
  - The workflow uses Kudra for extraction (OCR â†’ vision-based table extraction â†’ VLM generates summaries and semantic tags). Then LangChain agents with tools that leverage the metadata. When someone asks about datasets, the agent filters by content_type="table" and semantic_tags="datasets" before running vector search.
  - This enables multi-hop reasoning, precise citations ("Table 2 from Methods section" instead of "Chunk 47"), and intelligent routing based on query intent. For structured documents where hierarchy matters, metadata enrichment during extraction seems like the right primitive.

- [Structure-first RAG with metadata enrichment (stop chunking PDFs into text blocks) : r/LangChain](https://www.reddit.com/r/LangChain/comments/1r9ur3x/structurefirst_rag_with_metadata_enrichment_stop/)
- Totally agree. Structure first extraction with metadata filtering makes RAG far more precise and lets you cite exact tables and sections.

- ## [Qwen3-VL for OCR: PDF pre-processing + prompt approach? : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1q92olo/qwen3vl_for_ocr_pdf_preprocessing_prompt_approach/)
- PaddleOCR-VL. You'll lose information with just a VLM , and higher risk of hallucination.
  - There's a lot of built in features in it - drawing bounding boxes and their positions, predicting of the class of the text - headers, footers, body, tables. In my testing it's able to handle logos, signatures very well - it doesn't try to produce text for it, it'll crop them out and within the mark down/JSON it'll tell you where it belongs. It works well with scans and blurs as well. It makes post processing a lot easier.
  - And it's just 0.9B parameters.

- I think the problem with VLM is the way we are currently processing the image - models will usually cut up the image to image patches and learning features from them and the relation between the input text and the patches.
  - In my experience, providing the extracted text to "ground" or as a point of reference does help guide the model - similar to RAG
  - So my best guess is to try using OCR to extract the text, in the prompt include both the image , extracted text, and your instructions for what you want to extract.

- The preprocessing step matters more than the model choice here. PDF2image + PIL for image extraction works well, but you'll want to handle page segmentation carefully since contracts often have multi-column layouts or embedded tables that can confuse the model.
  - For Qwen3-VL specifically, you don't need a specialized package. Standard image preprocessing through transformers works fine. The key is your prompt structure. Instead of asking for "OCR, " frame it as "convert this document page to markdown" or "extract the text and structure." Qwen3 responds better to task framing than technical terms. Also watch out for token limits with 30B, contracts can get long and you might need to chunk pages.
  - I've been working on document processing pipelines at vectorflow.dev and see this pattern a lot. The bigger issue is usually downstream, once you have the text. How are you planning to handle the extracted content? Contracts have hierarchical structure that's easy to lose if you're not preserving section relationships and metadata during processing.
  - What's your target output format looking like? JSON with preserved document structure or flattened markdown?

- I'm training Qwen3-VL-2B for structured invoice extraction (to a fixed json format). Working pretty good so far. But I haven't tested other models yet, will do so because I would love to be able to go lower than 2B

- Looks like Qwen publishes an â€˜OCR cookbookâ€™ here: https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/ocr.ipynb
