---
title: lib-aikit-langgraph-community-langchain
tags: [community, langchain, langgraph]
created: 2025-09-05T07:18:02.221Z
modified: 2025-09-05T07:18:16.308Z
---

# lib-aikit-langgraph-community-langchain

# guide

# discuss-stars
- ## 

- ## 

- ## 

- ## [I deleted 400 lines of LangChain and replaced it with a 20-line Python loop. My AI agent finally works. : r/AI_Agents](https://www.reddit.com/r/AI_Agents/comments/1p227ra/i_deleted_400_lines_of_langchain_and_replaced_it/)
  - I rewrote the entire logic using: Raw Python (for control flow). Standard OpenAI API (for the intelligence). A simple while loop (for the agentic behavior).
  - Here is the dumb `while` loop that replaced my entire stack

- Youâ€™re not alone. I hit the same wall with LangChain and later with Autogen. Once the abstractions get too clever, you spend more time debugging the framework than building the agent.
  - I moved back to raw Python for the same reasons you mentioned. A simple loop with explicit messages and tool calls is honestly the cleanest mental model. You control the tokens, the reasoning, the retries, everything.
  - The only place where I still lean on external tools is when the agent needs to interact with real sites. I use Hyperbrowser there because I donâ€™t want to maintain Playwright sessions or proxies myself. Everything else runs in plain Python.
  - In my experience, frameworks are great for demos but raw control flow is what makes agents predictable in production. Curious if you plan to add memory or retries next.
- Yeah this matches my experience. Once the abstractions start getting clever you spend half the day figuring out what the framework is doing behind the scenes.
  - The simple loop with explicit messages has been the easiest mental model for me too. I am thinking about adding a small memory layer next but still deciding how to keep it lightweight.

- Been there done that. Try OpenAI Agent SDK. Is quite simple and no abstractions. That's one of the reason I did this small framework: agente. Is quite simple and practical and do the job for simple agents.

- LangChain is solid when it comes to utilities like document loaders, text splitters and all the integrations. My only real trouble was with the AgentExecutor and the reasoning loop.
  - I would not use Langchain for control flow, that's why they created langgraph. If you're using Langchain for control flow, you're using Langchain for the wrong purpose. AgentExecutor is quite outdated for building agents, especially if you want to build multi agents that work together in the background with human in the loop feedback, time travel, llm structured output, multi threads for in chat memory management as well as memory store for long term / overall profile memory. Langchain is good for rag, embedding, easy way to switch llm, etc

- Did you try Google's ADK ? If so how do you think it compares to OpenAI's ?
  - Google abstractions are a bit more rigid in terms of their workflow agents but thereâ€™s ways to do more fine grained control flows. More gluing on OpenAI. Both are pretty good, weâ€™re using adk in prod
  - When the agent got stuck or started inventing arguments, the abstraction made it much harder to debug than simply checking the raw message list.

- I spent hours debugging a LangChain agent that kept looping only to realized the agent executor was hiding a system prompt that confused the model. Then I switched to a solution like yours and it worked like a charm. Honestly I feel frameworks are great for prototyping but when it comes to production code they become black boxes.
  - I had a similar experience. The moment the agent executor hid the real system prompt I spent more time debugging the framework than the agent itself. Switching to a simple loop made everything clearer. I agree that frameworks are great for prototyping but once things get serious the black box feeling becomes a problem.

- I get the idea behind LangGraph and the workflow control but that was exactly where I kept running into trouble. For simple agents it felt heavier than what I needed. Raw Python gave me the same control with fewer moving parts, so for now I am keeping things minimal.

- I ripped LangChain out of my system 6 months ago and never looked back. The abstractions were built way before we knew what the abstractions should be and they kind of just get in the way. I spend more time debugging LangChain than debugging my agent.

- Same though using smolagents from huggingface now and itâ€™s pretty lightweight. Really not too different then what I was building in the past
# discuss-roadmap
- ## 

- ## 

- ## 

- ## 
# discuss-internals
- ## 

- ## 

- ## 

- ## 

- ## [is the openai package still the best approach for working with LLMs in Python? : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1puqqjv/is_the_openai_package_still_the_best_approach_for/)
- It works well. You can recreate everything with the standard `requests` library, but why would you? It's just streaming requests and JSON at the end of the day

- Their API is the de facto standard for interacting with LLMs, so it stands to reason their lib/package is the best for interacting with said API.
  - If you're running everything on the same machine/VM/container, you can skip the whole API and integrate the inference code with your own code/logic directly without the added complexity of the API/client/serialization/deserialization.

- Litellm is a defacto middleware that tries to support all the features of all the providers with no extra cruft

- Yes I donâ€™t like the agent frameworks like langchain or crewai either. Personally I went down the route of using raw CUDA some of the time, and writing compilers that compile DSLs to PTX some of the time.

- ## [Need help in streaming the graph's agent response from backend to frontend _202411](https://github.com/langchain-ai/langgraphjs/issues/668)
- ğŸ‘· 202509: Please use `useStream()` for first-class streaming handling from LangGraph to your FE of choice

- ## âš–ï¸ã€°ï¸ [Data stream protocol support Â· Issue Â· langchain-ai/langgraphjs _202409](https://github.com/langchain-ai/langgraphjs/issues/504)
  - It would be fantastic if langgraph supported vercels new data stream protocol so that langgraph could basically be drop in for react apps already using the vercel ai SDK client side

- langgraph provided an example https://www.assistant-ui.com/examples/stockbroker
  - It adopted Stream Protocol, however using langgraph server is required

- ğŸ‘· 202506: Closing this as Won't Fix
  - Upcoming AI SDK v5 will likely steer away from custom streaming protocol in favour of SSE 
  - We do now have a `useStream` hook that is tailor made for integrating LangGraph into React apps

- ## [å¦‚ä½•è¯„ä»·LangChain LCEL? - çŸ¥ä¹](https://www.zhihu.com/question/634584770)
- LangChainçš„è¡¨è¾¾å¼è¯­è¨€ï¼ˆLCELï¼‰é€šè¿‡é‡è½½__or__è¿ç®—ç¬¦çš„æ€è·¯ï¼Œæ„å»ºäº†ç±»ä¼¼Unixç®¡é“è¿ç®—ç¬¦çš„è®¾è®¡ï¼Œå®ç°æ›´ç®€æ´çš„LLMè°ƒç”¨å½¢å¼ã€‚
  - å¦å¤–ï¼ŒChainä¹Ÿå¯ä»¥åˆ†å‰ã€åˆå¹¶ï¼Œç»„åˆå‡ºæ›´å¤æ‚çš„DAGè®¡ç®—å›¾ç»“æ„ã€‚
  - åŸºäºLCELç¡®å®èƒ½æè¿°æ¯”è¾ƒå¤æ‚çš„LangChainè®¡ç®—å›¾ç»“æ„ï¼Œä½†ä¾ç„¶æœ‰DAGå¤©ç„¶çš„è®¾è®¡é™åˆ¶ï¼Œå³ä¸èƒ½æ”¯æŒâ€œå¾ªç¯â€ã€‚
  - äºæ˜¯LangChainç¤¾åŒºæ¨å‡ºäº†ä¸€ä¸ªæ–°çš„é¡¹ç›®â€”â€”LangGraphï¼ŒæœŸæœ›åŸºäºLangChainæ„å»ºæ”¯æŒå¾ªç¯å’Œè·¨å¤šé“¾çš„è®¡ç®—å›¾ç»“æ„ï¼Œä»¥æè¿°æ›´å¤æ‚çš„ï¼Œç”šè‡³å…·å¤‡è‡ªåŠ¨åŒ–å±æ€§çš„AIå·¥ç¨‹åº”ç”¨é€»è¾‘ï¼Œæ¯”å¦‚æ™ºèƒ½ä½“åº”ç”¨ã€‚
- åˆæ¥ä¸€ä¸ªtensorflowï¼Œæ»¡çœ¼æ—¢è§†æ„Ÿï¼Ÿä¸ºå•¥è°éƒ½æƒ³åœ¨è¯­è¨€ä¸Šå†åšä¸€ä¸ªè¯­è¨€ï¼Œä½ è¯´ä½ åšäº†è®¡ç®—å›¾è°ƒåº¦åšäº†æè‡´ä¼˜åŒ–ä½ ä¸Šactoréƒ½è¡Œï¼Œæœ€åè¿˜æ˜¯åšäº†ä¸ªçš®ä½†æ˜¯å¢åŠ äº†debugéš¾åº¦ï¼Œä½ è¯´ä½ åŠ ä¸å°±æ˜¯å› ä¸ºè°ƒè¯•ä»¥åŠè§‚æµ‹æ€§æ›´å·®äº†å—ï¼Œé¡ºä¾¿è¦ç‹‚å¹è‡ªå®¶ã€‚ä¸çŸ¥é“ä¼šä¸ä¼šåƒtf debuggerä¸€æ ·å‡ºæ¥ä¸ªlang debugger

# discuss-ai-providers
- ## 
- ## ğŸ¤” raise self._make_status_error_from_response(err.response) from None
  - `openai.InternalServerError: Error code: 502`

- ğŸ‘·:  when using local LM Studio, you have to turn off global mode for proxy like clash, you can use rule/direct mode
- ## [Langchain throws an error when generating structured output using Qwen3-32B deployed via vllm. Â· Issue Â· langchain-ai/langchain _202505](https://github.com/langchain-ai/langchain/issues/31335)
- This was not a langchain issue, but an issue that was caused by a bug in the Qwen3/early Qwen2.x chat template, which assumed that `message.content` would always be present in every assistant/tool call message. 
  - When running structured outputs or tool calling via OpenAI-compatible APIs (e.g., LangChain, Roo, LM Studio, etc.), this would throw errors such as: ` key 'content' not found` or `Input should be a valid string [type=string_type, input_value=None, ...]`

- ## [Local LLM with LM Studio Server: Error in POST payload when using langchain_openai. OpenAIEmbeddings for embedding API. Â· Issue Â· langchain-ai/langchain _202405](https://github.com/langchain-ai/langchain/issues/21318)
  - [ERROR] 'input' field must be a string or an array of strings
- Setting `check_embedding_ctx_length` to False fixes the problem.

```python
OpenAIEmbeddings(check_embedding_ctx_length=False,  openai_api_key="sk-1234", base_url="http://localhost:8999/v1",model="nomic-ai/nomic-embed-text-v1.5-GGUF")
```

- ## [New `init_chat_model` format using LM Studio? _202507](https://forum.langchain.com/t/new-init-chat-model-format-using-lm-studio/422)
- Yes, you can use `init_chat_model()` with LM Studio, since it exposes an OpenAI compatible API (usually via `localhost:1234/v1` ). 
  - To make this work, you need to pass the `model_provider="openai"` so LangChain uses the correct internal logic.

```python
from langchain.chat_models import init_chat_model
model = init_chat_model(
    model="your-model-id",  # e.g. "gpt-3.5-turbo" or "lmstudio-llama2"
    model_provider="openai",  # because LM Studio mimics OpenAI's API
    base_url="http://localhost:1234/v1",
    api_key="not-needed"  # LM Studio accepts any string here
)
```

- The `init_chat_model()` function is just a higher level wrapper introduced to standardize model initialization across providers (OpenAI, Anthropic, HuggingFace, etc.). 
  - Youâ€™re not losing anything by sticking with the classic `ChatOpenAI` constructor in fact, 
  - for LM Studio and other self-hosted setups, using `ChatOpenAI` directly may give you more explicit control and transparency.
- So both are valid, use what feels cleanest for your setup.
- I found the spot where I think the OpenAI compatibility may not be working. With the OpenAIEmbeddings. 
  - In the below code, if I switch to using the Ollama variant, this works fine. The OpenAI variant gives me a bad request error from LMStudio.
  - I tried different models too - I still get the same â€œinputâ€ field error.
  - I found on some random page from google - adding `check_embedding_ctx_length=False` , to the embedding constructor fixes the error.
- ## ğŸ”¡ [ChatOpenAI â€” LangChain documentation](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)
- `ChatOpenAI` can be used with OpenAI-compatible APIs like LM Studio, vLLM, Ollama, and others. 

# discuss
- ## 

- ## 

- ## 

- ## [How do you test multi-turn conversations in LangChain apps? Manual review doesn't scale : r/LangChain _202511](https://www.reddit.com/r/LangChain/comments/1p2726e/how_do_you_test_multiturn_conversations_in/)
- Manually reviewing conversation flows doesnâ€™t scale, and static scripts break whenever prompts or context change. Using autonomous testing agents like your Penelope approach is a strong solution, especially when combined with an LLM judge to evaluate outcomes.
  - Frameworks like CoAgent (coa.dev) complement this by providing structured evaluation, monitoring, and observability for multi-turn interactions. They help teams detect regressions, track context management issues, and ensure agent decisions remain consistent as prompts, tools, and workflows evolve.

- ## [Disadvantages of Langchain/Langgraph in 2025 : r/LangChain _202507](https://www.reddit.com/r/LangChain/comments/1m2skwu/disadvantages_of_langchainlanggraph_in_2025/)
- As it has always been, the best thing you can do is learn to understand what the tool solves and how that tool particularly solves it. If you know why you would need to use it and how it solves that problem, then pivoting to different system will become just understanding how they specifically do it, as concepts often transfer between tools.
- Langchain has a lot of features, but honestly, most of them donâ€™t do much for me. The only parts I really like are the abstractions for:
  - Messages (AIMessage, HumanMessage, etc)
  - Chat models (ChatOpenAI, ChatGoogleGenerativeAI, etc)
  - Tools (creating custom tools)
  - What I find useful is how you can swap out models and the rest just works, since everythingâ€™s pretty standardized (though there are some edge cases).
  - Outside of that, though, it gets pretty painful.
  - I think the underlying ideas behind Langgraph are solid, but the steep learning curve and the sense of vendor lock-in kept me from fully committing to it.
- [A story of using langchain/langgraph : r/LangChain _202507](https://www.reddit.com/r/LangChain/comments/1m3kn6x/a_story_of_using_langchainlanggraph/)
  - I hope you enjoy my story of working with langchain and langgraph these past 2, almost 3 years. Also, this short story is about sticking with langchain and langgraph. Not doing things your own or choosing to integrate other libraries. But we'll talk about that towards the end.
- Langchain before LCEL
  - So I started using langchain in its beginnings, I think most of you will remember all the StuffDocumentsChain and MapReduceChain, so this was way before the current LCEL way of doing. 
  - We started using langchain just because it was one of the first "popular" libraries in the space.
  - Our uses cases at the time were simple, The most common one, and that we also had was to have a generic RAG platform where users would deposit their documents and we'd handle all the infrastructure (auth, database, frontend, endpoints etc.), embeddings, vector store etc. At the time RAG was "painful"
- Langchain and LCEL
  - Then langchain introduced LCEL. It's a declarative way to chain together "chains". It's nice in theory, but in practice, for complex use cases, it adds another layer of complexity. 
  - And that layer of complexity is solved with langgraph. And this is a recurrent theme with langchain. New layers of complexity that get solved by introducing new tools. 
  - To put it simply, LCEL = pipelining runnables. And Runnables would be this general interface that's somewhat synonym to "processing". It's something you can run.
  - The big advantage of LCEL is that it solves these rigid ways of creating chains through the langchain built-ins, and offers an easy way to compose chains, and even reuse them in some scenarios. You can think that it is syntactic sugar
  - The main drawback of LCEL is debuggability. For a moderately complex chain, if you don't filter the logs, you'd get tons of details that just make it hard to read them.
  - The other major drawback is how to inject data at runtime in the chain.
  - The other major drawbacks of LCEL is that the composability and reusability of chains that it claimed was also one of its weaknesses. The easily composable chains are the ones that come from some langchain built-ins like their parsers (say StrOutputParser) or the base chat models (say ChatOpenAI) etc. But that is very limited. 
- Langgraph
  - I think langgraph came to be by transposing what we saw in the real world, developers making LLMs do different things and then make them interact with each other.
  - Langgraph solves many of the issues above because it forces you to define your state or data schema beforehand, so when you want to use the graph (or the chain written as a graph), you're more likely to adapt the code around it to that state, rather than write the chain in a way that adapts to the code around it. Also, you have more granular control on how its mutated and when.
  - But langgraph still suffers from a lot of the burden of the 1000 abstractions that langchain introduces, and also from how complex langgraph itself is. 
  - Another drawback of working with constantly evolving things is, you are somewhat dependent on how good the library developers handle their updates.
- The advice I'd give for using langgraph in production is the same I'd give to any other code in production. Use Pydantic, or at least dataclasses with validation. Think clearly of your state and what you need to be passed through the graph and for what reason. Type hinting obviously, though it's a pain with langchain sometimes. 
  - One another advice I'd give is, as you are writing your graph's code, think of its evaluation and write its evaluation code.
- I think, one good practice (and obviously this will depend a lot on your use cases) is to separate data access from the nodes of the graph as much as possible. Make the agents or LLMs or whatever have access to the data through tools. Whether those tools are defined as nodes in the graph or are bound to the LLM is not important, the important part is not to have a non-tool node that fetches data.
  - You can restrict parts of the graph from access some data, but data injected dynamically in the state through nodes is harder to reason about and it's just better handled with a tool in that case. 
  - And obviously, any compute heavy processing should not be a node in your langgraph graphs.
- I know there are a bunch of other libraries such as autogen, crewai, pydantic-ai. I have not tested them in their current form, but I have tested them in their beginnings and they fell short of features compared to langgraph. langgraph allowed to do much more complex workflows.
- You made a good point â€” LLMs are just functions, and it's about learning how to use them effectively.
- ## [LangChain bad, I get it. What about LangGraph? : r/LocalLLaMA _202407](https://www.reddit.com/r/LocalLLaMA/comments/1dxj1mo/langchain_bad_i_get_it_what_about_langgraph/)
- My main grudge against langchain was the fact that behind the initial impression of good abstractions you immediately met with discrepancies in behaviors of internal implementations - how messages are passed around, how system prompt is defined, how parameters can be proxied to a model, how external APIs are called - you named it. And every little integration is written in a slightly different way.
  - In general, seeing this low consistency level makes me want to avoid using the project, as it's an indication of the overall level of quality there.
- Hmm as a LISP and Haskell enthusiast, it seems like lang-graph is trying to implement a DSL but doing it in a weird way because in Python there is no easy way to represent Python code as a code graph or abstract syntax tree.
- For entirely offline solution, I have a project LangGraph-GUI, this use ollama.
- I think the main thing LangGraph adds is, a state machine framework for human in the loop with time travel.
  - you get a flowchart graph way of doing things, and you get a nice chart. But it won't do anything you couldn't do writing your control flow in Python. 
  - In the future I could see people building low-code GUI tools to create a workflow interactively on top of LangGraph without typing a bunch of boilerplate.
  - If you didn't like LangChain by itself, you're not going to like using it within some LangGraph nodes. It seems more about adding a control flow framework to LangChain.
- LangGraph adds very little to itself. `langchain-core` is a huge dependency. If we think of it, a chain is just a linear graph. 
  - LangGraph library only implements "a way" to declare a workflow, checkpointer and manage state. These could be theoretically just 3 files but LangGraph took care of edge cases, is organised and and tested. 
  - Majority of the actual functionality after compiling your graph comes form `Runnable` and its variants defined in `langchain-core` . Even graph visualisation method is defined there and not in LangGraph.
  - So, if you code your rag using requests or openai or re modules, LangGraph will eventually wrap it with Runnable primitive. If you are okay with that, which I am, then great!
- OK, I think if all you want is a runnable you can subclass or decorate a python function with `@chain` , but the reason to use langgraph instead would be, you like the whole state graph framework
- LangChainâ€™s original strength was its ability to connect LLMs with actual information sourcesâ€”databases, APIs, PDFs, tools like Gmail or Slack. Thatâ€™s what made it practical.
  - As we move toward more agentic workflows, the biggest bottleneck isnâ€™t flow control or state managementâ€”itâ€™s connectors.
  - Thereâ€™s a serious tooling gap hereâ€”and not for lack of ideas. Itâ€™s that the most powerful tools are not open.
  - Big tech players (Google, Microsoft, OpenAI) are prioritizing integrations within their own ecosystemsâ€”Gemini, Copilot, GPTsâ€”rather than enabling external developers to build agentic SaaS on top of their platforms.
- perhaps using anthropic's mcp is one solution?wrap the mcp server with your own llm and it does do a pretty god job
- ## [Should I learn LangGraph instead of LangChain? : r/LangChain _202408](https://www.reddit.com/r/LangChain/comments/1env9og/should_i_learn_langgraph_instead_of_langchain/)
  - while I reading the documents of LangChain about Agents I saw warning note. I think LangChain won't support building Agents with Langchain anymore. They will use LangGraph to create Agents anymore. 
- Knowing Langchain will make your Langgraph learning journey quite easy since they both use LCEL. That said, Langgraph is the most capable framework for building Agents currently. I wouldn't even look at Autogen or CrewAI. They have a quickstart on their page, just search Langgraph tutorial and see how you feel about it. It's not that hard but like anything, requires practice
- ğŸ‘· Erick from LangChain here. Would highly recommend starting with LangGraph for your entry point if you're working with agents. 
  - you can also follow the migration guides if you're used to working with the legacy AgentExecutor (still supported, but much less controllable than langgraph)!
  - You can use them completely separately, or together. 
  - LangGraph is much lighter-weight in the sense it's just orchestrating python functions (nodes and conditional edges), and you can choose to use LangChain integrations/other abstractions within your nodes.
- ## [langchainåˆ°åº•è¯¥æ€ä¹ˆä½¿ç”¨ï¼Œå¤§å®¶åœ¨é¡¹ç›®ä¸­å®è·µæœ‰æˆåŠŸçš„æ¡ˆä¾‹å—? - çŸ¥ä¹](https://www.zhihu.com/question/609483833)
- å¦‚æœç›®çš„å¿«é€Ÿæ­å‡ºå¯ç”¨demo, æ˜¯å¯ä»¥è€ƒè™‘ç”¨æ¡†æ¶çš„, langchainè¿™é‡Œå°±å¸®äº†æˆ‘å¾ˆå¤š
  - ç¬¬ä¸€ä¸ªä¼˜ç‚¹, æ–¹ä¾¿æ¨¡å‹åˆ‡æ¢
  - é•¿æœŸè®°å¿†ConversationSummaryBufferMemoryä¼šè‡ªåŠ¨å¸®æˆ‘æ€»ç»“å¯¹è¯å†å², å¹¶åœ¨æ¯æ¬¡å¯¹è¯ä¸­å¸¦ç€è¿™æ®µæ€»ç»“. å¥½å¤„æ˜¯ç¡®å®æ€»ç»“äº†, ä½†å¥½åƒåˆæ²¡æ€»ç»“. å› ä¸ºå¸¦çš„ç»†èŠ‚å¤ªå¤šäº†, å®é™…ä¸Šå¹¶æ²¡æœ‰çœå¤šå°‘token.
  - çŸ­æœŸè®°å¿†, ä¹Ÿå°±æ˜¯Redis History Messageå®è´¨ä¸Šå°±æ˜¯æŠŠlistå­˜å‚¨åˆ°Redisé‡Œäº†, æ²¡æœ‰å¤ªå¤§æŠ€æœ¯å«é‡.
  - æœ€å¤§çš„é—®é¢˜æ˜¯ä»€ä¹ˆå‘¢, æ˜¯Langchainé‡Œé¢æœ‰ä¸¤ç±»æ¨¡å‹llmå’Œchatmodel.
  - å› ä¸ºå†å²åŸå› , langchainå¤§éƒ¨åˆ†ä»£ç å’Œæ–‡æ¡£éƒ½æ˜¯åŸºäºllmçš„, æ¯”å¦‚æˆ‘å‘é€ä¸€æ®µå¯¹è¯, è®©llmå›å¤, æˆ‘å‘é€çš„æ˜¯ä¸€æ®µæ–‡æœ¬. è€ŒOpenAIå®˜ç½‘ä¾‹å­ä¸­, æ˜¯å‘é€çš„ä¸€ä¸²message list
  - ä¸ºäº†æ›´å¥½åœ°ç¬¦åˆOpenAIçš„ä½¿ç”¨æ–¹æ³•, æˆ‘å°±ä»åŸºäºllmçš„ä»£ç , æ”¹ä¸ºäº†åŸºäºchatmodel.
- ## [å­¦ä¹ AIï¼Œæ˜¯ä¸æ˜¯è¦å­¦ä¹ LangChainè¿™ä¸ªæ¡†æ¶ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/1899527932420002416)
- å¦‚æœå­¦ä¹ åº”ç”¨è½åœ°ï¼Œé‚£LangChainï¼ŒSGLangï¼ŒvLLMæ˜¯å¾ˆå€¼å¾—çœ‹çš„æ¡†æ¶ã€‚
  - å¦‚æœå­¦ä¹ æ¨¡å‹ç®—æ³•è®­ç»ƒï¼Œé‚£Deepspeedï¼ŒMegatronï¼ŒTorchæ˜¯å¾ˆå¥½çš„æ¡†æ¶ã€‚
- å­¦ä¹ è¿™äº›æ¡†æ¶çš„æ„ä¹‰ï¼Œå¹¶ä¸æ˜¯è¯´ä½ ä¸€å®šè¦åœ¨å®é™…å·¥ä½œä¸­ç”¨æŸä¸€ä¸ªæ¡†æ¶ï¼Œè€Œæ˜¯é‡åˆ°äº†ç±»ä¼¼çš„é—®é¢˜ï¼Œå¯ä»¥å»å‚è€ƒç°æœ‰çš„è§£æ³•ã€‚
  - åƒ Langchain å…¶å®å°è£…äº†å¾ˆå¤šä¸œè¥¿ï¼Œè°ƒè¯•ä¹Ÿéå¸¸å¤æ‚ï¼Œä½†ä»–æ•´ä½“è®¾è®¡çš„æ€è·¯è¿˜æ˜¯å€¼å¾—å€Ÿé‰´å’Œå­¦ä¹ ã€‚ç®€å•æ¥è¯´ï¼Œå°±æ˜¯å®ƒè¿‡åº¦å°è£…äº†ï¼Œä½†ä½ ä¸€ç‚¹éƒ½ä¸äº†è§£çš„è¯ï¼Œè¿è®¾è®¡æ€è·¯éƒ½æ²¡æœ‰ã€‚
- åªæ˜¯å­¦ç‚¼ä¸¹ï¼Œé‚£æ˜¯å®Œå…¨æ²¡å¿…è¦å­¦Langchainçš„ã€‚å¦‚æœè¦åšLLMå¼€å‘ï¼Œæˆ‘è§‰å¾—å…ˆç…§ç€OpenAI API Documentä¾è‘«èŠ¦ç”»ç“¢ï¼ŒåŒæ—¶åšè¿‡å·¥ä¸šé¡¹ç›®äº†ï¼Œæœ‰è½¯ä»¶å·¥ç¨‹çš„ç»éªŒï¼Œå†å»å­¦ä¹ Langchainä¹Ÿä¸è¿Ÿã€‚
  - å­¦å®Œä»¥åå¯èƒ½ä¼šå‘ç°ç»å¤§å¤šæ•°åœºæ™¯ä¸ä¼šç”¨Langchainï¼Œä½†ä¹Ÿä¼šå‘ç°Langchainæä¾›çš„æ€æƒ³åœ¨å·¥ç¨‹é‡Œè¿˜æ˜¯å€¼å¾—å€Ÿé‰´çš„ã€‚
- langchainæ˜¯ä¸ªåŒ…è£…ä¸¥ä¸¥å®å®çš„æ¡†æ¶ï¼Œä¸œè¥¿å¤ªå¤šå¤ªé‡äº†ï¼Œä¼šè®©ä½ é™·å…¥å­¦ä¹ å®ƒçš„apiä¸­å»ï¼Œä»è€Œé”™è¯¯çš„ä»¥ä¸ºlamgchainå°±æ˜¯å…¥å£ä¼¼çš„
- è™½ç„¶é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå·²ç»è¡¨æ˜LLMså¯ä»¥ä¸ºç®—æœ¯å’Œå¸¸è¯†æ¨ç†ç­‰ä»»åŠ¡ç”Ÿæˆæ¨ç†ç—•è¿¹ï¼Œä½†å…¶ç¼ºä¹å¯¹å¤–éƒ¨ä¸–ç•Œçš„è®¿é—®å¯èƒ½ä¼šå¯¼è‡´äº‹å®è™šæ„ç­‰é—®é¢˜ã€‚
  - ReActé€šè¿‡è®©LLMsä¸ºä»»åŠ¡ç”Ÿæˆå£å¤´æ¨ç†ç—•è¿¹å’ŒåŠ¨ä½œæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è¯­è¨€æ¨¡å‹å¯ä»¥æ ¹æ®è¾“å…¥ç”Ÿæˆå“åº”ï¼Œä½†ReActé€šè¿‡å…è®¸æ¨¡å‹ä¸å…¶ç¯å¢ƒäº’åŠ¨æ¥å¢å¼ºè¿™ä¸€ç‚¹ã€‚è¿™ç§äº’åŠ¨æ˜¯é€šè¿‡æ¨¡å‹å¯ä»¥ç”¨æ¥æé—®æˆ–æ‰§è¡Œä»»åŠ¡ä»¥è·å–æ›´å¤šä¿¡æ¯å’Œæ›´å¥½åœ°ç†è§£æƒ…å†µçš„æ–‡æœ¬åŠ¨ä½œå®ç°çš„ã€‚
- ## [Agent/LangGraph é¢è¯•å…«è‚¡æ–‡ï¼šæ ¸å¿ƒéš¾é¢˜20é¢˜ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1914230995034564014)
- é—®é¢˜ 1ï¼š LangGraph çš„æ ¸å¿ƒè®¾è®¡ç†å¿µæ˜¯ä»€ä¹ˆï¼Ÿå®ƒä¸ä¼ ç»Ÿçš„é¡ºåºé“¾ï¼ˆå¦‚ LangChain Expression Language (LCEL) ä¸­çš„ | æ“ä½œç¬¦ï¼‰ç›¸æ¯”ï¼Œåœ¨å¤„ç†å¤æ‚å¤šæ­¥éª¤ä»£ç†å·¥ä½œæµæ–¹é¢ï¼Œæœ‰å“ªäº›æ˜¾è‘—ä¼˜åŠ¿ï¼Ÿè¯·ä»çŠ¶æ€ç®¡ç†å’Œæ§åˆ¶æµä¸¤ä¸ªè§’åº¦æ·±å…¥é˜è¿°ã€‚
  - LangGraphï¼š æä¾›äº†å†…ç½®çš„çŠ¶æ€ç®¡ç†æœºåˆ¶ã€‚æ¯ä¸ªèŠ‚ç‚¹éƒ½å¯ä»¥è¯»å–å’Œä¿®æ”¹å…±äº«çš„çŠ¶æ€ï¼Œå¹¶ä¸”è¿™äº›ä¿®æ”¹ä¼šåœ¨èŠ‚ç‚¹ä¹‹é—´ä¼ é€’ã€‚è¿™ç§çŠ¶æ€å¯ä»¥æ˜¯ä»»æ„å¯åºåˆ—åŒ–çš„ Python å¯¹è±¡
  - ä¼ ç»Ÿé¡ºåºé“¾ï¼š é¡ºåºé“¾æœ¬è´¨ä¸Šæ˜¯æ— çŠ¶æ€çš„ï¼Œæ•°æ®ä»ä¸€ä¸ªç»„ä»¶æµå‘ä¸‹ä¸€ä¸ªç»„ä»¶ï¼Œä½†æ²¡æœ‰ä¸€ä¸ªé›†ä¸­çš„ã€å¯ä¿®æ”¹çš„å…±äº«çŠ¶æ€ã€‚æ¯ä¸ªç»„ä»¶åªæ¥æ”¶ä¸Šä¸€ä¸ªç»„ä»¶çš„è¾“å‡ºä½œä¸ºè¾“å…¥ã€‚å¦‚æœéœ€è¦ç»´æŠ¤çŠ¶æ€ï¼Œé€šå¸¸éœ€è¦é€šè¿‡ä¸Šä¸‹æ–‡å¯¹è±¡æˆ–å¤–éƒ¨å˜é‡æ¥æ‰‹åŠ¨ä¼ é€’ï¼Œè¿™åœ¨å¤æ‚åœºæ™¯ä¸‹å®¹æ˜“å‡ºé”™ä¸”éš¾ä»¥ç®¡ç†ã€‚
- é—®é¢˜ 2ï¼š åœ¨ LangGraph ä¸­ï¼ŒStateGraph å’Œ MessageGraph æœ‰ä½•åŒºåˆ«
  - StateGraph ä½¿ç”¨ä¸€ä¸ªæ˜ç¡®å®šä¹‰çš„ã€å¯åˆå¹¶çš„ï¼ˆmergeableï¼‰çŠ¶æ€å¯¹è±¡ä½œä¸ºå›¾çš„å…¨å±€çŠ¶æ€ã€‚
  - MessageGraph æ˜¯ StateGraph çš„ä¸€ä¸ªç‰¹åŒ–ï¼Œå®ƒå°†çŠ¶æ€éšå¼åœ°å¤„ç†ä¸ºä¸€ç³»åˆ—æ¶ˆæ¯ï¼ˆBaseMessage å¯¹è±¡ï¼‰ã€‚æ¯ä¸ªèŠ‚ç‚¹æ¥æ”¶ä¸€ä¸ªæ¶ˆæ¯åˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼ˆé€šå¸¸æ˜¯æœ€æ–°çš„æ¶ˆæ¯ï¼‰ï¼Œå¹¶è¿”å›ä¸€ä¸ªæˆ–å¤šä¸ªæ¶ˆæ¯ä½œä¸ºè¾“å‡ºï¼Œè¿™äº›æ¶ˆæ¯ä¼šè‡ªåŠ¨æ·»åŠ åˆ°å›¾çš„å…¨å±€æ¶ˆæ¯åˆ—è¡¨ä¸­ã€‚
- é—®é¢˜ 3ï¼š LangGraph å¦‚ä½•å®ç°å›¾çš„åŠ¨æ€æ€§ï¼Ÿè¯·ç»“åˆ add_edge, add_conditional_edges, set_entry_point å’Œ set_finish_point ç­‰æ–¹æ³•ï¼Œè¯¦ç»†é˜è¿°å…¶åœ¨æ„å»ºå¤æ‚å·¥ä½œæµä¸­çš„ä½œç”¨
  - å…è®¸å·¥ä½œæµæ ¹æ®è¿è¡Œæ—¶æ¡ä»¶è¿›è¡Œåˆ†æ”¯ã€å¾ªç¯å’Œè·³è½¬ã€‚
- 
- 
- 
- 
- 
- 
- 
