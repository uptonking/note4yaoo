---
title: lib-db-app-community-distributed
tags: [community, database, distributed]
created: 2023-10-26T19:03:45.307Z
modified: 2023-10-26T19:04:00.318Z
---

# lib-db-app-community-distributed

> about database distributed/decentralized

# guide
- openraft's raft implementation is solid!
# discuss-stars
- ## 

- ## I learned today that TimescaleDB deprecated their distributed (multi-node) support
- https://twitter.com/a_prout/status/1777813707327684750
  - It reminded me that distributed databases come with a cost
  - [Distributed SQL Databases And The Costs of Being Distributed | by Adam Prout | Medium _202401](https://medium.com/@adamprout/distributed-sql-databases-and-the-costs-of-being-distributed-ab7b38b0fa50)

- ## ğŸ†šï¸ğŸ¤” [æœ‰äº†åˆ†å¸ƒå¼æ•°æ®åº“ï¼Œæ˜¯ä¸æ˜¯å°±ä¸éœ€è¦åˆ†åº“åˆ†è¡¨äº†ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/356734992)
- åˆ†å¸ƒå¼æ•°æ®åº“ä¸èƒ½é˜»æ­¢ä½ æŠŠä¸€ä¸ªè¡¨æˆ–è€…åº“æçš„å’Œå®‡å®™äº‰å¤§å°, ç”¨å¥½æ•°æ®åº“æ°¸è¿œéœ€è¦äººçš„å‚ä¸ï¼Œç³»ç»Ÿæ— æ³•è§£å†³è¿™ä¸ªé—®é¢˜

- ä¸éœ€è¦åˆ†åº“åˆ†è¡¨äº†ï¼Œä½†æ˜¯å¤§å¤šè¿˜ä¼šè®©ä½ æŒ‡å®šæ‹†åˆ†é”®ã€‚ å³ä½¿ SQL ä¸å¸¦ sharding key ä¹Ÿèƒ½æ‰§è¡Œï¼Œä½†æ˜¯æ€§èƒ½ï¼ˆå°¤å…¶æ˜¯ latencyï¼‰ä¹Ÿä¼šæ‰“ä¸ªæŠ˜æ‰£ã€‚ no silver bullet

- åªæ˜¯ä¸è¦æ‰‹åŠ¨åˆ†åº“åˆ†è¡¨ï¼Œå¹¶ä¸ä»£è¡¨å†…éƒ¨å®ç°æ²¡æœ‰åˆ†åº“åˆ†è¡¨ï¼Œæ•°æ®åº“äº§å“å†…éƒ¨æŒ‰ç…§æŸç§è§„åˆ™æ¥å¸®åŠ©ç”¨æˆ·è‡ªåŠ¨åœ¨çº¿æ‰©å®¹è€Œå·²å¦å¤–è‡ªåŠ¨åˆ†åº“åˆ†è¡¨è™½å¸¦æ¥äº†ä¾¿åˆ©ï¼Œä½†ä¹Ÿå¸¦æ¥äº†å…¶ä»–é—®é¢˜ï¼Œå¦‚ä¸€äº›åŠŸèƒ½å—é™ï¼Œæˆ–è€…æ€§èƒ½ä¸‹é™ç­‰ç­‰

- åˆ†å¸ƒå¼æ•°æ®åº“ä¹Ÿæ— æ³•é¿å…ï¼Œå¯¹äºæŸäº›ç‰¹æ®Šè¡¨ï¼Œè¦åœ¨è¿ç»´å±‚é¢æ‰‹åŠ¨è®¾ç½®åˆ†è¡¨é€»è¾‘ï¼Œä½†æ˜¯è‡³å°‘åœ¨åº”ç”¨å±‚é¢å¯ä»¥ä¸åˆ†äº†
  - æŸäº›å¸å¥—å‹æ•°æ®ï¼Œåˆ†è¡¨å…¶å®ä¸æ­¢æ˜¯æ€§èƒ½é—®é¢˜ï¼Œç»“ç®—åæ•°æ®å°±ä¸èƒ½åŠ¨äº†ï¼Œåˆ†è¡¨ä¼šæ›´å¥½ç®¡ç†

- è¡¨å¤§äº†å°±è¦ shardingï¼Œå’Œåˆ†å¸ƒä¸åˆ†å¸ƒå¼æ²¡å…³ç³»ï¼Œï¼Œï¼Œ

- ## [ç›®å‰ç”µå•†ç½‘ç«™æœåŠ¡ç«¯æ˜¯åˆ†åº“åˆ†è¡¨å¤šï¼Œè¿˜æ˜¯ä½¿ç”¨åˆ†å¸ƒå¼æ•°æ®åº“å¦‚TiDBï¼ŒPolarDBå¤šï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/449874451)
- æ¶æ„å¸ˆåº”è¯¥è€ƒè™‘ç›®å‰ä¸šåŠ¡å‘å±•çš„é˜¶æ®µ, é€‰æ‹©åˆé€‚çš„è§£å†³æ–¹æ¡ˆ, è¿‡æ—©ä¼˜åŒ–æ˜¯ä¸‡æ¶ä¹‹æº.
- é¦–å…ˆèƒ½åšMySQLåˆ†åº“åˆ†è¡¨è¿™ä¸ªä½“é‡åˆ°æ—¶å€™ï¼Œä¸€èˆ¬éƒ½å·²ç»æ˜¯æ²‰ç§¯æŒ‰å¹´è®¡çš„æ•°æ®äº†ï¼Œå®ƒä»¬ä¹‹é—´çš„è¡¨ç»“æ„å…³ç³»ä¸€å®šæ˜¯å¾ˆå¤æ‚çš„ï¼Œnewsqlæ¨¡ä»¿çš„æ˜¯SQLçš„ç¿»è¯‘å’Œè§£é‡Šï¼Œä½†è¦è®©å¦‚æ­¤å¤§ä½“é‡çš„å…³ç³»å‹æ•°æ®å¯¼å…¥åˆ°tidbçš„key/valueå­˜å‚¨ä¸­ï¼Œç„¶åè¿˜èƒ½è¡”æ¥å¥½è¡¨ä¹‹é—´çš„ç»“æ„å…³è”ï¼Œè¿™å…¶ä¸­çš„å·¥ä½œé‡ä¸æ˜¯è¯´è¯´é‚£ä¹ˆç®€å•çš„ï¼Œå­˜å‚¨è¿‡ç¨‹é—®é¢˜ï¼Œåˆ—æ•°é—®é¢˜ï¼Œåœ¨è¿ç§»è¿‡ç¨‹ä¸­å„ä¸ªéƒ½ä¼šæ•´æ­»å·¥ç¨‹å¸ˆã€‚
  - å…¶æ¬¡tidbè‡ªèº«çš„æ¶æ„ä¹Ÿæ˜¯éå¸¸åºå¤§ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨äº†rocksdbåï¼Œå¯¹å†…å­˜å’ŒSSDçš„å¼ºçƒˆéœ€æ±‚ï¼Œè€Œä¸”éœ€è¦æ›´å¤šçš„é›†ç¾¤èŠ‚ç‚¹ï¼Œé‚£ä¹ˆè¿™å°±è¦å¤§åŠ¨ç¡¬ä»¶èµ„æºé…ç½®äº†ã€‚è€Œä¸”tidbã€pdã€tikvï¼Œtisparkçš„å››ç»„é›†ç¾¤å¤æ‚åº¦è¿œè¿œè¶…è¿‡å‡ ä¸ªåˆ†åº“çš„MySQLï¼Œé‚£ä¹ˆè¿ç»´å·¥ç¨‹å¸ˆæ˜¯å¦èƒ½çŸ­æœŸå†…æ‹¿çš„ä½è¿™å°±æ˜¯å¾ˆå¤§çš„ç–‘é—®äº†ã€‚

- å¤§å¤šæ•°è¿˜æ˜¯åœ¨ä½¿ç”¨åˆ†åº“åˆ†è¡¨ä¸­é—´ä»¶ï¼Œç”¨MySQLåšå­˜å‚¨èŠ‚ç‚¹æ„æˆè‹¥å¹²ä¸ªå­˜å‚¨é›†ç¾¤å­˜å‚¨æ•°æ®åˆ†ç‰‡ã€‚

- TiDBå±äºçœ‹èµ·æ¥å¾ˆç¾å¥½ï¼Œä½†å®é™…æ²¡é‚£ä¹ˆç¾å¥½çš„äº‹æƒ…ï¼Œæ¯”å¦‚å¤æ‚JOINï¼Œæ•°æ®åœ¨å¤šä¸ªèŠ‚ç‚¹é—´æ¥å›ä¼ é€’å¯¼è‡´çš„ç½‘ç»œé—®é¢˜ã€‚æ²¡æœ‰ä»€ä¹ˆæ•°æ®åº“åŒ…æ²»ç™¾ç—…ï¼Œæ ¹æ®ä¸åŒéœ€æ±‚æ¥åšä¸åŒå–èˆå§

- ## [Kafka ä¸ºä»€ä¹ˆè¦æŠ›å¼ƒ ZooKeeperï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/624795964)
- å¤§æ•°æ®çš„ç”Ÿæ€å˜äº†ï¼Œå°¤å…¶æ˜¯hadoopå…¨å®¶æ¡¶æˆä¸ºæ—¶ä»£çš„çœ¼æ³ªäº†ã€‚
  - ä»¥å‰å„å…¬å¸è‡ªå·±æ­ä¸€å¥—é›†ç¾¤åœ¨æœºæˆ¿é‡Œï¼Œhdfsï¼Œzookeeperè¿™äº›ä¸œè¥¿å±äºæ˜¯å…¬å…±çš„æœåŠ¡ã€‚å„ç§ä¹±ä¸ƒå…«ç³Ÿçš„ç»„å»ºéƒ½ä¾èµ–zookeeperä¸æ˜¯ç´¯èµ˜ï¼Œæ˜¯å……åˆ†åˆ©ç”¨èµ„æºåŠ å¼ºå¯é æ€§ã€‚
  - ä½†æ˜¯ç°åœ¨æ˜¯äº‘è®¡ç®—çš„æ—¶ä»£ï¼Œè®²ç©¶çš„æ˜¯å¼¹æ€§ä¼¸ç¼©ã€‚é›†ç¾¤åœ¨äº‘ä¸Šï¼Œæœºå™¨ï¼Œå®¹å™¨çš„æ•°é‡å¤§å¹…å¢åŠ è€Œä¸”å˜åŒ–å¾ˆå¿«ï¼Œç”šè‡³æœ‰å¾ˆå¤šæ‰˜ç®¡å’ŒsaasæœåŠ¡ä½ éƒ½ä¸åœ¨ä¹æœ‰å¤šå°‘æœºå™¨ã€‚
  - è¿™æ ·ä¸¤å¥—åˆ†å¸ƒå¼ç³»ç»Ÿä¹‹é—´çš„ä¾èµ–å°±æˆäº†é—®é¢˜ã€‚å¦‚æœä½ è¦æ–°å¢å‡ ç™¾ä¸ªAæœåŠ¡èŠ‚ç‚¹ï¼Œé‚£ä¹ˆå®ƒä¾èµ–çš„BæœåŠ¡èŠ‚ç‚¹è¦ä¸è¦æ‰©å®¹å‘¢ï¼Ÿå¦‚æœè¦æ‰©çš„è¯ï¼Œå¢åŠ å¤šå°‘ï¼Œæ€ä¹ˆä¿è¯æ‰©å®¹çš„è¿‡ç¨‹ä¸¤å¥—ç³»ç»Ÿçš„äº¤æµä¸å‡ºé—®é¢˜ï¼Ÿ
  - è¿™å°±æ˜¯è¦å»é™¤zookeeperçš„åŸå› ï¼Œä¹Ÿæ˜¯hdfsè¢«ç±»s3æœåŠ¡å–ä»£çš„åŸå› 

- SparkçŸ­æ—¶é—´å†…ä¸ä¼šè¿‡æ—¶ã€‚hadoopè¦å­¦çš„æ˜¯é›†ç¾¤æ€ç»´ï¼Œéƒ¨ç½²è¿ç»´è¿™ç§ä¸œè¥¿ä¸ç”¨ä¸‹åŠŸå¤«ã€‚
  - å›½å†…hadoopè¿˜èƒ½æ’‘ä¸€æ®µæ—¶é—´ï¼Œå› ä¸ºsaasåŒ–ç¨‹åº¦ä½ï¼Œè‡ªå»ºé›†ç¾¤çš„å…¬å¸è¿˜æ˜¯æœ‰ï¼Œä¸Šäº‘çš„å…¬å¸å¾ˆå¤šæ˜¯ç±»s3çš„å¯¹è±¡å­˜å‚¨æ­é…çº¯è®¡ç®—çš„ç±»emræœåŠ¡ï¼Œä¸å…¶è¯´æ˜¯hadoopç”Ÿæ€çš„æ®‹ä½™ï¼Œæ›´åƒæ˜¯hive metastoreçš„ç”Ÿæ€ã€‚
  - å…¨ä¸Šsaaså…¨æ‰˜ç®¡çš„ä¹Ÿæœ‰ï¼Œéƒ½æ˜¯æœ‰é’±çš„å…¬å¸ã€‚
  - è‡³äºå‡ å¤§å‚æ™®éå–œæ¬¢è‡ªç ”ï¼Œæ¶‰åŠåˆ°çš„ä¸œè¥¿æ¯”hadoopè¦åº•å±‚ã€‚

- ä½ è¿™åªæ˜¯åº•å±‚å­˜å‚¨å˜åŒ–ï¼ˆHDFS->S3)ï¼Œæœ¬è´¨ä¸Šæ²¡å¤ªå¤§å·®åˆ«ï¼Œä¸è¿‡ä¸€ä¸ªæ˜¯å¯¹è±¡å­˜å‚¨ï¼Œä¸€ä¸ªæ˜¯ç±»æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨ï¼Œå¯¹äºè®¡ç®—æœåŠ¡æ¥è¯´ï¼Œæ²¡çœ‹å‡ºæœ¬è´¨å·®åˆ«

- ZookeeperåŠ kafka çš„æ¶æ„ï¼Œæœ‰ä¸‰å±‚è§’è‰²ï¼Œä¸€å±‚æ˜¯zookeeper ï¼Œæä¾›åŸºç¡€çš„çŠ¶æ€æŒä¹…åŒ–å’ŒçŠ¶æ€é€šçŸ¥æœåŠ¡ã€‚ä¸€å±‚æ˜¯controller ï¼ŒåŸºäºzookeeper æä¾›çš„æœåŠ¡ï¼Œç»™æ¾æ•£çš„broker æä¾›ç»Ÿä¸€çš„çŠ¶æ€æœåŠ¡ï¼Œä½†å®ƒæœ¬èº«æ˜¯æ²¡æœ‰çŠ¶æ€æœåŠ¡çš„ï¼Œå®ƒæ˜¯ä¾èµ–zookeeper çš„æœåŠ¡æ¥åšä¸»æ§ã€‚ä¸€å±‚æ˜¯broker ï¼Œæ— çŠ¶æ€æœåŠ¡ï¼Œå› ä¸ºä»–ä»¬æ— çŠ¶æ€ï¼Œæ— æ³•è‡ªå‘ç»„ç»‡èµ·æ¥ï¼Œæ‰€ä»¥éœ€è¦controller ä¸ºä»–ä»¬åšä¸»æ§ã€‚å…¶ä¸­æœ‰ä¸€ä¸ªbroker å…¼èŒäº†controller è§’è‰²ã€‚
  - è¿™ä¸ªæ¶æ„æœ¬æ¥æ²¡ä»€ä¹ˆé—®é¢˜ï¼Œä½†æ˜¯å¦‚æœè¦ä¼˜åŒ–ï¼Œä¹Ÿæ˜¯å¯ä»¥çš„ã€‚zookeeper æœ¬æ¥æä¾›äº†çŠ¶æ€æœåŠ¡ï¼Œä½†æ˜¯å› ä¸ºå®ƒä¸æ˜¯kafka çš„ä¸€éƒ¨åˆ†ï¼Œæ‰€ä»¥kafka ä¸å¾—ä¸è®¾è®¡ä¸€ä¸ªcontroller æ¥åšä¸»æ§ã€‚å‡å¦‚controller æœ¬èº«å°±å¯ä»¥æä¾›çŠ¶æ€æœåŠ¡ï¼Œé‚£ä¸Šé¢çš„ä¸‰å±‚æ¶æ„å°±å¯ä»¥ç®€åŒ–æˆä¸¤å±‚ï¼Œä¹Ÿå°±æ˜¯ä¸€å±‚controller ï¼Œæä¾›ä¸»æ§æœåŠ¡ï¼Œä¸€å±‚broker ï¼Œæ— çŠ¶æ€å·¥ä½œæœåŠ¡ã€‚
  - Kafka æŠ›å¼ƒzookeeper å°±æ˜¯åšäº†è¿™æ ·çš„ä¼˜åŒ–ï¼Œè‡ªå·±å¼€å‘äº†ä¸€ä¸ªåŸºäºraft å…±è¯†ç®—æ³•çš„ä¸€è‡´æ€§æœåŠ¡kraftï¼Œä¸ºé›†ç¾¤æä¾›ä¹‹å‰zookeeper çš„çŠ¶æ€æœåŠ¡çš„åŒæ—¶ï¼Œä¹Ÿä¸ºbroker æä¾›ä¸»æ§æœåŠ¡ï¼Œä¹Ÿå°±æ˜¯controllerã€‚è¿™æ ·ç›¸æ¯”ä¹‹å‰çš„æ¶æ„è¿˜æœ‰ä¸€ä¸ªå¾ˆå¤§çš„ä¼˜ç‚¹ï¼Œé‚£å°±æ˜¯controller çš„æ•…éšœåˆ‡æ¢ä¼šéå¸¸å¿«ï¼Œè€Œä¸”åˆ‡æ¢æ—¶é—´å‡ ä¹ä¸éšé›†ç¾¤è§„æ¨¡è€Œçº¿æ€§å¢é•¿ã€‚
  - åŸå› æ˜¯ï¼Œä»¥å‰çš„æ¶æ„ï¼Œcontroller åªæœ‰ä¸€ä¸ªï¼Œå¦‚æœåšcontroller çš„æ•…éšœåˆ‡æ¢ï¼Œé‚£ä¹ˆæ–°controller éœ€è¦å…¨é‡ä»zookeeperåŒæ­¥é›†ç¾¤æ‰€æœ‰å…ƒæ•°æ®ä¿¡æ¯å¹¶æ„å»ºåˆ°å†…å­˜ï¼Œæ¥ä¸ºåšä¸»æ§æœåŠ¡åšå‡†å¤‡ï¼Œè¿™äº›å…ƒæ•°æ®ä¿¡æ¯åŒ…æ‹¬topic å’Œåˆ†åŒºä¿¡æ¯ï¼Œå¦‚æœæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡é›†ç¾¤ï¼Œtopic å’Œåˆ†åŒºå¾ˆå¤šï¼Œè¿™ä¸ªè¿‡ç¨‹å°†ä¼šå¾ˆè€—æ—¶ï¼Œä¹Ÿå°±ä¼šé€ æˆæ›´ä¹…çš„åœæœºæ—¶é—´ã€‚è€Œkraft çš„æ¶æ„ï¼Œcontrolleræœ‰å¤šä¸ªï¼Œåªæ˜¯åªæœ‰ä¸€ä¸ªæ˜¯leader æä¾›ä¸»æ§æœåŠ¡ï¼Œå…¶ä»–çš„ä½œä¸ºfollower ï¼Œä¼šå®æ—¶åŒæ­¥leaderçš„å…ƒæ•°æ®ä¿¡æ¯ï¼Œä¹Ÿå°±æ˜¯å…ƒæ•°æ®åœ¨å¤šä¸ªcontroller é‡Œé¢æ˜¯å‡ ä¹ä¿æŒä¸€è‡´çš„(raft åè®®ä¿è¯çš„)ï¼Œæ‰€ä»¥æ•…éšœåˆ‡æ¢çš„æ—¶å€™ï¼Œå‡ ä¹ä¸éœ€è¦å†åŒæ­¥å…ƒæ•°æ®ï¼Œå°±å¯ä»¥å®Œæˆcontroller åˆ‡æ¢ã€‚

- æœ‰ç‚¹æ„æ€ï¼ŒFoundation DBä¹Ÿæ˜¯å¤šä¸ªcontrollerçš„æ€è·¯

- ## ğŸŒ° When Discord started partitioning their data, they ran into unexpected issues.
- https://twitter.com/ProgressiveCod2/status/1734532331338342487
  - The main reason was their partitioning strategy.
  - In Discord's data model, every message belonged to a channel and was stored in a table known as Messages.
  - The Messages table was partitioned based on Channel ID and Bucket. The Bucket is basically a static time window.
  - This scheme had a serious implication: A Discord server with hundreds of thousands of members could potentially generate enough messages in a short period. This can overwhelm a partition in no time.
  - With reads more expensive than writes in Cassandra, this arrangement led to hot partitions.
  - How did Discord deal with this issue?
  - [SDC#8 - Database Replication Under the Hood](https://newsletter.systemdesigncodex.com/p/database-replication-under-the-hood)

- ## ğŸ†šï¸ å¦‚æœä»Šå¤©è¿˜æœ‰äººæ´¥æ´¥ä¹é“ä»€ä¹ˆåˆ†è¡¨åˆ†å¸ƒå¼æµ·é‡è§„æ¨¡é«˜å¹¶å‘ï¼Œåªèƒ½è¯´æ˜ä¸€ä»¶äº‹ï¼šè¿™å“¥ä»¬è¿‡å»åå¹´å·²ç»åœæ­¢å­¦ä¹ äº†ï¼Œè¿˜æ´»åœ¨ä¸Šä¸€ä¸ªæ—¶ä»£é‡Œã€‚
- https://twitter.com/GobeUncleWang/status/1725311478528721026

- ### ğŸ‘‡ğŸ» æ”¯æŒåˆ†å¸ƒå¼

- å¯¹äºæµ·é‡æ•°æ®è€Œè¨€ï¼Œä¾ç„¶ä¼šè£…ä¸è¿›ä¸€å°æœºå™¨é‡Œï¼Œæ‰€ä»¥è¿˜æ˜¯ä¼šéœ€è¦åˆ†å¸ƒå¼ã€‚ 
  - è‡³äºé‚£äº›ä¸€ä¸Šæ¥å°±å‡ ä¸ª PB çš„å­˜å‚¨ä¹Ÿåªæ˜¯èƒ½å­˜è€Œå·²ï¼Œè¦ç®—çš„æ—¶å€™ç“¶é¢ˆç«‹åˆ»å°±å‡ºæ¥äº†ã€‚
  - æœ€å¥½çš„çš„ç³»ç»Ÿè¿˜æ˜¯æ—¢èƒ½åœ¨é«˜æ€§èƒ½èŠ‚ç‚¹ä¸Šé«˜æ•ˆç‡è¿è¡Œï¼Œåˆèƒ½æ”¯æŒåˆ†å¸ƒå¼
- å¦å¤–ï¼Œå¦‚æœæ‰€ç”¨çš„ç¡¬ä»¶æ²¡æœ‰ mainframe é‚£ç§ç¡¬ä»¶å®¹é”™çš„èƒ½åŠ›ï¼Œå•ä¸ªèŠ‚ç‚¹å‡ºç°ä»»ä½•æ•…éšœéƒ½ä¼šå¯¼è‡´æœåŠ¡ä¸­æ–­
  - å®¹ç¾è¿™ç§äº‹é›†ä¸­å¼æ•°æ®åº“ä¹Ÿæœ‰ä¸»ä»å¤åˆ¶ï¼Œå…¶å®ä¹Ÿæ˜¯å¹¿ä¹‰ä¸Šçš„åˆ†å¸ƒå¼ï¼Œå°±åƒæˆ‘ä»¬ä¹Ÿä¸ä¼šç®¡ä¸€ä¸ªCPUå†…çš„ä¸¤ç™¾ä¸ªæ ¸å«åˆ†å¸ƒå¼ä¸€æ ·ï¼Œé€šå¸¸ä¹Ÿä¸ä¼šç®¡è¿™ç§å«åˆ†å¸ƒå¼å°±æ˜¯äº†ã€‚

- APè¿˜æ˜¯éœ€è¦å­˜ç®—åˆ†ç¦»åšåˆ†å¸ƒå¼ã€‚TPåšåˆ†å¸ƒå¼æ€§èƒ½éƒ½æŸå¤±äº†ï¼Œè¿™ä¸ªtradeofféšç€å•æœºç¡¬ä»¶å˜å¼ºè¶Šæ¥è¶Šä¸åˆ’ç®—
- OLAPçš„æ•°æ®ä»“åº“åœºæ™¯ï¼Œæ˜æ˜¾ç°åœ¨å†å¼ºçš„å•æœºè‚¯å®šæ˜¯ä¸è¡Œçš„ï¼Œé¡¶å¤šé¡¶å¤šçš„å°±æ˜¯æœåŠ¡ä¸€ä¸ªç”¨æˆ·åŒæ—¶æŸ¥è¯¢ä½¿ç”¨ï¼Œå¤šä¸ªæœºå™¨åŒæ—¶åä½œæ¥æœåŠ¡ä¸€ä¸ªè¯·æ±‚æ‰èƒ½ç»™ä¸€ä¸ªå³å¸­åŒ–çš„ç»Ÿè®¡åˆ†æåœºæ™¯æä¾›å¾ˆå¥½çš„ä½“éªŒï¼Œç”šè‡³æ›´å¥½çš„æœåŠ¡ä¸€ä¸ªå¤šäººåŒæ—¶ä½¿ç”¨çš„å³å¸­åŒ–ä½¿ç”¨åœºæ™¯

- ä½ è¿™æ˜æ˜¾æ˜¯è¯¯å¯¼ã€‚åªè¦éœ€æ±‚è¶…è¶Šå•æœºä¸Šé™ï¼Œä½ å°±å¾—åˆ†å¸ƒå¼ã€‚ä½ è§è¿‡HPCæ˜¯å•æœºå—ï¼Ÿè¿CPUéƒ½å¾—æ˜¯åˆ†å¸ƒå¼ç½‘çŠ¶ç»“æ„ï¼Œå°±å› ä¸ºå¤„ç†éœ€æ±‚è¿œè¶…â€œå•å¤„ç†ç»“æ„â€çš„èƒ½åŠ›ä¸Šé™ï¼Œå¿…é¡»é›†ç¾¤åŒ–å¤„ç†ã€‚

- è¿™æ–‡ç« æ²¡è€ƒè™‘åˆ°å®é™…å·¥ç¨‹åœºæ™¯ã€‚å•æœºå¿…ç„¶æ˜¯ä¸å®‰å…¨çš„ï¼Œå¤–éƒ¨å› ç´ æ¯”å¦‚ç½‘ç»œæŠ–åŠ¨ï¼Œæ–­ç½‘æ–­ç”µï¼Œbugå¯¼è‡´è¿›ç¨‹å¼‚å¸¸ç»ˆç»“ï¼Œå†…éƒ¨å› ç´ æ¯”å¦‚ç‰¹æ€§æ›´æ–°/ç‰ˆæœ¬æ›´æ–°å¯¼è‡´çš„é‡å¯ã€‚å•æœºå¿…ç„¶å­˜åœ¨ä¸€å®šæ—¶é—´çš„åœæœºï¼Œè¿™åœ¨å®é™…çš„å·¥ç¨‹åœºæ™¯ä¸­æ˜¯æ— æ³•å¿å—çš„ã€‚
  - ä½ è¯´çš„å«å†—ä½™å’Œæ–‡ç« è¯´çš„åˆ†å¸ƒå¼æ˜¯ä¸¤å›äº‹ã€‚æ–‡ç« çš„åˆ†å¸ƒå¼æ˜¯æŒ‡åŒä¸€ä¸ªä»»åŠ¡é€šè¿‡ç‰¹å®šçš„åè®®åˆ†æ•£åˆ°ä¸åŒçš„ä¸»æœºä¸ŠååŒå¤„ç†ï¼Œå•æœºéœ€è¦æ€ä¹ˆåšå†—ä½™ï¼Œåˆ†å¸ƒå¼ä¹Ÿæ˜¯ä¸€æ ·è¦åšçš„ã€‚

- å­˜å‚¨èƒ½å­˜ ä¸ä»£è¡¨è¦è®¡ç®— æŸ¥æ‰¾çš„æ—¶å€™ èƒ½å¿«é€Ÿç®—å¾—å‡ºæ¥ æŸ¥å¾—å‡ºæ¥ã€‚ 

- ### ğŸ‘‡ğŸ» æ”¯æŒå•æœº

- ä½ å•æœºæ€ä¹ˆä¿è¯availability
  - ä¸»ä»

- åˆ†å¸ƒå¼æ˜¯å½“å¹´ç¡¬ä»¶ç®—åŠ›ä¸å¤Ÿçš„æ—¶ä»£æ²¡æœ‰åŠæ³•çš„åŠæ³•ã€‚å½“æ—¶ä¸æ˜¯æ¯ä¸ªå…¬å¸éƒ½èƒ½ä¹°å¾—èµ·ä¸€ç™¾å¤šä¸ªCPUçš„æœåŠ¡å™¨ï¼Œç°åœ¨ç¡¬ä»¶ä»·æ ¼ä¸‹æ¥äº†ï¼Œæ€§èƒ½ä¸Šå»äº†ï¼Œä¸ªäººç”µè„‘éƒ½èƒ½64æ ¸ï¼Œç¡¬ä»¶è¿™ç§å¼ºæ‚çš„æ€§èƒ½ï¼ŒåŸºæœ¬ä¸Šä¼ä¸š90%çš„éœ€æ±‚ï¼Œä¹°ä¸¤ä¸‰å°æœåŠ¡å™¨å°±èƒ½æ»¡è¶³ï¼Œå†ä¹Ÿä¸éœ€è¦è¢«è¿«æä¸€å †èŠ±é‡Œèƒ¡å“¨çš„é…ç½®æ¥å®ç°åˆ†å¸ƒå¼ã€‚

- å¤§è§„æ¨¡æƒ…å†µä¸‹çš„æ•°æ®åˆ†ç‰‡ä¸å‡ï¼ˆå€¾æ–œï¼‰é€ æˆè´Ÿè½½ä¸å‡è¡¡ï¼Œè¿™ä¾ç„¶æ˜¯ä¸€ä¸ªå›ºæœ‰çš„æœªæ›¾è§£å†³å¥½çš„é—®é¢˜ï¼Œåˆ†åº“åˆ†è¡¨å¯¹åº”çš„æ˜¯å…³ç³»è¡¨è¿™æ ·çš„æ•°æ®ï¼Œä½†å¯¹äºé€’å½’çš„å›¾æ•°æ®æ€ä¹ˆåšåˆ°åˆ†å¸ƒå¼æƒ…å†µä¸‹çš„å¤„ç†ï¼ˆrpcå¼€é”€å¦‚ä½•ä¼˜åŒ–è°ƒåº¦ï¼‰ï¼Œä¹Ÿæ²¡æœ‰æˆç†Ÿæ–¹æ¡ˆå§
  - å•æœºå†…éƒ¨å°±èƒ½è°ƒæ•´

- è¦æ˜¯æœ‰åº•å±‚åº“èƒ½ç”¨H100è¿™æ ·çš„æ˜¾å¡åšåŠ é€Ÿï¼Œç¡¬ä»¶çš„ä¸Šé™ä¼šæ›´ææ€–ã€‚

- å®¹ç¾ä¸æ˜¯åˆ†å¸ƒå¼

- 
- 
- 

- ## ğŸ•›ğŸ”¥ [A history of the distributed transactional database | Hacker News_201812](https://news.ycombinator.com/item?id=18600404)
- 
- 
- 

- ## ğŸ”¥ [We put a distributed database in the browser and made a game of it | Hacker News_202307](https://news.ycombinator.com/item?id=36680535)
- Is this something similar to couch/pouchdb?
  - TigerBeetle is more domain-specific, i.e. focused on financial transactions and high-performance, high-availability. There are just two entity types in the database: accounts and transfers between accounts. 
- > In comparison to {C, P}ouchDB, I think the question is around offline-first availability.
  - Got it, thanks! Yeah that is indeed not how TigerBeetle works. If you ever cannot connect to the cluster, you keep retry messages (idempotently) until you connect and the message succeeds.

# discuss-cap
- ## 

- ## 

- ## 

- ## What is the difference between Linearizability and Serializability? 
- https://twitter.com/vaibhaw_vipul/status/1764525904045220312
- Linearizability in context of consistency refers to the "C" in CAP theorem. 
  - It is synonym of "atomic consistency". 
  - It is a guarantee about single operation on single objects. Writes appear instantaneous.  
- Serializability is the "I" or isolation in ACID. 
  - It is a guarantee about transaction or groups of one or more operations over one or more objects. 
  - It guarantees that the execution of a set of transactions (usually containing read and write operations) over multiple items is equivalent to some serial execution (total ordering) of the transactions. 
  - Serializability is a mechanism for guaranteeing database correctness.
  - Combining serializability and linearizability yields strict serializability

- In practice, your database is unlikely to provide serializability, and your multi-core processor is unlikely to provide linearizabilityâ€”at least by default. 
  - As the above theory hints, achieving these properties requires a lot of expensive coordination.
# discuss-clock
- ## 

- ## 

- ## 

- ## How can you order events in a distributed system?
- https://twitter.com/Franc0Fernand0/status/1767167089385541760
- Distributed systems don't have a global clock to order events. They use logical clocks to measure time in terms of logical operations.
- There are two popular types of logical clocks.
- A Lamport clock uses a single counter initialized to 0 for each process.
  - A process increases its counter before each operation:
  - when sending a message, a process attaches a copy of its counter
  - when receiving a message, a process merges its and the received counter. The merge takes the maximum of the two counters.
- Vector clocks address this problem.
  - Such a clock uses a list of N counters for each process, where N is the number of processes.
  - A process increases its counter in its list before each operation:
  - when sending a message, a process attaches a copy of its list.
  - when receiving a message, a process merges its and the received list. The merge takes the maximum of each counter in the two lists.
- This mechanism gives a partial ordering between lists by comparing corresponding counters.
- One problem with vector clocks is that the required memory grows linearly with the process.
  - There are, however, variants of vector clocks that can fix this.

# discuss-decentralized/p2p
- ## 

- ## 

- ## ğŸ”¥[OrbitDB: Peer-to-Peer Databases for the Decentralized Web | Hacker News_202103](https://news.ycombinator.com/item?id=26310094)
- 
- 
- 

- ## ğŸ”¥ [OrbitDB: Peer-to-peer databases for the decentralized web | Hacker News_202004](https://news.ycombinator.com/item?id=22918467)
- 
- 
- 

- ## ğŸ”¥ [OrbitDB â€“ serverless, peer-to-peer database on top of IPFS | Hacker News_201812](https://news.ycombinator.com/item?id=18748542)
- I'm looking for a distributed scalable P2P triple store (or graph database) to store and retrieve RDF using SPARQL.
  - SOLID (Tim Berners-Lee, RDF)
  - GUN (ours, graph)

# discuss-protocol-raft/paxos âš–ï¸
- ## 

- ## 

- ## 

- ## Log stacking can happen when using Raft/Paxos to replicate DBs w/ their own WALs. 
- https://twitter.com/strlen/status/1762933075783405649
  - Apache Kudu consolidates: the tablet WAL is also the Raft log. 
  - There have been experimental in other systems by disabling local WAL and using consensus log for recovery. 
  - Is there a generic solution?

- https://twitter.com/jorandirkgreef/status/1763073571616694751
  - This is why TigerBeetleâ€™s global consensus protocol and local storage engine were co-designed:
  - not only to share the WAL, but also to
  - run directly on a raw block device
  - without requiring a filesystem

- I don't think it's possible in a multi-node system.  Whether you call it the log or the network.

- Back in the day we built Baardskeerder, an append-only B-tree'ish thing, to act as both the log and the database for Arakoon, a multi-Paxos key-value-store. Earlier version used traditional separation in log files and something BDB-like (TokyoCabinet).

- In Redpanda the only log is a Raft log.

- I believe AWS has such a generic building block _internally_ seen it called various names â€“ AlfBus, transaction journal, and perhaps what they called Grover 
  - I am familiar with Grover, I worked on Aurora in 2016-2018. It is its storage system publicly discussed in the Aurora papers. It does provide centralized storage for both the data and log pages. Not sure about any changes since then.

- ## å¶ç„¶å‘ç°å‰‘æ¡¥å¤§å­¦ 2020 å¹´å¯¹æ¯” Paxos å’Œ Raft çš„ä¸€ç¯‡è®ºæ–‡ï¼Œé¢˜ç›®å¾ˆæœ‰æ„æ€ï¼šâ€œæˆ‘ä»¬å°±å…±è¯†åè®®è¾¾æˆå…±è¯†äº†å—ï¼Ÿâ€
- https://twitter.com/qtmuniao/status/1760552518051070087
  - ä¸»è¦æ€è·¯æ˜¯ç”¨ Raft çš„æœ¯è¯­å’Œæ€è·¯å»è¡¨è¿° MultiPaxosï¼Œä»¥æ˜æ™°ä¸¤è€…çš„åŒºåˆ«å’Œè”ç³»ã€‚

- ## ğŸ†šï¸ [Paxos vs. Raft: Have we reached consensus on distributed consensus? | Hacker News _202107](https://news.ycombinator.com/item?id=27831576)
- Since the article mentions Google as the outlier preferring Paxos, I may be able to shed some light from a few years ago.
  - The Paxos, paxosdb, and related libraries (despite the name, all are multi-paxos) are solid and integrated directly into a number of products (Borg, Chubby, CFS, Spanner, etc.). There are years of engineering effort and unit tests behind the core Paxos library and so it makes sense to keep using and improving it instead of going off to Raft. As far as I am aware the Google Paxos implementation predates Raft by quite a while.
- This makes sense to me. Very few of us have the resources to maintain, for example, the kind of globally synced (way beyond typical NTP) clock infrastructure that Google has (TrueTime)

- I hope that the "Paxos vs Raft" debate can die down, now that engineers are learning TLA+ and distributed systems more thoroughly. These days we can design new protocols and prove their correctness, instead of always relying on academics. For example, at MongoDB we considered adopting the reconfiguration protocol from Raft, but instead we designed our own and checked it with TLA+. 

- ğŸ‘¥ 
- https://twitter.com/tweeshan/status/1756534657892679871
- Raft has the lowest availability and performance. IIUC Paxos has the best availability (because any replica can lead) but costs 2 round trips.
# discuss-sharding
- ## 

- ## 

- ## Top 4 data sharding algorithms explained.
- https://twitter.com/alexxubyte/status/1785330458408124790
  - We are dealing with massive amounts of data. Often we need to split data into smaller, more manageable pieces, or â€œshardsâ€. 
- Here are some of the top data sharding algorithms commonly used
- ğŸ”¹ Range-Based Sharding
  - This involves partitioning data based on a range of values. 
  - For example, customer data can be sharded based on alphabetical order of last names, or transaction data can be sharded based on date ranges. 
- ğŸ”¹ Hash-Based Sharding
  - In this method, a hash function is applied to a shard key chosen from the data (like a customer ID or transaction ID). 
  - The result determines which shard the data will go to. 
  - This tends to distribute data more evenly across shards compared to range-based sharding. However, we need to choose a proper hash function to avoid hash collision.
- ğŸ”¹ Consistent Hashing
  - This is an extension of hash-based sharding that reduces the impact of adding or removing shards. 
  - It distributes data more evenly and minimizes the amount of data that needs to be relocated when shards are added or removed.
- ğŸ”¹ Virtual Bucket Sharding
  - Data is mapped into virtual buckets, and these buckets are then mapped to physical shards. 
  - This two-level mapping allows for more flexible shard management and rebalancing without significant data movement.

- ## There are two main ways to partition a table.
- https://twitter.com/Franc0Fernand0/status/1780571942438818192
  - Vertical partitioning moves columns to different tables. It's simple to understand and keeps data about certain features together. However, a new partition will be required in case of further data growth.
  - Horizontal partitioning divides a table into new tables with the same columns but fewer rows. Each new table represents a different data store.

- There are 3 popular ways to partition a table horizontally:
  1. Hash-based maps rows to new tables by applying a hash function to one or more columns.
  2. Range-based splits the data using ranges of values in one or more columns.
  3. Directory-based uses a dedicated lookup service to store the partition schema.

- It's flexible because it abstracts the schema and the number of machines from the application.
  - But it's more complex, and connecting to the service slows the performance.

- ## ğŸ†šï¸ Sharding is overloaded, do you mean table partitioning, page partitioning, node partitioning or something else? Ignoring hash/range partition trade offs, separate discussion.
- https://twitter.com/sunbains/status/1769017995664437733
- Table partitioning is on the same node and consistency is preserved but problems with global indexes and foreign keys. 
  - The former complicates your queries. Usually suitable as a poor manâ€™s time series type solution. 
  - You can drop old data by simple dropping the old partition, itâ€™s not a general solution for scalability.
- With node partition like CitusDB or Vitesse, you have the above problems plus bigger problems with cross node consistency. 
  - It becomes the application problem, not good either. 
  - Manually shard the data and the application has to be aware of the location of the data. 
  - Resharding when you outgrow your shards is the stuff of nightmares. 
  - DDL. takes that pain to a different level, you have to have worked on very large data sets to understand it. 
  - Foreign keys forget it. Given the consistency issues, good luck.
- Automatic sharding at the page level, this is the architecture pioneered by Google Spanner. 
  - This gives strong consistency guarantees, no manual resharding, DDL is easy and can be scaled by using all the nodes in the network (look at TiDB Distributed eXecution Framwotk for an example). 
  - No stopping the world on DDL, see F1 online schema change paper. 
  - Foreign keys and constraints and all the benefits of RDBMS with proper distributed SQL queries. 
  - This isnâ€™t the future. You can start small, try it with http://tiup.io , HA out of the box, excellent observability tools built in too.
  - TiDB does this and is MySQL compatible.

- Yugabyte and CockroachDB are Postgres compatible.

- ## Sharding is the most proven database scaling pattern. The rest is just noise
- https://twitter.com/isamlambert/status/1768341699334668676
- Itâ€™s basically the only pattern, but itâ€™s not limited to sql of course
  - 
- Consistent hashing is another pattern... Sharding is just preferred because you can shard per account. A single account will (typically) never access another account's data.
  - Okay, okay... You have to split up the data to scale horizontally, of course. I guess ANOTHER scaling pattern would be MemSQL in order to process more/faster on a single instance.
# discuss
- ## 

- ## Byzantine failures only come up in crypto conversations. Never in distributed databases or other distributed system projects. 
- https://twitter.com/sunbains/status/1787376127708696928
  - Non-crypto  seem to care only about fail-stop.
- I don't think it's true that non-crypto distributed systems only care about fail-stop. Omission faults, authentication-detectable corruption, latency-related reorderings, and faults caused by failure recovery are typically considered.
  - I omitted the  faults you mention only for brevity. I agree they are considered. The Red Panda article you posted also says that the fsync issue  is a small part of Byzantine failures and in my understanding is related to the point above.

- ## ğŸ¤”â“ Modern distributed databases require scaling in two axis. Lots of small objects and small number of large objects.
- https://twitter.com/sunbains/status/1780293307861737643
  - In concrete terms, 100â€™s of millions of small tables and 10â€™s of thousands of very large tables.

- Often the way to solve for both is to turn one into the other somehow, then solve for that
  - The example I was thinking of was chopping one big table into multiple ranges to effectively get "many small tables" (I know not 1:1) that are easy to distribute over many machines

- ## Eventual Consistency (EC) means itâ€™s impossible to diverge (a safety invariant: if all messages are sent, states are equal). 
- https://twitter.com/_Felipe/status/1777667242403999970
  - SEC(Strong EC) means the messages will, in fact, be eventually sent (a liveness property).
- Leslie Lamportâ€™s TLA+ is an extension of LTL (Linear Temporal Logic).

- ## ğŸ§© Database guarantees durability using WAL, and it is possible due to LSN 
- https://twitter.com/sunbains/status/1771190591034102129
  - Databases dump every single operation in WAL before it is applied to the actual data, and the Log Sequence Number (LSN) is a unique identifier associated with each record added to the WAL.
  - LSNs are monotonically increasing enabling replicas to keep track of the replication. The master node keeps a record of the highest LSN it has written. The difference between the master_lsn and the LSN on the replica helps the database determine the Replication Lag.
- Thus LSNs are what enable us to achieve and implement three critical features of any durable database, and they are
  1. recovery from failure and rebuilding in-memory state of the database
  2. asynchronous replication between master and replica
  3. point-in-time snapshots by replaying changes from WAL until desired LSN

- You donâ€™t necessarily need a WAL and LSN for replication, they are orthogonal(æ­£äº¤çš„). 
  - The idea of a WAL is to first write to the log (the A in WAL) then data files. 
  - Also, it doesnâ€™t have to be a transaction aware log either. It can simply be a physical log and you can build the transaction log on top of it along with the replication, both synchronous and asynchronous.

- ## Scaling Database: When and How to Shard
- https://twitter.com/sahnlam/status/1764539121782112444
  - Database sharding refers to splitting data across multiple database servers and is commonly used for scaling. 
  - However, sharding introduces major operational and infrastructure complexity that should be avoided unless absolutely necessary.

- ğŸ’¡ Alternative Scaling Approaches
  - Vertical Scaling:  Use more powerful single database servers with more CPUs, memory, storage and I/O bandwidth. Much simpler to manage than sharding.
  - SQL Optimization: Tune SQL queries and database schema to maximize performance on a single server using proper indexes, efficient SQL, etc.
  - Caching: Use in-memory caches like Redis to reduce database load by serving common queries from the cache instead of hitting the database every time.
  - Read Replicas + Load Balancer: Add horizontal read scaleability without full complexity of sharding. Directs reads across replicas.
- These optimization approaches should be exhausted before considering sharding.

- ğŸ’¡ Sharding Methods
  - Vertical Sharding: Split database into columnar tables or sections vs rows. For example, having one table for names and another table for emails.
  - Horizontal Sharding: Split database into row partitions distributed evenly across multiple servers. Methods include range based, directory based, and hash based sharding.
- When sharding, use the simplest approach that meets requirements to minimize complexity. Seek to avoid sharding until necessary despite the scaling benefits. The infrastructure and operational overheads often outweigh gains.

- ## Distributed Services can't exist without Eventual Consistency.
- https://twitter.com/RaulJuncoV/status/1763220110251032900
- These are the 3 patterns to deal with it.
- A practical example: Imagine you have two services: Orders and Invoices. When the customer places an order, you must generate the associated invoice.
- 1. Background Synchronization Pattern
  - You save the order and then update/create the invoice in the background.
  - The job in schedule checks for new or changed orders and creates or updates invoices.
  - This approach decouples the services and reduces the latency on the user-facing service (Orders service).
  - You get Eventual Consistency but slowly.

- 2. Orchestrated Request-Based Pattern
  - You save the order and send it to the Invoices service. Unlike the previous pattern, this one attempts to process the entire distributed transaction. 
  - It requires some kind of Orchestrator Service, which can be a separate service or an existing one.
  - It requires compensating transactions.

- 3. Event-Based Pattern
  - Services emit events, and others listen to them to update their data. 
  - You save the order and emit an event of "order-created." The Invoices service listens for these events and creates a new invoice based on the order. 
  - The services do not need to know about each other and can scale independently.

- TL; DR
  - Background Synchronization offers simplicity and decouples services but introduces delays in data consistency.
  - Orchestrated Request-Response offers a controlled flow with reliability but increases response times.
  - Event-based brings loose coupling and scalability and allows services to react to changes at the cost of complexity.

- Eventual Consistency is the price for better performance, scalability, and availability.

- I prefer Event-Based but I have used background synchronization a lot in the past. You can set up synchronization between your database instances releasing the Application layer from that responsibility.
- If the latency to move data over is not a problem, Background Synchronization is the right solution.

- ## This summarizes quite well the diff between Data Sharding vs. Distributed SQL
- https://twitter.com/FranckPachot/status/1761673558206407031
  - Sharding with table families is not one logical DB: a single hierarchy of foreign key and no global index
  - Distributed SQL provides all relational SQL features at global level

- ## There's a difference between a database designed to scale up an existing MySQL or Postgres install and a distributed database that has chosen to try and be compatible with MySQL or Postgres.
- https://twitter.com/isamlambert/status/1758872040646615090
- Let me market my names:
  * AlloyDB, Neon, Aurora = DisaggSQL
  * Planetscale = ShardSQL
  * CockroachDB, TiDB, YugabyteDB = DistSQL
  * Oracle, MySQL, Postgres = TradSQL
  * MongoDB is DistMQL

- AlloyDB is a good example of a distributed database that is trying to be Postgres compatible.

- Scale up an existing MySQL or Postgres or scale out an existing setup? When folks try to scale out an existing installed: they tend to start building something like a distributed database and then get into frequent buy vs build debates

- ## The purpose of distributed systems has changed drastically over the last 2 decades.
- https://twitter.com/YingjunWu/status/1745213563755749806
  - When MapReduce first emerged, the need for dist. systems was to get better perf - single node wasn't powerful enough.  
  - But now, in the cloud era, we we can easily rent machines with big DRAM from AWS. Yet, people still need distributed systems, but primarily for high availability.
  - I bet more and more data systems will be optimized for scaling up, not scaling out. Examples include RisingWave, DuckDB, Neon, and many others.

- But it's trickier to manage in terms of autoscaling? Assume at peak time we need say 100 cores, but only for 1-2 hours, and for the rest of the day we need say 20 cores. We would need to use different machine types through the day which is not well automated scenario now.
  - That's a very good point. Yes, right now we still need a multi-node cluster in this case, as migrating terabytes of states from one machine to another is always a pain.
  - The idea of 'S3 as a primary storage, ' as adopted in many modern systems, can mitigate the problem. To scale up (let's say we just have one single machine), we only need to create a new machine and allow the new machine to load the entire states from S3. But unfortunately, at the moment (atm), S3 is not fast enough and loading data is still quite slow.
  - I'm keeping a close eye on S3 Express. If AWS really delivers low-latency S3 at low cost, then it may address the problem nicely.
- Great insights! In my personal opinion, scaling out could be still relevant and important for certain customers who anticipate explosive growth in data usages and want to position for the future (a few years out) in choosing DB vendors..

- ## DuckDB is great, but switching my code from single-node processing to distributed processing should be a no-brainer and invisible to me, like switching a flag. 
- https://twitter.com/ananthdurai/status/1733040083056701865
  - What to do with the current duckdb ecosystem to enable it? Is PySpark api support the answer for it?
- Write everything in SQL and then use SQLGlot to auto-transpile to Spark if you ever need it.  If you can avoid it, I wouldn't recommend using the Pyspark API (in duckdb or spark) because you get locked into it https://

- Check clickhouse-local.
  - Apart from clickhouse, if you use compute engine agnostic storage format (e.g. delta) and compute engine agnostic transformations engine (e.g. dbt), migration should be as easy as swapping?

- ## ğŸª§ å¿«é€Ÿäº†è§£ä¸‹è´Ÿè½½å‡è¡¡6å¤§ç®—æ³•
- https://x.com/qloog/status/1797255930905776240

- ## ğŸª§ 6 Load Balancing ALGORITHMS you Must Know
- https://twitter.com/AmigosCode/status/1725507946950344949
1. Round Robin
 - Allocates incoming requests to each server in a circular sequence. It ensures an equal distribution of the workload among servers.
2. Sticky Round Robin
 - Similar to Round Robin, but with the added feature of maintaining session persistence. Once a client is assigned to a server, subsequent requests from that client continue to be directed to the same server.
3. Least Time
 - Assigns requests to the server with the least expected processing time. This algorithm considers factors like server response time and current load.
4. Least Connections
 - Directs traffic to the server with the fewest active connections. This helps distribute the load more evenly across servers, preventing overload on any single server.
5. IP/URL Hash
 - Uses a hash function on the client's IP address or URL to determine which server should handle the request. This ensures that requests from the same client are consistently directed to the same server.
6. Weighted Round Robin
 - Similar to Round Robin, but assigns different weights to servers based on their capacity or processing power. Servers with higher weights receive more requests, allowing for proportional load distribution.

- Each of these load balancing algorithms plays a crucial role in optimizing the performance and reliability of server clusters by efficiently distributing incoming requests.

- ## æœ‰å•¥å·¥å…·å¯ä»¥æ¯”è¾ƒå¥½çš„åœ¨å•æœºæ¨¡æ‹Ÿåˆ†å¸ƒå¼çš„ç¯å¢ƒå—ï¼Ÿä¸»è¦æ˜¯æƒ³æ¨¡æ‹Ÿä¸€ä¸‹å¸¦å®½é™åˆ¶å’Œç½‘ç»œå»¶è¿Ÿ
- https://twitter.com/leiysky/status/1725430083106836637
- sudo tc qdisc add dev wlan0 root netem delay 1000ms
- chaos-mesh + minikube ï¼ˆæˆ–è€…ç›´æ¥ tcã€‚å¯ä»¥é™åˆ¶ bandwidthã€‚éƒ½æ˜¯ç”¨çš„ tc qdiscï¼Œé™æµæ˜¯ tbf å»¶è¿Ÿæ˜¯ netemã€‚
- æˆ‘ç”¨ Clojure å†™çš„ä¸€ä¸ªå·¥å…·ï¼Œjepsen

- ## ğŸ’¡ Database Sharding Explained
- https://twitter.com/arcnotes/status/1719692415995425014
  - [Database Sharding Explained](https://architecturenotes.co/database-sharding-explained/)

- ## ğŸ”¥ [From coding bootcamp graduate to building distributed databases | Hacker News_202102](https://news.ycombinator.com/item?id=26209594)
- 
- 
- 

- ## ğŸ”¥ [The Inner Workings of Distributed Databases | Hacker News_202304](https://news.ycombinator.com/item?id=35602983)
- 
- 
- 

- ## ğŸ”¥ [Technical Challenges Developing a Distributed SQL Database | Hacker News_201904](https://news.ycombinator.com/item?id=19759101)
- 
- 
- 

- ## ğŸ”¥ [How does database sharding work? | Hacker News_202304](https://news.ycombinator.com/item?id=35476518)
- 
- 
- 

- ## ğŸ›¢ï¸ğŸ”¥ [Azure Cosmos DB, a globally distributed database | Hacker News_201705](https://news.ycombinator.com/item?id=14308814)
- 
- 
- 

- ## ğŸ”¥ [Distributed algorithms in NoSQL databases | Hacker News_201709](https://news.ycombinator.com/item?id=15367003)
- 
- 
- 

- ## ğŸ”¥ [Principles of Sharding for Relational Databases | Hacker News_201708](https://news.ycombinator.com/item?id=14971650)
- 
- 
- 
