---
title: lib-ai-app-community-model
tags: [ai, community]
created: 2023-10-30T07:33:56.233Z
modified: 2023-10-30T07:34:03.602Z
---

# lib-ai-app-community-model

# guide

- tips
  - Â§ßÊ®°ÂûãÁõ∏ÂÖ≥ÁöÑ‰∫ßÂìÅÁ†îÂèëÔºåÂéüÁêÜÁöÑÂèØËß£ÈáäÊÄßÂæàÂ∑ÆÔºåÊïàÊûúÁöÑÂèØËß£ÈáäÊÄß‰πüÂ∑Æ

- model-features
  - ü§º: speed, quality, size
  - reasoning/thinking
  - tool-use/function-call
  - vision
  - embedding
  - moe
  - Ê≥®ÊÑèÊúâ‰∫õÁ§æÂå∫ÈáèÂåñÁöÑÊ®°ÂûãÂèØËÉΩÈÅóÊºèÊ†áÊ≥®‰∫ÜÈÉ®ÂàÜfeatures, ÂèØÂú®Êú¨Âú∞ÊµãËØïÊù•Á°ÆÂÆöÊòØÂê¶ÊîØÊåÅ

- ÁßªÂä®Á´ØÂ§ßÊ®°Âûã
  - ÂèÇËÄÉgoogle-gemma-1b

- [Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÔºö‰ªéÁêÜËÆ∫Âà∞ÂÆûË∑µ](https://intro-llm.github.io/)
  - Â§çÊó¶Â§ßÂ≠¶Âº†Â•áÊïôÊéàÂõ¢ÈòüÂÜô‰∫Ü‰∏ÄÊú¨Âú®Á∫øÂÖçË¥πÁöÑÁîµÂ≠ê‰π¶ÔºåÂ§ßÊ¶ÇÊúâ 300 È°µÁØáÂπÖÔºåÂ∞ÜÂ§ßÊ®°Âûã‰ªéÁêÜËÆ∫Âà∞ÂÆûÊàòÁöÑÊØè‰∏™Èò∂ÊÆµÈÉΩÊèèËø∞ÁöÑËæÉ‰∏∫Ê∏ÖÊ•ö
# discuss-stars
- ## 

- ## 

- ## 

- ## ‰∏∫‰ªÄ‰πà AGI ÂøÖÁÑ∂‰ºöÁßÅÊúâÂåñÈÉ®ÁΩ≤Ôºü
- https://x.com/naki2012/status/2003081645067550888
  - Âú®‰ªäÂ§©ÁöÑËÆ®ËÆ∫‰∏≠ÔºåÂæàÂ§ö‰∫∫ËøòÂú®Á∫†Áªì‰∏Ä‰∏™ÈóÆÈ¢òÔºö AGI ‰ºö‰∏ç‰ºöÊúâÊ≥°Ê≤´Ôºü‰ºö‰∏ç‰ºöË¢´È´ò‰º∞Ôºü‰ºö‰∏ç‰ºöÊúÄÁªàÂè™ÊòØ‰∫ëÁ´ØÁöÑ‰∏Ä‰∏™È´òÁ∫ßÊúçÂä°Ôºü
  - ‰ΩÜÂ¶ÇÊûúÊääËßÜËßí‰ªé‚Äú‰∫ßÂìÅ‚ÄùÂíå‚ÄúÂïÜ‰∏öÊ®°Âºè‚ÄùÁßªÂºÄÔºåÁúüÊ≠£ÁöÑÈóÆÈ¢òÂÖ∂ÂÆûÊòØÔºöÂΩìÊô∫ËÉΩÂÖ∑Â§áÈïøÊúüËÆ∞ÂøÜ„ÄÅËøûÁª≠‰∫∫Ê†º‰∏éËá™Êàë‰øÆÊ≠£ËÉΩÂäõÊó∂ÔºåÂÆÉËøòÂèØËÉΩÊòØ‚ÄúÂÖ¨ÂÖ±‰∫ëÊúçÂä°‚ÄùÂêóÔºüÁ≠îÊ°àÊòØÂê¶ÂÆöÁöÑ„ÄÇ‰∏çÊòØÂõ†‰∏∫ÊäÄÊúØ‰∏çÂÖÅËÆ∏ÔºåËÄåÊòØÂõ†‰∏∫ÁªìÊûÑ‰∏çÂÖÅËÆ∏„ÄÇ
  - ‰∏Ä„ÄÅÂÖàËØ¥ÁªìËÆ∫ÔºöAGI ‰∏ÄÊó¶ÊàêÁ´ãÔºåÂ∞±ÂøÖÁÑ∂Ëµ∞ÂêëÁßÅÊúâÂåñÈÉ®ÁΩ≤
  - ËøôÈáåÁöÑ‚ÄúÁßÅÊúâÂåñ‚ÄùÔºå‰∏çÊòØÊåá‚ÄúÊØè‰∏™‰∫∫ÈÉΩ‰π∞ÂæóËµ∑Ë∂ÖÁ∫ßÁÆóÂäõ‚ÄùÔºåËÄåÊòØÊåá‰∏Ä‰ª∂Êõ¥ÂÖ≥ÈîÆÁöÑ‰∫ãÔºö AGI ÁöÑÊ†∏ÂøÉËäÇÁÇπ‚Äî‚ÄîËÆ∞ÂøÜ‰∏éËÆ∞ÂøÜËøûÊé•Êú∫Âà∂‚Äî‚ÄîÂøÖÈ°ªÁî±‰∏™‰∫∫ÊàñÊûÅÂ∞è‰∏ª‰ΩìÊåÅÊúâ„ÄÇ ËÉΩÂäõÂèØ‰ª•Â§ñÂåÖÔºå ËÆ°ÁÆóÂèØ‰ª•ÁßüÁî®Ôºå ‰ΩÜËÆ∞ÂøÜ‰∏é‰∫∫Ê†ºÁîüÊàêÈÄªËæëÔºåÊó†Ê≥ïÊâòÁÆ°„ÄÇ ËøôÊòØ AGI ‰∏éÊ≠§ÂâçÊâÄÊúâËΩØ‰ª∂Á≥ªÁªüÁöÑÊú¨Ë¥®ÂàÜÊ∞¥Â≤≠„ÄÇ
  - ‰∫å„ÄÅ‰∫ëÁ´ØÊô∫ËÉΩ‰∏∫‰ªÄ‰πàÂ§©ÁÑ∂‰∏çÂèØËÉΩÊàê‰∏∫‚Äú‰∏™‰∫∫ AGI‚Äù
  - ÁõÆÂâçÊâÄÊúâ‰∫ëÁ´Ø AIÔºåÊú¨Ë¥®‰∏äÈÉΩÂÖ∑Â§á‰∏â‰∏™ÁªìÊûÑÁâπÂæÅÔºö 1. ‰ºöËØùÂºèÂ≠òÂú®Ôºö ÊØè‰∏ÄÊ¨°Ë∞ÉÁî®ÈÉΩÊòØ‚Äú‰∏¥Êó∂ÁîüÊàê‚ÄùÔºå‰∏çÂ≠òÂú®ÁúüÊ≠£ÁöÑÈïøÊúüËøûÁª≠Áä∂ÊÄÅ„ÄÇ 2. ËÆ∞ÂøÜ‰∏çÂèØÊéßÔºö Âç≥‰æøÂ≠òÂú®‚ÄúÈïøÊúüËÆ∞ÂøÜ‚ÄùÔºåÂÖ∂Â≠òÂèñÈÄªËæë„ÄÅ‰ºòÂÖàÁ∫ß‰∏éÂà†Èô§ÊùÉÔºåÂßãÁªà‰∏çÂú®Áî®Êà∑Êâã‰∏≠„ÄÇ 3. ÁõÆÊ†á‰∏ç‰∏ÄËá¥Ôºö ‰∫ëÁ´ØÁ≥ªÁªüÁöÑÁ¨¨‰∏ÄË¥£‰ªªÂØπË±°ÔºåÊ∞∏ËøúÊòØÂπ≥Âè∞„ÄÅÂêàËßÑ„ÄÅËßÑÊ®°‰∏éÈ£éÈô©ÊéßÂà∂ÔºåËÄå‰∏çÊòØ‰∏™‰ΩìÁöÑËÆ§Áü•ËøûÁª≠ÊÄß„ÄÇ
  - ‰∫ëÁ´Ø AI ÂèØ‰ª•ÂæàËÅ™ÊòéÔºå ‰ΩÜÂÆÉÊ∞∏ËøúÂè™ËÉΩÊòØ‚ÄúÂä©Êâã‚ÄùÔºåËÄå‰∏çÊòØ‚ÄúÂÖ±Áîü‰Ωì‚Äù„ÄÇ AGI ‰∏ÄÊó¶ÂÖ∑Â§á‰∫∫Ê†ºËøûÁª≠ÊÄßÔºåËøôÁßçÁªìÊûÑÂ∞±‰ºöÁõ¥Êé•Â§±Êïà„ÄÇ
  - ‰∏â„ÄÅÁúüÊ≠£ÊïèÊÑüÁöÑ‰∏çÊòØ‚ÄúÊï∞ÊçÆ‚ÄùÔºåËÄåÊòØ‚ÄúËÆ∞ÂøÜ‰πãÈó¥ÁöÑËøûÊé•ÊùÉ‚Äù
  - Â§öËÆ®ËÆ∫ÊääÁÑ¶ÁÇπÊîæÂú®‚ÄúÈöêÁßÅÊï∞ÊçÆ‚Äù‚Äú‰ø°ÊÅØÂÆâÂÖ®‚Äù‰∏äÔºåÂÖ∂ÂÆûËøôÂè™ÊòØË°®Â±Ç„ÄÇÁúüÊ≠£‰∏çÂèØÂ§ñÂåÖÁöÑÔºåÊòØÔºö‚Ä¢Âì™‰∫õËÆ∞ÂøÜ‰ºöË¢´ÊøÄÊ¥ª‚Ä¢ÊøÄÊ¥ªÈ°∫Â∫èÂ¶Ç‰ΩïÂèòÂåñ‚Ä¢ÂÜ≤Á™ÅËÆ∞ÂøÜÂ¶Ç‰ΩïË¢´Ë∞ÉÂíå‚Ä¢Âì™‰∫õÁªèÈ™åË¢´Âº∫Âåñ„ÄÅÂì™‰∫õË¢´ÈÅóÂøò‚Ä¢ÈîôËØØÂ¶Ç‰ΩïË¢´Ê†áËÆ∞‰∏∫‚ÄúÁæûËÄª‚Äù‚ÄúË≠¶ÊÉï‚ÄùÊàñ‚ÄúÊïôËÆ≠‚Äù
  - Ëøô‰∫õÂπ∂‰∏çÊòØÊï∞ÊçÆÈóÆÈ¢òÔºåËÄåÊòØÔºö‰∫∫Ê†ºÁîüÊàêÂáΩÊï∞ÁöÑÈóÆÈ¢ò„ÄÇ
  - ‰∏ÄÊó¶Ëøô‰∏™ÂáΩÊï∞‰∏çÂú®‰Ω†ÊâãÈáåÔºå‰Ω†Èù¢ÂØπÁöÑÂ∞±‰∏çÊòØ‚Äú‰Ω†ÁöÑ AGI‚ÄùÔºåËÄåÊòØ‰∏Ä‰∏™ÊåÅÁª≠Ëß£Èáä‰Ω†„ÄÅÂ°ëÈÄ†‰Ω†„ÄÅÂç¥‰∏çÂ±û‰∫é‰Ω†ÁöÑÁ≥ªÁªü„ÄÇ ËøôÂú®ÁªìÊûÑ‰∏äÊòØ‰∏çÂèØÊé•ÂèóÁöÑ„ÄÇ
  - Âõõ„ÄÅ‰∏∫‰ªÄ‰πà‚Äú‰∏ªËÑë‚ÄùÂøÖÈ°ªÊòØÊú¨Âú∞„ÄÅÁßÅÊúâ„ÄÅÊåÅÁª≠Â≠òÂú®ÁöÑ
  - ‚Ä¢Áâ©ÁêÜ‰∏é‰∏ªÊùÉ‰∏äÈÉΩÂ±û‰∫é‰∏™‰∫∫
  - ‰∫î„ÄÅËøô‰∏çÊòØÊäÄÊúØÁêÜÊÉ≥‰∏ª‰πâÔºåËÄåÊòØÂéÜÂè≤ËßÑÂæãÁöÑÈáçÊºî
  - ÂõûÁúãÂéÜÂè≤ÔºåÊØè‰∏ÄÊ¨°ÂÖ≥ÈîÆËÉΩÂäõÈÉΩ‰ºöÁªèÂéÜÂêåÊ†∑ÁöÑË∑ØÂæÑÔºö
  - ‚Ä¢ËÆ°ÁÆóÔºö‰ªéÂ§ßÂûãÊú∫ ‚Üí PC ‚Üí ‰∏™‰∫∫ËÆæÂ§á
  - ‚Ä¢Â≠òÂÇ®Ôºö‰ªé‰∏≠ÂøÉÊúçÂä°Âô® ‚Üí Êú¨Âú∞Á°¨Áõò ‚Üí ÁßÅÊúâ NAS
  - ‚Ä¢ÈÄö‰ø°Ôºö‰ªéÂõΩÂÆ∂Á≥ªÁªü ‚Üí ËøêËê•ÂïÜ ‚Üí ‰∏™‰∫∫ÁªàÁ´Ø
  - Âá°ÊòØ‰∏é‚ÄúËá™ÊàëËøûÁª≠ÊÄß‚ÄùÂº∫Áõ∏ÂÖ≥ÁöÑËÉΩÂäõÔºåÊúÄÁªàÈÉΩ‰ºö‰∏ãÊ≤âÂà∞‰∏™‰Ωì„ÄÇ
  - AGI Â¶ÇÊûúÈïøÊúüÂè™Â≠òÂú®‰∫é‰∫ëÁ´ØÔºåÈÇ£ÂÆÉÊ≥®ÂÆöÂè™ÊòØ‚ÄúÈ´òÁ∫ßËá™Âä®Âåñ‚ÄùÔºåËÄå‰∏ç‰ºöÊàê‰∏∫‚Äú‰∏™‰ΩìÊô∫ËÉΩÂª∂Â±ï‚Äù„ÄÇ
  - ÂÖ≠„ÄÅÁúüÊ≠£ÁöÑÂàÜÁïåÁ∫øÔºåÂÖ∂ÂÆûÂ∑≤ÁªèÂá∫Áé∞‰∫Ü
  - Êú™Êù•‰∫∫‰∏é AI ÁöÑÂ∑ÆÂºÇÔºå‰∏çÂú®‰∫é‚ÄúÁî®‰∏çÁî® AI‚ÄùÔºåËÄåÂú®‰∫éÔºö ‚Ä¢Êúâ‰∫õ‰∫∫‰ΩøÁî®ÁöÑÊòØÈöèÊó∂ÂèØË¢´ÊõøÊç¢„ÄÅË¢´ÂçáÁ∫ß„ÄÅË¢´ÈáçÁΩÆÁöÑÊô∫ËÉΩÊúçÂä° ‚Ä¢Êúâ‰∫õ‰∫∫Êã•ÊúâÁöÑÊòØ‰∏Ä‰∏™ÈïøÊúüÂ≠òÂú®„ÄÅ‰∏éËá™Ë∫´ËÆ∞ÂøÜÈó≠ÁéØÁªëÂÆöÁöÑÊô∫ËÉΩÊ†∏ÂøÉ
  - ÂâçËÄÖÊòØÁî®Êà∑ÔºåÂêéËÄÖÊòØÂÖ±ÁîüËÄÖ„ÄÇËøôÊù°ÂàÜÁïåÁ∫ø‰∏ÄÊó¶ÂΩ¢ÊàêÔºåÂ∞ÜÊØî‚ÄúÊòØÂê¶ËÅîÁΩë‚Äù‚ÄúÊòØÂê¶‰ªòË¥π‚ÄùÊõ¥Ê∑±Âàª„ÄÇ
  - ‰∏É„ÄÅÁªìËØ≠ÔºöAGI ÁßÅÊúâÂåñÔºå‰∏çÊòØÈÄâÊã©ÔºåËÄåÊòØÂøÖÁÑ∂
  - ËÆ∞ÂøÜ‰∏ªÊùÉ‰∏çÂèØÂ§ñÂåÖÔºå‰∫∫Ê†ºÁîüÊàê‰∏çÂèØÊâòÁÆ°„ÄÇ
  - ÊâÄ‰ª•ÔºåAGI ÁöÑÊú™Êù•ÂΩ¢ÊÄÅÔºå‰∏çÊòØ‚ÄúÊõ¥Â§ßÁöÑ‰∫ë‚ÄùÔºå ËÄåÊòØÔºö ÁßÅÊúâ‰∏ªËÑë √ó Â§ñÈÉ®ËÉΩÂäõ √ó Âçè‰ΩúÂºèÊô∫ËÉΩÁΩëÁªú„ÄÇ

- ‰∏çÊòØÔºåËÄåÊòØ„ÄÇÊàëÁé∞Âú®ÁúãÂà∞Ëøô‰∏™ÔºåÁ¨¨‰∏ÄÂèçÂ∫îÂ∞±ÊòØËøôÊÆµËØùÊòØAIÁîüÊàêÁöÑÔºåÊàñËÄÖËá≥Â∞ëÊòØ‰∫∫ÂíåAIËÅäÂá∫Êù•ÁöÑÂÜÖÂÆπ„ÄÇ

- ## üí° ‰∏ãÂçàÁªô‰∫∫ÂÅö‰∫Ü‰∏Ä‰∏™Â∞èÂ∞èÁöÑÂ±ïÁ§∫„ÄÇ‰∏Ä‰∏™ChatbotÔºåÂÖ∂ÂÆû‰πüÂ∞±‰∏âÂõõÁôæÂ≠óÁöÑÊèêÁ§∫ËØçÔºåÂÆö‰πâ‰∫Ü‰∏ÉÂÖ´‰∏™Â∑•ÂÖ∑„ÄÇ
- https://x.com/wwwgoubuli/status/2001256235296022575
  - ÊºîÁ§∫ÁöÑÊó∂ÂÄôÔºåÊàëËÆ©ÂØπÈù¢ÊîæÂºÄÂ∞ùËØïÔºå‰πüÊèêÂâçÂ£∞Êòé‰∫ÜÁõÆÂâçËøòÊòØDemoÈò∂ÊÆµÔºå‰ºöÊúâ‰∫õÈóÆÈ¢ò„ÄÇ‰ΩÜÂØπÊñπ‰ªçÁÑ∂Ë¢´ÊàëËøô‰∏™ÊºîÁ§∫‰∏≠‰ΩìÁé∞ÁöÑËá™‰∏ªÂØªÊâæÂ∑•ÂÖ∑Ëß£ÂÜ≥ÈóÆÈ¢òÁöÑË°å‰∏∫ÈúáÊÉä‰∫Ü„ÄÇ
  - ‰ª•‰∏∫ÊàëÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ÂæàÂ§çÊùÇÁöÑÂ∑•‰ΩúÊµÅÔºåÊàëËØ¥Ê≤°ÊúâÂ∑•‰ΩúÊµÅÔºåÂÖ∂ÂÆûÂ∞±Âè™Êúâ‰∏ÄÂ±Ç„ÄÇ
  - ÂØπÊñπÁôæÊÄù‰∏çÂæóÂÖ∂Ëß£„ÄÇÊâÄ‰ª•ËØ¥ÔºåËÆ§Áü•Â∑ÆËÇØÂÆöÊòØÁúüÂÆûÂ≠òÂú®ÁöÑ„ÄÇ
  - ËøôÊòØ‰∏Ä‰∏™ÂûÇÁõ¥È¢ÜÂüüÁöÑAgent„ÄÇÂéüÂÖàÂÆ¢Êà∑ÁöÑÈóÆÈ¢òÊòØÔºåÂú®‰ªñ‰ª¨‰πãÂâçÊâÄÂÅöÁöÑAIÁöÑÂ∞ùËØï‰∏≠ÔºåÊúâ‰∏ÄÈÉ®ÂàÜÂ∑≤ÁªèÂèëÊå•ÂæóÂæà‰∏çÈîô‰∫Ü„ÄÇ‰ΩÜÊòØÂΩìÊúâÁöÑÊó∂ÂÄô‰∏çÂæó‰∏çÂ§ÑÁêÜË∂ÖÈïøÁöÑÊï∞ÊçÆÁöÑÊó∂ÂÄôÔºå‰∏¢ÁªôLLMÊÄªÊòØÊúâÂêÑÁßçÈîôËØØÂá∫Áé∞„ÄÇ
  - ÊàëÊâæ‰ªñ‰ª¨ÁöÑÁ†îÂèëÈóÆËøáÔºåÊèêÂèñ‰∫ÜÂá†‰∏™Âú®‰ΩøÁî®AI‰πãÂâçÁöÑÔºå‰ªñ‰ª¨Â§ÑÁêÜÁöÑÂ∑•ÂÖ∑„ÄÇÁÑ∂ÂêéÂ∞ÜËøô‰∫õÂ∑•ÂÖ∑ÁöÑÊèèËø∞„ÄÅ‰ΩúÁî®„ÄÅÈôêÂÆöËåÉÂõ¥Á≠â‰∫§Áªô‰∫ÜLLMÔºå Âπ∂ÂëäËØâÊ®°Âûã‰∏çË¶ÅËá™Â∑±ËØïÂõæËß£ÂÜ≥ÈóÆÈ¢òÔºåËÄåÊòØË¶ÅÁî®Â∑•ÂÖ∑Ëß£ÂÜ≥ÈóÆÈ¢ò„ÄÇ Ê≤°‰∫Ü

- ‰∫ãÂÆû‰∏äÂæàÂ§öÂÖ¨Âè∏ÊâÄÂÅöÁöÑagentÊàñËÄÖchatbotËøûpromptÁöÑ‰∏äÈôêÈÉΩÊ≤°ÊúâËææÂà∞ÔºåÂ∞±ÂºÄÂßãËÄÉËôëÊêûsftÔºåÊêûRLHFÔºå‰∏Ä‰∏™Â∞èÂ∞èÁöÑÈúÄÊ±Ç‰∏∫‰∫ÜÊâÄË∞ìÁöÑ‰ºòÂåñÊåáÊ†áÊêûÁöÑËáÉËÇø‰∏çÂ†™

- ## üìå LLM Âá∫Êù•‰πãÂêéÔºåÂú®Â∫îÁî®Â±ÇÁöÑÊäòËÖæ‰ªéÊú™ÂÅúÊ≠á„ÄÇ‰ªé Prompt Ë∞É‰ºòÂà∞ Workflow ÈÖçÁΩÆÔºåÂÜçÂà∞ Agent ÊûÑÂª∫ÔºåÊúÄÁªàÁõÆÁöÑÈÉΩÊòØ‰∏ÄÊ†∑ÁöÑÔºöËÆ© LLM Êõ¥Â•ΩÂú∞‰∏∫‰∫∫Á±ªÂπ≤Ê¥ªÔºåÊääÊú∫Âô®ÁöÑÊÄßËÉΩÂéãÊ¶®Âà∞ÊûÅËá¥„ÄÇ
- https://x.com/Barret_China/status/1973188130091180466
- ÂØπ LLM ÁöÑÂéãÊ¶®ÔºåÂèØ‰ª•ÂàÜ‰∏∫‰∏§‰∏™Áª¥Â∫¶„ÄÇ
- ‰∏ÄÊòØÂ∏ÆÂä©ÂÆÉÊâæÂà∞ÊúÄ‰ºòÁÆóÊ≥ïÔºåËÆ©Êé®ÁêÜÂ∞ëËµ∞ÂºØË∑Ø„ÄÇ
  - ‰∏∫Ê≠§Êàë‰ª¨Âá†‰πéÊääËÉΩÊÉ≥Âà∞ÁöÑË∑ØÂ≠êÈÉΩËµ∞‰∫Ü‰∏ÄÈÅçÔºåËÆ© LLM Â≠¶‰ºöÂèçÊÄùÔºàreflection„ÄÅself-consistency„ÄÅself-criticsÔºâÔºåÂ≠¶‰ºöÊé®ÁêÜÂíåËßÑÂàíÔºàreasoning„ÄÅplanning„ÄÅchain-of-thought„ÄÅtree-of-thoughtÔºâÔºõÂ≠¶‰ºöËÆ∞ÂøÜÔºàshort-term memory„ÄÅlong-term memoryÔºâÔºå‰∏çËá≥‰∫éÂØπËØù‰∏ÄÈïøÂ∞±Â§±ÂøÜÔºõÂ≠¶‰ºöÊâæÁü•ËØÜÔºàRAG„ÄÅknowledge graphÔºâÔºåÂú®Â§ñÈÉ®‰∏ñÁïåÈáåË°•ÂÖÖ‰∫ãÂÆûÔºõÂ≠¶‰ºöÊûÑÂª∫‰∏ä‰∏ãÊñáÔºàcontext buildingÔºâÔºåÂú®ÊúâÈôê token ÈáåÂ°û‰∏ãÊõ¥Â§öÊúâÊïà‰ø°ÊÅØÔºõÂ≠¶‰ºöÁî®Â∑•ÂÖ∑Ôºàtool-useÔºåfunction callingÔºåMCPÔºâÔºåÊää‰∫ãÊÉÖ‰∫§ÁªôÂ§ñÈÉ®Á®ãÂ∫èÂéªË∑ëÔºåËÄå‰∏çÊòØÂÖâÈù†Ëá™Â∑±ÁîüÊàêÔºõÁ≠âÁ≠â„ÄÇ
  - Ëøô‰∫õ‰∏úË•øÔºåËØ¥Âà∞Â∫ïÈÉΩÊòØÊäÄÂ∑ßÂíåÊú∫Âà∂ÔºåÊú¨Ë¥®ÁõÆÁöÑÊòØËÆ© LLM Êõ¥Âø´ÁêÜËß£‰∫∫Á±ªË¶ÅÂπ≤Âï•ÔºåÂõ¥ÁªïÁõÆÊ†áÔºàgoal-orientedÔºâÂ∞ΩÂèØËÉΩÊâæÂà∞‰∏ÄÊù°‰ª£‰ª∑ÊúÄÂ∞èÁöÑË∑ØÔºåË∑ëÂà∞ÊúÄ‰ºòËß£‰∏äÂéª„ÄÇ
- Á¨¨‰∫å‰∏™Áª¥Â∫¶ÔºåÊòØÂØπÊó∂Èó¥ÁöÑÂéãÊ¶®ÔºåËÆ© LLM ÂèØ‰ª•ÂÅöÂà∞ 7√ó24 Â∞èÊó∂‰∏çÂÅúÊ≠á„ÄÇ
  - ÂΩìÊàë‰ª¨ÂØπ LLM Êúâ‰∫ÜÊõ¥Ê∑±ÂÖ•ÁöÑÁêÜËß£‰πãÂêéÔºåÂæàÂÆπÊòìÊÉ≥Âà∞ÊääÂÆÉÊâìÈÄ†ÊàêÂ±û‰∫éËá™Â∑±ÊàñÁªÑÁªáÁöÑ‚ÄúÊï∞Â≠óÂëòÂ∑•‚ÄùÔºåÂÆÉ‰∏çÁü•Áñ≤ÊÉ´„ÄÅ‰∏ç‰ºöÊä±ÊÄ®ÔºåÂèØ‰ª•ÊåÅÁª≠ËøêËΩ¨„ÄÅ‰∏çÊñ≠Â≠¶‰π†„ÄÇ
- Â§ßÈÉ®ÂàÜ‰∫∫‰ªäÂ§©Áî® AI ÁöÑÊñπÂºèÔºåËøòÂÅúÁïôÂú®Êü•ËµÑÊñô„ÄÅÊÄªÁªìÂÜÖÂÆπ„ÄÅÂÜôÂë®Êä•ÊúàÊä•Ëøô‰∫õÂçïÁÇπÂú∫ÊôØ‰∏äÔºåÂ¶ÇÊûúË¶ÅÁúüÊ≠£ÊûÑÂª∫‰∏ÄÂêç‚Äú‰∏çÂÅúÊ≠áÁöÑ AI Êï∞Â≠óÂëòÂ∑•‚ÄùÔºåÂÖâÈù†Ëøô‰∫õËøò‰∏çÂ§ü„ÄÇÊàë‰ª¨ÈúÄË¶ÅÂÖàËßÑÂàíÂá∫Â±û‰∫éËá™Â∑±ÁöÑ AI Êï∞Â≠óÂ∑•ÂéÇ ‚Äî‚ÄîÊÉ≥Ê∏ÖÊ•öË¶ÅÈÄ†Âá∫Êù•ÁöÑ‚Äú‰∫ßÂìÅ‚ÄùÊòØ‰ªÄ‰πàÔºåÊòØÊ≤âÊ∑ÄÁü•ËØÜÁöÑÁ≥ªÁªüÔºåÊòØËá™Âä®ÂåñÁöÑ‰∏öÂä°ÊµÅÁ®ãÔºåËøòÊòØ‰∏Ä‰∏™ÂèØ‰ª•ÈïøÊúüËø≠‰ª£ÁöÑÊúçÂä°„ÄÇ
  - Âú®ËøôÂ∫ßÂ∑•ÂéÇÈáåÔºåAI ÊòØÁîü‰∫ßÁ∫ø‰∏äÁöÑÊâßË°åËÄÖÔºåÂÆÉË¥üË¥£ÂÖ∑‰ΩìÁöÑÂä†Â∑•‰∏é‰∫ßÂá∫ÔºõËÄå‰∫∫Á±ªÁöÑËßíËâ≤ÂèëÁîü‰∫ÜËΩ¨ÂèòÔºå‰ªé‚Äú‰∫≤Ëá™Âπ≤Ê¥ªÁöÑÂ∑•‰∫∫‚ÄùÂèòÊàê‚ÄúÁõëÂ∑•‰∏éÁÆ°ÁêÜËÄÖ‚Äù„ÄÇ ‰∫∫Á±ª‰∏çÂÜç‰∫≤ÊâãÂÆåÊàêÊØè‰∏ÄÊ≠•ÔºåËÄåÊòØË¶ÅËÆæËÆ°ÊµÅÊ∞¥Á∫øÔºåËÆæÂÆöËßÑÂàôÔºåÂà∂ÂÆöÊåáÊ†áÔºåÁõëÊéßË¥®ÈáèÔºåÂπ∂Âú®ÈúÄË¶ÅÊó∂Ë∞ÉÂ∫¶ËµÑÊ∫ê„ÄÇÊç¢Âè•ËØùËØ¥ÔºåAI ÁöÑ‰ª∑ÂÄº‰∏çÂú®‰∫éÊõøÊàë‰ª¨‚ÄúÂπ≤‰∏ÄÁÇπÊ¥ª‚ÄùÔºåËÄåÂú®‰∫éÂ∏ÆÊääÊï¥Êù°ÊµÅÊ∞¥Á∫øË∑ëËµ∑Êù•ÔºåËÄå‰∫∫Á±ªÊõ¥ÂÉèÊòØ‚ÄúÊï∞Â≠óÂ∑•ÂéÇÁöÑÁÆ°ÁêÜËÄÖ‚Äù„ÄÇ
  - ÂΩìËøô‰∏§‰∏™Áª¥Â∫¶ÁªìÂêàËµ∑Êù•Êó∂ÔºåÁúüÊ≠£ÁöÑÊãêÁÇπÂ∞±Âá∫Áé∞‰∫Ü„ÄÇLLM ‰∏çÂÜçÂè™ÊòØ‰∏Ä‰∏™ÂÜ∑ÂÜ∞ÂÜ∞ÁöÑÂ∑•ÂÖ∑ÔºåËÄåÊòØÈÄêÊ∏êÂèòÊàê‰∫ÜÂèØ‰ª•ÈïøÊúüÂçè‰ΩúÁöÑ‰ºô‰º¥„ÄÇÂÆÉÊó¢ËÉΩÊâøÊãÖÈáçÂ§çÊÄßÂä≥Âä®Ôºå‰πüËÉΩÂú®Â§çÊùÇÈóÆÈ¢ò‰∏äÊèê‰æõÊ¥ûËßÅ„ÄÇÂÆÉ‰∏ç‰ªÖ‰ªÖÊòØ‚ÄúÂ∏Æ‰Ω†ÂÅö‰∫ã‚ÄùÔºåÊõ¥ÊòØ‚ÄúÂíå‰Ω†‰∏ÄËµ∑ÂÅö‰∫ã‚Äù„ÄÇ
  - Êú™Êù•ÁöÑÂ∑ÆË∑ùÔºå‰∏çÂú®‰∫éË∞ÅËÉΩÂÜôÂá∫Êõ¥ÊºÇ‰∫ÆÁöÑ PromptÔºåËÄåÂú®‰∫éË∞ÅËÉΩÊää LLM ÁúüÊ≠£ËûçÂÖ•Âà∞Ëá™Â∑±ÁöÑÊó∂Èó¥ÂíåÁªÑÁªáÈáåÔºåÂΩ¢ÊàêÁ®≥ÂÆöÁöÑÁîü‰∫ßÊñπÂºè„ÄÇ
  - Âõ†Ê≠§Ôºå‰ºö‰∏ç‰ºöÁî®„ÄÅÁî®Âà∞‰ªÄ‰πàÊ∑±Â∫¶„ÄÅËÉΩÂê¶ÊåÅÁª≠‰ºòÂåñÔºåËøô‰∫õÊâçÊòØÈïøÊúüÁöÑÁ´û‰∫âÂäõÊù•Ê∫ê„ÄÇË∞ÅËÉΩÊää AI ËøêË°åÊàê‚ÄúÂ∑•ÂéÇ‚ÄùÔºåËÆ©Ëá™Â∑±‰ªéÊâßË°åËÄÖËΩ¨‰∏∫ÁõëÂ∑•ÂíåÁÆ°ÁêÜËÄÖÔºåË∞ÅÂ∞±ËÉΩÂú®Êú™Êù•ÁöÑÊó•Â∏∏Â∑•‰ΩúÂíå‰∏öÂä°‰∏≠ÔºåËé∑ÂæóÁúüÊ≠£ÂèØÂ§çÁî®„ÄÅÂèØÁ¥ØÁßØÁöÑ‰ºòÂäø„ÄÇ

- ‰ª£Á†ÅÊäΩË±°ÁúüÂÆû‰∏ñÁïåÁöÑÊó∂‰ª£Âø´Ë¶ÅÁªìÊùü‰∫ÜÔºåÊ¨¢ËøéÊù•Âà∞token ÊäΩË±°ÁúüÂÆû‰∏ñÁïåÁöÑÊó∂‰ª£„ÄÇ

- Â¶ÇÊûúTOKENÊäïÂÖ•‰∫ßÂá∫ÊòØÊ≠£ÂêëÁöÑÔºåÊÅ®‰∏çÂæó‰ªñ‰∏ç‰ºëÊÅØ 

- ÈóÆÈ¢òÊù•‰∫ÜÔºåÊàëË¶ÅÂÅö‰ªÄ‰πàÔºåAIË¶ÅÊÄé‰πàÂ∏ÆÊàëÂÅö

- ## üì± [Á´Ø‰æßÊ®°Âûã‰ºöÊòØ AI ÊäÄÊúØÊºîËøõÁöÑ‰∏ã‰∏Ä‰∏™ „ÄåÂøÖ‰∫â‰πãÂú∞„Äç ÂêóÔºüÂΩìÂâçËêΩÂú∞Èù¢‰∏¥Âì™‰∫õÊ†∏ÂøÉÁì∂È¢àÔºü - Áü•‰πé](https://www.zhihu.com/question/1914319403023032351)
- TO CÂú∫ÊôØÂ∫îÁî®ÊúÄÂ§ßÁâπÁÇπÊòØÁ°¨‰ª∂ÂèÇÂ∑Æ‰∏çÈΩê„ÄÇÊÄßËÉΩÂ∑ÆËÉΩÂ§ßÂà∞10Â§öÂÄç‰ª•‰∏ä„ÄÇ
  - ÊäÄÊúØÊ†àÂøÖÈ°ªÊª°Ë∂≥ÊâÄÊúâ‰∏ªÊµÅÁ°¨‰ª∂ÁªìÊûÑÂ∑ÆÂºÇÂíåÊÄßËÉΩÊÄßËÉΩ‰∏ãÔºå‰øùÊåÅÁõ∏ÂØπ‰∏ÄËá¥ÁöÑ‰ΩøÁî®‰ΩìÈ™å„ÄÇËøôÊòØ‰∏Ä‰ª∂ÊûÅÁ¥ØÁöÑÊ¥ªÂÑø„ÄÇ
  - ÊúçÂä°Á´Ø‰∏ã‰∏™ÂºÄÊ∫êÊ®°ÂûãÊê≠‰∏™WEBÊúçÂä°ÂÜô‰∏™HTMLÂ∞±ËÉΩÂçñÊúçÂä°‰∫Ü„ÄÇÁ´Ø‰æßÊÉ≥ÂÜÖÂµåAIÊ®°Âûã‰∫ßÂìÅÂåñÔºåÂêåÊó∂ËøòË¶ÅËß£ÂÜ≥ÂÆûÊó∂ÊÄßÔºåË¶ÅËß£ÂÜ≥ÁöÑÂ∑•Á®ãÈóÆÈ¢òË¶ÅÂ§ö10ÂÄç‰∏çÊ≠¢„ÄÇË¶ÅËá™Â∑±‰ºòÂåñÊ®°Âûã„ÄÇÁîöËá≥Ë¶ÅËá™Â∑±Êê≠Âª∫Ê®°ÂûãÁªìÊûÑÔºåËá™Â∑±ÂáÜÂ§áÊï∞ÊçÆËÆ≠ÁªÉ„ÄÇ
- ÁõÆÂâçÁúãÁ´Ø‰æßÂÆåÂÖ®Ë∞à‰∏ç‰∏äAIÊºîËøõÂøÖ‰∫â‰πãÂú∞„ÄÇ‰∏ÄÊù•Á°¨‰ª∂ËæÉÂº±‰∏îÂèÇÂ∑Æ‰∏çÈΩêÔºåÂØºËá¥TO CÂ∑•Á®ã‰øùÈöú‰ΩìÈ™åÊûÅÂ∫¶Âõ∞ÈöæÂíåÂ§çÊùÇ„ÄÇ‰∫åÊù•PCÈ´òÁÆóÂäõÁ°¨‰ª∂ÂÆåÂÖ®‰∏çÊòØÂ§ß‰ºóÊ∂àË¥πÁ∫ßÁöÑÂîÆ‰ª∑ÂíåÂÆö‰ΩçÔºåÊúÄÂêéÔºåËøòÊ≤°Êúâ‰ªÄ‰πàÁ´Ø‰æßAIËêΩÂú∞‰∫ßÂìÅÁöÑÂ∫îÁî®ËåÉÂºè„ÄÇ
  - ‰ªéÂ∫îÁî®‰ΩìÈ™å‰∏äÔºåÈÇ£‰∫õÂÖÅËÆ∏Âª∂Ëøü500ÊØ´Áßí‰ª•‰∏äÔºåÂØπÂ∏¶ÂÆΩÈúÄÊ±Ç‰∏çÂ§ßÁöÑÂ∫îÁî®ÔºåÈÉΩÂèØ‰ª•ÊîæÂà∞‰∫ëÁ´Ø„ÄÇ
- ÁúüÈúÄË¶ÅÂÅöÂà∞Á´Ø‰æßÁöÑÔºåÂÖ∂ÂÆûÊòØÈÇ£‰∫õÈúÄË¶ÅÂç≥Êó∂ÂìçÂ∫îÁöÑÂ∫îÁî®Âú∫ÊôØ„ÄÇ
  - ‰∏∫Êï∞‰∏çÂ§öÁöÑÂ∫îÁî®Á±ªÂà´ÔºåÊØîÂ¶ÇÂç≥Êó∂‰∫§‰∫íÂΩ±ÂÉèÁõ∏ÂÖ≥ÔºåËØ≠Èü≥Áõ∏ÂÖ≥ÁöÑÔºåËØ∏Â¶ÇÊ∏∏ÊàèÔºåÈúÄË¶ÅÂç≥Êó∂‰∫§‰∫íÁöÑÊï∞Â≠ó‰∫∫Ôºå3DËôöÊãüAI‰∫∫ÔºåËôöÊãü‰∫∫Áõ¥Êí≠Á≠âÔºåÂèØËÉΩ‰ºöÂØπÁ´Ø‰æßAIÊúâÈÉ®ÂàÜÈúÄÊ±Ç
- ËøôÁ±ªÂ∫îÁî®Êúâ3‰∏™ÁâπÂæÅÔºö 
  - 1 Âç≥Êó∂ÊÄßÁõ¥Êé•ÂΩ±Âìç‰ΩìÈ™åÔºå‰∏çËÉΩÁ≠â„ÄÇ 
  - 2 ÂØπÁÆóÂäõË¶ÅÊ±ÇÁõ∏ÂØπ‰ΩéÔºåËøêÁÆóÊú¨Âú∞ÁÆóÂäõËÉΩÂ§üÊª°Ë∂≥ÔºåÂπ∂‰∏ç‰∏ÄÂÆöÈùûË¶Å‰∫ëÁÆóÂäõÊîØÊåÅ„ÄÇ 
  - 3 ÂØπÂ∏¶ÂÆΩË¶ÅÊ±ÇËæÉÂ§ßÔºå‰∫ëÊúçÂä°ÁöÑÊàêÊú¨ËøáÈ´ò‰ΩøÂæóÂïÜ‰∏öÊ®°Âºè‰∏çÊàêÁ´ãÔºåÊâÄ‰ª•ÈúÄË¶ÅÊîæÂÖ•Êú¨Âú∞„ÄÇ
- MOBILEÁ¶ªÁ´Ø‰æßÂèØËÉΩÊõ¥Ëøú„ÄÇ Áî±‰∫éÁÆóÂäõÊûÅ‰∏∫‰Ωé‰∏ãÔºåÂè™ÊúâÂæàÂ∞ëÁöÑÊÉÖÂÜµÊâçÈúÄË¶ÅÊâãÊú∫‰∏äÂç≥Êó∂ÂìçÂ∫îAIÊé®ÁêÜÁªôÂá∫ÁöÑÁªìÊûú„ÄÇÂç≥ÈÇ£‰∫õÊääÊâãÊú∫‰Ωú‰∏∫‰ø°Âè∑‰º†ÊÑüÂô®ÔºåÂç≥Êó∂ÂØπ‰ø°Âè∑ËøêÁÆóÁöÑÂú∫ÂêàÔºöÊØîÂ¶ÇË°®ÊÉÖÊçïÊçâ„ÄÅÊâãÂäø„ÄÅËÇ¢‰ΩìÂä®‰ΩúÊçïÊçâÂíåËØÜÂà´ÔºåËØ≠Èü≥„ÄÅËøêÂä®Êï∞ÊçÆÂ§ÑÁêÜÂíåÁîüÁâ©‰ø°Âè∑Â§ÑÁêÜÁ≠â„ÄÇ 
  - Â¶ÇÊûúÊâãÊú∫Ê≤°Êúâ‰∏ìÁî®Á•ûÁªè/Âº†ÈáèËäØÁâáÔºåGPUËøòÈúÄË¶ÅÊâøÊãÖÊ∏≤Êüì3DÂõæÂΩ¢ÔºåÈÇ£‰πàËÉΩË∑ëÁöÑAIÊ®°Âûã‰ºöÊõ¥Âä†ÂèóÈôê„ÄÇ
  - ÁõÆÂâçÊµãËØïÔºåÊüêÊâãÊú∫Áî®NPUË∑ëÊ®°ÂûãÊØî‰∏çÁî®ÊúÄÂ§öËÉΩÂø´10ÂÄç„ÄÇ 
  - ÊâãÊú∫ËäØÁâáÂïÜÊòØÂê¶ÊúâÂä®ÂäõÂèëÂ±ïÔºåË¶ÅÁúãÊâãÊú∫‰Ωú‰∏∫Êï∞ÊçÆ‰º†ÊÑüÂô®Âç≥Êó∂Â§ÑÁêÜÊï∞ÊçÆËÉΩÂ∏¶Êù•Â§öÂ§ßÁöÑÂ∫îÁî®Â∏ÇÂú∫„ÄÇ

- Êú¨Âú∞ÈÉ®ÁΩ≤Ê®°ÂûãÁöÑ‰ºòÂäøÂú®‰∫é‰ΩéÂª∂ËøüÔºåËøôÂú®ÊàëÁúãÊù•ÂÖ∂ÂÆû‰πüÊòØ‰∏™Áõ∏ÂØπÂÅè‰º™ÁöÑ‰ºòÂäø„ÄÇ
  - ‰∫ãÂÆû‰∏äÔºåÁé∞‰ª£ÁΩëÁªúÁéØÂ¢ÉÂÆûÈôÖ‰∏ä‰ª•ÂèäË∂≥Â§üÂø´ÈÄüÂíåÁ®≥ÂÆöÔºåÊó†ËÆ∫ÊòØÊµÅÈáèËøòÊòØWiFiÔºåËæÖ‰ª•CDNËæπÁºòËäÇÁÇπ‰ºòÂåñÂêéÔºåÁªùÂ§ßÂ§öÊï∞‰∏ªÊµÅAIÂ∫îÁî®ÁöÑ‰∫ëÁ´ØÂìçÂ∫îÈÉΩËÉΩÁ®≥ÂÆöÂú®Âá†ÂçÅÊØ´Áßí‰ª•ÂÜÖ„ÄÇ
- Á´Ø‰æßËÆæÂ§áÂèóÈôê‰∫éÂäüËÄó„ÄÅÁÉ≠Èáè„ÄÅÁÆóÂäõÁ≠âÁâ©ÁêÜÊù°‰ª∂ÔºåÂæÄÂæÄÂè™ËÉΩÈÉ®ÁΩ≤ËΩªÈáèÂåñÊ®°Âûã

- ÁßªÂä®ËÆæÂ§á‰∏äÈïøÊúüËøêË°å‰∏Ä‰∏™ÊúâÁ´û‰∫âÂäõÁöÑÂ§ßÊ®°Âûã‰ªçÁÑ∂‰∏çÁé∞ÂÆûÔºåË¥üËΩΩ„ÄÅËµÑÊ∫êÂç†Áî®„ÄÅÁîµÈáèÊ∂àËÄó„ÄÅÂèëÁÉ≠Á≠âÁ≠âÈóÆÈ¢òÔºåÂæàÈöæÂÅöÂà∞Â•ΩÁöÑ‰ΩìÈ™åÔºåÊõ¥‰∏çË¶ÅËØ¥Áâ©ËÅîÁΩëËÆæÂ§á„ÄÅÊô∫ËÉΩÈü≥ÁÆ±Á≠â‰ΩéÂäüËÄó‰∫ßÂìÅ‰∫Ü„ÄÇ
  - ÂæàÂ§öÂ£∞Áß∞Ëá™Â∑±Áî®‰∫ÜÁ´Ø‰æßÂ§ßÊ®°ÂûãÁöÑÔºåÂÆûÈôÖ‰∏ä‰ªçÁÑ∂Á¶ª‰∏çÂºÄ‰∫ëÁ´ØÁöÑÂçèÂêåÈÖçÂêàÔºåÊàñËÄÖËØ¥‰∏ªË¶ÅÈù†‰∫ëÁ´ØÔºåÂ§áÁî®ÊñπÊ°àÂèØËÉΩÊòØÁ´Ø‰æßÔºå‰ΩÜÊääÁ´Ø‰æßÊãøÂá∫Êù•Â§ßËÇÜÂÆ£Êâ¨ÔºåÊù•ÂÅöÂπøÂëäËÄåÂ∑≤„ÄÇ

- üêõ Á´Ø‰æßÊ®°ÂûãÁöÑÂä£Âäø
  - Á´Ø‰æßÊ®°Âûã‰ªçÁÑ∂ÂèóÈôê‰∫éËÆæÂ§áÊú¨Ë∫´ÁöÑÂÜÖÂ≠ò„ÄÅÂäüËÄóÂíåÁÆóÂäõ
  - ‰∫ëÁ´ØÊ®°ÂûãÂèØ‰ª•ÈöèÊó∂ÂçáÁ∫ßÔºåÁÉ≠Êõ¥Êñ∞Êú∫Âà∂‰øùËØÅÊâÄÊúâÁî®Êà∑Á¨¨‰∏ÄÊó∂Èó¥‰∫´ÂèóÊñ∞‰∏Ä‰ª£Ê®°Âûã
  - Á´Ø‰æßÊ®°ÂûãÂØπ‰∫é‰∏çÂêåËÆæÂ§áÂíåÁ°¨‰ª∂Êû∂ÊûÑÈúÄË¶ÅÂàÜÂà´ÈÄÇÈÖç

- Á´Ø‰æßÈÄÇÂêàÁöÑÂú∫ÊôØËøòÊòØÈöêÁßÅ„ÄÅÊûÅ‰ΩéÂª∂ËøüÔºåÊàñËÄÖÂÖ®Â§©ÂÄô„ÄÇ

- Â¶ÇÊûúÂÆåÂÖ®Ë∑ëÂú®Á´Ø‰æßÔºåÈÇ£ÂïÜ‰∏öÊ®°ÂºèÊÄé‰πàÂÅöÔºüÁé∞Âú®ÁöÑ‰∏ªÊµÅÊòØËÆ¢ÈòÖÂà∂Ôºå‰ΩÜÂÆåÂÖ®Êú¨Âú∞ÁöÑËÆ¢ÈòÖÂà∂ÔºåÂê∏ÂºïÂäõÊØîËæÉ‰ΩéÔºåÁ†¥Ëß£È£éÈô©ÊØîËæÉÈ´òÂêß

- ## deepseek 3fs ÂÖ∂ÂÆû‰∏çÊòØÂæàÁêÜËß£ËøôÊ†∑ÁöÑÊÑè‰πâÊòØÂï•‚Ä¶ Â∑≤ÁªèËÑ±Á¶ª FS ÁöÑÈÄöÁî®Êé•Âè£‰∫ÜÔºå‰∏∫Âï•Ë¶ÅÁ°¨ÊåÇ‰∏Ä‰∏™ FUSE VFS Â±Ç‚Ä¶ Áõ¥Êé•ÊëíÂºÉ VFS Ëµ∞‰∏™ÁßÅÊúâÁöÑ protocol ‰∏çÂπ≤ÂáÄÂ§ö‰∫Ü‚Ä¶
- https://x.com/silsrc/status/1895390926098571505
- > Most applications use FUSE client, which has a low adoption barrier. Performance-critical applications are integrated with the native client.

- ÊàëÁöÑÁ¨¨‰∏ÄÂèçÂ∫îÊòØÁé∞ÊúâÊ°ÜÊû∂Â∫îËØ•Ê≤°ÂäûÊ≥ïÂ§ßÊâπÈáèÊîπÁî®ÁßÅÊúâÁöÑ APIÔºåÊØîÂ¶ÇÂú® Python ÊàëÂ∞±ÊòØË¶ÅÁî® pathlib. Path ÂØπÊñá‰ª∂ÂÅöÁÇπÁÆÄÂçïÊìç‰ΩúÔºåÈÇ£Á°ÆÂÆûÂè™ËÉΩÊòØÊåÇ FUSE ‰∏äÂéª‰∫Ü

- ## ËÅä‰∏ÄËÅäÂõΩÂÜÖÂ§ßÊ®°ÂûãÁöÑÂÆâÂÖ®Êú∫Âà∂Ôºö ‰∏ÄËà¨ÊòØ‰∏§Â•óÔºåÂàÜÂà´‰ΩúÁî®‰∫étrain-timeÂíåtest-time„ÄÇ
- https://x.com/9hills/status/1840786446153921017
  - ËÆ≠ÁªÉÁöÑÊó∂ÂÄôÂ¢ûÂä†ÂÆâÂÖ®Âíå‰ª∑ÂÄºËßÇÂØπÈΩêÁöÑSFTÂíåÂÅèÂ•ΩÂØπÈΩêÊï∞ÊçÆ„ÄÇÊúÄÁªàÊïàÊûúÁ±ª‰ººÂºÄÊ∫êÁöÑQwen2Ê®°ÂûãÔºåÊúâÁÇπÁî®‰ΩÜÊòØÂæàÂÆπÊòìË¢´Jailbreak„ÄÇ
  - Êé®ÁêÜÊó∂Â¢ûÂä†ÂÆâÂÖ®ÁÆóÂ≠êÔºåÂÖ∑‰ΩìÊúâÂá†Áßç
- ÊäÄÊúØÈöæÁÇπÊúâ‰∏§‰∏™Ôºö
  1. ËÆ≠ÁªÉÂàÜÁ±ªÂô®ÁöÑÂ§ßÈáèÈùûÂÆâÂÖ®Êï∞ÊçÆÔºåÊâÄ‰ª•ËØ¥‰Ω†ÂÖàÊàê‰∏∫ÂèçË¥ºÊâçËÉΩËØÜÂà´ÂèçË¥º„ÄÇ
  2. Ê®°ÂûãË¶ÅÂÅöÁöÑË∂≥Â§üÂ∞èË∂≥Â§üÂø´ÔºåÊúÄÂ∞èÂåñÂΩ±ÂìçÊ®°ÂûãttftÂíåtps„ÄÇ
  3. ÊµÅÂºèÂæàÈöæÔºåÊüêÊ®°ÂûãÊúÄÊó©ÊòØ‰∏ÄÂè•Âè•ËæìÂá∫ÁöÑÔºåÂêéÊù•ÊâçÊîπÊàêtokenÁ∫ßÊµÅÂºè„ÄÇ
- ËØ∑Êïô‰∏Ä‰∏ãÔºåÁé∞Âú®Â∏ÇÂú∫‰∏äÁöÑÂ§ßÊ®°ÂûãÔºåÊÄé‰πàÁü•ÈÅì‰ªñ‰ª¨ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÊòØÂ§ö‰πÖÁöÑÂë¢Ôºü
  - ÂèØ‰ª•ÈóÆ‰∏Ä‰∫õÁâπÂÆöÊó∂Èó¥ÁöÑÊñ∞ÈóªÊù•È™åËØÅÔºå‰ΩÜÊòØÂÖ∂ÂÆûÊ≤°ÂÖ≥Á≥ª„ÄÇÊ®°ÂûãÁöÑÁ≤æÁ°ÆÁü•ËØÜ‰∏çÈáçË¶ÅÔºå‰πüÂÖÖÊª°ÂπªËßâ„ÄÇ

- ## Attention is *Not* All You NeedÔºåËøòÊòØÊúâ‰∫∫Âú®Â∞ùËØï transformer ‰ª•Â§ñÁöÑÊû∂ÊûÑÔºå
- https://x.com/liumengxinfly/status/1835251398692508114
  - ÊØïÁ´ü transfermor Êé®ÁêÜÂ§çÊùÇÂ∫¶Âú®Êï∞Â≠¶‰∏äÊòØÊó†Ê≥ïÁ∫øÊÄßÊâ©Â±ïÁöÑÔºåÊó©Êôö‰ºöËµ∞Âà∞Áì∂È¢à

- ## ÂõΩ‰∫ß188‰∏™Â§ßÊ®°ÂûãÁöÑexcelÊñáÊ°£Ôºö Âåó‰∫¨69 ‰∏äÊµ∑22 Êù≠Â∑û15 Âπø‰∏ú26‰∏™ Ê±üËãè15‰∏™
- https://twitter.com/FinanceYF5/status/1730912502312296935
  - [ÂõΩ‰∫ßÂ§ßÊ®°Âûã188‰∏™list - Feishu Docs](https://zw73xyquvv.feishu.cn/wiki/WXLmwBbYuiTobkkJ6Ojc2cxqnj0?sheet=2XjJlJ&table=tblS2Jv7isKtSODz&view=vewfCdOf0U)

# discuss-llm-architecture
- ## 

- ## 

- ## [Local model registry to solve duplicate GGUFs across apps? : r/LocalLLM _202512](https://www.reddit.com/r/LocalLLM/comments/1pygyhi/local_model_registry_to_solve_duplicate_ggufs/)
  - I'm running into storage issues with multiple local LLM apps. I downloaded Olmo3-7B through Ollama, then wanted to try Jan.ai's UI and had to download the same 4GB model again. Now multiply this across Dayflow, Monologue, Whispering, and whatever other local AI tools I'm testing.
  - Each app manages its own model directory. No sharing between them. So you end up with duplicate GGUFs eating disk space.
  - Feels like this should be solvable with a shared model registry - something like how package managers work. Download the model once, apps reference it from a common location. Would need buy-in from Ollama, LMStudio, Jan, LibreChat, etc. to adopt a standard, but seems doable if framed as an open spec.
  - I'm guessing the OS vendors will eventually bake something like this in, but that's years away. Could a community-driven library work in the meantime? Or does something like this already exist and I'm just not aware of it?

- They're just files. You can remove duplicates yourself and replace them with symlinks to whichever copy you choose to make the "primary".

- Jan has Import option(to use downloaded GGUF files from any folder).
  - Koboldcpp also does this just with browse GGUF option.
  - For Oobabooga, I used symlinks option.
- I found that buried in the Jan.ai UI under Settings / Model Providers / Llama.cpp / import.

- 
- 
- 
- 
- 
- 
- 
- 

- ## ü§î [How do you make agents deterministic? : r/AI_Agents](https://www.reddit.com/r/AI_Agents/comments/1pv2gfk/how_do_you_make_agents_deterministic/)
  - I have been talking to many business and a common concern has been lack of reliability of ai agents.
- In my experience, determinism does not come from the model, it comes from the system design around it. You cannot prompt an LLM into behaving like a rule engine. Business rules need to live in code or configuration, not in free text prompts. The agent‚Äôs job is to interpret context and propose actions, not decide what is allowed.
  - What works well is separating concerns very clearly. Rules, policies, and exceptions are encoded as deterministic logic or tables.
  - For reliability, many teams also constrain where and how agents can act. When agents need to interact with real systems, running them in predictable environments like hyperbrowser helps keep execution consistent and auditable, which is critical in regulated workflows.

- Typically variance results from ambiguous instructions or situations. Use this method to identify specific points in your conversation and see how you can improve the context.
  - Use something like Langfuse to run experiments and trace execution
  - Run the same experiment multiple times.
  - Use python to scrape the data and compare outputs across runs
  - Compute deltas in your outputs
  - Check if specific inputs prove to have more variance in output
- Correct. In fact you should be focusing more on the sad paths because those are the edge cases you need to test.

- Use regular workflow automation tools instead of agents. Or make your tools highly deterministic and have obvious tool selection criteria.

- 90% of it is prompt engineering and a feedback loop. You have another AI at the end that is fed the last AIs user, systems messages and outputs and feeback from client "like i expected it to do this", this will give you a feedback loop on what went wrong in prompt.

- I have been trying to achieve this with my own framework. Its possible. Not in the sense of 1:1 deterministic responses but the core logic of agent output can be deterministic. People have written some methods which I mostly followed. Plus my framework has agent driven, smart retries, which enforces responds to be constrained. As a result my framework, I can deliver basic crud applications with different domain with exact same gaps and similar bugs. They all look very identical. After I get back from holiday I will announce my work. Looking forward to hear your feedbacks.

- ## üèòÔ∏èüß© [Agents | OpenCode](https://opencode.ai/docs/agents/)
- Agents are specialized AI assistants that can be configured for specific tasks and workflows. 
- There are two types of agents in OpenCode; primary agents and subagents.
- Primary agents are the main assistants you interact with directly. 
  - These agents handle your main conversation and can access all configured tools.
  - OpenCode comes with two built-in primary agents, Build and Plan. 
  - Build is the default primary agent with all tools enabled. This is the standard agent for development work where you need full access to file operations and system commands.
  - Plan is a restricted agent designed for planning and analysis. We use a permission system to give you more control and prevent unintended changes. This agent is useful when you want the LLM to analyze code, suggest changes, or create plans without making any actual modifications to your codebase.
- Subagents are specialized assistants that primary agents can invoke for specific tasks. 
  - OpenCode comes with two built-in subagents, General and Explore. 
  - General is a general-purpose agent for researching complex questions, searching for code, and executing multi-step tasks. Use when searching for keywords or files and you‚Äôre not confident you‚Äôll find the right match in the first few tries.
  - Explore is a fast agent specialized for exploring codebases. Use this when you need to quickly find files by patterns, search code for keywords, or answer questions about the codebase.

- ## ‰∏∫‰ªÄ‰πàÁé∞Âú® AI ÂÜôÂâçÁ´Ø‰∏Ä‰∏ãÂ≠êËøô‰πàÂº∫‰∫ÜÔºåÂ∞èÁ±≥ÁöÑ MiMo ËÆ∫ÊñáÁöÑËøôÊÆµ‰ªãÁªç‰∫Ü‰ªñ‰ª¨Â¶Ç‰ΩïËÆ≠ÁªÉÊ®°ÂûãÂÜôÂâçÁ´ØÁöÑÔºåÂÖ≥ÈîÆÊòØËøôÂè•Ôºö
- https://x.com/linexjlin/status/2002383307414409514
  - Êàë‰ª¨ÁöÑÂü∫‰∫éËßÜËßâÁöÑÈ™åËØÅÂô®ÈÄöËøáÂØπÂΩïÂà∂ÁöÑËßÜÈ¢ëÁâáÊÆµÊâßË°åÊÉÖÂÜµËøõË°åËØÑÂàÜÔºåÁªºÂêàËØÑ‰º∞ËßÜËßâË¥®Èáè„ÄÅÂäüËÉΩÂáÜÁ°ÆÊÄßÂíåÂèØÊâßË°åÊÄßÔºå‰ªéËÄåÁ°Æ‰øùÂ•ñÂä±Êú∫Âà∂ËÉΩÂ§üÂêåÊó∂ÂÖºÈ°æÂ§ñËßÇË°®Áé∞‰∏éË°å‰∏∫ÊïàÊûú„ÄÇ
  - ÂéüÁêÜ‰∏äÂ∞±ÊòØÊ®°ÂûãÊ†πÊçÆ prompt ÂÜôÂ•Ω‰ª£Á†ÅÂêéÂÜçÁî® Playwright Êìç‰ΩúÂΩïÂà∂ÊàêËßÜÈ¢ëÔºåÁÑ∂ÂêéÔºå‰∫§Áªô‰∏Ä‰∏™ËßÜËßâÈ™åËØÅÂô®ÔºàÂ∫îËØ•ÊòØ‰∏Ä‰∏™‰∏ìÈó®ËÆ≠ÁªÉÁöÑËßÜÈ¢ëÁêÜËß£Ê®°ÂûãÔºâ ËøõË°åÊâìÂàÜÔºå‰ª•Êèê‰æõÂ•ñÂä±‰ø°Âè∑„ÄÇ

- ËøòÂú®Êç£ÈºìXXPOÁöÑÊàëËßâÂæóËá™Â∑±ÊúõÂ∞òËé´Âèä‰∫Ü
  - ÈáçÂ§çÊÄßÂ∑•‰Ωú‰∫§Êé• AI  ÊòØÂØπÁöÑÔºå‰∫∫Â§öÂá∫ÁöÑÁ≤æÂäõÂèØ‰ª•Áî®Âú®ÂÆ°ÁæéÊääÂÖ≥‰∏ä„ÄÇ

- Â∫îËØ•ÊòØËÆ≠‰∫Ü‰∏Ä‰∏™ÂæàÂ∞èÁöÑËßÜËßâÂà§Âà´Ê®°ÂûãÔºåÂâçÊúüÂä†‰∫ÜÂæàÂ§ö‰∫∫Â∑•ËØÑÂà§ÁöÑÊï∞ÊçÆÔºåËøòÊòØÊå∫ÊâéÂÆûÁöÑ

- ÊâÄ‰ª•Áé∞Âú®Ê∏ÖÊ•ö‰∫ÜÔºåÂÖ∂ÂÆûËøòÂæó‰æùÈù†‰∫∫‰∏∫ÂàõÂª∫ÂæàÂ§öËÆ≠ÁªÉÊï∞ÊçÆ

- ËøôÊ¨°ÁªèËøá‰∫ÜÂ§öÂ∞ëÂ∑•Á®ãÂ∏àÂ§öÂ∞ëÂ∞èÊó∂ÁöÑÊâìÁ£®

- ## [Is direct tool use a trap? Would it be better for LLMs to write tool-calling code instead? : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1px089u/is_direct_tool_use_a_trap_would_it_be_better_for/)
- Look into smolagents from HF, it uses something pretty similar to this.

- "better" is defined by what's model was trained on. Code path is definitely more scalable, but also more unsafe. Currently, most models are primarily trained on direct calls mode, so code calling requires taking context space with explanations and represents another failure mode for the model.
  - Source: agents in my company can decide to call their tools via code, it's pretty hard to make them choose it, unless very specifically targeting to do so.

- Scales much better to give the ai access to a virtual folder structure with code for calling different functions. Just dumping everything into context is silly.

- I use each approach. Function calling API‚Äôs are my approach pretty much 100% of the time if I am building an agent which is integrating with another system with a contract and the job of the agent is to talk to the other system(s).
  - A sandbox is better for broader use cases where the work is all internal.
  - I often define APIs using tool calls that map to python functions running inside the sandbox which is a mix of the two.
  - My opinion is that it completely depends on the situation 

- if you are going to allow a model to write and run arbitrary code, it is riskier and should be sandboxed in most situations. I think smolagents does have the option to constrain to just the functions you are given it or write and run other things as well. I am not 100 percent sure they can guarantee this.

- Yes, this is true. I've had great success providing a restrictive JS environment and typescript definition files and using this instead of JSON tool calls, especially when executing steps which would otherwise require multiple tool calls. However, it does depend on having a model that has been trained extensively on code.

- ## üí° [anthropic blog on code execution for agents. 98.7% token reduction sounds promising for local setups : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/)
  - instead of direct tool calls, model writes code that orchestrates tools
  - basic idea: dont preload all tool definitions. let model explore available tools on demand. data flows through variables not context
  - for local models this could be huge. context limits hit way harder when youre running smaller models
  - the privacy angle is interesting too. sensitive data never enters model context, flows directly between tools
  - cloudflare independently discovered this "code mode" pattern according to the blog
  - main challenge would be sandboxing. running model-generated code locally needs serious isolation
  - tools like cursor and verdent already do basic code generation. this anthropic approach could push that concept way further
  - wondering if anyone has experimented with similar patterns locally

- FYI, this pattern already exists in HFs smolagents, they use model-generated code to execute tools instead of JSON tool calls
  - yep, smolagents is definitely already using this pattern.
- it's up to you how you execute this. The whole approach should depend on strong sandboxing. smolagents can run generated code in a restricted executor, same assumption Anthropic makes in the blog
- The searchable filesystem approach to tool definitions was the most interesting bit for me, very clean way to avoid preloading huge schemas, whether you use code or JSON

- Yes, though in my case I have the model generating a DAG of steps it wants to run instead of arbitrary code, which reduces the sandboxing needed, avoids non-terminating constructs, etc.
  - Token-efficiency is a side-benefit from my perspective. Moving to the plan->execute pattern also makes problems tractable for smaller models, many of which are able to understand instructions and produce "code" of some sort, but which may struggle to pluck details out of even a relatively short context window with the needed accuracy.
- I really like the DAG / plan‚Üíexecute approach , especially for sandboxing and small models. It feels aligned with the same idea of keeping data and state out of the model context, just with tighter structure. Do you generate the full DAG upfront, or refine it during execution?
  - 2 modes. The model can propose a dag using a planning tool and then the user can discuss/iterate it, or auto mode where it just runs.

- if you are writing the function why call an MCP server? Why not just do what the MCP does?
  - MCP is more easily reusable.
- I'd second that; any reasonably shaped API should work really, but this way you avoid installing any packages and browsing for the API docs. It's a way for the model to discover the API instead of being fed how to use it.

- ## Use Anthropic's tool search to give your agent hundreds of tools without filling its context window.
- https://x.com/aisdk/status/2000886249306120473
- Or you could use http://mcpz.it which came out in Feb this year and was the first tool to actually identify and solve the tooling issues with MCPs 

- Game-changer for multi-tool agents  Tool search + deferLoading = hundreds of tools without context explosion? Claude-Sonnet-4.5 agents just scaled massively‚ÄîVercel AI SDK making production agents real. 

- ## ü§î [How to make LLM output deterministic? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1plbe8i/how_to_make_llm_output_deterministic/)
  - I am working on a use case where i need to extract some entities from user query and previous user chat history and generate a structured json response from it. The problem i am facing is sometimes it is able to extract the perfect response and sometimes it fails in few entity extraction for the same input ans same prompt due to the probabilistic nature of LLM. I have already tried setting temperature to 0 and setting a seed value to try having a deterministic output.
  - does setting seed value really work? In my case it seems it didn't improve anything.

- Because of certain GPU optimizations, LLMs are technically random even at temperature = 0 IIRC. llama.cpp has a similar issue. And you can run into something similar in training as well for a given training seed unless you configure some knobs if I'm not misremembering.

- Here's a really good blog post around LLM determinism: https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/
  - If you were to host your LLM locally, both vLLM and SGLang have done work on providing deterministic / batch invariant inference

- change your mindset, when you work with llm it is non-deterministic, whatever you do there is still tiny chance that it can't deliver deterministic response. always handling the non-deterministic part is crucial in all llm base application
  - for me personally, try to prompt the model to wrap the anser around xml tag is quite reliable like `<Answer>what ever llm response</Answer>` and going from there

- Literally impossible to make them fully deterministic because input itself affects the inference matrix.

- The problem with using cloud LLM APIs is that your requests will get batched with others which introduces nondeterminism, even with temperature sampling disabled.
  - It‚Äôs relatively easy to achieve this if you run a model yourself and set the batch size = 1, however.

- ## [Claude Skills are just .cursorrules, change my mind : r/ClaudeAI](https://www.reddit.com/r/ClaudeAI/comments/1oj109n/claude_skills_are_just_cursorrules_change_my_mind/)
- It's all just prompt really. Basically an LLM only has prompt and output.

- Cursor rules are like CLAUDE.md files. They are loaded into context on every AI interaction. Claude Skills are executable capabilities that can be invoked _when needed_. 
  - Nope, cursorrules have markdown header thingies with a description and can either be auto-loaded, or invoked by the LLM which will only see their descriptions before invoking them.

- They can run packaged scripts when needed.

- The marketplace/plugin system is the big differentiator, imo

- üí° I think it was presented as revolutionary because it's adaptable for more than just coding. It can execute the scripts and call tools to use for specific purposes. I think the fact that it's accessible to Claude that AI web means that it's accessible to more individuals that may not be using cursor or something else for coding so available for other use cases

- I wish Claude Code could lazy load MCPs as same as skills.

- I'm a Cursor user and I'd love to replicate what I see people getting out of skills.
  - cursor rules are only loaded at the start of a chat. You can have 50 rules, and logic for which ones to load so that cursor only ever loads a few per chat, but it's still always at the start of a chat.
  - My understanding of Claude Skills is that they can be accessed more-or-less at any time. If I want to e.g. file a bug midway through a chat, if the "File a bug" rule wasn't included at the start, Cursor doesn't seem to re-read its Cursor rules and pick up the newly-needed ones, whereas it sounds like Claude Code will.

- ## [Â¶Ç‰ΩïÁúãAnthropicÊúÄÊñ∞ÂèëÂ∏ÉÁöÑClaude SkillsÔºü‰ºöÊõø‰ª£MCPÂêóÔºü - Áü•‰πé _202510](https://www.zhihu.com/question/1962512846630941008)
- ËØ¥‰∏™Êö¥ËÆ∫ÔºåAI AgentÊÉ≥Ë¶ÅËêΩÂú∞ÔºåÈúÄË¶ÅÁöÑÂè™ÊúâÂº∫Â§ßÁöÑÊ®°ÂûãÂü∫Â∫ßÔºå‰ªÄ‰πàSkill„ÄÅMCP„ÄÅ... ÈÉΩÂè™ÊòØÊ∑ªÂ§¥ÔºåÈÄöËøá‰ª£Á†ÅÂæàÂÆπÊòìÂÆûÁé∞„ÄÇ
- SkillÂ∞±ÊòØ‰∏Ä‰∏™Ê†áÂáÜÂåñÁöÑÊñá‰ª∂Â§πÔºåÁî®Êù•ÊâìÂåÖAgentÂÆåÊàêÁâπÂÆö‰ªªÂä°ÊâÄÈúÄÁöÑÁü•ËØÜÂíåÂ∑•ÂÖ∑„ÄÇ
  - ÂèØ‰ª•ÊääÂÆÉÁêÜËß£ÊàêÁªôÊ®°ÂûãÁöÑËØ¥Êòé‰π¶ÊàñÊ†áÂáÜ‰Ωú‰∏öÁ®ãÂ∫èÔºàSOPÔºåÊàñËÄÖ‰πãÂâçÊØîËæÉÁÅ´ÁöÑÊ¶ÇÂøµÔºöSPECÁöÑÂ¢ûÂº∫ÁâàÔºâ„ÄÇ
  - AnthropicËøôÊ¨°‰∏ç‰ªÖÂèëÂ∏É‰∫ÜÊ¶ÇÂøµÔºåËøòÁõ¥Êé•ÂºÄÊ∫ê‰∫Ü‰∏Ä‰∏™GitHub‰ªìÂ∫ì, ÈáåÈù¢ÂåÖÂê´‰∫ÜÊâÄÊúâ20‰∏™Â∑¶Âè≥ÁöÑÂÆòÊñπSkillÁöÑÊ∫êÁ†ÅÁ§∫‰æã
- ‰∏Ä‰∏™SkillÊñá‰ª∂Â§πÈÄöÂ∏∏ÂåÖÂê´ËøôÂá†ÈÉ®ÂàÜÔºö
  - SKILL.mdÔºöÊ†∏ÂøÉÊñá‰ª∂ÔºåÂøÖÈ°ªÂ≠òÂú®„ÄÇÈáåÈù¢Áî®YAMLÂÜôÂÖÉÊï∞ÊçÆÔºàÂêçÂ≠ó„ÄÅÊèèËø∞ÔºâÔºåÁî®MarkdownÂÜôËØ¶ÁªÜÁöÑÊåá‰ª§ÔºåÂëäËØâClaudeÂú®‰ªÄ‰πàÊÉÖÂÜµ‰∏ã„ÄÅ‰ª•ÂèäÂ¶Ç‰Ωï‰ΩøÁî®Ëøô‰∏™Skill„ÄÇ
  - scripts/ÔºöÂ≠òÊîæÂèØÊâßË°åÁöÑPython„ÄÅShellËÑöÊú¨„ÄÇÊØîÂ¶ÇPDFÂ§ÑÁêÜSkillÈáåÔºåÂ∞±Êúâfill_fillable_fields.pyËøôÁßçÁ°ÆÂÆöÊÄßÊûÅÂº∫ÁöÑ‰ª£Á†Å„ÄÇ
  - references/ÔºöÂ≠òÊîæÂèÇËÄÉÊñáÊ°£„ÄÇÊØîÂ¶ÇAPIÊñáÊ°£„ÄÅÊï∞ÊçÆÂ∫ìSchema„ÄÅÂÖ¨Âè∏ÊîøÁ≠ñÁ≠âÔºåËøô‰∫õÊòØÁªôClaudeÁúãÁöÑÁü•ËØÜÂ∫ì„ÄÇ
  - assets/ÔºöÂ≠òÊîæËµÑÊ∫êÊñá‰ª∂„ÄÇÊØîÂ¶ÇPPTÊ®°Êùø„ÄÅÂÖ¨Âè∏Logo„ÄÅReactÈ°πÁõÆËÑöÊâãÊû∂Á≠âÔºåËøô‰∫õÊòØClaudeÂú®ÊâßË°å‰ªªÂä°Êó∂Áõ¥Êé•‰ΩøÁî®ÁöÑÊñá‰ª∂ÔºåËÄå‰∏çÊòØÈòÖËØªÁöÑ
- ‰∏Ä‰∏™Skill = ‰ªªÂä°ËØ¥Êòé‰π¶ SKILL.md + Â∑•ÂÖ∑‰ª£Á†Å (scripts) + ‰∏ì‰∏öÁü•ËØÜ (references) + Á¥†ÊùêËµÑÊ∫ê (assets)„ÄÇ
  - ÂÆÉÊääÂÆåÊàê‰∏Ä‰∏™ÁâπÂÆö‰ªªÂä°ÊâÄÈúÄÁöÑ‰∏ÄÂàáÈÉΩÊâìÂåÖÂ•Ω‰∫ÜÔºåÊú¨Ë¥®‰∏äÂ∞±ÊòØ‰∏ÄÁßç‰ª£Á†ÅÂíåËµÑÊ∫êÁöÑÁªÑÁªáÊñπÂºèÔºå‰∏ÄÁßçÁ∫¶ÂÆö‰ºò‰∫éÈÖçÁΩÆÁöÑÁêÜÂøµ„ÄÇ
- üí° Claude SkillsËÆæËÆ°ÁöÑÁ≤æÈ´ìÔºå‰πüÊòØÂÆÉÂíåÁÆÄÂçïRAG/MCP/FunctionCallingÁöÑÊúÄÂ§ßÂå∫Âà´„ÄÇÂÆÉÂ∞±ÊòØ‰∏ÄÂ•óËÅ™ÊòéÁöÑÔºå‰∏∫‰∫ÜËäÇÁúÅ‰∏ä‰∏ãÊñáÁ™óÂè£ËÄåËÆæËÆ°ÁöÑÂàÜÂ±ÇÂä†ËΩΩÁ≠ñÁï•„ÄÇ 
  - Á¨¨‰∏ÄÂ±ÇÔºöÂÖÉÊï∞ÊçÆÔºàName + DescriptionÔºâ„ÄÇËøôÈÉ®ÂàÜ‰ø°ÊÅØÈùûÂ∏∏ÁÆÄÁü≠Ôºå‰ºöÂ∏∏È©ªÂú®ClaudeÁöÑËÑëÊµ∑Èáå„ÄÇÂΩìÁî®Êà∑ÊèêÂá∫‰∏Ä‰∏™‰ªªÂä°Êó∂ÔºåClaude‰ºöÂø´ÈÄüÊâ´ÊèèÊâÄÊúâÂèØÁî®SkillÁöÑÊèèËø∞ÔºåÂà§Êñ≠Âì™‰∏™ÂèØËÉΩÁõ∏ÂÖ≥„ÄÇËøôÊòØÁ¨¨‰∏ÄÈÅìÁ≠õÈÄâÔºåÊàêÊú¨ÊûÅ‰Ωé„ÄÇ
  - Á¨¨‰∫åÂ±ÇÔºöSKILL.md„ÄÇÂΩìClaudeËÆ§‰∏∫Êüê‰∏™SkillÁõ∏ÂÖ≥Êó∂ÔºåÂÆÉÊâç‰ºöÂéªÂä†ËΩΩSKILL.mdÈáåÁöÑËØ¶ÁªÜÊåá‰ª§„ÄÇËøôÈÉ®ÂàÜÂÜÖÂÆπÂëäËØâClaudeÂÆåÊàê‰ªªÂä°ÁöÑÂÖ∑‰ΩìÊ≠•È™§„ÄÅÂ∫îËØ•ÈÅµÂæ™ÁöÑËßÑÂàô„ÄÅ‰ª•ÂèäÂ¶Ç‰Ωï‰ΩøÁî®Êñá‰ª∂Â§πÈáåÁöÑÂÖ∂‰ªñËµÑÊ∫ê„ÄÇËøôÊ≠•ÁöÑ‰∏ä‰∏ãÊñáÊ∂àËÄó‰∏≠Á≠â„ÄÇ
  - Á¨¨‰∏âÂ±ÇÔºöËÑöÊú¨ÂíåÂèÇËÄÉÊñáÊ°£„ÄÇÂè™ÊúâÂΩìSKILL.mdÈáåÁöÑÊåá‰ª§ÊòéÁ°ÆË¶ÅÊ±ÇÔºåÊàñËÄÖClaudeÂú®ÊâßË°å‰∏≠Âà§Êñ≠ÈúÄË¶ÅÊó∂ÔºåÂÆÉÊâç‰ºöÂéªËØªÂèñscripts/ÈáåÁöÑ‰ª£Á†ÅÊàñreferences/ÈáåÁöÑÊñáÊ°£„ÄÇËøôÊ≠•ÁöÑ‰∏ä‰∏ãÊñáÊ∂àËÄóÊòØÊåâÈúÄÁöÑÔºåÈÅøÂÖç‰∫Ü‰∏ÄÊ¨°ÊÄßÊääÊâÄÊúâ‰∏úË•øÈÉΩÂ°ûËøõÂéª„ÄÇ
- Ëøô‰∏™Êú∫Âà∂ÁöÑÂ•ΩÂ§ÑÊòæËÄåÊòìËßÅÔºåÊûÅÂ§ßÂú∞ËäÇÁúÅ‰∫ÜÂÆùË¥µÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£„ÄÇÂÆÉÂÖàÂá≠ÁªèÈ™åÂà§Êñ≠Áî®Âì™‰∏™SOPÔºåÁÑ∂ÂêéÁøªÂºÄSOPÁÖßÁùÄÂÅöÔºåÈÅáÂà∞ÂÖ∑‰ΩìÈóÆÈ¢òÂÜçÊü•ÈòÖÈôÑÂΩïÊàñÂ∑•ÂÖ∑ÊâãÂÜå„ÄÇËøôÂ•óÈÄªËæëÔºåÊàë‰ª¨Áî®‰ª£Á†ÅÂΩìÁÑ∂‰πüËÉΩÂÆûÁé∞Ôºå‰ΩÜSkillsÊääÂÆÉÊ†áÂáÜÂåñ‰∫Ü„ÄÇ

- ÂÆÉÂíåMCPÊòØ‰ªÄ‰πàÂÖ≥Á≥ªÔºå‰ºöÊõø‰ª£ÂêóÔºüÁõ¥Êé•ÂõûÁ≠îÔºåÂÆåÂÖ®‰∏çÊòØ‰∏ÄÂõû‰∫ãÔºå‰∏ç‰ºöÊõø‰ª£ÔºåÁîöËá≥ÊòØ‰∫íË°•ÁöÑ„ÄÇ
  - MCPÊòØ‰∏ÄÁßçÈÄö‰ø°ÂçèËÆÆ„ÄÇÂÆÉÂÆö‰πâ‰∫ÜAgentÔºàÂÆ¢Êà∑Á´ØÔºâÂ¶Ç‰Ωï‰∏é‰∏Ä‰∏™Êö¥Èú≤‰∫ÜÂ∑•ÂÖ∑ÁöÑÊúçÂä°ÔºàÊúçÂä°Á´ØÔºâËøõË°åÊ†áÂáÜÂåñÁöÑ‰∫§ÊµÅ„ÄÇÂÆÉËß£ÂÜ≥ÁöÑÊòØAgent‰∏éÂ§ñÈÉ®Â∑•ÂÖ∑Â¶Ç‰ΩïÂØπËØùÁöÑÈóÆÈ¢ò„ÄÇ
  - Claude SkillsÊòØ‰∏ÄÁßçËÉΩÂäõÂ∞ÅË£ÖÊ†ºÂºè„ÄÇÂÆÉÂÆö‰πâ‰∫ÜAgentËá™Ë∫´Â∫îËØ•ÂÖ∑Â§áÂì™‰∫õÁü•ËØÜ„ÄÅÂ∑•‰ΩúÊµÅÂíåÂÜÖÈÉ®Â∑•ÂÖ∑„ÄÇÂÆÉËß£ÂÜ≥ÁöÑÊòØAgentÂ¶Ç‰ΩïÊÄùËÄÉÂíåË°åÂä®ÁöÑÈóÆÈ¢ò„ÄÇ
  - SkillÈáåÁöÑÁü•ËØÜÂèØ‰ª•ÊåáÂØºAgentÂ¶Ç‰ΩïÊõ¥ÊúâÊïàÂú∞Âéª‰ΩøÁî®‰∏Ä‰∏™ÈÅµÂæ™MCPÂçèËÆÆÁöÑÂ∑•ÂÖ∑„ÄÇ‰∏Ä‰∏™AgentÂÆåÂÖ®ÂèØ‰ª•Âä†ËΩΩ‰∏Ä‰∏™SkillÔºåÁÑ∂ÂêéÊ†πÊçÆSkillÈáåÁöÑÊåá‰ª§ÔºåÂéªË∞ÉÁî®‰∏Ä‰∏™ËøúÁ®ãÁöÑMCPÊúçÂä°Âô®

- ÊúÄÂ§ßÁöÑ‰ª∑ÂÄºÊòØÔºöAnthropicÊää‰ªñ‰ª¨Âú®Áîü‰∫ßÁéØÂ¢É‰∏≠ÊâìÁ£®Âá∫ÁöÑ‰∏ÄÂ•óAgentËÉΩÂäõÁÆ°ÁêÜÁöÑËÆæËÆ°Ê®°ÂºèÂºÄÊ∫ê‰∫Ü„ÄÇÊàë‰ª¨ÂÆåÂÖ®ÂèØ‰ª•ÊääËøô‰∏™Ê®°ÂºèÂÄüÈâ¥ËøáÊù•ÔºåÁî®Âú®Ëá™Â∑±ÁöÑAgent‰ΩìÁ≥ªÈáåÔºå‰∏çÁÆ°‰Ω†Áî®ÁöÑÊòØQwen„ÄÅDeepseekÔºåËøòÊòØÂà´ÁöÑÊ®°Âûã
  - ÂΩì‰Ω†ÁöÑAgentËÉΩÂäõË∂äÊù•Ë∂äÂ§öÊó∂ÔºåÊÄé‰πàÁÆ°ÁêÜÔºü‰∏Ä‰∏™Âá†ÂçÉË°åÁöÑSystem PromptÔºü‰∏Ä‰∏™ÂåÖÂê´Âá†ÂçÅ‰∏™Â∑•ÂÖ∑ÂáΩÊï∞ÁöÑÂ§ßÊùÇÁÉ©Êñá‰ª∂ÔºüËøô‰∫õÈÉΩÂæàÈöæÁª¥Êä§„ÄÇ
  - ËÄåSkillsÊèê‰æõ‰∫Ü‰∏ÄÁßçËß£ËÄ¶ÁöÑ„ÄÅÊ®°ÂùóÂåñÁöÑÊñπÊ°à„ÄÇ‰Ω†Âõ¢ÈòüÈáåÁöÑAgent‰∏çÂÜçÊòØ‰æùËµñ‰∏Ä‰∏™Â∑®Â§ßÁöÑ„ÄÅÈöæ‰ª•Áª¥Êä§ÁöÑsystem_prompt.txtÔºåËÄåÊòØ‰∏Ä‰∏™Áî±Âá†ÂçÅ‰∏™Ê†áÂáÜÂåñÁöÑSkillÊñá‰ª∂Â§πÁªÑÊàêÁöÑËÉΩÂäõÂ∫ìÔºåÊØè‰∏™SkillÈÉΩÂèØ‰ª•Áã¨Á´ãÁâàÊú¨ÊéßÂà∂„ÄÅÊµãËØïÂíåËø≠‰ª£„ÄÇ

- Âú®Â≠óËäÇÂÆû‰π†ÂÅöÈÄöÁî®AgentÁ†îÂèëÁöÑÊó∂ÂÄôÔºåAgentÈúÄË¶ÅÊîØÊåÅÂá†ÂçÅÁßç‰∏çÂêå Â∑•ÂÖ∑/Êé•Âè£/Âπ≥Âè∞ Êù•ÂÆåÊàê‰∫îËä±ÂÖ´Èó®ÁöÑ‰ªªÂä°ÔºåÂØπÊ≠§ÔºåÂêå‰∫ã‰ª¨ÂØπÁü•ËØÜÁöÑÁÆ°ÁêÜÊèêÂá∫‰∫Ü‰∏Ä‰∏™knowledgeËåÉÂºèÔºö
  - ÊØè‰∏™knowledgeÂÆö‰πâÂ•Ωtitle, description, used whenÔºàÂú®‰ªÄ‰πà‰ªªÂä°/Â∑•ÂÖ∑/Âπ≥Âè∞Âá∫Áé∞Êó∂Ôºå‰ΩøÁî®ËØ•knowledgeÔºâ„ÄÇËøô‰∫õ‰Ωú‰∏∫metadataÔºåÊØè‰∏™knowledgeÁöÑmetadata‰πüÂ∞±‰∏âË°åÂ≠óÂ∑¶Âè≥„ÄÇknowledgeÁöÑÊ≠£ÊñáÊòØ‰∏Ä‰∏™markdownÔºå‰ºöÂåÖÂê´SOP„ÄÅDos„ÄÅDon'ts„ÄÅÁîöËá≥ÁÆÄÂçïÁöÑËÑöÊú¨„ÄÇ
  - AgentÂêØÂä®‰∏ÄÊ¨°‰ªªÂä°Êó∂ÔºåÂÖàÊ†πÊçÆpromptËÆ©LLMÊ†πÊçÆÊèê‰æõÁöÑmetadata‰∏ªÂä®Âè¨ÂõûÂÖ∂ËÆ§‰∏∫Áî®Âæó‰∏äÁöÑknowledgeÔºåÂÜçÈÄöËøáÂ∑•Á®ãÊâãÊÆµÊääÂÆåÊï¥ÁöÑmdÊãºËøõpromptÈáåÔºåÊîØÊåÅÂÆåÊàêÂêéÁª≠‰ªªÂä°„ÄÇ
  - ÁúãÂÆåAnthropicÊèêÂá∫ÁöÑClaude SkillsÔºåÊÑüÂèπÂΩìÊó∂Âêå‰∫ãÁêÜÂøµÁöÑÂÖàËøõÔºå‰πüÊÑüÂèπË°å‰∏öÈú∏‰∏ªÁöÑÁîüÊÄÅËØùËØ≠ÊùÉ‚Äî‚ÄîÂ¶ÇÊûúÊòØË±ÜÂåÖÊàñËÄÖseedÊèêÂá∫ËøôÊ†∑‰∏Ä‰∏™ËåÉÂºèÔºåËÇØÂÆöÂæó‰∏çÂà∞Â¶ÇÊ≠§Â∑®Â§ßÁöÑÂÖ≥Ê≥®ÂíåË∑üËøõ„ÄÇ
  - ÂêåÊó∂‰πüÊâøËÆ§ÔºåClaude SkillsÁöÑÂÆö‰πâÂÜÖÊ∂µÂíåËßÑËåÉÊØîÂΩìÊó∂Êàë‰ª¨Âõ¢ÈòüÊèêÂá∫ÁöÑknowledgeÊõ¥Âä†ÂÖ®Èù¢ÂíåÂèØÂæ™ÔºåÂè™‰∏çËøáÊú¨Ë¥®‰∏äÊ≤°ÊúâÂ§™Â§öËøõÊ≠•Ôºå‰ªçÁÑ∂ÊòØcontext engineeringÁöÑ‰∏ÄÁßç„ÄÇË¶ÅÊÉ≥ËÆ©Ê®°ÂûãÂÖÖÂàÜÂèëÊå•Â•ΩSkillsÁöÑËÉΩÂäõÔºåÊúÄÁªàËøòÊòØË¶Å‰æùËµñÊõ¥Â•ΩÁöÑÊ®°ÂûãÔºåÊõ¥Âº∫ÁöÑÊé®ÁêÜ„ÄÇ

- ËøôÁßçËåÉÂºèÊòØÂºÄÂèëAGENTÁöÑÂ∏∏ËßÑÊñπÂºèÔºåÊ≤°Âï•ÂÖàËøõÁöÑ„ÄÇÂ∑•Á®ãÈáèÂ§ß‰∏ÄÁÇπÁöÑAGENTÈÉΩ‰ºöÊûÑÂª∫Ëá™Â∑±ÁöÑÁü•ËØÜÂ∫ìËåÉÂºèÔºåÊ†∏ÂøÉÂ∞±ÊòØÁªìÊûÑÂåñËá™Â∑±ÁöÑÂÜÖÂÆπ

- ÂÜô function call ÁöÑÊó∂ÂÄôÂ∞±‰ºöÁî®Âà∞ÂëÄÔºå‰∏çÂêåÁöÑÊòØÂè™ËÄÉËôëÂà∞‰ª£Á†ÅÂ±ÇÈù¢ÁöÑÂ∞ÅË£Ö

- ‰ªéskillÁöÑÊñáÊ°£ÊèèËø∞Êù•Áúã ÂÆÉÂ∞±ÊòØÂçïÁ∫ØÂú∞ÊääÊèèËø∞‰∏¢ÁªôÂ§ßÊ®°Âûã Ëá≥‰∫éÂ§ßÊ®°ÂûãÂÆûÈôÖ‰ºö‰∏ç‰ºöfollow ÈÇ£Â∞±ÂÆåÂÖ®ÁúãÂøÉÊÉÖ‰∫Ü ‰ªéËøô‰∏ÄÁÇπÊù•ËØ¥ skillÊñπÊ°àÂú®ÊÄßËÉΩ‰∏éÁ°ÆÂÆöÊÄßËøôÂùóÂøÖÁÑ∂ÊòØÊØîÁúüÊ≠£ÁöÑtoolsÂ∑Æ‰∏çÂ∞ëÁöÑ
  - ÊÄª‰ΩìÊù•ËØ¥ skillÂü∫Êú¨ÂèØ‰ª•ÁÆóÊòØprompt‰πã‰∏äÁöÑÂàùÁ∫ßËØ≠Ê≥ïÁ≥ñ ËôΩÁÑ∂ÊØîËæÉÈ∏°ËÇã ‰ΩÜÂØπÁî®Êà∑ËÄåË®Ä ÊÄªÂΩíÊòØËÉΩËß£ÂÜ≥‰∏Ä‰∫õÂú∫ÊôØ‰∏ãÁöÑÈóÆÈ¢òÁöÑ Ëµ∑Á†ÅÊúâ‰∫Ü‰∏ÄÂ•óÊåáÂØºÂ§ßÊ®°Âûã‰ΩøÁî®Êñ∞Â∑•ÂÖ∑ÁöÑ‰∏¥Êó∂Ëß£ÂÜ≥ÊñπÊ°à‰∫Ü

- ## [OpenAI Ë∞∑Ê≠åËÅîÊâãÊé®Âá∫ AGENTS.mdÔºåËÉΩÂê¶Êàê‰∏∫ÁºñÁ®ã Agent ÁöÑ„ÄåÂÆòÊñπËØ¥Êòé‰π¶„ÄçÔºü - Áü•‰πé _202508](https://zhuanlan.zhihu.com/p/1941669020068709122)
- ‰∫âËÆ∫‰∏ÄÔºöAGENTS.md vs. CONTRIBUTING.md
  - README.md Êàñ CONTRIBUTING.md ‰∏çÂ§üÁî®ÂêóÔºü
  - ËøôÊï¥‰ª∂‰∫ãÊú¨ËØ•Âú® CONTRIBUTING.md ÈáåËß£ÂÜ≥„ÄÇAGENTS.md ÈáåÁöÑÂÜÖÂÆπÔºåÂíå‰∫∫Á±ªË¥°ÁåÆËÄÖÊÉ≥‰∫ÜËß£ÁöÑ‰∏úË•øÊ≤°‰ªÄ‰πà‰∏§Ê†∑„ÄÇ
  - Áªô Agent ÁöÑÊñáÊ°£ÂøÖÈ°ª È´òÂ∫¶Á≤æÁÇºÔºåÂõ†‰∏∫ËøáÂ§öÁöÑÂÜÖÂÆπ‰ºöÊ∂àËÄóÂÆùË¥µÁöÑ API tokenÔºåÂ¢ûÂä†ÊàêÊú¨ÔºåÁîöËá≥Èôç‰ΩéËæìÂá∫Ë¥®Èáè„ÄÇ

- ‰∫âËÆ∫‰∫åÔºöÊñá‰ª∂ vs. Êñá‰ª∂Â§πÔºåÂçï‰Ωì vs. ÁªìÊûÑÂåñ
  - ÊúâÁªèÈ™åÁöÑÂºÄÂèëËÄÖÊèêÂá∫ÔºåÂØπ‰∫éÂ§ßÂûãÂ§çÊùÇÈ°πÁõÆÔºå‰∏Ä‰∏™Â∑®Â§ßÁöÑ Markdown Êñá‰ª∂ÂæàÂø´‰ºöÂèòÂæóÈöæ‰ª•Áª¥Êä§„ÄÇ
  - ËÆ∏Â§öÂºÄÂèëËÄÖÂª∫ËÆÆÈááÁî®Êõ¥ÊúâÁªÑÁªáÁöÑÊñá‰ª∂Â§πÁªìÊûÑÔºå‰æãÂ¶Ç‰∏Ä‰∏™ÈöêËóèÁöÑ .agents ÁõÆÂΩï

- ‰∫âËÆ∫‰∏âÔºöÊ†πÁõÆÂΩïÊ±°ÊüìÈóÆÈ¢ò

- ‰∫âËÆ∫ÂõõÔºöÁªßÊâøËøòÊòØË¶ÜÁõñÔºü
  - AGENTS.md ÊîØÊåÅÂú®Â≠êÁõÆÂΩï‰∏≠ÂµåÂ•óÔºå‰ΩÜÂÖ∂ËßÑÂàôÊòØ „ÄåÊúÄËøëÊñá‰ª∂‰ºòÂÖà„Äç„ÄÇ

- [ÂëäÂà´Ê∑∑‰π±ÔºåÁî® AGENTS.md Áªü‰∏Ä‰Ω†ÁöÑ AI ÂºÄÂèëÂ∑•ÂÖ∑ËßÑÂàô - Áü•‰πé](https://zhuanlan.zhihu.com/p/1951785160124109343)
  - AGENTS.md Âè™ÊòØ‰∏Ä‰∏™ÂçïÁã¨ÁöÑÊñá‰ª∂ÔºåÊ≤°Ê≥ïÂÉè Cursor ÁöÑ `.cursor/rules` ‰∏ÄÊ†∑ÊîØÊåÅÈùûÂ∏∏Â§öÁöÑÁã¨Á´ãÁöÑËßÑÂàô„ÄÇ
  - ÊàëÁöÑÂª∫ËÆÆÊòØÔºå‰Ω†ÂèØ‰ª•ÂíåÂõ¢ÈòüÂïÜÂÆö‰∏Ä‰∏™Â≠òÊîæÂêÑÁ±ªËßÑÂàôÁöÑÂÖ¨ÂÖ±ÁõÆÂΩïÔºåÊØîÂ¶Ç `.ai/rules/`„ÄÇ ÁÑ∂Âêé‰Ω†ÂèØ‰ª•Âú® AGENTS.md ‰∏≠Ë°•ÂÖÖ rules ‰ø°ÊÅØÂÜÖÂÆπ„ÄÇ

- [Switching to AGENTS.md : r/cursor](https://www.reddit.com/r/cursor/comments/1nqwz02/switching_to_agentsmd/)
  - Don't dump all rules into this file. You can link to other rules from this file. Agents should be able to reason about it and read the right documentation.

- [Claude Code and Claude.md: Should you spread your product doumentation and plans and agent instructions over multiple files? : r/ClaudeAI](https://www.reddit.com/r/ClaudeAI/comments/1lr1g0d/claude_code_and_claudemd_should_you_spread_your/)
  - Yes, progressively split the knowledge across multiple md files.
  - Rule of thumb that works for me. Claude.md is for nouns. Slash commands are for verbs. Meaning Claude.md is about where and what things are, and then slash commands are about how to do the thing.

## üå∞ [How to write a great agents.md: Lessons from over 2, 500 repositories - The GitHub Blog _202511](https://github.blog/ai-and-ml/github-copilot/how-to-write-a-great-agents-md-lessons-from-over-2500-repositories/)

- We recently released a new GitHub Copilot feature: custom agents defined in agents.md files. Instead of one general assistant, you can now build a team of specialists: a @docs-agent for technical writing, a @test-agent for quality assurance, and a @security-agent for security analysis

- What works in practice: Lessons from 2, 500+ repos
  - Put commands early: Put relevant executable commands in an early section: npm test, npm run build, pytest -v. Include flags and options, not just tool names.
  - Code examples over explanations: One real code snippet showing your style beats three paragraphs describing it. 
  - Set clear boundaries: Tell AI what it should never touch (e.g., secrets, vendor directories, production configs, or specific folders). ‚ÄúNever commit secrets‚Äù was the most common helpful constraint.
  - Be specific about your stack: Say ‚ÄúReact 18 with TypeScript, Vite, and Tailwind CSS‚Äù not ‚ÄúReact project.‚Äù Include versions and key dependencies.
  - Cover six core areas: Hitting these areas puts you in the top tier: commands, testing, project structure, code style, git workflow, and boundaries. 

- ## [Q: When will there be fast and competent SLMs for laptops? : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pcurp8/q_when_will_there_be_fast_and_competent_slms_for/)
  - Qwen3-30B-A3B and GPT-OSS-20B both uses Mixture-of-Experts instead of dense layers for their SLM
  - Kimi-Linear and Qwen3-Next-80B-A3B moved along to use "mixed attention" (majority of layers with linear attention) to speed things up AND have longer contexts
  - Not enough people getting into ternary attention like BitNet a4.8 / BitNet v2 or ternary quantization (PTQ)
  - Whatever layer routing is to reduce the amount of RAM needed, including Ouro-2.6B-Thinking these days and Mixture-of-Depths back in 2024
  - Are all of these different techniques conflicting with one another? If it is just a lack of funding for fine-tuning/modding an existing SLM into something fast (assuming QAFT and RL), how much would it cost to crowdfund a project like this?

- It depends on your standards, I believe that for the average Joe, something like Ling Mini 2.0 would already check those requirements (fast -> 1B active is doable for most modern laptops at 20+ tok/s) (Competent -> 16B total parameters makes it decent enough for 99% of the tasks an average person would likely use it for)

- For most people, Llama3 8B was the moment where their laptop could handle a ton of their work and queries locally.

- I use Ling mini to correctly format the ocr result of screenshots. Its the fastest and adheres well to long system prompt. All on cpu.

- We already have that. Llama 3.2 3B for writing. Gemma 3 for multimodal. Qwen 3 4B for stem. Granite 4 for rag.
  - The issue is that the active/ dense parameter count determines the limit of how intelligent the overall model is. The mixture of experts kinda determines how big the model's encyclopaedia is.
  - Its possible to argue that llama 3.2 3B, gemma 3 4B and Qwen 3 30b are around the same level in writing. But Qwen 3 30b is clearly the one with more knowledge.
  - There is also the requirement for AI to have ethics and emotions, which is frankly way too complex to fit into an SLM without lobotomising it.
  - Laptop ram is just too limited in bandwidth and capacity

- I am tempted to suggest RAG-ing an SLM into being better than just being slowed down by a 32B dense model (8B or 14B on higher-performance laptops), and since dense models are "more intelligent" compared to MoE... Maybe BitNet is a more or flexible layer activation is a workaround then?

- Bitnet and ternary are out as they need specific hardware

- ## [I'm surprised how simple Qwen3 VL's architecture is. : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/)
- Most machine learning architecture isn‚Äôt really that complicated when you look at it in code. Plus, in software development simplicity = better.

- training pipeline probably much more complicated

- To be honest... The entire domain of LLMs and even VLMs are fairly simple... Working in self driving for over 5 years exposed to bespoke perception and multi task models, it shocked me how simple LLMs are, especially training it from the model side.
  - The literal loss function for LLMs during pretraining and finetuning is just cross entropy... Compare that to something more complicated like YOLO, it's actually insane in terms of difference of complexity.
  - Really the solution now.... Stack some transformers, use a LM head, chunk input for VLMs into patches... Pretty damn simple I have to say

- The nicest part of Qwen3-VL is that most of the ‚Äúmagic‚Äù comes from small, well-chosen inductive biases rather than a baroque stack. It‚Äôs basically ViT ‚Üí lightweight bridge ‚Üí plain decoder LLM, with two tasteful upgrades: interleaved 3D positional encoding (i-MRoPE) and DeepStack feature fusion.

- In my experience it is not very good at producing accurate coordinates of items it sees

- ## [Finally DeepSeek supports interleave thinking : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pbal3o/finally_deepseek_supports_interleave_thinking/)
  - If a thinking model supports multi-step tool calls and can incorporate thinking from historical steps during these calls, then this model supports interleaved thinking.
  - So far, among open-source models, only GPT-OSS, Kimi K2 Thinking, and MiniMax M2 support it, and I believe this feature is crucial for agents.
  - Interleave thinking lets an AI agent reason, act, and observe in tight loops, so it can adapt step-by-step to new information instead of blindly following a fixed plan.

- Why is special support needed? Each request to an LLM is whole conversation, and you can eliminate previous thinking blocks at each request. What am I missing here?

- ## [Experimenting with Multiple LLMs at once? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p4pre3/experimenting_with_multiple_llms_at_once/)
- I do mostly coding, so different families of models get wildly different python training data. Having each do the same coding task and then have another model pick the best components of the script for a new third script works really well.
  - Open web UI also has channels, which is a discord style chat room. You can tell the models you have to collaborate with each other on a project and they will take turns with sections of code.

- I had them do a few rounds of back and forth checking each other's work. It produced noticably better results than either model could on their own - but the time it took made it pointless. It was faster to just use a larger/slower model, allow a reasoning model to go nuts with thinking tokens, or just iterate myself. 
  - For tasks that aren't time sensitive there's some value there. 

- I like the idea of this and have experimented. I don‚Äôt have a great way to have them collaborate in real time, but using two to check each others work just seems smart to me.

- Andrej Karpathy just posted about a vibe coded project he did called LLM Council that seems pretty cool: https://github.com/karpathy/llm-council

- Here's mine: GitHub.com/irthomasthomas/llm-consortium It's cli based but you can also save a multi-model consortium and use it like a regular model. Then you can use llm-model-gateway to serve that on a openai proxy and use it like a normal model in your tools.

- ## [Instead of either one huge model or one multi-purpose small model, why not have multiple different "small" models all trained for each specific individual use case?   r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1ovsb2x/instead_of_either_one_huge_model_or_one/)
- This is already how MoE (Mixture of Experts) models work. There are plenty of them on huggingface, though I think the separation of the 'experts' often isn't quite so clean. You also have to consider some other things -- your 20B param 'python only' expert model is almost unusable if it doesn't also understand variable names and comments and instructions in natural language, there does have to be some baseline knowledge to make a coding model actually useful, not *just* code in the training data.
- That's kinda what a MOE model is/does. Each of the "experts" are specialists in different trained things and it picks the active ones to use based on the prompt from the user.

- some people mentioned MoE, but MoE works differently. MoE makes decisions for each token separately, not for whole answers. So training many small, specialized models is not the same thing as using MoE.

- Your idea is exactly what Microsoft is attempting with its Phi series of models. They intend to build an ecosystem of models trained on very specific tasks, and then have an agentic system that uses a simple model to process the users request, decide the best model to use, and use that model to answer the question.
  - I believe Google is doing something similar with Gemini, and there are rumors that GPT 5 functions the same way.

- Perfect routing is not a solved problem. In general you have to get part of the way through solving a problem before figuring out exactly what is necessary to solve it.

- ## Every LangGraph user I know is making the same mistake! They all use the popular supervisor pattern to build conversational agents.
- https://x.com/akshay_pachaar/status/1983874390149484881
  - The pattern defines a supervisor agent that analyzes incoming queries and routes them to specialized sub-agents. Each sub-agent handles a specific domain (returns, billing, technical support) with its own system prompt.
  - This works beautifully when there's a clear separation of concerns.
  - The problem is that it always selects just one route.
  - For instance, if a customer asks: "I need to return this laptop. Also, what's your warranty on replacements?"
  - The supervisor routes this to the Returns Agent, which knows returns perfectly but has no idea about warranties.
  - This gets worse as conversations progress because real users don't think categorically. They mix topics, jump between contexts, and still expect the agent to keep up.
  - This isn't a bug you can fix since this is fundamentally how router patterns work.
  - Now, let's see how we can solve this problem.
  - Instead of routing between Agents, first, define some Guidelines.
  - Each guideline has two parts: - Condition: When it gets activated? - Action: What should the agent do?
  - Based on the user's query, relevant guidelines are dynamically loaded into the Agent's context.
  - This approach is actually implemented in Parlant - a recently trending open-source framework (15k+ stars).
  - Instead of routing between specialized agents, Parlant uses dynamic guideline matching. At each turn, it evaluates ALL your guidelines and loads only the relevant ones, maintaining coherent flow across different topics.
  - Another key advantage is that dynamic guidelines keep the system prompt clean, ensuring the agent receives only the right instructions at the right time.

- Hmm won‚Äôt a fully connected sub agent flow with planning enabled work here where one agent can route to any sub agent if it can‚Äôt handle the task. So if realize the user needs more than one thing to be done the agent plans the steps and assigns to the required agent , task is executed result return until the final agent and one agent gives a combined answer

- It's written in python, nothing is stopping a string split and route two agents and join intelligently at the end.

- This seems like another form of "skills" that claude just released. Where the different skills are used to solve the flows and problems independently.  Its just making sure that all the parts are handled.
  - Yes the ideas are certainly relatable

- You can have the supervisor select a set of relevant agents instead of just one though
  - I get your point. However, real users don't think categorically. They mix topics, jump between contexts, and still expect the agent to keep up at every turn of the conversation. You need something that can dynamically load context at every turn. That's where @ParlantIO shines!
- I agree that this per turn dynamic context loading for the supervisor is effective however if there are agents added as tools to one of these guidelines then effectively its like dynamically selecting a subset of composite agents and then ReAct on this context. I actually did something very similar in one of my agents about data analysis where the per turn strategy would be selected dynamic (model, prompt and tools). I think as i said subset selection is just one of the possible single route out of all possible subset routes.

- ## [I Built Pocket Flow, an LLM Framework in just 100 Lines ‚Äî Here is Why _202503](https://medium.com/@zh2408/i-built-an-llm-framework-in-just-100-lines-83ff1968014b)
- After a year of struggling with bloated frameworks, I decided to strip away anything unnecessary. The result is Pocket Flow, a minimalist LLM framework in just 100 lines of code.

- After a year of building LLM applications from scratch, I had a revelation: beneath all the complexity, LLM systems are fundamentally just simple directed graphs.
  -  By stripping away the unnecessary layers, I created Pocket Flow ‚Äî a framework with zero bloat, zero dependencies, and zero vendor lock-in, all in just 100 lines of code.
- We also support batch processing, asynchronous execution, and parallel processing for both nodes and flows.

- Unlike other frameworks, Pocket Flow deliberately avoids bundling vendor-specific APIs. 
  - No Vendor Lock-in: You‚Äôre free to use any model you want, including local models like OpenLLaMA, without changing your core architecture.

- [I Built an LLM Framework in 179 Lines‚ÄîWhy Are the Others So Bloated?  : r/LangChain _202502](https://www.reddit.com/r/LangChain/comments/1iwrhuu/i_built_an_llm_framework_in_179_lineswhy_are_the/)
- I had a look at the code. You've built a simple state machine. 
  - State machine are almost aways the worst time of choice when it comes to composition. 
  - In other words organising everything in flows of Nodes while still writing code is worse then for example carefully providing the data structures yourself and deal with the concurrency and dataflow based on the application specific requirements.
- Thanks for the feedback and example. I opted for a state machine for its simplicity! I'm open to finding a way to balance simple state transitions with tailored data structures though

- how is this different to the approach LangGraph took?
  - We think LangGraph‚Äôs approach can feel rigid and tends to enforce a strictly linear, single-threaded execution model! We also want to do more than just manage state transitions!!
  - For example, we can handle asynchronous capabilities‚Äîlike node cloning to avoid race conditions and dedicated AsyncParallelBatchNodes and AsyncParallelBatchFlows‚Äîto enable parallel execution. This means you can run I/O-bound tasks concurrently (such as multiple LLM calls) without being restricted to a single-threaded, linear flow.

- ## ü§î [I built an AI orchestration platform that breaks your promot and runs GPT-5, Claude Opus 4.1, Gemini 2.5 Pro, and 17+ other models together - with an Auto-Router that picks the best approach : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o76n9d/i_built_an_ai_orchestration_platform_that_breaks/)
  - I've been frustrated with choosing between AI models - GPT-5 is great at reasoning, Claude excels at creative writing, Gemini handles data well, Perplexity is best for research - so I built LLM Hub to orchestrate them all intelligently.
  - [LLM HUB - AI Pipeline Orchestration](https://llm-hub.tech/)
  - The Core Problem: Each AI has strengths and weaknesses. Using just one means compromising on quality.
  - üí° The Solution: LLM Hub coordinates 20+ models across 4 execution modes:
- 4 EXECUTION MODES:
  - Single Mode - One model, one response (traditional chat)
  - Sequential Mode - Chain models where each builds on the previous (research ‚Üí analysis ‚Üí writing)
  - Parallel Mode - Multiple models tackle the same task, synthesized by a judge model
  - üåü Specialist Mode (the game-changer) - Breaks complex tasks into up to 4 specialized segments, routes each to the expert model, runs them in parallel, then synthesizes everything

- ## Code2Video - ÈÄöËøá‰ª£Á†ÅÁîüÊàêÊïôËÇ≤ËßÜÈ¢ëÁöÑ AI Agent Ê°ÜÊû∂ÔºåÊù•Ëá™Êñ∞Âä†Âù°ÂõΩÁ´ãÂ§ßÂ≠¶ Show Lab ÁöÑÊúÄÊñ∞Á†îÁ©∂ÔºåËÆ∫ÊñáÂíåÂºÄÊ∫êÈ°πÁõÆÈÉΩÂèëÂ∏É‰∫Ü„ÄÇ
- https://x.com/shao__meng/status/1974280130420961548
  - ÈÄöËøá AI Agent ÁöÑÊñπÂºèÁîüÊàêÁ±ª‰ºº 3Blue1Brown ÁöÑËßÜÈ¢ëÔºåÂæàÊúâË∂£ÔºåÂí±‰ª¨‰∏ÄËµ∑ÁúãÁúã„ÄÇ
  - ÊñπÊ≥ïÊ†∏ÂøÉÔºö‰∏âÊô∫ËÉΩ‰ΩìÂçè‰Ωú, ËÆæËÆ°Ê°ÜÊû∂ÂàÜËß£‰ªªÂä°‰∏∫‰∏â‰∏™Âçè‰ΩúÊô∫ËÉΩ‰ΩìÔºåÂΩ¢ÊàêÊ®°ÂùóÂåñÁÆ°ÈÅìÔºö
  - ¬∑ PlannerÔºàËßÑÂàíËÄÖÔºâÔºö‰ªéÂ≠¶‰π†‰∏ªÈ¢òÔºàÂ¶Ç‚ÄúÁ∫øÊÄßÂèòÊç¢‰∏éÁü©Èòµ‚ÄùÔºâÁîüÊàêÂ§ßÁ∫≤ÂíåÊïÖ‰∫ãÊùøÔºåÁ°Æ‰øùÊó∂Èó¥ËøûË¥ØÊÄßÔºàÂ¶ÇÊ¶ÇÂøµÂºïÂÖ•„ÄÅÊâ©Â±ïÂíåÂõûÈ°æÔºâ„ÄÇÂÆÉÈõÜÊàêÂ§ñÈÉ®Êï∞ÊçÆÂ∫ìÔºåÊ£ÄÁ¥¢ÂèÇËÄÉÂõæÂÉèÂíåËßÜËßâËµÑ‰∫ßÔºàÂ¶ÇÂõæÊ†áÔºâÔºåÂπ∂ËÄÉËôëÂèó‰ºóÊ∞¥Âπ≥ÔºàÂ¶ÇÈ´ò‰∏≠Áîü vs. Â§ßÂ≠¶ÁîüÔºâÔºå‰ª•ÊèêÂçá‰∫ãÂÆûÂáÜÁ°ÆÊÄßÂíåËßÜËßâ‰∏ÄËá¥ÊÄß„ÄÇ
  - ¬∑ CoderÔºàÁºñÁ†ÅËÄÖÔºâÔºöÂ∞ÜÊïÖ‰∫ãÊùøËΩ¨Êç¢‰∏∫ÂèØÊâßË°å Manim Âä®Áîª‰ª£Á†Å„ÄÇÈááÁî®Âπ∂Ë°åÂêàÊàêÁ≠ñÁï•Âä†ÈÄüÁîüÊàêÔºåÂπ∂ÂºïÂÖ• ScopeRefineÔºàËåÉÂõ¥ÂºïÂØº‰øÆÂ§çÔºâÊú∫Âà∂Ôºö‰ªéË°åÁ∫ßÔºàÂ±ÄÈÉ®‰øÆÂ§çÈîôËØØË°åÔºâÈÄêÊ≠•Êâ©Â±ïÂà∞ÂùóÁ∫ßÂíåÂÖ®Â±ÄÈáçÁîüÔºåÁ°Æ‰øù‰ª£Á†ÅÈ´òÊïà‰∏îÊó†ËØ≠Ê≥ïÈîôËØØÔºåÂêåÊó∂‰øùÊåÅË∑®ËäÇ‰∏ÄËá¥ÊÄß„ÄÇ
  - ¬∑ CriticÔºàÊâπËØÑËÄÖÔºâÔºö‰ΩøÁî®ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÂíåÈîöÁÇπËßÜËßâÊèêÁ§∫ÔºàÂ¶ÇÂç†Áî®Ë°®Âíå‰ΩçÁΩÆÁΩëÊ†ºÔºâËø≠‰ª£Á≤æÁÇºÊ∏≤ÊüìËßÜÈ¢ë„ÄÇÈíàÂØπÁ©∫Èó¥ÈóÆÈ¢òÔºàÂ¶ÇÈáçÂè†ÊàñÁ©∫Êó∑Âå∫ÂüüÔºâÔºåÂÆÉÊèê‰æõÂÖ∑‰ΩìËß£ÂÜ≥ÊñπÊ°àÔºàÂ¶ÇË∞ÉÊï¥ÂØπË±°‰ΩçÁΩÆÊàñÁº©ÊîæÔºâÔºåÊèêÂçáÂ∏ÉÂ±ÄÊ∏ÖÊô∞Â∫¶ÂíåÊïôËÇ≤Âê∏ÂºïÂäõ„ÄÇ
  - Êï¥‰∏™ÊµÅÁ®ã‰ªéÁî®Êà∑Êü•ËØ¢ËæìÂÖ•ÔºåÂà∞ËæìÂá∫ÊïôËÇ≤ËßÜÈ¢ëÔºåÈÄöÂ∏∏ÈúÄÊï∞ÂàÜÈíüÊ∏≤ÊüìÔºåËøú‰ºò‰∫éÁ´ØÂà∞Á´ØÂÉèÁ¥†ÁîüÊàê„ÄÇ

- ## [Â¶Ç‰ΩïÁúãÂæÖËßÇÁÇπÔºöAI ÁöÑÂÖ≥ÈîÆÁÇπ‰∏çÊòØpromptÔºåËÄåÊòØContext EngineeringÔºü - Áü•‰πé](https://www.zhihu.com/question/1923364519964545063)
- ContextËøú‰∏çÊ≠¢ÊòØ‰∏ÄÂè•promptÔºåÂÆÉÂåÖÊã¨Ôºö
  - Êåá‰ª§/Á≥ªÁªüÊèêÁ§∫ÔºöÂÆö‰πâÊ®°ÂûãË°å‰∏∫ÁöÑÂàùÂßãÊåá‰ª§
  - Áî®Êà∑ÊèêÁ§∫ÔºöÊù•Ëá™Áî®Êà∑ÁöÑÂç≥Êó∂‰ªªÂä°ÊàñÈóÆÈ¢ò
  - Áä∂ÊÄÅ/ÂéÜÂè≤ÔºöÂΩìÂâçÂØπËØùÁöÑÁü≠ÊúüËÆ∞ÂøÜ
  - ÈïøÊúüËÆ∞ÂøÜÔºöË∑®Â§öÊ¨°ÂØπËØùÊî∂ÈõÜÁöÑÊåÅ‰πÖ
  - Áü•ËØÜÂ∫ìÊ£ÄÁ¥¢‰ø°ÊÅØ(RAG)ÔºöÊù•Ëá™ÊñáÊ°£„ÄÅÊï∞ÊçÆÂ∫ìÊàñAPIÁöÑÂ§ñÈÉ®Áü•ËØÜ
  - ÂèØÁî®Â∑•ÂÖ∑ÔºöÊ®°ÂûãÂèØ‰ª•Ë∞ÉÁî®ÁöÑÊâÄÊúâÂäüËÉΩÂÆö‰πâ
  - ÁªìÊûÑÂåñËæìÂá∫ÔºöÂØπÊ®°ÂûãÂìçÂ∫îÊ†ºÂºèÁöÑÂÆö‰πâ
- ËøôÂÖ∂ÂÆûÂ∞±ÊòØÂú®ÊûÑÂª∫‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ„ÄåAIÂ∑•‰ΩúÁéØÂ¢É„ÄçÔºåËÄå‰∏çÊòØÁÆÄÂçïÂú∞ÁªôAI‰∏ãÊåá‰ª§„ÄÇ

- Âç≥‰Ωø‰Ω†ÂÜô‰∏çÂ•ΩpromptÔºåÂ∏ÇÈù¢‰∏ä‰πüÊúâÂæàÂ§öAIÂ∑•ÂÖ∑ÂèØ‰ª•Â∏Æ‰Ω†ÂÅöprompt enhancement
  - ÊúüÂæÖAIËÉΩ„ÄåÊúâËÆ∞ÂøÜÔºåÊáÇÁî®Êà∑„Äç„ÄÇÂõ†Ê≠§ÔºåContext EngineeringÔºà‰∏ä‰∏ãÊñáÂ∑•Á®ãÔºâÂ∞±ÂºÄÂßãË¢´ÂÖ≥Ê≥®Âà∞„ÄÇ
  - context engineering, ÊòØÊåáÈÄöËøáÁ≥ªÁªüÊÄßÊûÑÂª∫„ÄÅÁÆ°ÁêÜÂíå‰ºòÂåñ AI Ê®°ÂûãÁöÑËæìÂÖ•‰∏ä‰∏ãÊñáÔºå‰ª•ÊèêÂçáÊ®°ÂûãÂú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑÁêÜËß£‰∏éËæìÂá∫ËÉΩÂäõ„ÄÇ
  - Á¨¨‰∏ÄÊòØÂéÜÂè≤‰∫§‰∫íÊï∞ÊçÆ„ÄÇ
  - Á¨¨‰∫åÔºåÊòØcontextÁöÑÊÄªÁªì„ÄÇÂØπËØùÂèØ‰ª•‰∏ÄÁõ¥Âª∂Áª≠Ôºå‰ΩÜÊ®°ÂûãÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£ÊúâÈôêÔºåÊÄé‰πàËÆ∞‰Ωè‰πãÂâçÁöÑÂØπËØùÔºüÂΩìÁÑ∂ÊòØÂéãÁº©Ôºå‰æãÂ¶ÇÊääÊúÄËøë5Êù°ÂØπËØùÁöÑÂÜÖÂÆπÂéüÂ∞Å‰øùÁïôÔºåÂÜç‰πãÂâçÁöÑÂÜÖÂÆπÊÄªÁªì‰∏Ä‰∏ã„ÄÇ
  - ‰ΩÜÊõ¥ÈáçË¶ÅÁöÑÔºåÊòØÈ¢ÜÂüüÁü•ËØÜ‰∏éËÉåÊôØ‰ø°ÊÅØ„ÄÇ
- AIÁöÑËÉΩÂäõÊù•Ê∫ê‰∫é‰∏§‰∏™ÊñπÈù¢ in-weights memoryÔºàËÆ≠ÁªÉÈõÜÈáåÂ≠¶Âà∞ÁöÑÔºâ Âíå in-context memoryÔºàÂèÇËÄÉËµÑÊñôÈáåÂ≠¶Âà∞ÁöÑÔºâ„ÄÇ in-weights memoryÂü∫Êú¨‰∏äÊòØÂæàÈöæ‰øÆÊîπÁöÑ, ‰ΩÜin-context memory Êõ¥ÂÆπÊòì‰øÆÊîπÂíåÊõ¥Êñ∞Ôºå‰Ω†Âè™Ë¶ÅÊèê‰æõÊñ∞ÁöÑËµÑÊñôÔºåÊ≠£Á°ÆÁöÑËµÑÊñôÔºåÊ®°ÂûãÂ∞±ËÉΩÊúâ„ÄåÊñ∞Áü•„Äç

- Â§çÊùÇÁöÑAIÂ∫îÁî®Ôºå‰∏çÊòØÈù†chatËæìÂÖ•ÊèêÁ§∫ËØçËøô‰πàÁÆÄÂçïÔºåËÄåÊòØÈúÄË¶ÅËá™Âä®ÂåñÂ§öÊ¨°ÂíåÂ§ßÊ®°ÂûãÁöÑ‰∫§‰∫íÔºåËøô‰∏™Ëá™Âä®ÁöÑËøáÁ®ã‰∏≠ÔºåÈúÄË¶ÅËÉΩÂ§üËá™Âä®‰∫ßÁîüÁªôÂ§ßÊ®°ÂûãÁöÑÊèêÁ§∫ËØç„ÄÇ

- Agent = LLM + Prompt + Â∑•ÂÖ∑Ë∞ÉÁî®Ôºå‰ΩÜ‰ªéÂ∑•Á®ãËßÜËßíÊù•ÁúãÔºåËøô‰∫õÊú¨Ë¥®ÈÉΩÊòØContext EngineeringÔºå‰πüÂ∞±ÊòØ‰∏ä‰∏ãÊñáÂ∑•Á®ã„ÄÇ
  - Context EngineeringÂ∞±ÊòØÂú®ÊØèÊ¨°Ë∞ÉÁî®ÈáåÔºåÊääÊ®°ÂûãÂÆåÊàê‰ªªÂä°ÊâÄÂøÖÈúÄÁöÑ‰ø°ÊÅØÊåâÂØπÁöÑÊ†ºÂºè„ÄÅÂú®ÂØπÁöÑÊó∂Êú∫ÂáÜÁ°ÆÊâìÂåÖËøõÂéª„ÄÇ

- ‰∏ä‰∏ãÊñáÂ∑•Á®ãÔºåAIÁªòÂõæÁî®Êà∑Êó©Â∞±Â§©Â§©Âú®Áî®‰∫Ü
  - ÂΩì‰Ω†‰ΩøÁî® InpaintÔºàÂ±ÄÈÉ®ÈáçÁªòÔºâ ÂäüËÉΩÊó∂ÔºåAI‰∏çÊòØÂá≠Á©∫ÊÉ≥Ë±°ÔºåËÄåÊòØÂü∫‰∫é‰Ω†Áïô‰∏ãÁöÑÁîªÂ∏É„ÄÅËæπÁºòÁ∫øÊù°„ÄÅÂÖâÁ∫øÊñπÂêë„ÄÅÂ∑≤ÊúâÁîªÈ£éÊù•Ë°•ÂÖ®Âå∫ÂüüÔºåËøôÂ∞±ÊòØ‚ÄúÂõæÂÉè‰∏ä‰∏ãÊñá‚Äù
  - ÂΩì‰Ω†ËøõË°å OutpaintÔºàÂõæÂÉèÊâ©Â±ïÔºâ Êó∂ÔºåAIÂøÖÈ°ªÂèÇËÄÉÂéüÂõæÁöÑÊûÑÂõæÈÄªËæë„ÄÅËâ≤ÂΩ©Ê∏êÂèò„ÄÅÈ£éÊ†ºÁ∫πÁêÜ„ÄÅ‰∫∫Áâ©ÈÄèËßÜÊù•ÁîüÊàêËá™ÁÑ∂Ë°îÊé•ÁöÑÈÉ®ÂàÜ„ÄÇËøôÁßçÂØπ‰∏ä‰∏ãÊñáÁöÑ‚ÄúÁêÜËß£‚ÄùÂíå‚ÄúÂª∫Ê®°‚ÄùÔºåÊØîÊèêÁ§∫ËØçÊú¨Ë∫´Êõ¥ÈáçË¶Å
  - ÂΩì‰Ω†Áî® PhotoshopÂÜÖÁΩÆÁöÑFireflyËøõË°åÂ±ÄÈÉ®‰øÆÊîπÂàõÊàêÂºèÂ°´ÂÖÖÊó∂ÔºåÁúüÊ≠£Ëµ∑ÂÜ≥ÂÆö‰ΩúÁî®ÁöÑ‰∏çÊòØ‰Ω†ËØ¥‰∫Ü‚ÄúÁªôÊàëÂä†‰∏Ä‰∏™ÁÅØÂ°î‚ÄùÔºåËÄåÊòØAIÊòØÂê¶ÂáÜÁ°ÆËØªÂèñ‰∫ÜÂΩìÂâçÁîªÈù¢ÊòØÂ§úÊôö„ÄÅÊòØÊ∞¥Ëæπ„ÄÅÊúâÂÖâÊ∫êÊäïÂ∞Ñ„ÄÅÊúâÈÄèËßÜÊ∂àÂ§±ÁÇπ„ÄÇ
  - ‰∏äÈù¢Ëøô‰∫õÂäüËÉΩÔºåÂì™ÊÄï‰Ω†Ê†πÊú¨‰∏çÂÜôÊèêÁ§∫ËØçÔºåÂ•ΩÁöÑÂ∑•ÂÖ∑‰πüËÉΩÁªô‰Ω†ËÑëË°•Â•ΩÔºåÊØîÂ¶ÇÊàëÁé∞Âú®Ê†πÊú¨Êîæ‰∏çÂºÄËÆ¢ÈòÖÁöÑPhotoshop AI„ÄÇ

- 
- 
- 
- 
- 

- ## DeepSeekÊúÄÂ§ßÁöÑÂàõÊñ∞ÔºåÊòØ‰∏çÈúÄË¶ÅÂ§ßÈáèÁöÑ‰∫∫Â∑•Ê†áÊ≥®ÔºåËÄåÊòØÁõ¥Êé•‰ªéÂÖ∂‰ªñÂ§ßÊ®°ÂûãËí∏È¶èÊàñËÄÖ‰ΩøÁî®Áæ§‰ΩìÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÁÆóÊ≥ïÔºàGRPOÔºâ„ÄÅCoTÔºàËá™ÊàëÂèçÊÄùÔºâÊù•ÁªôÂ§ßÊ®°ÂûãÂèçÈ¶àÔºå
- https://x.com/seclink/status/1888011462008005030
  - Â∞±Áõ∏ÂΩì‰∫éÂÆåÂÖ®‰ΩøÁî®RLÔºàÊàñËÄÖÂè¶‰∏Ä‰∏™Âü∫Á°ÄÂ§ßÊ®°ÂûãÔºâÊù•Êõø‰ª£‰∫∫Â∑•Ê†áÊ≥®‰∫Ü„ÄÇ
  - ËøôÂÆûÈôÖ‰∏äÊòØÊä¢‰∫ÜScale AI ËøôÁßçÂÖ¨Âè∏ÁöÑËõãÁ≥ïÔºåDeepSeekÁâõX‰πãÂ§ÑÂú®‰∫éÔºåÂæàÂ§öËÄÅÂ§ñ‰∏ÄÂºÄÂßã‰∏ç‰ø°ÔºåÁÑ∂ÂêéÁÖßÁùÄËÆ∫ÊñáÈáåÁöÑÊñπÊ≥ïÂø´ÈÄüÔºàÂ±Ä‰øÉÂú∞ÔºâÂ§çÁé∞ÔºåÂç¥ÂèëÁé∞Á´üÁÑ∂‰πüËÉΩÂ§çÁé∞ÊàêÂäü„ÄÇ

- cotÂíåËí∏È¶è‰πãÂâçÔºåÂ∞±ÈÄöËøágrpoËøõË°årlËé∑Âæó‰∫ÜÁõ∏ÂΩì‰∏çÈîôÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ„ÄÇ‰Ω†ËøôÊï¥ÁêÜÁöÑ‰∏çÊ∏ÖÊô∞„ÄÇ

- ËøôÊ≤°Êúâ‰ªª‰ΩïÂàõÊñ∞ÔºåGPT1Â∞±Áî®‰∫ÜÂêåÊ†∑ÁöÑÊñπÊ≥ïÔºåËÄå‰∏îÁé∞Âú®‰∏ç‰ΩøÁî®ÊòØÊúâÁêÜÁî±ÁöÑ„ÄÇÂõ†‰∏∫deepseekÊ®°ÂûãÊú¨Ë∫´‰∏çË°åÔºå‰∏Ä‰∏™queryÈúÄË¶ÅRL chainÊâçËÉΩËææÂà∞ÂèØÊé•ÂèóÁöÑÁ≠îÊ°àÔºåËá¥‰Ωøinference ÊïàÁéá‰ΩéÂà∞ÂèØÊÄïÔºåËôΩÁÑ∂training‰æøÂÆúÔºå‰ΩÜÊòØoperation costË¶ÅÂ§öÂ•ΩÂá†ÂÄçÔºåÂæó‰∏çÂÅøÂ§±

- ‰Ω†Ëøô‰∏™ËØ¥ÁöÑÂÆåÂÖ®‰∏çÂØπÔºåCoTÊòØGPTÂèëÊòéÁöÑÔºåËí∏È¶è‰πüÊó©Â∞±Êúâ‰∫ÜÔºåRL‰πüÊó©Â∞±Êúâ‰∫ÜÔºådeepseekÊòØÂèëÊòé‰∫ÜRLÈáåÁöÑGRPO

- ## I read up on DeepSeek‚Äôs learning algo, GRPO. GRPO: group relative policy optimization
- https://x.com/virattt/status/1885102056546910672
- How GRPO works:
  1 ‚Ä¢ model generates a group of answers
  2 ‚Ä¢ compute score for each answer
  3 ‚Ä¢ compute avg score for entire group
  4 ‚Ä¢ compare each answer score to avg score
  5 ‚Ä¢ reinforce model to favor higher scores
  - This process is repeated, allowing the model to learn and improve over time.
- Other methods like PPO, use a value function model to do reinforcement learning.
  - GRPO does not, which reduces memory and computational overhead when training.

- GRPO was proposed for the first time in Feb 2024 in DeepSeekMath paper. It is not new.
  - Back then they used Neural Networks for Rewards (PRM/ORM). The magic happened when DeepSeek  replaced PRM/ORM with the exact reward (Verified Reward).

- ## üî° OpenAI's Deep Research is just a search+read+reasoning in a while-loop, right? here is my replicate of it in nodejs, using gemini-flash and jina reader
- https://x.com/hxiao/status/1886250705415229627
- If it includes evaluating js driven websites, including images Then yes 
  - To really replicate that, you'd probably wanna just use puppeteer, save the whole page as an image, extract the info from that, and then crunch that data

# discuss-multi-agents üèòÔ∏è
- ## 

- ## 

- ## 

- ## üÜö ‰ªäÂ§©ÂæàÊúâË∂£Ôºå‰∏§ÂÆ∂Áü•ÂêçÁöÑÂÖ¨Âè∏ÂêÑÂá∫‰∫Ü‰∏ÄÁØáÊñáÁ´†Ôºå‰∫âËÆ∫Ë¶Å‰∏çË¶Å‰ΩøÁî®Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü„ÄÇ
- https://x.com/oran_ge/status/1933754019010539923
  - Claude ÁöÑÂÆòÊñπ Anthropic ÔºöÂ¶Ç‰ΩïÊûÑÂª∫Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü
  - Devin ÁöÑÂÆòÊñπ Cognition Ôºö‰∏çË¶ÅÊûÑÂª∫Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü
- ËøôÊ†∏ÂøÉÁöÑ‰∫âËÆÆÁÇπÂú®‰∫éÔºöContext ‰∏ä‰∏ãÊñáÂà∞Â∫ïÂ∫îËØ•ÂÖ±‰∫´ËøòÊòØÂàÜÂºÄÔºü
  - Claude ËøôËæπÁöÑËßÇÁÇπÊòØÔºåÊêúÁ¥¢‰ø°ÊÅØÁöÑÊú¨Ë¥®ÊòØÂéãÁº©ÔºåÂçï‰∏™Êô∫ËÉΩ‰ΩìÁöÑ‰∏ä‰∏ãÊñáÊúâÈôêÔºåÈù¢ÂØπÊó†ÈôêÁöÑ‰ø°ÊÅØÔºåÂéãÁº©ÊØîÂ§™Â§ßÂ∞±‰ºöÂ§±Áúü„ÄÇËøôÊòØÈõÜ‰ΩìÊô∫ÊÖßÔºå‰∏ÄËµ∑Âçè‰ΩúËé∑ÂæóÁöÑËÉúÂà©„ÄÇ
  - Devin ËøôËæπÁöÑËßÇÁÇπÊòØÔºåÂ§ö‰∏™Êô∫ËÉΩ‰ΩìÁöÑ‰∏ä‰∏ãÊñá‰∏ç‰∏ÄËá¥Ôºå‰ºöÂØºËá¥‰ø°ÊÅØÂâ≤Ë£Ç„ÄÅËØØËß£„ÄÅ‰ªñ‰ª¨Ê±áÊä•ÁªôËÄÅÊùøÁöÑ‰ø°ÊÅØÁªèÂ∏∏ÂÖÖÊª°‰∫ÜÁüõÁõæ„ÄÇ
  - ËøôËÆ©ÊàëÊÉ≥Âà∞ÔºåËΩØ‰ª∂Â∑•Á®ã‰ªéÊù•‰∏çÊòØËøΩÊ±ÇÂÆåÁæéÔºåËÄåÊòØÊåÅÁª≠Ëø≠‰ª£„ÄÇ

- Ê≤°Âï•ÁüõÁõæÁöÑ„ÄÇÁúãÈúÄÊ±Ç„ÄÇopen-ended ÁöÑÈóÆÈ¢òÊØîËæÉÈÄÇÂêà multi-agentÔºõÁõÆÊ†áÂæàÂÖ∑‰ΩìÁöÑËØùÔºåÂ∞±ÊØîËæÉÈÄÇÂêàÂçï‰∏™ agent

- ÊÑüËßâÂíåÁé∞ÂÆû‰∏≠ÁöÑ‰∏§ÂÆ∂‰∏çÂêåÁ≠ñÁï•ËøêËê•ÁöÑÂÖ¨Âè∏Â∑Æ‰∏çÂ§öÔºåÊ≤°ÊúâÁªùÂØπÁöÑÂØπÂíåÈîôÔºåÈÉΩËÉΩËµ∞Âá∫Êù•ÁöÑÂèØËÉΩÊÄß‰πüË∂ÖÂ§ß„ÄÇ

- ## @KuraAIAgents ÈÄöËøáÂàõÊñ∞ÁöÑ‰∫îÈáç Agent Êû∂ÊûÑÔºàËßÑÂàí„ÄÅÊâßË°å„ÄÅËØÑ‰º∞ÔºâÂÆûÁé∞‰∫Ü 87% ÁöÑÊµèËßàÂô®Ëá™Âä®ÂåñÂáÜÁ°ÆÁéáÔºåË∂ÖË∂ä Claude ËÆ°ÁÆóÊú∫Êìç‰Ωú 28 ‰∏™ÁôæÂàÜÁÇπÔºåÂêåÊó∂ÊîØÊåÅ‰ΩéÊàêÊú¨Ê®°ÂûãÊõøÊç¢ÊñπÊ°à
- https://x.com/shao__meng/status/1857586562588094918
  - ÂåÖÂê´5‰∏™‰∏ìÈó®ÁöÑ AgentÔºåÂÖ∂‰∏≠3‰∏™Ê†∏ÂøÉ Agent ÂΩ¢Êàê‰∏Ä‰∏™Âæ™ÁéØÁ≥ªÁªü
  - Âú® WebVoyager Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó 87% ÁöÑÊàêÁª©
  - ÊØî Claude ÁöÑËÆ°ÁÆóÊú∫Êìç‰ΩúÈ´òÂá∫ 28%
- ‰∫î‰∏™Ê†∏ÂøÉ AgentÔºö
a) ÂàùÂßãËßÑÂàíËÄÖ(Initial Planner)
- Ë¥üË¥£Âà∂ÂÆöÈ´òÂ±ÇÊ¨°ËÆ°Âàí
- ‰ΩøÁî® OpenAI o1 Ê®°ÂûãËøõË°åÊé®ÁêÜ
b) Âæ™ÁéØËßÑÂàíËÄÖ(Agent Loop Planner)
- ËØÑ‰º∞‰ªªÂä°ÊòØÂê¶ÂÆåÊàêÊàñ‰∏çÂèØËÉΩÂÆåÊàê
- ‰∏∫ÊâßË°åËÄÖÊèê‰æõ‰∏ã‰∏ÄÊ≠•Êåá‰ª§
- Ê†πÊçÆÈúÄË¶Å‰øÆÊîπËÆ°Âàí
c) ÊâßË°åËÄÖ(Executor)ÂÖ∑Â§á‰∏âÈ°πÊ†∏ÂøÉÊäÄËÉΩÔºö
- ÁΩëÂùÄÂØºËà™ÂíåËøîÂõû
- ËØªÂèñÂΩìÂâçÈ°µÈù¢Êï∞ÊçÆ
- ÊâßË°åÂ±èÂπïÊìç‰Ωú(ÁÇπÂáª„ÄÅÊªöÂä®„ÄÅËæìÂÖ•)
d) Âæ™ÁéØËØÑËÆ∫ËÄÖ(Agent Loop Critic)
- ËØÑ‰º∞ÊâßË°åËÄÖÁöÑË°®Áé∞
- ÁâπÂà´Âú®Â§çÊùÇÁïåÈù¢Êìç‰Ωú‰∏≠Ëµ∑ÂÖ≥ÈîÆ‰ΩúÁî®
e) ÊúÄÁªàËØÑËÆ∫ËÄÖ(Final Critic)
- ËØÑ‰º∞Êï¥‰∏™‰ªªÂä°ËΩ®Ëøπ
- ÂøÖË¶ÅÊó∂Êèê‰æõÂèçÈ¶àÂπ∂ÂêØÂä®Êñ∞ÁöÑÂæ™ÁéØ

- ## OpenAI ÂèëÂ∏ÉÂ§ö Agent ÁºñÊéíÊ°ÜÊû∂ËÉåÂêéÁöÑÊÄùËÄÉ‰ª•ÂèäÂÆûË∑µËøáÁ®ã
- https://x.com/tuturetom/status/1845634978530693494
  - Ê†∏ÂøÉÊòØ OpenAI ÁöÑÂ∑•Á®ãÂ∏àÂú®ÊÄùËÄÉ  Agent ÁöÑ„ÄåË∑ØÁî±„Äç + „ÄåÁßª‰∫§„ÄçÁ≠âËÉΩÂäõÊó∂ÊãìÂ±ïÂá∫Êù•ÁöÑ‰∏Ä‰∏™Á§∫‰æãÔºåËøõËÄåÂèëÁé∞Ëøô‰∏™Á§∫‰æãÂéüËØ≠ÂæàÊôÆÈÄÇÔºåÊâÄ‰ª•ÂºÄÂèë‰∫Ü Swarm Ê°ÜÊû∂
  - ÊâÄÊúâ‰ºüÂ§ßÁöÑÊÄùËÄÉÈÉΩÊ∫ê‰∫éÂë®Êú´‰∏ö‰ΩôÂ∑•‰Ωú

- ## OpenAI ÊÇÑÊÇÑÂºÄÊ∫ê‰∫ÜÊûÑÂª∫Â§ö‰ª£ÁêÜÊô∫ËÉΩ‰ΩìÂçèÂêåÊ°ÜÊû∂ÔºöSwarm
- https://x.com/aigclink/status/1844936446416912628
  - Áî®‰∫éÊûÑÂª∫„ÄÅÁºñÊéíÂíåÈÉ®ÁΩ≤Â§ö‰ª£ÁêÜ

# discuss-ai-format/interop
- ## 

- ## 

- ## 

- ## [Use YAML over JSON when dumping into prompts for ~2x token saving : r/ChatGPTCoding _202509](https://www.reddit.com/r/ChatGPTCoding/comments/1nl7xux/use_yaml_over_json_when_dumping_into_prompts_for/)
  - [YAML vs. JSON: Which Is More Efficient for Language Models? _202307](https://medium.com/better-programming/yaml-vs-json-which-is-more-efficient-for-language-models-5bc11dd0f6df)
  - It's been pointed out in the comments (with sass) that minifying your JSON is another, perhaps even better, alternative than transforming to YAML. So now there's two options for saving tokens.

- Does the guy who wrote the article know that you don't need to use whitepaces in JSON and you can minify it to consume less space than YAML? Generally speaking, JSON is more space-efficient and compact than YAML.

- Just remove the spaces and condence the JSON into a single line. LLMs don't care about spaces, it's a visual thing for us.

- Thought LLM's don't count white space as context... or if they did, it would be incredibly minimal
  - They kind of have to, if only to correctly write Python
  - ASCII art too

- Another point is accuracy... some like XML more as well - and there is BAML. If i just wanna save money I could get a cheaper model too.
- xml is also what Claude officially recommends for better accuracy.

- I use YAML and JSON, because i use the CMS Drupal since 2006 - so this fits quite well in my workflow

- TOML is actually more verbose when it comes to complex data structures.
  - Which makes sense since it was designed to be a JSON/YAML mappable language for better human readability.
# discuss-local-llm-usecases
- resources
  - [Use Cases | Claude](https://claude.com/resources/use-cases)

- ## 

- ## 

- ## 

- ## [Real world use cases for small LLM on edge devices : r/LocalLLaMA _202409](https://www.reddit.com/r/LocalLLaMA/comments/1ffzsy0/real_world_use_cases_for_small_llm_on_edge_devices/)
- Small local models can make many factual mistakes, because it's impossible to compress the entire world model into 2 GB. However, they can be great analyzers (for their size) of the existing text.
  - Summarizing news articles, extracting insights from long web pages, finding a fact that you're searching for in a long text (a book?) and so on. Imagine it as a personal assistant in daily activities.

- Function calling. If it can do basic language to function translation, this is a perfect use. So it allows human voice or text interaction with complicated systems that would otherwise require custom coding. Also, text to structured responses, like using embeds or RAG to return well-defined SQL queries that might result from a wide variety of human specifications.

- All of these tasks involve LLM now, and are simple enough that they can be done on edge. In fact, most of them already do. Sure a cloud service can also do them, sometime better, but with a recurring cost that scale with usage. Most business that sell hardware, and user that buy it, will prefer the fixed upfront cost for R&D that small model.
  - OCR
  - Live transcription and translation
  - Summarization
  - Autocorrect
  - Basic voice control

- Analysis of secured data in offline mode . Maybe finance data , healthcare data , research data. Embed it on drone and check for suitable usecases

- ## [The curious case of Qwen3-4B (or; are <8b models *actually* good?) : r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1p76wtf/the_curious_case_of_qwen34b_or_are_8b_models/)
  - how good are the smaller models at answering some of the sort of questions I might ask of them, chatting, instruction following etc?
  - I ran each model's output against the "council of AI elders", then got GPT 5.1 (my paid account craps out today, so as you can see I am putting it to good use) to run a tally and provide final meta-commentary.
  - The results, per GPT 5.1: GPT-OSS 20B and Qwen 3-4B emerged as the strongest overall performers. Mid-tier models like DeepThink 7B and Qwen 2.5 7B produced competent technical content but struggled severely with the style transform, while Phi-Mini 4B showed the weakest combination of accuracy, coherence, and instruction adherence.
  - The results align closely with real-world use cases: larger or better-trained models excel at technical clarity and instruction-following, whereas smaller models require caution for detail-sensitive or persona-driven tasks, underscoring that the most reliable workflow continues to be ‚Äústrong model for substance, optional model for vibe.‚Äù

- I‚Äôm using qwen3-4b 2507 instruct for simple tasks such as meeting summarization, keeping track of to-do‚Äôs and the likes. So far, it has performed quite well

- ## [My Journey to finding a Use Case for Local LLMs : r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1p1xy0q/my_journey_to_finding_a_use_case_for_local_llms/)
  - I have a lot of phone pictures of recipes, and a lot of inherited cookbooks. The thought of gathering the ones I really liked into one place was daunting. The recipes would get buried in mountains of photos of cats (yes, it happens), planes, landscapes etc. Google photos is pretty good at identifying recipe images, but not the greatest.
  - I landed on qwen3-vl:8b. It was able to take the image (with very strict prompting) and output the exact text from the image. I did have to verify and do some editing here and there. I was happy
  - I then turned to GPT-OSS:20b since it is newer, and asked it to convert the recipe text to json-ld recipe schema compatible format.
  - Now I can take a pic of any recipe I want, run it through the qwen-vl:8b model for OCR, verify the text, then have GPT-OSS:20b spit out json-ld recipe schema text that can be imported into the mealie database. (And verify the json-ld text again, of course).
  - I'm not using any system prompts at this time. Here is the prompt

- You could now take this even further and query it with a prompt on your cell phone.

- My use case is that I use local vlm to the content of the download file, then rename following the format I define and use bash script to route it to the its dedicated folder. Now every time a file is downloaded, it is renamed and route to its location automatically.
# discuss-local-llm-xp/tips
- ## 

- ## 

- ## 

- ## [I benchmarked 7 Small LLMs on a 16GB Laptop. Here is what is actually usable. : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pzxtnr/i_benchmarked_7_small_llms_on_a_16gb_laptop_here/)
  - I tested Qwen 2.5 (14B), Mistral Small (12B), Llama 3 (8B), and Gemma 3 (all 4-bit quants) to see which ones I could actually run without crashing my laptop.
  - Qwen 2.5 (14B): The smartest for coding, but it eats 11GB System RAM + Context. On a 16GB laptop, if I opened 3 Chrome tabs, it crashed immediately (OOM).

- They are all ancient models by llm age so its kinda an invalid test. The king of low spec ram is mamba2 and sliding window attention. Try Granite 4, or Nemotron Nano 2

- I'm not considering 1-2 tokens/s "usable". Even 10 tokens/s is barely usable for the most part.

- ## [I'm curious whether people ask for the model's name in their prompts when testing on LMArena (ChatBot Arena). : r/LocalLLaMA _202504](https://www.reddit.com/r/LocalLLaMA/comments/1jtfzci/im_curious_whether_people_ask_for_the_models_name/)
  - After all, by doing this, users can know the names of the models being A/B tested beforehand, which could bias the ongoing test to some extent.

- Models never know a thing about themselves; no serious LLM user in 2025 will ask model about itself.

- well, the model's output might not be correct at all: deepseek sometimes call itself gpt. so you never now who these models actually are.

- ## [Hard lesson learned after a year of running large models locally : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)
  - I‚Äôm running everything off a workstation with a single RTX‚ÄØ3090, Ubuntu 22.04, llama.cpp for smaller models and vLLM for anything above 30‚ÄØB parameters.
  - My goal has always been to avoid cloud dependencies and keep as much computation offline as possible, so I‚Äôve tried every quantization trick and caching tweak I could find.
  - The biggest friction point has been scaling beyond 13‚ÄØB models.
  - My takeaway so far is that local first inference is viable for small to medium models, but there‚Äôs a hard ceiling unless you invest in server grade hardware or cluster multiple GPUs.
  - Quantization helps, but you trade some quality and run into new bugs.
  - For privacy sensitive tasks, the trade‚Äëoff is worth it; for fast iteration, it‚Äôs been painful compared to cloud based runners.

- vLLM works great if model + context fit into VRAM, but it doesn't do CPU offloading well - use llama.cpp for anything that spills over to RAM.

- instead of trying to use "smarter bigger" models to achieve whatever youre trying to achieve, its more reliable to use multiple parallel instances (via vllm for example) of smaller model that can communicate with each other in distinct roles to create a system that produces accurate results.

- You can also rent a card in the cloud for near local inference.

- ## [Useful patterns for building HTML tools _202512](https://simonwillison.net/2025/Dec/10/html-tools/)
- https://x.com/vista8/status/2003803470449787263
  - Áî®Â§ßÊ®°Âûã‰∏§Âπ¥ÂÅö‰∫Ü150‰∏™Â∑•ÂÖ∑„ÄÇËøô‰∏çÂ•áÊÄ™ÔºåÂº∫ÁöÑÊòØ‰ªñÂè™Áî®ÂçïÈ°µHTMLÂÅöÔºÅ
  - ‰∏Ä‰∏™ HTML Êñá‰ª∂ÔºåÊâìÂºÄÂ∞±ËÉΩÁî®Ôºå‰∏çÈúÄË¶ÅÂÆâË£ÖÔºå‰∏çÈúÄË¶ÅÊ≥®ÂÜå„ÄÇ
  - ‰ªñÁâπÂà´Âº∫Ë∞É "‰∏çË¶ÅÁî® React"„ÄÇ Âõ†‰∏∫ JSX ÈúÄË¶ÅÊûÑÂª∫ÔºåËøô‰ºöËÆ©Êï¥‰∏™ÊµÅÁ®ãÂèòÂæóÈ∫ªÁÉ¶„ÄÇ ‰øùÊåÅÁÆÄÂçïÔºåÂ∞±ÊòØ‰øùÊåÅÂèØÁª¥Êä§„ÄÇ
- ‰ºòÂäø‰πüÂæàÊòéÊòæÔºö
  - ÂèØ‰ª•Áõ¥Êé•‰ªé ChatGPT Êàñ Claude ÈáåÂ§çÂà∂Á≤òË¥¥Âá∫Êù•
  - ÊâîÂà∞ GitHub Pages ‰∏äÂá†ÁßíÈíüÂ∞±ËÉΩÁî®
  - ‰∏çÈúÄË¶Å npmÔºå‰∏çÈúÄË¶ÅÊûÑÂª∫Ê≠•È™§
  - ‰∏§Âπ¥ÂêéËøòËÉΩÊâìÂºÄÔºå‰∏ç‰ºöÂõ†‰∏∫‰æùËµñËøáÊúüÂ¥©Ê∫É
- Áä∂ÊÄÅÂ≠òÂú®Âì™ÈáåÔºü Ê≤°ÊúâÂêéÁ´ØÊï∞ÊçÆÂ∫ìÊÄé‰πàÂäûÔºüSimon Êúâ‰∏§‰∏™ÂäûÊ≥ïÔºö
  - ÊääÁä∂ÊÄÅÂ≠òÂú® URL Èáå„ÄÇ ÊØîÂ¶ÇÔºå‰ªñÂÅö‰∫Ü‰∏Ä‰∏™ 24x24 ÁöÑÂõæÊ†áÁºñËæëÂô®Ôºå‰Ω†ÁîªÁöÑÂõæÊ†áÁõ¥Êé•ÁºñÁ†ÅÂú® URL Èáå„ÄÇËøôÊ†∑‰Ω†ÂèØ‰ª•Êî∂ËóèÔºåÂèØ‰ª•ÂàÜ‰∫´ÔºåÊâìÂºÄÈìæÊé•Â∞±ËÉΩÁªßÁª≠ÁºñËæë„ÄÇ
  - ÊääÂÖ≥ÈîÆÁöÑ‰ø°ÊÅØÂ≠òÂú® localStorage Èáå„ÄÇ ÊØîÂ¶Ç API key ËøôÁßçÊïèÊÑü‰ø°ÊÅØÔºå‰∏çËÉΩÂá∫Áé∞Âú® URL ÈáåÔºå‰πü‰∏çËÉΩÂèëÂà∞ÊúçÂä°Âô®„ÄÇ localStorage ËÆ©Êï∞ÊçÆÂè™Â≠òÂú®Áî®Êà∑ÁöÑÊµèËßàÂô®Èáå„ÄÇ ‰ªñËøòÁî® localStorage ÂÅöËá™Âä®‰øùÂ≠ò„ÄÇ ÂÜôÂ≠óÊï∞ÁªüËÆ°Â∑•ÂÖ∑Êó∂ÔºåÊØèÊ¨°ËæìÂÖ•ÈÉΩ‰ºö‰øùÂ≠òÔºåËøôÊ†∑‰∏çÂ∞èÂøÉÂÖ≥ÊéâÊ†áÁ≠æÈ°µ‰πü‰∏ç‰ºö‰∏¢ÂÜÖÂÆπ„ÄÇ
- CORSÔºàË∑®ÂüüËµÑÊ∫êÂÖ±‰∫´ÔºâÂê¨Ëµ∑Êù•ÂæàÊäÄÊúØÔºå‰ΩÜÁêÜËß£ÂÆÉÂæàÈáçË¶Å„ÄÇ
  - iNaturalist ÂèØ‰ª•Êü•Âä®Áâ©ÁöÑËßÇÂØüËÆ∞ÂΩï
  - PyPI ÂèØ‰ª•Ëé∑Âèñ Python ÂåÖÁöÑ‰ø°ÊÅØ
  - GitHub ÁöÑÂÖ¨ÂºÄ‰ªìÂ∫ìÂÜÖÂÆπÈÉΩÂèØ‰ª•Áõ¥Êé•Ëé∑Âèñ
  - Bluesky Âíå Mastodon ÁöÑ API ‰πüÂæàÂºÄÊîæ
  - Simon ÁîöËá≥Áî® GitHub Gists Êù•ÊåÅ‰πÖÂåñÊï∞ÊçÆ„ÄÇ Âõ†‰∏∫ Gist ÁöÑ API ÊîØÊåÅ CORSÔºå‰Ω†ÂèØ‰ª•ÂÅö‰∏Ä‰∏™Á∫ØÂâçÁ´ØÂ∑•ÂÖ∑ÔºåÊääÁî®Êà∑ÁöÑÊï∞ÊçÆ‰øùÂ≠òÂà∞‰ªñ‰ª¨Ëá™Â∑±ÁöÑ Gist Èáå„ÄÇ
- Êñá‰ª∂‰∏çÈúÄË¶Å‰∏ä‰º† `<input type="file">` ‰∏çÂè™ÊòØÁî®Êù•‰∏ä‰º†Êñá‰ª∂„ÄÇ JavaScript ÂèØ‰ª•Áõ¥Êé•ËØªÂèñÊñá‰ª∂ÂÜÖÂÆπÔºåÂú®ÊµèËßàÂô®ÈáåÂ§ÑÁêÜ„ÄÇ
- Python Âíå WebAssembly ËøôÊòØÊúÄËÆ©‰∫∫ÂÖ¥Â•ãÁöÑÈÉ®ÂàÜ„ÄÇ Pyodide ËÆ© Python ÂèØ‰ª•Âú®ÊµèËßàÂô®ÈáåËøêË°å„ÄÇ
  - ‰∏çÊòØÁé©ÂÖ∑Á∫ßÂà´ÁöÑËøêË°åÔºåÊòØÁúüÊ≠£ÁöÑ PythonÔºåÂåÖÊã¨ Pandas Âíå matplotlib„ÄÇ
- Tesseract OCR„ÄÅSQLite„ÄÅÂõæÁâáÂéãÁº©Â∫ìÔºåËøô‰∫õÂéüÊú¨Áî® C Êàñ C++ ÂÜôÁöÑËΩØ‰ª∂ÔºåÁé∞Âú®ÈÉΩËÉΩÂú®ÊµèËßàÂô®ÈáåË∑ë„ÄÇ
- Ëøô‰∫õÂ∑•ÂÖ∑ÁúüÂèØ‰ª•Ëß£ÂÜ≥ÈóÆÈ¢ò„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåËøôÁßçÊñπÂºèËÆ©‰Ω†ËÉΩÂø´ÈÄüÂÆûÈ™å„ÄÇÊÉ≥Ê≥ïÂà∞ÂéüÂûãÂèØËÉΩÂè™ÈúÄË¶ÅÂá†ÂàÜÈíü„ÄÇ‰∏çÁî®ÊãÖÂøÉÈÉ®ÁΩ≤Ôºå‰∏çÁî®ÊãÖÂøÉÁª¥Êä§Ôºå‰∏çÁî®ÊãÖÂøÉÊàêÊú¨„ÄÇ
- ÊÄé‰πàËµ∑Ê≠•Â∞ùËØïÔºü ‰∏Ä‰∏™ GitHub ‰ªìÂ∫ìÔºåÂºÄÂêØ GitHub Pages„ÄÇ ÁÑ∂ÂêéÂ∞±ÂèØ‰ª•ÂºÄÂßã‰∫Ü„ÄÇ Áî® ChatGPT Êàñ Claude ÁîüÊàê‰∏Ä‰∏™Â∑•ÂÖ∑ÔºåÂ§çÂà∂Á≤òË¥¥Âà∞‰ªìÂ∫ìÈáåÔºåÂá†ÁßíÈíüÂêéÂ∞±ËÉΩÂú®ÁΩë‰∏äËÆøÈóÆ„ÄÇ ‰∏çÈúÄË¶ÅÂÆåÁæéÔºå‰∏çÈúÄË¶ÅÂ§çÊùÇ„ÄÇ ‰ªé‰∏Ä‰∏™Ëß£ÂÜ≥‰Ω†Ëá™Â∑±ÈóÆÈ¢òÁöÑÂ∞èÂ∑•ÂÖ∑ÂºÄÂßã„ÄÇ ÁÑ∂ÂêéÊÖ¢ÊÖ¢ÁßØÁ¥ØÔºåÊÖ¢ÊÖ¢ÊîπËøõ„ÄÇ ‰∏§Âπ¥Âêé‰Ω†ÂèØËÉΩ‰πü‰ºöÊúâÂá†ÂçÅ‰∏™ÁîöËá≥‰∏äÁôæ‰∏™ËøôÊ†∑ÁöÑÂ∑•ÂÖ∑„ÄÇ ÊØè‰∏Ä‰∏™ÈÉΩÂú®Êüê‰∏™Êó∂ÂàªÂ∏ÆÂà∞‰Ω†ÔºåÊØè‰∏Ä‰∏™ÈÉΩËÆ©‰∏ã‰∏Ä‰∏™Êõ¥ÂÆπÊòìÂÅöÂá∫Êù•„ÄÇ
- ËøôÂ∞±ÊòØ HTML Â∑•ÂÖ∑ÁöÑÈ≠ÖÂäõÔºöÁÆÄÂçï„ÄÅÂÆûÁî®„ÄÅÂèØÊåÅÁª≠„ÄÇ

- ## [Senior engineer struggles with learning LLMs foundations : r/LLMDevs _202512](https://www.reddit.com/r/LLMDevs/comments/1plxnq0/senior_engineer_struggles_with_learning_llms/)
  - I've been using ollama and openai to create some interesting side projects and to learn more about LLMs, but I think I'm hugely lacking solid foundations. Please provide me with a structure learning material for a senior engineer with some knowledge of LLMs, thanks
- Andrej Karpathy‚Äôs series on YouTube and Stanford‚Äôs CME course on LLMs and Transformers which is published to YouTube for free.
  - I had a quick look at the Stanford's course and I think it's exactly what I needed, thanks

- You need to learn four critical things
  - Your agent's core product logic is the prompt/instructions you send to an LLM. You will be spending time here with domain experts too to construct good instructions so that the model aligns to your policy. There is no magic bullet. You iterate and evaluate until you are satisfied. Making investments in evals is worth it.
  - If you want to build an agentic application, then you need to expose tools to different models. These are essentially APIs that you have today, both internal or external which the model will instruct you to run and return its results as string. 
  - There are two agentic loops, one is called the inner loop where you agent interacts with an LLM until the LLM is done (stop_reason=finish). There is an outerloop which runs to route traffic to/from agents (if you have a multi-agent architecture), ensure that only good traffic is reaching your agents and that if multiple agents need to be engaged it would be handled outside your core product logic.
  - You need exceptional observability to know what happens, how things fail, etc. And you need to account for different models in your stack so that you can easily improve performance, and/or latency and/or cost.

- ## [ÂíåAIÂØπËØùÔºå‰∏çË¶Å‰ΩøÁî®‚Äú‰Ω†‚Äù ](https://linux.do/t/topic/1293395)
- AI Â§ßÁ•û Karpathy ÂàÜ‰∫´‰∫Ü‰∏Ä‰∏™ÂèçÁõ¥ËßâÁöÑËßÇÁÇπÔºöË∑üÂ§ßÊ®°ÂûãËÅäÂ§©Êó∂Ôºå‰∏çË¶Å‰ΩøÁî® ‚Äú‰Ω†‚Äù Ëøô‰∏™Â≠óÔºå‰πüÂ∞±ÊòØ‰∏çË¶ÅÊääÂÆÉ‰ª¨ÂΩì ‚Äú‰∫∫‚Äù„ÄÇ
  - Karpathy ÊåáÂá∫ÔºåËøôÁßçÈóÆÊ≥ïÂÖ∂ÂÆûÊòØÂú®Áªô AI ÈôçÊô∫„ÄÇÂõ†‰∏∫ LLM Êú¨Ë¥®‰∏äÂπ∂Ê≤°ÊúâËá™ÊàëÊÑèËØÜÔºåÂÆÉÊõ¥ÂÉèÊòØ‰∏Ä‰∏™Êã•ÊúâÊµ∑ÈáèÁü•ËØÜÁöÑÊ®°ÊãüÂô®„ÄÇ
  - ÂΩì‰ΩøÁî® ‚Äú‰Ω†‚Äù Ëøô‰∏™Â≠óÁöÑÊó∂ÂÄôÔºåÊØîÂ¶ÇÈóÆ ‚Äú‰Ω†ÁöÑÁúãÊ≥ï‚ÄùÔºåÊ®°ÂûãÂ∞±‰ºöÁ´ãÂàªË¢´Ëß¶Âèë‰∏ÄÁßçÁâπÂÆöÁöÑ‰∫∫Ê†ºÂµåÂÖ•„ÄÇ
  - ÂÆÉ‰ºöÂº∫Ë°åËÆ©Ëá™Â∑±ÊâÆÊºî‰∏Ä‰∏™Á§ºË≤å„ÄÅÂÆâÂÖ®‰ΩÜÊúâÁÇπÊó†ËÅäÁöÑ AI Âä©Êâã„ÄÇËøôÊó∂ÂÄôÂÆÉÂêêÂá∫Êù•ÁöÑÂæÄÂæÄÊòØÈÇ£‰∫õÂõõÂπ≥ÂÖ´Á®≥„ÄÅÊª¥Ê∞¥‰∏çÊºèÔºå‰ΩÜÂÆûÈôÖ‰∏äÊ≤°Âï•Ê∑±Â∫¶ÁöÑËΩ¶ËΩ±ËæòËØùÔºå‰πüÂ∞±ÊòØÂ§ßÂÆ∂Â∏∏ËØ¥ÁöÑ ‚ÄúAl Âë≥‚Äù„ÄÇ
  - Áõ∏ÂèçÔºåKarpathy Âª∫ËÆÆÊàë‰ª¨Ë¶ÅËΩ¨ÂèòÊÄùË∑ØÔºåË¶ÅÊÉ≥Ëß£ÈîÅ AI ÁúüÊ≠£ÁöÑÂÆûÂäõÔºåÂ∞±Êää AI ÂΩìÊàêÊ®°ÊãüÂô®Áî®„ÄÇ
  - ‰∏æ‰∏™‰æãÂ≠êÔºå‰∏çË¶ÅÈóÆ ‚Äú‰Ω†ÊÄé‰πàÁúã‚ÄùÔºåËÄåÊòØË¶ÅËÆæÂÆöÂÖ∑‰ΩìÁöÑÊÉÖÂ¢ÉÂíåËßíËâ≤„ÄÇ‰Ω†ÂèØ‰ª•ÈóÆ ‚ÄúÂ¶ÇÊûúÊòØ‰∏â‰ΩçËµÑÊ∑±‰∫ßÂìÅÁªèÁêÜÂùêÂú®‰∏ÄËµ∑ËÆ®ËÆ∫Ëøô‰∏™ÂäüËÉΩÔºå‰ªñ‰ª¨‰ºöÊèêÂá∫Âì™‰∫õÂ∞ñÈîêÁöÑÊâπËØÑ‚ÄùÔºåÊàñËÄÖ ‚ÄúËØ∑‰ª•‰∏Ä‰ΩçËØ∫Ë¥ùÂ∞îÁªèÊµéÂ≠¶Â•ñÂæó‰∏ªÁöÑËßÜËßíÂàÜÊûêËøô‰∏™Áé∞Ë±°‚Äù„ÄÇ
- ËØÑËÆ∫Âå∫‰πüÊúâ‰∏çÂ∞ëÈ´òÊâãË°•ÂÖÖÔºå‰∏ç‰ªÖË¶ÅÂ∞ëÁî® ‚Äú‰Ω†‚ÄùÔºå‰πüË¶ÅÂ∞ëÁî® ‚ÄúÊàë‚Äù„ÄÇ
  - Âõ†‰∏∫ÂΩì‰Ω†Ë°®Ëææ ‚ÄúÊàëËßâÂæó.‚Äù ÁöÑÊó∂ÂÄôÔºåAI ‰∏∫‰∫ÜËÆ®Â•ΩÁî®Êà∑ÔºåÂæÄÂæÄ‰ºöÈ°∫ÁùÄ‰Ω†ÁöÑËØùËØ¥ÔºåËøôÂ∞±ÂØºËá¥‰∫ÜÊâÄË∞ìÁöÑÈòøË∞ÄÂ•âÊâøÁé∞Ë±°ÔºåËÄå‰∏çÊòØÂü∫‰∫éÂÆ¢ËßÇ‰∫ãÂÆûÁªôÂá∫Á≠îÊ°à„ÄÇ
- ‰∏ãÊ¨°ÂÜôÊèêÁ§∫ËØçÁöÑÊó∂ÂÄôÔºåËØïËØïÊàíÊéâ ‚Äú‰Ω†‚Äù Âíå ‚ÄúÊàë‚Äù Ëøô‰∏§‰∏™Â≠ó„ÄÇËÆ© AI ÂéªÊ®°ÊãüÈÇ£‰∫õÂÖ∑‰ΩìÁöÑ‰∏ìÂÆ∂„ÄÅÂõ¢ÈòüÁîöËá≥ÂèçÊñπËæ©ÊâãÔºå‰Ω†‰ºöÂèëÁé∞ÂÆÉÁöÑÂõûÁ≠îË¥®ÈáèËÉΩÊèêÂçáÂ•ΩÂá†‰∏™Ê°£Ê¨°„ÄÇ

- ÈÇ£‰πàÊØîÂ¶Ç ‚Äú‰Ω†ÊòØ‰∏Ä‰∏™Áâ©ÁêÜÂ≠¶È¢ÜÂüü‰∏ì‰∏ö‰∫∫Â£´‚Äù„ÄÅ‚Äú‰Ω†ÊòØ‰∏Ä‰∏™Á©øÂ∞èË£ôÂ≠êÁöÑÁÆóÊ≥ïÈ´òÊâã‚Äù„ÄÅ‚Äú‰Ω†ÊòØ‰∏Ä‰∏™Áå´Â®ò‚Äù ÁöÑ promptÔºåÂà∞Â∫ïËØ•‰∏çËØ•Áî®Âë¢Ôºü‰ª•ÂâçÊàëÁúãÂæàÂ§ö‰∫∫ÈºìÂä±Âú®ÈúÄË¶ÅÁâπÂÆöÂ∑•‰ΩúÂú∫ÊôØÊó∂Ëøô‰πàÂÅöÔºåËøôÈáåÈù¢Êúâ‰Ω†Ôºå‰ΩÜÊòØÊåáÂÆö‰∫Ü‰∫∫Ê†ºÁöÑÁ±ªÂûã
  - Êàë‰πüÂú®ÊÉ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÂèØ‰ª•ÊîπÂÜôÊàê ‚Äú‰Ωú‰∏∫‰∏Ä‰∏™ xxx‚ÄùÔºå‚ÄúÊâÆÊºî‰∏ÄÂêç xxx‚ÄùÔºåÁÑ∂ÂêéÂêéÈù¢ÂÖ®ÈÉ®Áî®Á•à‰ΩøÂè•ËØï‰∏Ä‰∏ã
- ‚Äú‰Ω†‚Äù ÁöÑËØùÊÑüËßâÊòØÂ∑≤ÁªèÊåáÂÆö‰∫Ü‰∏Ä‰∏™‰∫∫ÔºåËÄåÁî® ‚ÄúËØ∑‰ª•‚Äù ÁöÑËØùÊÑüËßâ‰ºö‰ªéËØ•È¢ÜÂüü‰∏≠ÊØè‰∏™‰∫∫‰∏çÂêåËßÜËßíÊù•ÊÄùËÄÉÔºåÂèØËÉΩ‰ºöÊõ¥ÂÖ®Èù¢‰∏ÄÁÇπÔºü

- ÁúÅÊµÅÔºåÂ§öÁî®ËßíËâ≤ÊâÆÊºîÊ≥ï„ÄÇ Ê≠§‰∫ãÂú®ËÑëÁ≠ãÊÄ•ËΩ¨ÂºØ‰∏≠‰∫¶ÊúâËÆ∞ÂΩï„ÄÇ ÈùûÊÄùËÄÉÊ®°Âûã‰Ω†Áõ¥Êé•ÊèêÈóÆ ËÑëÁ≠ãÊÄ•ËΩ¨ÂºØÁ±ªÂûãÁöÑÈ¢òÁõÆ Â§ßÂ§öÊ®°ÂûãÂæàÂèØËÉΩË¢´Ê¨∫È™ó„ÄÇ ‰ΩÜ‰Ω†Âº∫Ë∞ÉËøôÊòØËÑëÁ≠ãÊÄ•ËΩ¨ÂºØÔºå ËÆ©‰ªñÂéªÊâÆÊºîÁ±ª‰ººÈ¢ÜÂüüÁöÑÈ´òÊâãÔºå Âç≥‰ΩøÊòØÈùûÊÄùËÄÉÊ®°Âûã‰πüËÉΩÁ†¥Ëß£Èô∑Èò±

- ‚Äú‰Ω†ÊòØ‰∏Ä‰∏™‰∏ì‰∏öÁöÑ swift Á®ãÂ∫èÂëò‚Äù  ‚Üí  ‚ÄúËØ∑‰Ωú‰∏∫‰∏Ä‰∏™‰∏ì‰∏öÁöÑ swift Á®ãÂ∫èÂëòÔºåxxxx‚Äù Ëøô‰πàÁêÜËß£ÂØπÂêó

- Ëøô‰∫õÊäÄÂ∑ßÁõ¥Êé•Ë∞ÉÁî® api ÊïàÊûúÊúÄÊòéÊòæÂêßÔºüÁΩëÈ°µÁâàÁöÑÂ§ßÊ®°ÂûãÂíåÂÖ∂‰ªñÂ∑•ÂÖ∑ÈáåË∞É APIÔºåÂü∫Êú¨ÈÉΩÈ¢ÑËÆæ‰∫ÜËßíËâ≤„ÄÇÂç≥‰Ωø‰∏çÁî®Á¨¨‰∏Ä‰∫å‰∫∫Áß∞ÔºåÊïàÊûúÊèêÂçá‰πü‰∏çÂ§ßÂêßÔºü

- ## [8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/)
  - https://github.com/lemonade-sdk/lemonade
  - Lemonade v9.0.6 came out today, making it really easy to run many models at the same time... and this is the best demo I could think of. Hope it makes someone laugh today!
  - This is a Ryzen AI MAX 395+ (aka Strix Halo). The models are between 3 and 8B parameters (size on disk is visible at the start of the video).
  - Lemonade is a free and open local LLM server made by AMD to make sure we have something optimized for AMD PCs. Today we released a new version that lets many LLMs run at the same time.
  - I made this quick HTML/CSS/JS app to demo the capability. It loads 8 LLMs, has them share a chat history, and then keeps prompting them until they vote yes or no on the user's question.
  - In the backend, there are 8 llama-server processes running on the Strix Halo's GPU. The web app talks to lemonade server at http://localhost:8000, and then lemonade routes the request to the right llama-server process based on the model ID in the request.
  - I thought it was funny the LLMs couldn't come to a final decision on this question. Just like people!

- A tie? That's no good. There should have been 9 LLMs, like the Supreme Court.

- Are they debating each other? Seems like they don‚Äôt spend much time disagreeing. I want to see one where they are forced into consensus or it doesn‚Äôt end (or maybe time it out and score it then)
  - There‚Äôs 5 rounds of debate. First round they are supposed to give a hot take. Rounds 2 and 3 they‚Äôre supposed to react to each other (shared chat history). Rounds 4 and 5 they‚Äôre supposed to vote.

- ## ["We're in an LLM bubble, not an AI bubble" - Here's what's actually getting downloaded on HuggingFace and how you can start to really use AI. : r/LlamaFarm _202512](https://www.reddit.com/r/LlamaFarm/comments/1pb2wr2/were_in_an_llm_bubble_not_an_ai_bubble_heres/)
  - Encoder-only models (BERT family) account for 45% of HuggingFace downloads, nearly 5x more than decoder-only LLMs at 9.5%. 
  - Every one of these model families exists because someone realized the "one model to rule them all" approach was failing for their use case
  - This is "Mixture of Experts" at the application level. Many small, specialized models working together instead of one massive model trying to do everything.

- ## [What broke when you tried to take local LLMs to production? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p91p4k/what_broke_when_you_tried_to_take_local_llms_to/)
- Ollama broke a lot of the time because we dev‚Äôd on a Mac but pushed production to nvidia. Switching to vllm has largely solved this and pushed it to more of an interface rather than model problem.

- Hosting several Models on one GPU is always really annoying with vLLM. You basically have to fiddle with the memory utilisation until all models fit. There is no clean way I found to calculate memory usage. You just have two tweak, boot, look at the logs and repeat. And even if you got all models to fit, vLLM sometimes crashes with cuda out of memory exceptions when loading the models since there seems to be a peak in memory consumption on boot up.

- Why doesn't anyone mention tabbyAPI/exllamav3? It's much more memory-efficient than AWQ, also it loads quicker, and the quantization is more optimized. Also K/V cache can be quantized between 2 and 8 bits seperately (=more room for context)

- ## [Why do you use open-source LLMs ? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p16kxx/why_do_you_use_opensource_llms/)
- Freedom. I can do whatever I want without having my data stolen.

- The benefit of open source LLMs for me is that you can access the internal layers to do things like attention injection, intermediate layer feature extraction, attention map extraction, token-wise early stoppage, adaptive layer skips, conversion to latent attention, conversion to mamba or gated delta net hybrid etc
  - This is it. Intervening mathematically in something that can talk like a human is an otherworldly experience. I do it almost every day and it makes me feel like an alchemist tinkering with a homunculus. There‚Äôs nothing else like it. Most other intellectual activities feel shallow by comparison.

- I just think it's very cool to have a summary of the vast wealth of human text and knowledge in a 30GB file on my computer.

- I want to own my capabilities. I don‚Äôt want my coding skills to be tied to some external service that I have zero control over.

- ## [What do you use local LLMs for? What is your use case? : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1ms4gmz/what_do_you_use_local_llms_for_what_is_your_use/)
- They are best used for small context tasks like classification/sentiment, asking it summarize, pull out keywords, grammar, etc.
  - Not for long conversations or complex tasks, but system prompt, user request, and then get a single response.
  - For example, I've been experimenting with a local LLM checking chat messages for rule-breaking in a Discord server, flag the suspicious once, and then I checked the flagged messages and take action myself.

- I used it to generate a synthetic dataset for training my first neural network, with successful results.

- Structured data extraction from private documents (millions of documents)
  - We have produced an annotated dataset of a few thousand records. We used this to fine tune a model and evaluate performance. We are getting >0.98 F1 score which is a margin of error we are willing to tolerate given the scale and time saved... It's for a very specific type of short documents. Our extraction pipeline has multiple extraction and validation steps.

- Data extraction and classification.

- ## [What are you using your local models for ? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1oz6k5j/what_are_you_using_your_local_models_for/)
- Confidential research. In SillyTavern. Just yet with a quantized Skyfall-31b from Drummer, apparently some blend of Mistral Small or whatever. It's fun.

- Qwen3-VL-30B-A3B-Thinking is heating my home by processing video. I've posted about it before but llm-ffmpeg-edit.bash handles the logic & llm-python-vision-multi-images.py handles sending the images/frames to the LLM backend.
  - I was using Mistral 3.2 (24B dense) before Qwen3-VL got support. The speed increase from 24B -> 30B-A3B has been incredible, while maintaining accuracy.

- I'm bound by the legal terms of my employment to not discuss the technologies we use there, but am free to talk about my personal use-cases.
- STEM research assistant -- I give it my technical notes (usually physics and/or math) and a question, and get back helpful replies. My go-to is Phi-4-25B, and when it's not smart enough I escalate to Tulu3-70B, sometimes Qwen3-235B pipelined with Tulu3-70B.
- Creative writing -- Cthulhu-24B, Big-Tiger-Gemma-27B-v3, or Valkyrie-49B-v2. Mostly sci-fi (space opera or Murderbot fanfic).
- Evol-Instruct and synthetic dataset generation or augmentation -- again, mostly Phi-4-25B or Tulu3-70B, though recently I have been using Valkyrie-49B-v2 to bulk up a RAG database of technical troubleshooting advice/solutions. To my surprise Valkyrie is a lot better at this than Tulu3-70B, even though they are derived from similar models (Tulu3 from Llama-3.1, Valkyrie from Llama-3.3-Nemotron-Super-49B-v1.5 which in turn is based on Llama-3.3).
- Persuasion research -- studying the capacity for LLM inference to change people's minds. Big-Tiger-Gemma-27B-v3 is excellent at this.
- Wikipedia-backed RAG for general question-and-answer. I use Big-Tiger-Gemma-27B-v3 for this as well.
- Describing images so I can index them in a locally hosted search engine. Qwen2.5-VL-72B is still the best vision model I've yet used, but I haven't had a chance yet to compare it against Qwen3-VL-32B. I am hoping Qwen3 is better, despite having fewer than half as many parameters.
- I also run an IRC bot for a technical support channel, which is mostly GOFAI-driven but I've been working on a plugin for it to be RAG/LLM-driven too. That, too, uses Big-Tiger-Gemma-27B-v3.
- Recently I've been trying to use Phi-4 (14B) as a synthetic dataset rewriter, to salvage low-quality inferred data I would normally prune from the dataset. I read a paper suggesting even very small models (4B) are effective at this. So far my results have been mixed. I've been meaning to try Tiger-Gemma-12B-v3 as well; possibly Phi-4 just isn't the right model for this.
- GLM-4.5-Air for slow inference of entire programming projects (which I don't do much, since I don't want my coding skills to atrophy) or to find bugs in my own code.
- Qwen3-Coder-REAP-25B-A3B for fast FIM code inference. The model doesn't have to be smart to figure out what my "for"-statement is going to look like, but it does need to be fast enough that it can suggest a completion before I've finished typing the "for"-statement myself. I use the REAPed version of this model so that it fits in my GPU's VRAM (at Q4_K_M); the original 30B-A3B didn't quite fit.
- I'm also tentatively using Phi-4 as a judge, comparing two replies to the same prompt and telling me which is better. It's early days yet, for this project, and it might not be the right model for this. We will see.
- Sometimes I use Big-Tiger-Gemma-27B-v3 or Phi-4 (14B) for language translation (mostly Spanish to English, but sometimes German or Russian to English). Overall Big Tiger is better at this, though Phi-4 does surprisingly well, and is better than Big Tiger at taking situational context into account with its replies. It's also a lot faster than Big Tiger, which is sometimes important for translation tasks.

- ## [Are any of you using local llms for "real" work? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1otnj2k/are_any_of_you_using_local_llms_for_real_work/)
- The most "real" work I've done is that Qwen3-VL-30B-A3B-Thinking is currently going through videos 10-seconds at a time. 
  - Based on the bot's True/False boolean output a wrapping program keeps track of what segments `<thing I'm looking for>` is within. At the end, we're done using Qwen3-VL and the wrapping program uses the segment information to use FFMPEG to make a clipped version where `<thing I'm looking for>` should always be present.
  - A general example is you could have the bot look for every explosion in an action film. Maybe it considers muzzle flash from a gun to be an explosion, not technically wrong as far as I know. So you could prompt that it should only be explosions not from gunfire and try that.

- I use an LLM, a scraper to fetch security vulnerabilities (CVEs), and an internal API that lists my running services, and the LLM generates a daily report for me about whether any of the software I'm running might have been mentioned.

- Using qwen3 vl to parse vids to extract car plate numbers snd other vehicle markings

- I used a local LLM and a vector database to help me with my corporate taxes. I first fed all past 3 or 4 years of expenses along with their categorizations (meals, fuel, office supplies etc.) into a Postgres PG Vector database and embedded the data using the Nomic model. I then classified several hundred transactions for the past tax year using this vector set of data to categorized the transactions.

- I use nvidia/Llama3-ChatQA-1.5-8B to index 2M similar insurance docs using ollama. I load the index into Meilisearch and sell access to it. I did this after a trip to micro center and a little over $3k.

- I use them as zeroshot NER/Classification models. They're pretty decent at it out of the box. Training isn't too complicated, but for small repetitive tasks, they're often good enough.

- ## [Why we shifted to Spec-Driven Development (and how we did it) : r/ClaudeCode _202511](https://www.reddit.com/r/ClaudeCode/comments/1op8b6i/why_we_shifted_to_specdriven_development_and_how/)
  - Over the last few months we came up with our own Spec-Driven Development (SDD) flow that we feel has some benefits over other approaches out there. 
  - Specifically, using a structured execution workflow and including the results of the agent work. 
- In short: you design your docs/specs first, then use them as input into implementation. And then you capture what happens during the implementation (research, agent discussion, review etc.) as output specs for future reference. 
- The cycle is:
  - Input specs: product brief, technical brief, user stories, task requirements.
  - Workflow: research ‚Üí plan ‚Üí code ‚Üí review ‚Üí revisions.
  - Output specs: research logs, coding plan, code notes, review results, findings.
- By making the docs (both input and output) first-class artifacts, you force understanding, and traceability. The goal isn‚Äôt to create a mountain of docs. 
  - The goal is to create just enough structure so your decisions are traceable and the agent has context for the next iteration of a given feature area.
- First, worth mentioning this approach really only applies to a decent sized feature. Bug fixes, small tweaks or clean up items are better served just by giving a brief explanation and letting the agent do its thing.
- How we implemented it (step-by-step)
  - Define your prd.md**:** goals for the feature, user journey, basic requirements.
  - Define your tech_brief.md: high-level architecture, constraints, tech-stack, definitions.
  - For each feature/user story, write a requirements.md file: what the story is, acceptance criteria, dependencies.
  - For each task under the story, write an instructions.md: detailed task instructions
  - To start implementation, create a custom set of commands that do the following for each task
  - Commit these spec files alongside code so future folks (agents, humans) have full context.
  - Use folder conventions: e.g., project/story/task/requirements.md, ‚Ä¶/instructions.md etc. So it‚Äôs intuitive.
- Bonus: If you want a tool that automates this kind of workflow opposed to doing it yourself (input specs creation, task management, output specs), I‚Äôm working on one called Devplan that might be interesting for you.

- Bmad does exactly this: https://github.com/bmad-code-org/BMAD-METHOD

- have you tried any of BMAD, GitHub/Spec-kit or Privacy-AI/spec-kitty for a community fork with extensive git worktree support

- ## [Which model do you wish could run locally but still can‚Äôt? : r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1omoodc/which_model_do_you_wish_could_run_locally_but/)
- It's not really about particular models at this point. I'm way more interested in the local infrastructure around it. The main thing I'm still missing is simple knowledge-base integration for something like Open-WebUI. I would really like to just point the front-end at my local Kiwix-server and a 1TB-eBook collection, let it index to its heart's content for a few weeks and then have any model be able to reference and integrate all those information.

- Qwen3 Omni, how come no one is talking about a 30B model that can do video and speech?

- Something which I can install with just a dmg or exe file please
  - I tried a couple but I am not tech savvy and had to install a lot of other stuff to my laptop

- For me, it‚Äôs not that it‚Äôs a single model I can‚Äôt run‚Ä¶ it‚Äôs the fact that I can‚Äôt run multiple models.

- ## [What's the missing piece in the LLaMA ecosystem right now? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o5dh3v/whats_the_missing_piece_in_the_llama_ecosystem/)
  - For me, it's the data prep and annotation tools. The models are getting powerful, but cleaning and structuring quality training data for fine-tuning is still a major, manual bottleneck.

- Training-Data is the biggest issue for local ecosystem right now i think. There is so many datasets, but who knows about their real quality.
  - For me personally, finetuning an LLM is like 500x harder than a diffusion model, simply due to the lack of tooling. Unsloth is nice and all, but i dont want to run fucking Jupyter Notebooks, i want something akin to kohya_ss with as many of the relevant hyperparameters exposed.
- Hardware accessibility is only secondary. If you have a small Model, e.g. the Qwen3 0.6B full finetune should be possible on local hardware. If that proves to be effective, renting a GPU machine somewhere for a few bucks shouldnt be the issue.

- llms are very bad at image recognition, give it a civ or other strategy game screenshot and it gets nearly everything wrong.

- Benchmarks. There has been little to no progress in the past two years regarding how LLMs are evaluated. It‚Äôs still mostly huge catalogues of questions with predetermined answers. That‚Äôs a very poor system for testing intelligence.
# discuss
- ## 

- ## 

- ## 

- ## 

- ## 

- ## Áé∞Âú®ÊâÄÊúâÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåÊó†ËÆ∫ÂÆÉÂè∑Áß∞‰∏ä‰∏ãÊñáÁ™óÂè£ÂèØ‰ª•Â§öÂ∞ëÔºåËæìÂÖ•ÊòØÁúüÁöÑÂèØ‰ª•ÂæàÈïøÔºå‰ΩÜÊòØËæìÂá∫‰∏çËÉΩÂ§™ÈïøÔºåËæìÂá∫Èïø‰∫ÜÂ∞±ÂπªËßâ‰∏•ÈáçÔºåÁõ∏ÂØπÂ•Ω‰∏Ä‰∫õÁöÑÊòØ GeminiÔºå
- https://x.com/dotey/status/1995667479377707123
  - ÊâÄ‰ª•‰ΩøÁî®Êó∂Ôºå‰Ω†ÂèØ‰ª•ËæìÂÖ•ÂæàÂ§öËµÑÊñôÁªôÂÆÉÂèÇËÄÉÔºå‰ΩÜÊòØÊØèÊ¨°‰∏çË¶ÅËæìÂá∫Â§™Â§öÔºåÊØîÂ¶Ç‰∏ÄÊ¨°ÊúÄÂ§öËæìÂá∫Âá†ÂçÉÂ≠óÔºåÂ§ö‰∫ÜÂ∞±Ë¶ÅÂàÜÈ°µ„ÄÇ
- ÂèØ‰ª•ÈááÁî®ÂàÜÂá†Ê≠•ÔºåÁÑ∂ÂêéÊèê‰æõ‰∏Ä‰∏™checklistÁ°Æ‰øù‰ªñËá™Â∑±ÂÆ°Ê†∏
  - ÂØπÔºåtodo listÊúâÂä©‰∫éÊãâÂõûÊ≥®ÊÑèÂäõÂ•Ω‰∏Ä‰∫õÔºå‰ΩÜ‰∏ÄÊ†∑ËæìÂá∫ÂÜÖÂÆπ‰∏çËÉΩÂ§™Èïø

- ËØ¥‰∏ãÊàëÁöÑÁêÜËß£ÔºöÊØè‰∏™tokenËæìÂá∫ÈÉΩÊòØ‰∏Ä‰∏™Ê¶ÇÁéáÔºå‰∏æ‰∏™‰æãÂ≠êÔºöÊØîÂ¶ÇËØ¥‚ÄúÊàë‚ÄùËøô‰∏™Â≠óÂêéÈù¢Ë∑üÁùÄ‚Äú‰ª¨‚ÄùËøòÊòØ‚ÄúÁöÑ‚ÄùÔºåÂØπllmÊù•ËØ¥Â∞±ÊòØÂèñÊ¶ÇÁéáÈ´òÁöÑ„ÄÇ‰ΩÜÊòØÊ¶ÇÁéáÊúâ2‰∏™ÈóÆÈ¢òÔºö
  1. Â∞±ÁÆóÊØèÊ¨°ÈÉΩÂèñÊúÄÈ´òÁöÑÔºå0.9*0.9*0.9‚Ä¶ÔºåÊï¥‰ΩìÊù•ÁúãÔºåÁ¨¶ÂêàÈ¢ÑÊúüÁöÑÊ¶ÇÁéáË∂äÊù•Ë∂ä‰ΩéÔºåÊâÄ‰ª•ËæìÂá∫Â§™Â§ö‰πãÂêéÊ≤°Ê≥ï‰øùËØÅ‰∏çËµ∞ÂÅè
  2. ‰∏≠Èó¥Êüê‰∏Ä‰∏™‰ΩçÁΩÆÔºå‰∏§‰∏™tokenÁöÑÊ¶ÇÁéáÁõ∏ËøëÔºållmÈÄâÊã©‰∏çÂ∞èÂøÉËµ∞ÂÅè‰∫Ü‰πãÂêéÔºåÂêéÈù¢ËæìÂá∫ÁöÑtoken‰ºö‰∏ÄÁõ¥Âú®Ëøô‰∏™Ëµ∞ÂÅèÁöÑtoken‰∏äÁªßÁª≠ËæìÂá∫
  - Ë°•ÂÖÖ‰∏ÄÁÇπÔºåÁé∞Âú®LLMÊúâÊ∏©Â∫¶ËÆæÁΩÆÔºå‰∏çÊòØÁ∫ØË¥™ÂøÉÂèñÊúÄÂ§ßÊ¶ÇÁéáÁöÑ„ÄÇËÄå‰∏îÁÆóÂäõÊúâÈôêÔºåÊó†Ê≥ïÂÅöÂà∞ÂÖ®Â±ÄÊúÄ‰ºò„ÄÇÂè™ËÉΩËØ¥ÁØáÂπÖË∂äÁü≠ÔºåË∂äÂáÜÁ°Æ„ÄÇ Áé∞Âú®Â§ßÂÆ∂ÂÅöÁöÑCoTÔºå‰ª•ÂèäÂà©Áî®Agentic workflow‰∏çÊñ≠ÈáçÂÜôÔºåÂÖ∂ÂÆûÈÉΩÊòØÂú®ÈÄºËøëÂÖ®Â±ÄÊúÄ‰ºò„ÄÇ

- ‰∏™‰∫∫ÊÑüËßâÔºöGemini Âè∑Áß∞Êúâ 100 ‰∏áÁöÑ‰∏ä‰∏ãÊñáÔºåÂÖ∂ÂÆû‰∏ÄËà¨Ë∑ëÂà∞ 20-30 ‰∏áÂ≠óÂ∑¶Âè≥Â∞±‰∏çË°å‰∫Ü

- ËæìÂÖ•ÂíåËæìÂá∫ÂØπÊ®°ÂûãÊù•ËØ¥ÈÉΩÊòØ‰∏ÄÊ†∑ÁöÑÔºåÂè™Ë¶Å‰∏ä‰∏ãÊñáËøáÈïøÔºåÊ®°ÂûãÊàñÂ§öÊàñÂ∞ëÈÉΩ‰ºöÂá∫Áé∞Êåá‰ª§‰∏çË∑üÈöè„ÄÇ

- ## ‚è≥ [A Tribute to MetaAI and Stability AI - 2 Giants Who Brought us so Much Joy... And, 2025 is the Year they Die... So Sad! : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1p9ah2v/a_tribute_to_metaai_and_stability_ai_2_giants_who/)
- People think the trillion dollar spend is for the r&d and scale. Research is a fraction of the opex, and the scale is not about breaking through new model features but to suck in as much actual human usage data as possible to train exclusively better models.

- ## [China just passed the US in open model downloads for the first time : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p7alka/china_just_passed_the_us_in_open_model_downloads/)
- This analysis includes all models on HuggingFace, not just LLMs. The most downloaded models (by far) are smaller stuff like embedding models, classifiers, VAD, etc., and small base models like BERT and GPT-2. The most popular properly large instruction-tuned LLM is Qwen2.5-VL-3B-Instruct in 24th place.
  - I suspect a lot of downloads are coming from poorly configured CI/CD pipelines and stuff, rather than individual users

- I'm surprised this hasn't happened earlier, the only good open weight models from the US in the past like 6 months has been Gpt-oss.

- How tf Germany is third place?
  - They have Black Forest Labs
  - stable diffusion guys (now bfl aka flux) are from germany
- Sentence Transformers (SBERT) is originally German. Gets an ungodly number of downloads, and because the models are small people just grab it directly from HF instead of running a local mirror.
  - (Also LAION (CLAP, some popular CLIP-ViT models) and Stable Diffusion/Black Forest Labs (FLUX), though I don't think those get nearly as many downloads.)

- Chinese users predominantly go to modelscope

- ## üí∞ [How are Chinese AI models claiming such low training costs? Did some research : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p6cf2p/how_are_chinese_ai_models_claiming_such_low/)
  - deepseek claims $6M training cost. Everyones losing their minds cause ChatGPT-4 cost $40-80M and Gemini Ultra hit $190M.
  - glm-4.6: $8-12M estimated, 357B parameters (thats model size)
  - Kimi K2-0905: $25-35M estimated, 1T parameters total (MoE architecture, only ~32B active at once)
  - MiniMax: $15-20M estimated, Mid-range model, mid-range cost
  - üßÆ Training cost = GPU hours √ó GPU price + electricity + data costs.
  - ÁúüÂÆûÁöÑÊàêÊú¨Êï∞Â≠óÈÉΩÂ∫îËØ•Êé•Ëøë‰∏äÈù¢ÁöÑÂÖ¨Âºè
  - deepseeks $6M feels like marketing. You cant rent enough H100s for months and only spend $6M unless youre getting massive subsidies or cutting major corners.
  - glms $8-12M is more realistic. Still cheap compared to Western models but not suspiciously fake-cheap.
  - Kimi at $25-35M shows you CAN build competitive models for less than $100M+ but probably not for $6M.

- One difference could be the accounting methodology. I can for sure guarantee that not every training attempt is successful, and companies spend a fortune of gpu-hours on practice runs training smaller models; and then there might be a rollback or two to earlier checkpoints in the big run. Then imagine one company counting the entire cost, while the other accounts only for the end run, and boom - you got drastically different reported figures while effectively the same amount of spent money.
- In the paper for Deepseek they are actually never claiming 6 million - there saying at an assumed price per GPU hour (can‚Äôt remember from the top of my head) the final run would be around 6 million.
  - OpenAI also estimated GPT-OSS final training cost like this too iirc. They just didn't for other models.

- They were very transparent about this, and have stated multiple times that it was just the final training run in that estimate and explicitly did not include prior incremental runs.

- The costs listed are likely just the literal hardware cost for the final training run for the model. Every other aspect of the model training process is ignored.

- metas new ai team are getting 9 figure salaries lol makes it hard to profit when you‚Äôre paying people such insane salaries.

- Because they're leveraging Western frontier models. Let's be clear, the Chinese labs aren't doing any hard training. All they're really doing is distilling the hard work done by Western labs.
  - I remember it being reported that Western frontier models trained on copyrighted data (even pirated material).

- [Are Chinese AI models really that cheap to train? Did some research. : r/LLMDevs](https://www.reddit.com/r/LLMDevs/comments/1p77x5k/are_chinese_ai_models_really_that_cheap_to_train/)
  - It‚Äôs a combo of all those things, especially lying, plus they used a ton of distillation techniques - i.e. they trained a ton on the output of ChatGPT and other established models. 

- ## ÂÅö‰∫ßÂìÅÁ∫ßÊêúÁ¥¢‰ª•ÂêéÂèëÁé∞ 3.7 ÊïàÊûúÁâπÂ•Ω‰ΩÜÊòØÁâπÂà´Ë¥µÔºåÊâÄ‰ª•ÂøÖÈ°ªÂæóÂÅö content cache Èôç‰ΩéÊàêÊú¨„ÄÇ
- https://x.com/arvin17x/status/1896922111505285484
  - ‰ΩÜ‰∏∫‰∫ÜÂÅö context cache ÔºåÂèëÁé∞ÂøÖÈ°ªÂæóËÆ∞ÂΩï token usageÔºå‰∏çÁÑ∂ÊÑüÁü•‰∏çÂà∞ cache ÁöÑÊïàÊûú„ÄÇ

- ÂèØ‰ª•ÂàÜ‰∫´‰∏ã Context Cache ÁöÑÂéüÁêÜ‰πàÔºåÊàëËøò‰ª•‰∏∫ËøôÁßç‰∏úË•øÂè™ËÉΩÂú®Ê®°Âûã‰æõÂ∫îÂïÜ‰æßÂÅö„ÄÇ
  - Â∞±ÊòØÂú®Ê®°Âûã‰æõÂ∫îÂïÜ‰æßÂÅö„ÄÇÂè™ÊòØ OpenAI Âíå DeepSeek ÂÅöÁöÑÊòØÈùôÈªòÊñπÊ°àÔºåËÄå Anthropic Âíå Google ÊòØÈúÄË¶ÅÂºÄÂèëËÄÖÊâãÂä®ÂºÄÁöÑ
- Â∫îËØ•ÊòØ prompt caching Âêß
  - AnthropicÂÆ∂Ëá™Â∑±ÁöÑËØ¥Ê≥ïÊòØ prompt Caching. ‰ΩÜÊàëÊÑüËßâË°å‰∏öÈáåÊÑüËßâËøòÊòØ‰π†ÊÉØÂè´ context caching ÁöÑ„ÄÇ
  - ‰∏çËøá OpenAI ÂÆòÊñπÂíå Anthropic ÂÆòÊñπÈÉΩÁß∞‰πã‰∏∫ prompt caching

- ÂæàÂ•áÊÄ™ÔºåËøôÁé©ÊÑè‰∏∫Âï•‰∏çÈªòËÆ§ÂºÄÂêØ„ÄÇ
  - Êüê‰∫õÂú∫ÊôØ‰∏ãÁöÑÁ°Æ‰∏ç‰∏ÄÂÆöÈÄÇÂêàÔºåÂõ†‰∏∫ÂÜôÂÖ• cache ÁöÑÊàêÊú¨ÊòØÂéü‰ª∑ÁöÑ 1.25ÂÄç

- ## It is common to generate train and validation sets using random splitting.
- https://x.com/_avichawla/status/1898622288737767785
  - However, in many situations, it can be fatal for model building.
  - Consider building a model that generates captions for images.
  - Group shuffle split solves this.

- ## Áî±‰∫éDeepSeek-R1 ÁàÜÁÅ´ÔºåÊâÄ‰ª•‰∏∫Â§ßÂÆ∂Â∏¶Êù•‰ªÄ‰πàÊòØLLMËí∏È¶èÊäÄÊúØÁöÑÁ¨îËÆ∞„ÄÇ
- https://x.com/karminski3/status/1882233538042597423
  - Âá†‰∏™Âä©ËÆ∞ËØçÔºöÊïôÂ∏àÊ®°ÂûãÔºåÂ≠¶ÁîüÊ®°ÂûãÔºåËΩØÁõÆÊ†áÔºåÁ°¨ÁõÆÊ†á„ÄÇ

- https://x.com/ShanghaoJin/status/1882679738789216456
  - ‰Ω†Âê¨ËØ¥Ëøá‰ªÄ‰πàÂè´‚ÄúËí∏È¶è‚Äù‰πàÔºüËØ¥‰∏™Â§ßÁôΩËØùÔºöÂ∞±ÊòØÊãø‰∫∫ÂÆ∂ÁÆóÂá∫Êù•ÁöÑÊ®°ÂûãÂèÇÊï∞ÔºåË∑≥ËøáÊâÄÊúâÊï∞ÊçÆÊ∏ÖÊ¥ó„ÄÅËÆ≠ÁªÉÔºåÂÅöÊúÄÂêé‰∏ÄÁ®ã„ÄÇÂÖ∂ÂÆûÊ≤°Êúâ‰ªª‰ΩïÂàõÊñ∞
  - Â•ΩÂÉè‰∫∫ÂÆ∂ËØÅÊòé‰∫ÜœÄ=3.14Ôºå‰ªñÊãøÁªìÊûúÂéªÁÆó‰∫ÜÂúÜÈù¢ÁßØ„ÄÇËÆ©‰ªñÂÜçËá™Â∑±ÂéªËØÅÊòéÁÆó‰∏Ä‰∏™eÔºå‰ªñÂèàÊäìÁûé‰∫Ü
- ÂçÉ‰∏á‰∏çË¶ÅÁî®‚ÄúÂ§ßÁôΩËØù‚ÄùÊù•Ëß£ÈáäËá™Â∑±ÈÉΩÊ≤°ÂÆåÂÖ®ÁêÜËß£ÁöÑÊ¶ÇÂøµÔºå Âè™ËÉΩËÆ©Â§ñË°åÊãçÊâãÔºåÊáÇÁöÑ‰∫∫Âè™‰ºöÁ¨ëËØù‰Ω†„ÄÇ ‰Ω†ÂÆåÂÖ®‰∏çÁü•ÈÅìËí∏È¶èÊòØÂú®Âπ≤‰ªÄ‰πà„ÄÇÂ¶ÇÊûú‰Ω†Áü•ÈÅìÁöÑËØùÔºåÈÇ£Â∞±ÊòØÂÆåÂÖ®‰∏çÁü•ÈÅìDeepseek Âú®ÂÅö‰ªÄ‰πà„ÄÇ
- ‚ÄúËí∏È¶è‚ÄùËØ¥Ê≥ï‰∏çÊ≠£Á°Æ„ÄÇ1. Ëí∏È¶èÊïàÊûú‰∏ÄËà¨‰∏ç‰ºöË∂ÖËøáÂéüÊ®°Âûã 2. deepseekÁöÑ reasoningË°å‰∏∫ÂíåÂ∏ÇÈù¢‰∏äÂÖ∂‰ªñÊ®°Âûã‰∏ç‰∏ÄËá¥(ÊúâË∂ÖË∂ä‰∫∫Á±ªÊ†áÊ≥®ÁöÑÂ•áÂ¶ôË°å‰∏∫) 3. ÂºÄÊô∫ÂØπÂÜçËÆ≠ÁªÉÊ®°ÂûãÊúâÁ¶ÅÊ≠¢Âπ∂‰ºöÁõëÊéß APIÊª•Áî®

- ## ÊàëÊó•Â∏∏Áî® Cursor ÂÜô‰ª£Á†ÅÁöÑÂú∫ÊôØ‰πã‰∏ÄÔºö‚ÄúËØ∑ÂèÇËÄÉ‰ª£Á†Å @ XXX1 @ XXXn ÂÅö YYY ‰∫ã„ÄÇ‚Äù
- https://x.com/dotey/status/1869436413600731146
  - ÁÆÄÂçïÊù•ËØ¥Â∞±ÊòØËÆ© AI ÁÖßËë´Ëä¶ÁîªÁì¢ÔºåÈáçË¶ÅÁöÑÊòØÁªôÂá∫ÂÖÖË∂≥ÁöÑ‰∏ä‰∏ãÊñáÔºåËÆ© AI ÂèØ‰ª•Â≠¶‰π†ÂíåÊ®°‰ªø„ÄÇÂâ©‰∏ãÁöÑÂ∞±ÊòØ Review + AcceptÔºåÂæàÁÆÄÂçïÈ´òÊïà„ÄÇ
  - ÁâπÂà´Ë¶ÅÊ≥®ÊÑèÁöÑÊòØÁ¨¨‰∏Ä‰∏™‚ÄúËë´Ëä¶‚ÄùË¶ÅÊâìÁ£®Â•ΩÔºåËøôÊ†∑ÂêéÁª≠ÁöÑ‚ÄúÁì¢‚ÄùÊâç‰∏ç‰ºöÁîªÊ≠™„ÄÇ

- Êõ¥ÁÆÄÂçïÁöÑÂÅöÊ≥ïÊúâÊó∂ÂÄôÂèØ‰ª•Áõ¥Êé• @ git ÊüêÊ¨°Êèê‰∫§

- ÊàëÁé∞Âú®ÊòØÊñ∞È°πÁõÆÈáåÂàõÂª∫‰∏Ä‰∏™txtÊñá‰ª∂ÔºåÈáåÈù¢ÂÜô‰∏äÊÉ≥Ê≥ïÂíågptÂØπÊàëÊÉ≥Ê≥ïÁöÑÂª∫ËÆÆÔºåÁÑ∂ÂêéËÆ©cursor ÂèÇËÄÉËøô‰∏™Êñá‰ª∂Êù•ÂºÄÂèëÔºåÈáåÈù¢Êàë‰πüÊúâÊó∂‰ºöÂÜô‰∏äÊ≠•È™§ÔºåÈ¶ñÂÖàÂÆûÁé∞‰ªÄ‰πàÔºåÁÑ∂ÂêéÂÆûÁé∞‰ªÄ‰πàÔºåÂÅö‰∏ÄÊÆµ‰∫ÜÔºåËÆ©cursorÊ†πÊçÆËøô‰∏™Êñá‰ª∂Ê£ÄÊü•‰∏Ä‰∏ãÈ°πÁõÆÂÆåÊàêÂ∫¶ÔºåÂàóÂá∫Êù•Âì™‰∫õÊ≤°ÂÅöÔºåËøôÊ†∑ÂèçÂ§çËø≠‰ª£ÂêëÂâç

- LM ÂÖÖÂàÜËØÅÊòé‰∫Ü‰∫∫Á±ªÁöÑÊú¨Ë¥®Â∞±ÊòØÂ§çËØªÊú∫„ÄÇ Âì™ÈáåÊúâ‰ªÄ‰πàÊåá‰ª§ÈÅµÂæ™ÔºåÊé®ÁêÜÔºåÂ§ßÂÆ∂ÈÉΩÊòØ‰ªé‰∏çÂêåÁöÑÁª¥Â∫¶Áî®‰∏çÂêåÁöÑÊñπÂºèÂú®Â§çËØª‰∏Ä‰∫õ‰∏úË•øËÄåÂ∑≤

- ## Â¶ÇÊûúÊÉ≥Ë¶ÅËÆ© LLM Á®≥ÂÆöÁîüÊàê JSON ÂØπË±°ÔºåÊúÄÁÆÄÂçïÁöÑÊñπÂºèÂ∞±ÊòØ‰ΩøÁî® zod ÂÆö‰πâ schema Âπ∂ÈÖçÂêà @vercel ai sdkÁöÑ generateObject‰ΩøÁî®ÔºåÊØîÂ¶ÇËøôÈáåÊàëÊÉ≥Ë¶Å‰ªéÁΩëÈ°µÊñáÊú¨ÂÜÖÂÆπÊèêÂèñÁªìÊûÑÂåñÁöÑ‰ø°ÊÅØ„ÄÇ
- https://x.com/FeigelC35583/status/1819558128297648412
  - ËøôÁßçÊñπÂºèÂíåÂΩìÂàù langchain Âú® prompt ÈáåÂÜô‰∏ÄÂ§ßÂ†Üjson ÂÆö‰πâÊúâÊú¨Ë¥®Âå∫Âà´ÔºåÂú®‰∫é‰ΩøÁî®‰∫Ü function call ÁöÑËÉΩÂäõ
  - ‰ªéËØ∑Ê±Ç‰∏≠ÂèØ‰ª•ÁúãÂà∞ÔºåÊú¨Ë¥®‰∏äÊòØÂú®Ë∞ÉÁî®Ê®°ÂûãÁöÑÊó∂ÂÄôÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫ json ÁöÑ ÂáΩÊï∞, ÊèèËø∞ÊòØ respond with a json object, ÂÖ∂‰∏≠ÂèÇÊï∞ÊòØËá™Â∑±ÂÆö‰πâÁöÑ schemaÔºåÁÑ∂ÂêéÂú® tool_choice ‰∏≠ÈôêÂà∂ÂøÖÈ°ªË¶Å‰ΩøÁî®Ëøô‰∏™ json ÂáΩÊï∞ÔºåÈÇ£‰πàÊ®°ÂûãÂ∞±‰ºöËøîÂõûË∞ÉÁî®json ÂáΩÊï∞ÁöÑÂèÇÊï∞ÔºåÂç≥‰Ω†ÂÆö‰πâÁöÑ schema
  - Á§∫‰æã‰ª£Á†ÅÊù•Ëá™‰∫éhttps://github.com/DiscovAI/DiscovAI-crawl ÊàëÊ≠£Âú® building ÁöÑ‰∏Ä‰∏™Èù¢Âêë RAG Â∫îÁî®ÁöÑÁà¨Ëô´Âπ≥Âè∞
- Â∫îËØ•Âè™ÊúâGPTÁ≥ªÂàóËÉΩÁî®Âêß
  - ÊîØÊåÅfunction callÂ∞±ÂèØ‰ª•ÔºådeepseekÂ∫îËØ•‰πüÂèØ‰ª•ÁöÑ
- Âú®ËøôÂü∫Á°Ä‰∏ä„ÄÇÊàë‰ºöËÄÉËôë‰ΩøÁî®jsonrepairËøô‰∏™ÂåÖÔºåÊâãÂä®‰øÆÂ§ç‰∏ãÔºåÂ¢ûÂä†ÂÆπÈîô
- Â¶ÇÊûúÂ§ßÊ®°ÂûãÊ≤°ÊúâÊ≤°ÊúâËøîÂõûÂØπÂ∫îË¶ÅÊ±ÇÁöÑÂ≠óÊÆµÊï∞ÊçÆÔºåÊàñËÄÖËøîÂõûÈîô‰∫ÜÁ±ªÂûãÔºåÂÆÉ‰ºöÊÄé‰πàÊ†∑Ôºå‰ºöËá™Â∑±Ë°•ÂÖÖÁ©∫ÁöÑÔºåÊàñËÄÖËá™Âä®ËΩ¨Êç¢Á±ªÂûãÂêóÔºü
  - ‰∏ç‰ºöË°•ÂÖÖÔºå‰ºöthrow errorÔºå‰πüÂèØ‰ª•Áî®‰∏äÈù¢Êé®ÂèãÊé®ËçêÁöÑjsonrepairÊâãÂä®fix

- ËÉΩÊîØÊåÅÂºÄÊ∫êÊ®°ÂûãÂêó
  - ÂèñÂÜ≥‰∫éÊ®°ÂûãÊîØ‰∏çÊîØÊåÅfunction callÔºåÊîØÊåÅÁöÑËØùÂ∞±ÂèØ‰ª•ÔºåÊïàÊûúÁöÑËØùË¶ÅÁúãÊ®°ÂûãÁöÑËÉΩÂäõ
- Áî® function call ÊÑüËßâÊ®°ÂûãÁöÑËÉΩÂäõÈôç‰∫Ü‰∏Ä‰∏™Áª¥Â∫¶Ôºå‰∏çÂ¶ÇÁõ¥Êé•ÁªôÊñáÊú¨ÔºåÊàëËøòÊòØÊõ¥ÂñúÊ¨¢Áî®xmlËá™Â∑±ÊèêÂèñ„ÄÇ

- ÊàëÊòØÁî®‰º™‰ª£Á†Å‚ûïÁ±ªÂûãÂ£∞Êòé, ‰πüÊòØ‰∏ÄÊ†∑ÁöÑÁ®≥ÂÆöËæìÂá∫ json
- langchainÊ°ÜÊû∂‰∏≠ÊúâPydantic json Ëß£ÊûêÂô®ÂèØ‰ª•Áõ¥Êé•Áî®ÔºåÊú¨Ë¥®‰πüÊòØÁîüÊàêschemaÔºåÂÜçÈÖçÂêàÈáçËØïËß£ÊûêÂô®‰πüÂèØ‰ª•Á®≥ÂÆöÁîüÊàêjsonÊ†ºÂºè

- ## üí° LLMs are literally the most unreliable technology of all time (followed by **ing bluetooth)
- https://x.com/Steve8708/status/1819448686424084892
  - After an absurd amount of trial and error, we've internally created a set of rules for make LLMs considerably more reliable
  - our secrets: restrict the llm to only what rag provides

- what's your stance on AI for no-code? Do people prefer drag-and-drop vs prompting?
  - i think the winning move is combining both

- Bluetooth is hell and causes frustration daily.

- ## üå∞ Firefox will use Transformers.js to power on-device features
- https://x.com/osanseviero/status/1797291569348751848
  - In their PDF Editor to generate alt text for images
  - Improve translations
  - Fully offline, open-source and with <200M models
- [Experimenting with local alt text generation in Firefox Nightly - Mozilla Hacks - the Web developer blog _202405](https://hacks.mozilla.org/2024/05/experimenting-with-local-alt-text-generation-in-firefox-nightly/)
  - https://huggingface.co/Mozilla

    - Êèê‰æõ‰∫ÜÊï∞ÊçÆÈõÜÂíåÊ®°Âûã

- Offline and open-source is a big win for privacy-focused tools

- ## [langchainÂà∞Â∫ïËØ•ÊÄé‰πà‰ΩøÁî®ÔºåÂ§ßÂÆ∂Âú®È°πÁõÆ‰∏≠ÂÆûË∑µÊúâÊàêÂäüÁöÑÊ°à‰æãÂêó? - Áü•‰πé](https://www.zhihu.com/question/609483833)
- LangChain‰πãÊâÄ‰ª•Â§ßÁÅ´ÔºåÊòØÂõ†‰∏∫ÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÁ≥ªÂàóÊñπ‰æøÁöÑÂ∑•ÂÖ∑„ÄÅÁªÑ‰ª∂ÂíåÊé•Âè£ÔºåÂ§ßÂ§ßÈôç‰Ωé‰∫Ü AI Â∫îÁî®ÂºÄÂèëÁöÑÈó®ÊßõÔºå‰πüÊûÅÂ§ßÁÆÄÂåñ‰∫ÜÂ§ßÊ®°ÂûãÂ∫îÁî®Á®ãÂ∫èÁöÑÂºÄÂèëËøáÁ®ã„ÄÇ
  - LangChainÊ°ÜÊû∂ËÉåÂêéÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∫èÂàóÂàÜËß£‰∏∫ÂêÑ‰∏™ÈÉ®ÂàÜÔºåÂÖÅËÆ∏ÂºÄÂèë‰∫∫ÂëòÊ†πÊçÆËá™Â∑±ÁöÑÈúÄÊ±ÇÈ´òÊïàÂú∞ÂÆöÂà∂Â∑•‰ΩúÊµÅÁ®ã„ÄÇ
- LangchainÊúâ6Â§ßÊ†∏ÂøÉÊ®°ÂùóÔºö
  - ModelsÔºöÊ®°ÂûãÔºåÊòØÂêÑÁßçÁ±ªÂûãÁöÑÊ®°ÂûãÂíåÊ®°ÂûãÈõÜÊàê„ÄÇ
  - PromptsÔºöÊèêÁ§∫ÔºåÂåÖÊã¨ÊèêÁ§∫ÁÆ°ÁêÜ„ÄÅÊèêÁ§∫‰ºòÂåñÂíåÊèêÁ§∫Â∫èÂàóÂåñ„ÄÇ
  - MemoryÔºöËÆ∞ÂøÜÔºåÁî®Êù•‰øùÂ≠òÂíåÊ®°Âûã‰∫§‰∫íÊó∂ÁöÑ‰∏ä‰∏ãÊñáÁä∂ÊÄÅ„ÄÇ
  - IndexesÔºöÁ¥¢ÂºïÔºåÁî®Êù•ÁªìÊûÑÂåñÊñáÊ°£Ôºå‰ª•‰æøÂíåÊ®°Âûã‰∫§‰∫í„ÄÇÂåÖÊã¨ÊñáÊ°£Âä†ËΩΩÁ®ãÂ∫è„ÄÅÂêëÈáèÂ≠òÂÇ®Âô®„ÄÅÊñáÊú¨ÂàÜÂâ≤Âô®ÂíåÊ£ÄÁ¥¢Âô®Á≠â„ÄÇ
  - AgentsÔºö‰ª£ÁêÜÔºåÂÜ≥ÂÆöÊ®°ÂûãÈááÂèñÂì™‰∫õË°åÂä®ÔºåÊâßË°åÂπ∂‰∏îËßÇÂØüÊµÅÁ®ãÔºåÁõ¥Âà∞ÂÆåÊàê‰∏∫Ê≠¢„ÄÇ
  - ChainsÔºöÈìæÔºå‰∏ÄÁ≥ªÂàóÂØπÂêÑÁßçÁªÑ‰ª∂ÁöÑË∞ÉÁî®„ÄÇ
- LangChain ÈÄöÂ∏∏Ë¢´Áî®‰Ωú„ÄåÁ≤òÂêàÂâÇ„ÄçÔºåÂ∞ÜÊûÑÂª∫ LLM Â∫îÁî®ÊâÄÈúÄÁöÑÂêÑ‰∏™Ê®°ÂùóËøûÊé•Âú®‰∏ÄËµ∑„ÄÇ‰ΩøÁî®Langchain‰∏≠‰∏çÂêåÁªÑ‰ª∂ÁöÑÁâπÊÄßÂíåËÉΩÂäõÔºåÂèØ‰ª•ÊûÑÂª∫‰∏çÂêåÂú∫ÊôØ‰∏ãÁöÑÂ∫îÁî®ÔºåÂ¶ÇËÅäÂ§©Êú∫Âô®‰∫∫„ÄÅÂü∫‰∫éÊñáÊ°£ÁöÑÈóÆÁ≠î„ÄÅÁü•ËØÜÁÆ°ÁêÜ„ÄÅ‰∏™‰∫∫Âä©ÁêÜ„ÄÅAgentÊô∫ËÉΩ‰ΩìÁ≠âÁ≠â„ÄÇ

- ‰Ω†ÁöÑËøô‰∏™ËÆ§ËØÜÂ≠òÂú®‰∏Ä‰∫õÂÅèÂ∑ÆÔºåÈ¶ñÂÖàÔºå‰æùËµñAPI key ÊòØ‰∏∫‰∫Ü‰Ω†‰ΩøÁî®Â§ßÊ®°ÂûãÂéÇÂïÜÁöÑÊúçÂä°ÂíåÈâ¥ÊùÉÔºåËøôÊ≤°Êúâ‰ªÄ‰πàÊãâË∑®ÁöÑ„ÄÇÂæàÂ§öÁ¨¨‰∏âÊñπÁöÑÊúçÂä°ÈÉΩÈúÄË¶ÅÈâ¥ÊùÉÈ™åËØÅÔºåËøôÊòØÊØîËæÉ‰∏ªÊµÅÁöÑÊñπÂºè„ÄÇ
- ÂèØ‰ª•‰ºÅ‰∏öËá™Â∑±ÈÉ®ÁΩ≤Â§ßÊ®°ÂûãÔºåËøôÁßçÊàêÊú¨ÊòØÂæàÈ´òÁöÑ„ÄÇ‰ªéÊàë‰ª¨Ëá™Â∑±ÁöÑÂÆûÈ™åÊïàÊûúÊù•ÁúãÔºå13B ‰ª•‰∏ãÁöÑÂ§ßÊ®°ÂûãÂü∫Êú¨Â∞±ÊòØÁé©ÂÖ∑Ôºå‰ºòÂåñÂçäÂ§©Ë¥πÊó∂Ë¥πÂäõÔºåËÄå 34B ÊàñËÄÖÊõ¥Â§ßÁöÑÊ®°ÂûãÔºåÂÖ¨Âè∏ÈÉ®ÁΩ≤ÊàêÊú¨ÂèàÂæàÈ´ò„ÄÇ
- langchain ‰∏≠ÁöÑÁâπËâ≤ÊòØÂÆÉÁöÑ langchain expression language (LCELÔºâÔºåÊòØ‰∏ÄÁßçÁ±ª‰ºº linux ÁÆ°ÈÅìÂΩ¢ÂºèÁöÑË∞ÉÁî®ÊñπÂºèÔºåÂèØ‰ª•ÂæàÁÆÄÂçïÁöÑÂÆûÁé∞ÂÆÉÁöÑ chain Áõ∏ÂÖ≥ÁöÑÂäüËÉΩ„ÄÇËøô‰∏™ÔºåÂú®ÊàëÂÆûÈôÖ‰ΩøÁî®ÁöÑÊó∂ÂÄôÔºåÊ≤°ÊúâÊÉ≥Ë±°ÁöÑÈÇ£‰πàÂ•ΩÁî®ÔºåÂèØ‰ª•Ê†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂéªÂ≠¶‰π†„ÄÇ
- ÊúÄÂêéÔºålangchain ‰∏≠ËøòÊúâ‰∏Ä‰∏™Âè´ÂÅö langgraph ÁöÑÁªÑ‰ª∂ÔºåËÉΩÂ§üÂíå pytorch ‰∏ÄÊ†∑Áî®Êê≠ÁßØÊú®ÁöÑÊñπÂºèÂéªÊûÑÈÄ†‰∏Ä‰∏™ÊúâÂêëÊó†ÁéØÂõæ„ÄÅÂæ™ÁéØÁöÑÈìæÔºåÊØî LCEL Êõ¥È´òÁ∫ß„ÄÇ

- 
- 

- ## LLMÊêûÂèçÁºñËØëÔºå.not careÂíåJvavÁî®Êà∑ÂÜç‰πü‰∏çÁî®ÊäòËÖæ‰ªÄ‰πàÊ∑∑Ê∑Ü‰∫ÜÔºåÈÉΩÊ≤°Áî®‰∫Ü
- https://twitter.com/geniusvczh/status/1774053196039962758
  - ÊñáÁ´†ÈáåÂèçÁºñËØëÁöÑÊòØx86, x86ÈÉΩÂèØ‰ª•ÔºåILÈöæÂ∫¶Âè™‰ºöÊõ¥‰Ωé
- Â§ßÊ¶ÇÁúã‰∫Ü‰∏Ä‰∏ãÔºåÂ∞±ÊòØÊääÁºñËØëÂá∫ÁöÑÊ±áÁºñË∑üÊ∫ê‰ª£Á†ÅÂÅö‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑseq2seqÁöÑfine tuneÔºåËÆ≠ÁªÉÈõÜËøûÊ∑∑Ê∑ÜÈÉΩÊ≤°ÊúâÔºåÁ¶ªËÆ©ÊâÄÊúâÊ∑∑Ê∑ÜÈÉΩÊ≤°Áî®ÈÇ£Êõ¥ÊòØËøòÂ∑ÆÂæóËøú„ÄÇ
- 17Âπ¥googleÈÇ£ÁØátransformerÁöÑËÆ∫ÊñáÂ∞±Èù†ËøôÊ†∑ÂÆåÊàê‰∫ÜËá™ÁÑ∂ËØ≠Ë®ÄÁöÑÁøªËØëÔºåËøô‰∫õÈÉΩÊòØËøüÊó©ÁöÑ‰∫ãÔºåÂèçÁºñËØëÂíåÂèçÊ∑∑Ê∑ÜÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈÉΩÊòØÂèØ‰ª•ÊâπÈáèÁîüÊàêÁöÑÔºåÂÅöËµ∑Êù•ÁÆÄÂçïÂ§ö‰∫Ü
  - ÊàëËßâÂæóLLMÂØπ‰∫éÂèçÁºñËØëÂíåÂèçÊ∑∑Ê∑ÜÔºåÂèØËÉΩÊõ¥Â§ßÁöÑ‰ΩúÁî®Âú®‰∫éÁîüÊàê‰∫∫Á±ªÂèãÂ•ΩÁöÑÂèòÈáè/Á®ãÂ∫èÁªìÊûÑ„ÄÇÊØïÁ´üÂèçÁºñËØëÂíåÂèçÊ∑∑Ê∑ÜÊòØÁå´Èº†Ê∏∏ÊàèÔºåÊÄªÂèØ‰ª•ÊÉ≥Âá∫Êñ∞ÁÇπÂ≠êÔºå‰∫∫Á±ªÁöÑÂπ≤È¢ÑËøòÊòØÂøÖ‰∏çÂèØÂ∞ëÁöÑÔºåËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÂü∫‰∫éËßÑÂàô/Á®ãÂ∫èÂàÜÊûêÁöÑ‰º†ÁªüÊñπÊ≥ïÂèØËÉΩÊõ¥Â•ΩÔºåÁÑ∂ÂêéÂÜçÁî®LLMÁåúÂèòÈáèÂêç
- ‰∏∫‰∫ÜÊãâËµÑÈáëËÄåÂ∑≤ÔºåÈí±Áî≥ËØ∑Âà∞‰∫ÜËÆ∫ÊñáÂ∞±Ê≤°Âï•Áî®‰∫Ü‚Ä¶‚Ä¶Â§ÑÁêÜÂ±éÂ±±ÁïôÁªôÂ∑®Â§¥ÁöÑÁ®ãÂ∫èÂëòÂ∞±Ë°å‰∫ÜÔºåËøòËΩÆ‰∏çÂà∞Â≠¶ÊúØÂúàÊù•ÊåáÁÇπÊ±üÂ±±
- ËøôÁßçÂ±ÄÈôê‰∫éÂáΩÊï∞ÁöÑÂèçÊ∑∑Ê∑ÜÂï•Áî®ÈÉΩÊ≤°ÊúâÔºåÂØπ‰ªòÁÇπ‰∏âËÑöÁå´ÂäüÂ§´ÁöÑÊ∑∑Ê∑ÜËøòÂ∑Æ‰∏çÂ§ö

- ## ü™ß Á†îÁ©∂‰∫Ü‰∏Ä‰∏ãÊú¨Âú∞Â§ßÊ®°ÂûãÁöÑÂú∫ÊôØÔºö
- https://twitter.com/changmingY/status/1773336179296887162
  1. ‰∏çËÉΩËÅîÁΩëÁöÑÂõΩÂÜÖÁî®Êà∑
  2. ‰∏ÄËà¨Áî®Êà∑Êú∫Âô®ÈÖçÁΩÆËææ‰∏çÂà∞ÔºåÊïàÁéáÂ§™Â∑Æ
  3. Êú¨Âú∞Áü•ËØÜÂ∫ìÁÆóÊòØ‰∏Ä‰∏™ÂàöÊÄßÈúÄÊ±Ç
  4. ÂûÇÁõ¥È¢ÜÂüüÊ®°ÂûãË∂äÊù•Ë∂äÂ§ö, ‰∏Ä‰∏™hubÈõÜ‰∏≠‰ΩøÁî®
  5. Â∞èËÄåÁæéÁöÑÊ®°Âûã‰ºöË∂äÊù•Ë∂äÂ§öÔºåÂÆåÊàê‰∏Ä‰∏™ÁâπÂÆöÂäüËÉΩ

- ## ollama ÁöÑÁºñËØëÁé©ÁöÑÂ§™Ëä±‰∫ÜÔºåÂÖàÊòØÂêß llama.cpp Âú®‰∏çÂêå cpu Âíå gpu ÁöÑÂä®ÊÄÅÈìæÊé•Â∫ìÈÉΩÁºñËØë‰∫ÜÂá∫Êù•ÈÅøÂÖçÁî®Êà∑Âú®ËøêË°åÊó∂ÂÜçÂéªÁºñËØëÔºå
- https://twitter.com/liumengxinfly/status/1767073319956971891
  - ÁÑ∂ÂêéÁî® go ÁöÑ embed ÁâπÊÄßÁõ¥Êé•ÊääËøô‰∫õÂä®ÊÄÅÂ∫ìÂÖ®ÈÉΩÊâìÂåÖÂà∞ go ÁöÑ‰∫åËøõÂà∂ÈáåÔºåÁÑ∂ÂêéÂú®Áî® cgo Âíå dlfcn Âä†ËΩΩÂíåË∞ÉÁî® llama.cppÔºåÂÆûÁé∞‰∫Ü‰∏Ä‰∏™‰∫åËøõÂà∂Êñá‰ª∂ÂÖçÁºñËØëÔºåÂÖçÂÆâË£ÖÁöÑËß£ÂÜ≥ÊâÄÊúâÈóÆÈ¢ò

- https://twitter.com/holegots/status/1767427148506431665
  - ‰∏çËøáËøô‰∏™Êú¨Ë¥®‰πüÊòØ llama.cpp Â•óÂ£≥Âêß , Â∫ïÂ±ÇËøòÊòØ cpp, golang Âπ∂‰∏çÂèÇ‰∏éÂÆûÈôÖÁöÑÊé®ÁêÜ.

- ## ÊúÄÊñ∞ÁâàÁöÑ OpenAI Translator Â∑≤ÁªèÊó†ÁºùÊîØÊåÅÊú¨Âú∞Â§ßÊ®°Âûã‰∫ÜÔºàOllamaÔºâÔºåÊó†ÈúÄËÅîÁΩëÔºåÂø´ÈÄü‰æøÊç∑ÔºåÂÆâÂÖ®Á®≥ÂÆöÔºÅÂÜç‰πü‰∏çÊÄï OpenAI Ë¥¶Âè∑Ë¢´Â∞Å‰∫ÜÔºÅÁøªËØëÊïàÊûúÂØπÊØîÂ§ßÂÆ∂ÂèØ‰ª•Áúã‰∏Ä‰∏ãÊà™ÂõæÔºåÂ§ßÂÆ∂Âø´Êù•‰∏ãËΩΩ‰ΩìÈ™å‰∏Ä‰∏ãÂêßÔºÅ _202402
- https://twitter.com/yetone/status/1761607398819840511
- Áé∞Âú®Â•ΩÂÉèËøò‰∏çÊîØÊåÅËá™ÂÆö‰πâÊ®°ÂûãÔºüÂè™ÊúâÊúâÈôêÁöÑÂá†‰∏™Ê®°ÂûãÂèØ‰æõÈÄâÊã©ÔºåÊúÄÂ•ΩÊòØÊúâ‰∏Ä‰∏™ÊñáÊú¨Ê°ÜÂèØ‰ª•Ëá™ÂÆö‰πâËæìÂÖ•
- ËøôÊòØMistralÂ§öÂ§ßÁöÑÊ®°ÂûãÔºå7BÁöÑÂêóÔºü
  - ÊòØÁöÑ

- ‰∏çÁü•ÈÅìËøô‰∫õ7b 13bÁöÑÂ∞èÊ®°ÂûãÂì™‰∏™ÁøªËØëË¥®ÈáèÊõ¥È´ò

- ## ÈòøÈáå‰∫ëÁ´üÁÑ∂ÊîØÊåÅËøô‰πàÂ§öÊ®°Âûã‰∫Ü
- https://twitter.com/yihong0618/status/1746745371441967540
- http://ai.azure‰πüÂåÖÂê´‰∫ÜÂ•ΩÂ§öÊ®°ÂûãÔºåÊò®Â§©ÊÉäÂà∞‰∫Ü

- ## Ë∂äÊù•Ë∂äËßâÂæó RAG Ëøô‰∏úË•øÊúâÊÑèÊÄù„ÄÇ
- https://twitter.com/wwwgoubuli/status/1737471851654160548
  - ÂçäÂπ¥ÂâçÊé•Ëß¶Âà∞Ëøô‰∏™ËØçÁöÑÊó∂ÂÄôÂºÄÂßãÊàëËøòÊúâ‰∫õ‰∏çÂ±ëÔºåÊêúÁ¥¢ÂÜÖÂÆπÊèíÂÖ•Âà∞ÊèêÁ§∫ËØçÁÆó‰ªÄ‰πàÂòõÔºåÂ∞èÂ≠¶‰∫åÂπ¥Á∫ßÈÉΩËÉΩÊòéÁôΩ„ÄÇÂ∞§ÂÖ∂ÊòØÁúãÂà∞Èöè‰æø‰∏¢ÂêëÈáèÂ∫ìÈÉΩËÉΩË∑ëÂá∫‰∏™‰∏É‰∏ÉÂÖ´ÂÖ´ÔºåË∂äÂèëËßâÂæóËøô‰∏™ÁÆÄÂçï„ÄÇ
  - ‰ΩÜÁé∞Âú®ÁúüÁöÑÊêû‰∫ÜÂçäÂπ¥ÔºåÊàëË∂äÂèëÁöÑËßâÂæóËøôÊâçÊòØ‰∏ã‰∏Ä‰∏™Â§ßÂ§öÊï∞‰∫∫ÂèØ‰ª•ÂèÇ‰∏éÁöÑÈ£éÂè£„ÄÇÂÆÉÊúâÈó®Êßõ„ÄÇ
- ÊäÄÂ∑ßÂæàÂ§ö ÊâÄ‰ª•Â•ΩÁé© ‰ΩÜÈ£éÈô©ÊòØÂ§ßÈÉ®‰ªΩÊäÄÂ∑ßÈÉΩË¢´Ê®°ÂûãÊèê‰æõÂïÜÁé©ËøáÔºå80%ÈúÄÊ±ÇÈÉΩÂèØËÉΩË¢´‰ªñ‰ª¨Áõ¥Êé•Ë¶ÜÁõñ
  - RAG‰∏çÂ∞±ÊòØquery transformation/rewrite/expanding, hybrid search, reranking, etcÂêóÔºüÂΩìÁÑ∂ËøòÊúâ‰∫õÂÖ∂‰ªñÊäÄÂ∑ßÂï•IAG‰πãÁ±ªÁöÑ„ÄÇÊï∞ÊçÆingestion‰πüÊúâ‰∫õÊäÄÂ∑ßÔºå‰∏çËøáÊàëÁúã‰∏ªË¶ÅËøòÊòØÂú®query‰∏ä„ÄÇ Ëøô‰∫õÂ§ßÈÉ®ÂàÜOAI, Baichuan, Êúà‰πãÊöóÈù¢ÂÜÖÈÉ®ÈÉΩÊé¢Á¥¢Ëøá‰∫ÜÂêß
- RAG‰∏ÄÁúãÂ∞±ÊòØ‰∏Ä‰∏™ÊúâÈóÆÈ¢òÁöÑÂå∫ÂüüÔºåÂ§ßÊ®°ÂûãÈöèÊó∂‰∏ã‰∏ÄÊ¨°ÂçáÁ∫ßÂèØËÉΩÂ∞±‰ºöÊîπÂèòÊï¥‰∏™Ê°ÜÊû∂Ôºå3.5ËøòËÉ°ËØ¥ÂÖ´ÈÅìÔºå4Â∑≤ÁªèÂæàÂ§öÈÉΩÊòØÊúâÊ†πÊúâÊçÆÁöÑ‰∫Ü
- ÊêûÂà∞ÊúÄÂêéÔºåËøòÊòØÊ∏ÖÊ¥óÊï∞ÊçÆÔºåRAGÂè™Áî®ÁÆÄÂçïÁ≠ñÁï•Ëß£ÂÜ≥Â§ßÂ§öÊï∞ÈóÆÈ¢òÔºåÂèØËßÇÊµã„ÄÇÂâçÊèêÊòØÊâÄÊúâÂ§çÊùÇÁ≠ñÁï•ÈÉΩË¶ÅËØïËøáÊâçÁü•ÈÅì„ÄÇ

- ## LangChainÂºÄÊ∫ê‰∫ÜAnythingLLMÔºöÂèØ‰ª•‰∏é‰ªª‰ΩïÂÜÖÂÆπËÅäÂ§©ÁöÑÁßÅ‰∫∫ ChatGPTÔºåÂ∫îËØ•Â∞±ÊòØ‰ªñ‰ª¨Ëá™Â∑±ÊñáÊ°£Á≥ªÁªüÁî®ÁöÑÈÇ£‰∏ÄÂ•ó„ÄÇ
- https://twitter.com/op7418/status/1733893368974073873
  - An efficient, customizable, and open-source enterprise-ready document chatbot solution.
  - https://github.com/Mintplex-Labs/anything-llm /MIT/js/python

- ÊúâÊ≤°ÊúâËØ¶ÁªÜËØ¥ÊòéÔºüÊúÄÂ§ßÂèØ‰ª•ÊîØÊíëÂ§öÂ§ßÁöÑÊñáÊ°£Ôºü
  - Â∫îËØ•ÊòØ‰∏çÈôêÂ§ßÂ∞èÁöÑÔºåÊãÜÂºÄÂ∞±Â•Ω‰∫Ü
- ËØ¥Ê≤°ËØ¥Á°¨‰ª∂ÈúÄÊ±ÇÔºü

- ## Â§ßÊ®°ÂûãÁöÑËøô‰∫õ benchmark Â∫îËØ•ÊòØÂÖ®ÂÆáÂÆôÊúÄÊ≤°Áî®ÁöÑ benchmark ‰∫ÜÂêßÔºü
- https://twitter.com/yihong0618/status/1721401347533324688
- ‰πü‰∏çÊòØÂÖ®Ê≤°Áî®Ôºå‰πüÊúâ‰∏Ä‰∫õÊúâÁî®ÁöÑ, Â∞§ÂÖ∂ÁªÜÂàÜ‰ªªÂä°‰∏äÁöÑÔºåËøòÊòØÊå∫ÊúâÁî®ÁöÑ„ÄÇÂΩìÂâçÁõ∏ÊØîÂÖ∂‰ªñbenchmarkÔºåÂèØÊìç‰ΩúÁ©∫Èó¥Á°ÆÂÆûÂ§ß
- ÂÖ¨ÂºÄÁöÑÂè™ËÉΩÂÖ®ÁúãËá™Ëßâ

- ## ‰∏≠ÊñáÂºÄÊ∫êÊ®°ÂûãËôΩÂ§öÔºåÊï∞ÊçÆÈõÜÂç¥ÂæàÂ∞ëÂºÄÊ∫ê„ÄÇ
- https://twitter.com/9hills/status/1718828132046942218
  - ÁõÆÂâçËã±Êñá 7B ËßÑÊ®°ÁöÑ SOTA Ê®°ÂûãÊòØ zephyr-7b-beta„ÄÇÂÆÉÊîæÂºÉ‰∫ÜË¥®ÈáèÂèÇÂ∑Æ‰∏çÈΩêÁöÑÂºÄÊ∫êÊï∞ÊçÆÈõÜÔºå‰ΩøÁî®ChatGPTÂíåGPT-4 ÂÖ®Êñ∞Ê†áÊ≥®‰∫Ü UltraChat Âíå UltraFeedback Êï∞ÊçÆÈõÜÔºàÂ∑≤ÂºÄÊ∫êÔºâ„ÄÇÊòØ llama-index È°πÁõÆÂÆûÊµãÂá∫Êù•ÂîØ‰∏ÄËÉΩÂ§üÊîØÊåÅ Agent ÁöÑÂ∞èÂèÇÊï∞Ê®°Âûã„ÄÇ
- ‰∏≠ÊñáÊï∞ÊçÆÈõÜÈÉΩÊòØÊãøÊù•ÂçñÈí±ÁöÑ
