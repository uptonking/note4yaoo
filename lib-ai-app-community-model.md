---
title: lib-ai-app-community-model
tags: [ai, community]
created: 2023-10-30T07:33:56.233Z
modified: 2023-10-30T07:34:03.602Z
---

# lib-ai-app-community-model

# guide

- tips
  - å¤§æ¨¡å‹ç›¸å…³çš„äº§å“ç ”å‘ï¼ŒåŸç†çš„å¯è§£é‡Šæ€§å¾ˆå·®ï¼Œæ•ˆæœçš„å¯è§£é‡Šæ€§ä¹Ÿå·®

- model-features
  - ğŸ¤¼: speed, quality, size
  - reasoning/thinking
  - tool-use/function-call
  - vision
  - embedding
  - moe
  - æ³¨æ„æœ‰äº›ç¤¾åŒºé‡åŒ–çš„æ¨¡å‹å¯èƒ½é—æ¼æ ‡æ³¨äº†éƒ¨åˆ†features, å¯åœ¨æœ¬åœ°æµ‹è¯•æ¥ç¡®å®šæ˜¯å¦æ”¯æŒ

- ç§»åŠ¨ç«¯å¤§æ¨¡å‹
  - å‚è€ƒgoogle-gemma-1b

- [å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼šä»ç†è®ºåˆ°å®è·µ](https://intro-llm.github.io/)
  - å¤æ—¦å¤§å­¦å¼ å¥‡æ•™æˆå›¢é˜Ÿå†™äº†ä¸€æœ¬åœ¨çº¿å…è´¹çš„ç”µå­ä¹¦ï¼Œå¤§æ¦‚æœ‰ 300 é¡µç¯‡å¹…ï¼Œå°†å¤§æ¨¡å‹ä»ç†è®ºåˆ°å®æˆ˜çš„æ¯ä¸ªé˜¶æ®µéƒ½æè¿°çš„è¾ƒä¸ºæ¸…æ¥š
# discuss-stars
- ## 

- ## [Am I the only one who feels that, with all the AI boom, everyone is basically doing the same thing? : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1qk8zj1/am_i_the_only_one_who_feels_that_with_all_the_ai/)
- The AI was trained to regress to the mean and now everything is built by AI so everything now is gonna regress to the mean. You and I and everyone.

- A bunch of amateurs ( or just lazy programmers ) just got their hands on a new toy and they're going down the dunning-kruger curve and they think their prototype is a finished product.

- I find that AIs best use case for me personally is the role of a teacher or an advisor. So damn handy to be able to bounce dumb questions off it while youâ€™re learning something new. Contextual awareness is fantastic for my learning style.

- ## The best way to learn in the new AI era is to use open source projects as references and ask Codex or Claude to re-implement them, 
- https://x.com/LiMzba/status/2013096172752392584
  - it will let you to learn much more deeply and thoroughly than simply reading secondhand knowledge from elsewhere.

- ## ä¸ºä»€ä¹ˆ AGI å¿…ç„¶ä¼šç§æœ‰åŒ–éƒ¨ç½²ï¼Ÿ
- https://x.com/naki2012/status/2003081645067550888
  - åœ¨ä»Šå¤©çš„è®¨è®ºä¸­ï¼Œå¾ˆå¤šäººè¿˜åœ¨çº ç»“ä¸€ä¸ªé—®é¢˜ï¼š AGI ä¼šä¸ä¼šæœ‰æ³¡æ²«ï¼Ÿä¼šä¸ä¼šè¢«é«˜ä¼°ï¼Ÿä¼šä¸ä¼šæœ€ç»ˆåªæ˜¯äº‘ç«¯çš„ä¸€ä¸ªé«˜çº§æœåŠ¡ï¼Ÿ
  - ä½†å¦‚æœæŠŠè§†è§’ä»â€œäº§å“â€å’Œâ€œå•†ä¸šæ¨¡å¼â€ç§»å¼€ï¼ŒçœŸæ­£çš„é—®é¢˜å…¶å®æ˜¯ï¼šå½“æ™ºèƒ½å…·å¤‡é•¿æœŸè®°å¿†ã€è¿ç»­äººæ ¼ä¸è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›æ—¶ï¼Œå®ƒè¿˜å¯èƒ½æ˜¯â€œå…¬å…±äº‘æœåŠ¡â€å—ï¼Ÿç­”æ¡ˆæ˜¯å¦å®šçš„ã€‚ä¸æ˜¯å› ä¸ºæŠ€æœ¯ä¸å…è®¸ï¼Œè€Œæ˜¯å› ä¸ºç»“æ„ä¸å…è®¸ã€‚
  - ä¸€ã€å…ˆè¯´ç»“è®ºï¼šAGI ä¸€æ—¦æˆç«‹ï¼Œå°±å¿…ç„¶èµ°å‘ç§æœ‰åŒ–éƒ¨ç½²
  - è¿™é‡Œçš„â€œç§æœ‰åŒ–â€ï¼Œä¸æ˜¯æŒ‡â€œæ¯ä¸ªäººéƒ½ä¹°å¾—èµ·è¶…çº§ç®—åŠ›â€ï¼Œè€Œæ˜¯æŒ‡ä¸€ä»¶æ›´å…³é”®çš„äº‹ï¼š AGI çš„æ ¸å¿ƒèŠ‚ç‚¹â€”â€”è®°å¿†ä¸è®°å¿†è¿æ¥æœºåˆ¶â€”â€”å¿…é¡»ç”±ä¸ªäººæˆ–æå°ä¸»ä½“æŒæœ‰ã€‚ èƒ½åŠ›å¯ä»¥å¤–åŒ…ï¼Œ è®¡ç®—å¯ä»¥ç§Ÿç”¨ï¼Œ ä½†è®°å¿†ä¸äººæ ¼ç”Ÿæˆé€»è¾‘ï¼Œæ— æ³•æ‰˜ç®¡ã€‚ è¿™æ˜¯ AGI ä¸æ­¤å‰æ‰€æœ‰è½¯ä»¶ç³»ç»Ÿçš„æœ¬è´¨åˆ†æ°´å²­ã€‚
  - äºŒã€äº‘ç«¯æ™ºèƒ½ä¸ºä»€ä¹ˆå¤©ç„¶ä¸å¯èƒ½æˆä¸ºâ€œä¸ªäºº AGIâ€
  - ç›®å‰æ‰€æœ‰äº‘ç«¯ AIï¼Œæœ¬è´¨ä¸Šéƒ½å…·å¤‡ä¸‰ä¸ªç»“æ„ç‰¹å¾ï¼š 1. ä¼šè¯å¼å­˜åœ¨ï¼š æ¯ä¸€æ¬¡è°ƒç”¨éƒ½æ˜¯â€œä¸´æ—¶ç”Ÿæˆâ€ï¼Œä¸å­˜åœ¨çœŸæ­£çš„é•¿æœŸè¿ç»­çŠ¶æ€ã€‚ 2. è®°å¿†ä¸å¯æ§ï¼š å³ä¾¿å­˜åœ¨â€œé•¿æœŸè®°å¿†â€ï¼Œå…¶å­˜å–é€»è¾‘ã€ä¼˜å…ˆçº§ä¸åˆ é™¤æƒï¼Œå§‹ç»ˆä¸åœ¨ç”¨æˆ·æ‰‹ä¸­ã€‚ 3. ç›®æ ‡ä¸ä¸€è‡´ï¼š äº‘ç«¯ç³»ç»Ÿçš„ç¬¬ä¸€è´£ä»»å¯¹è±¡ï¼Œæ°¸è¿œæ˜¯å¹³å°ã€åˆè§„ã€è§„æ¨¡ä¸é£é™©æ§åˆ¶ï¼Œè€Œä¸æ˜¯ä¸ªä½“çš„è®¤çŸ¥è¿ç»­æ€§ã€‚
  - äº‘ç«¯ AI å¯ä»¥å¾ˆèªæ˜ï¼Œ ä½†å®ƒæ°¸è¿œåªèƒ½æ˜¯â€œåŠ©æ‰‹â€ï¼Œè€Œä¸æ˜¯â€œå…±ç”Ÿä½“â€ã€‚ AGI ä¸€æ—¦å…·å¤‡äººæ ¼è¿ç»­æ€§ï¼Œè¿™ç§ç»“æ„å°±ä¼šç›´æ¥å¤±æ•ˆã€‚
  - ä¸‰ã€çœŸæ­£æ•æ„Ÿçš„ä¸æ˜¯â€œæ•°æ®â€ï¼Œè€Œæ˜¯â€œè®°å¿†ä¹‹é—´çš„è¿æ¥æƒâ€
  - å¤šè®¨è®ºæŠŠç„¦ç‚¹æ”¾åœ¨â€œéšç§æ•°æ®â€â€œä¿¡æ¯å®‰å…¨â€ä¸Šï¼Œå…¶å®è¿™åªæ˜¯è¡¨å±‚ã€‚çœŸæ­£ä¸å¯å¤–åŒ…çš„ï¼Œæ˜¯ï¼šâ€¢å“ªäº›è®°å¿†ä¼šè¢«æ¿€æ´»â€¢æ¿€æ´»é¡ºåºå¦‚ä½•å˜åŒ–â€¢å†²çªè®°å¿†å¦‚ä½•è¢«è°ƒå’Œâ€¢å“ªäº›ç»éªŒè¢«å¼ºåŒ–ã€å“ªäº›è¢«é—å¿˜â€¢é”™è¯¯å¦‚ä½•è¢«æ ‡è®°ä¸ºâ€œç¾è€»â€â€œè­¦æƒ•â€æˆ–â€œæ•™è®­â€
  - è¿™äº›å¹¶ä¸æ˜¯æ•°æ®é—®é¢˜ï¼Œè€Œæ˜¯ï¼šäººæ ¼ç”Ÿæˆå‡½æ•°çš„é—®é¢˜ã€‚
  - ä¸€æ—¦è¿™ä¸ªå‡½æ•°ä¸åœ¨ä½ æ‰‹é‡Œï¼Œä½ é¢å¯¹çš„å°±ä¸æ˜¯â€œä½ çš„ AGIâ€ï¼Œè€Œæ˜¯ä¸€ä¸ªæŒç»­è§£é‡Šä½ ã€å¡‘é€ ä½ ã€å´ä¸å±äºä½ çš„ç³»ç»Ÿã€‚ è¿™åœ¨ç»“æ„ä¸Šæ˜¯ä¸å¯æ¥å—çš„ã€‚
  - å››ã€ä¸ºä»€ä¹ˆâ€œä¸»è„‘â€å¿…é¡»æ˜¯æœ¬åœ°ã€ç§æœ‰ã€æŒç»­å­˜åœ¨çš„
  - â€¢ç‰©ç†ä¸ä¸»æƒä¸Šéƒ½å±äºä¸ªäºº
  - äº”ã€è¿™ä¸æ˜¯æŠ€æœ¯ç†æƒ³ä¸»ä¹‰ï¼Œè€Œæ˜¯å†å²è§„å¾‹çš„é‡æ¼”
  - å›çœ‹å†å²ï¼Œæ¯ä¸€æ¬¡å…³é”®èƒ½åŠ›éƒ½ä¼šç»å†åŒæ ·çš„è·¯å¾„ï¼š
  - â€¢è®¡ç®—ï¼šä»å¤§å‹æœº â†’ PC â†’ ä¸ªäººè®¾å¤‡
  - â€¢å­˜å‚¨ï¼šä»ä¸­å¿ƒæœåŠ¡å™¨ â†’ æœ¬åœ°ç¡¬ç›˜ â†’ ç§æœ‰ NAS
  - â€¢é€šä¿¡ï¼šä»å›½å®¶ç³»ç»Ÿ â†’ è¿è¥å•† â†’ ä¸ªäººç»ˆç«¯
  - å‡¡æ˜¯ä¸â€œè‡ªæˆ‘è¿ç»­æ€§â€å¼ºç›¸å…³çš„èƒ½åŠ›ï¼Œæœ€ç»ˆéƒ½ä¼šä¸‹æ²‰åˆ°ä¸ªä½“ã€‚
  - AGI å¦‚æœé•¿æœŸåªå­˜åœ¨äºäº‘ç«¯ï¼Œé‚£å®ƒæ³¨å®šåªæ˜¯â€œé«˜çº§è‡ªåŠ¨åŒ–â€ï¼Œè€Œä¸ä¼šæˆä¸ºâ€œä¸ªä½“æ™ºèƒ½å»¶å±•â€ã€‚
  - å…­ã€çœŸæ­£çš„åˆ†ç•Œçº¿ï¼Œå…¶å®å·²ç»å‡ºç°äº†
  - æœªæ¥äººä¸ AI çš„å·®å¼‚ï¼Œä¸åœ¨äºâ€œç”¨ä¸ç”¨ AIâ€ï¼Œè€Œåœ¨äºï¼š â€¢æœ‰äº›äººä½¿ç”¨çš„æ˜¯éšæ—¶å¯è¢«æ›¿æ¢ã€è¢«å‡çº§ã€è¢«é‡ç½®çš„æ™ºèƒ½æœåŠ¡ â€¢æœ‰äº›äººæ‹¥æœ‰çš„æ˜¯ä¸€ä¸ªé•¿æœŸå­˜åœ¨ã€ä¸è‡ªèº«è®°å¿†é—­ç¯ç»‘å®šçš„æ™ºèƒ½æ ¸å¿ƒ
  - å‰è€…æ˜¯ç”¨æˆ·ï¼Œåè€…æ˜¯å…±ç”Ÿè€…ã€‚è¿™æ¡åˆ†ç•Œçº¿ä¸€æ—¦å½¢æˆï¼Œå°†æ¯”â€œæ˜¯å¦è”ç½‘â€â€œæ˜¯å¦ä»˜è´¹â€æ›´æ·±åˆ»ã€‚
  - ä¸ƒã€ç»“è¯­ï¼šAGI ç§æœ‰åŒ–ï¼Œä¸æ˜¯é€‰æ‹©ï¼Œè€Œæ˜¯å¿…ç„¶
  - è®°å¿†ä¸»æƒä¸å¯å¤–åŒ…ï¼Œäººæ ¼ç”Ÿæˆä¸å¯æ‰˜ç®¡ã€‚
  - æ‰€ä»¥ï¼ŒAGI çš„æœªæ¥å½¢æ€ï¼Œä¸æ˜¯â€œæ›´å¤§çš„äº‘â€ï¼Œ è€Œæ˜¯ï¼š ç§æœ‰ä¸»è„‘ Ã— å¤–éƒ¨èƒ½åŠ› Ã— åä½œå¼æ™ºèƒ½ç½‘ç»œã€‚

- ä¸æ˜¯ï¼Œè€Œæ˜¯ã€‚æˆ‘ç°åœ¨çœ‹åˆ°è¿™ä¸ªï¼Œç¬¬ä¸€ååº”å°±æ˜¯è¿™æ®µè¯æ˜¯AIç”Ÿæˆçš„ï¼Œæˆ–è€…è‡³å°‘æ˜¯äººå’ŒAIèŠå‡ºæ¥çš„å†…å®¹ã€‚

- ## ğŸ’¡ ä¸‹åˆç»™äººåšäº†ä¸€ä¸ªå°å°çš„å±•ç¤ºã€‚ä¸€ä¸ªChatbotï¼Œå…¶å®ä¹Ÿå°±ä¸‰å››ç™¾å­—çš„æç¤ºè¯ï¼Œå®šä¹‰äº†ä¸ƒå…«ä¸ªå·¥å…·ã€‚
- https://x.com/wwwgoubuli/status/2001256235296022575
  - æ¼”ç¤ºçš„æ—¶å€™ï¼Œæˆ‘è®©å¯¹é¢æ”¾å¼€å°è¯•ï¼Œä¹Ÿæå‰å£°æ˜äº†ç›®å‰è¿˜æ˜¯Demoé˜¶æ®µï¼Œä¼šæœ‰äº›é—®é¢˜ã€‚ä½†å¯¹æ–¹ä»ç„¶è¢«æˆ‘è¿™ä¸ªæ¼”ç¤ºä¸­ä½“ç°çš„è‡ªä¸»å¯»æ‰¾å·¥å…·è§£å†³é—®é¢˜çš„è¡Œä¸ºéœ‡æƒŠäº†ã€‚
  - ä»¥ä¸ºæˆ‘å®šä¹‰äº†ä¸€ä¸ªå¾ˆå¤æ‚çš„å·¥ä½œæµï¼Œæˆ‘è¯´æ²¡æœ‰å·¥ä½œæµï¼Œå…¶å®å°±åªæœ‰ä¸€å±‚ã€‚
  - å¯¹æ–¹ç™¾æ€ä¸å¾—å…¶è§£ã€‚æ‰€ä»¥è¯´ï¼Œè®¤çŸ¥å·®è‚¯å®šæ˜¯çœŸå®å­˜åœ¨çš„ã€‚
  - è¿™æ˜¯ä¸€ä¸ªå‚ç›´é¢†åŸŸçš„Agentã€‚åŸå…ˆå®¢æˆ·çš„é—®é¢˜æ˜¯ï¼Œåœ¨ä»–ä»¬ä¹‹å‰æ‰€åšçš„AIçš„å°è¯•ä¸­ï¼Œæœ‰ä¸€éƒ¨åˆ†å·²ç»å‘æŒ¥å¾—å¾ˆä¸é”™äº†ã€‚ä½†æ˜¯å½“æœ‰çš„æ—¶å€™ä¸å¾—ä¸å¤„ç†è¶…é•¿çš„æ•°æ®çš„æ—¶å€™ï¼Œä¸¢ç»™LLMæ€»æ˜¯æœ‰å„ç§é”™è¯¯å‡ºç°ã€‚
  - æˆ‘æ‰¾ä»–ä»¬çš„ç ”å‘é—®è¿‡ï¼Œæå–äº†å‡ ä¸ªåœ¨ä½¿ç”¨AIä¹‹å‰çš„ï¼Œä»–ä»¬å¤„ç†çš„å·¥å…·ã€‚ç„¶åå°†è¿™äº›å·¥å…·çš„æè¿°ã€ä½œç”¨ã€é™å®šèŒƒå›´ç­‰äº¤ç»™äº†LLMï¼Œ å¹¶å‘Šè¯‰æ¨¡å‹ä¸è¦è‡ªå·±è¯•å›¾è§£å†³é—®é¢˜ï¼Œè€Œæ˜¯è¦ç”¨å·¥å…·è§£å†³é—®é¢˜ã€‚ æ²¡äº†

- äº‹å®ä¸Šå¾ˆå¤šå…¬å¸æ‰€åšçš„agentæˆ–è€…chatbotè¿promptçš„ä¸Šé™éƒ½æ²¡æœ‰è¾¾åˆ°ï¼Œå°±å¼€å§‹è€ƒè™‘æsftï¼ŒæRLHFï¼Œä¸€ä¸ªå°å°çš„éœ€æ±‚ä¸ºäº†æ‰€è°“çš„ä¼˜åŒ–æŒ‡æ ‡æçš„è‡ƒè‚¿ä¸å ª

- ## ğŸ“Œ LLM å‡ºæ¥ä¹‹åï¼Œåœ¨åº”ç”¨å±‚çš„æŠ˜è…¾ä»æœªåœæ­‡ã€‚ä» Prompt è°ƒä¼˜åˆ° Workflow é…ç½®ï¼Œå†åˆ° Agent æ„å»ºï¼Œæœ€ç»ˆç›®çš„éƒ½æ˜¯ä¸€æ ·çš„ï¼šè®© LLM æ›´å¥½åœ°ä¸ºäººç±»å¹²æ´»ï¼ŒæŠŠæœºå™¨çš„æ€§èƒ½å‹æ¦¨åˆ°æè‡´ã€‚
- https://x.com/Barret_China/status/1973188130091180466
- å¯¹ LLM çš„å‹æ¦¨ï¼Œå¯ä»¥åˆ†ä¸ºä¸¤ä¸ªç»´åº¦ã€‚
- ä¸€æ˜¯å¸®åŠ©å®ƒæ‰¾åˆ°æœ€ä¼˜ç®—æ³•ï¼Œè®©æ¨ç†å°‘èµ°å¼¯è·¯ã€‚
  - ä¸ºæ­¤æˆ‘ä»¬å‡ ä¹æŠŠèƒ½æƒ³åˆ°çš„è·¯å­éƒ½èµ°äº†ä¸€éï¼Œè®© LLM å­¦ä¼šåæ€ï¼ˆreflectionã€self-consistencyã€self-criticsï¼‰ï¼Œå­¦ä¼šæ¨ç†å’Œè§„åˆ’ï¼ˆreasoningã€planningã€chain-of-thoughtã€tree-of-thoughtï¼‰ï¼›å­¦ä¼šè®°å¿†ï¼ˆshort-term memoryã€long-term memoryï¼‰ï¼Œä¸è‡³äºå¯¹è¯ä¸€é•¿å°±å¤±å¿†ï¼›å­¦ä¼šæ‰¾çŸ¥è¯†ï¼ˆRAGã€knowledge graphï¼‰ï¼Œåœ¨å¤–éƒ¨ä¸–ç•Œé‡Œè¡¥å……äº‹å®ï¼›å­¦ä¼šæ„å»ºä¸Šä¸‹æ–‡ï¼ˆcontext buildingï¼‰ï¼Œåœ¨æœ‰é™ token é‡Œå¡ä¸‹æ›´å¤šæœ‰æ•ˆä¿¡æ¯ï¼›å­¦ä¼šç”¨å·¥å…·ï¼ˆtool-useï¼Œfunction callingï¼ŒMCPï¼‰ï¼ŒæŠŠäº‹æƒ…äº¤ç»™å¤–éƒ¨ç¨‹åºå»è·‘ï¼Œè€Œä¸æ˜¯å…‰é è‡ªå·±ç”Ÿæˆï¼›ç­‰ç­‰ã€‚
  - è¿™äº›ä¸œè¥¿ï¼Œè¯´åˆ°åº•éƒ½æ˜¯æŠ€å·§å’Œæœºåˆ¶ï¼Œæœ¬è´¨ç›®çš„æ˜¯è®© LLM æ›´å¿«ç†è§£äººç±»è¦å¹²å•¥ï¼Œå›´ç»•ç›®æ ‡ï¼ˆgoal-orientedï¼‰å°½å¯èƒ½æ‰¾åˆ°ä¸€æ¡ä»£ä»·æœ€å°çš„è·¯ï¼Œè·‘åˆ°æœ€ä¼˜è§£ä¸Šå»ã€‚
- ç¬¬äºŒä¸ªç»´åº¦ï¼Œæ˜¯å¯¹æ—¶é—´çš„å‹æ¦¨ï¼Œè®© LLM å¯ä»¥åšåˆ° 7Ã—24 å°æ—¶ä¸åœæ­‡ã€‚
  - å½“æˆ‘ä»¬å¯¹ LLM æœ‰äº†æ›´æ·±å…¥çš„ç†è§£ä¹‹åï¼Œå¾ˆå®¹æ˜“æƒ³åˆ°æŠŠå®ƒæ‰“é€ æˆå±äºè‡ªå·±æˆ–ç»„ç»‡çš„â€œæ•°å­—å‘˜å·¥â€ï¼Œå®ƒä¸çŸ¥ç–²æƒ«ã€ä¸ä¼šæŠ±æ€¨ï¼Œå¯ä»¥æŒç»­è¿è½¬ã€ä¸æ–­å­¦ä¹ ã€‚
- å¤§éƒ¨åˆ†äººä»Šå¤©ç”¨ AI çš„æ–¹å¼ï¼Œè¿˜åœç•™åœ¨æŸ¥èµ„æ–™ã€æ€»ç»“å†…å®¹ã€å†™å‘¨æŠ¥æœˆæŠ¥è¿™äº›å•ç‚¹åœºæ™¯ä¸Šï¼Œå¦‚æœè¦çœŸæ­£æ„å»ºä¸€åâ€œä¸åœæ­‡çš„ AI æ•°å­—å‘˜å·¥â€ï¼Œå…‰é è¿™äº›è¿˜ä¸å¤Ÿã€‚æˆ‘ä»¬éœ€è¦å…ˆè§„åˆ’å‡ºå±äºè‡ªå·±çš„ AI æ•°å­—å·¥å‚ â€”â€”æƒ³æ¸…æ¥šè¦é€ å‡ºæ¥çš„â€œäº§å“â€æ˜¯ä»€ä¹ˆï¼Œæ˜¯æ²‰æ·€çŸ¥è¯†çš„ç³»ç»Ÿï¼Œæ˜¯è‡ªåŠ¨åŒ–çš„ä¸šåŠ¡æµç¨‹ï¼Œè¿˜æ˜¯ä¸€ä¸ªå¯ä»¥é•¿æœŸè¿­ä»£çš„æœåŠ¡ã€‚
  - åœ¨è¿™åº§å·¥å‚é‡Œï¼ŒAI æ˜¯ç”Ÿäº§çº¿ä¸Šçš„æ‰§è¡Œè€…ï¼Œå®ƒè´Ÿè´£å…·ä½“çš„åŠ å·¥ä¸äº§å‡ºï¼›è€Œäººç±»çš„è§’è‰²å‘ç”Ÿäº†è½¬å˜ï¼Œä»â€œäº²è‡ªå¹²æ´»çš„å·¥äººâ€å˜æˆâ€œç›‘å·¥ä¸ç®¡ç†è€…â€ã€‚ äººç±»ä¸å†äº²æ‰‹å®Œæˆæ¯ä¸€æ­¥ï¼Œè€Œæ˜¯è¦è®¾è®¡æµæ°´çº¿ï¼Œè®¾å®šè§„åˆ™ï¼Œåˆ¶å®šæŒ‡æ ‡ï¼Œç›‘æ§è´¨é‡ï¼Œå¹¶åœ¨éœ€è¦æ—¶è°ƒåº¦èµ„æºã€‚æ¢å¥è¯è¯´ï¼ŒAI çš„ä»·å€¼ä¸åœ¨äºæ›¿æˆ‘ä»¬â€œå¹²ä¸€ç‚¹æ´»â€ï¼Œè€Œåœ¨äºå¸®æŠŠæ•´æ¡æµæ°´çº¿è·‘èµ·æ¥ï¼Œè€Œäººç±»æ›´åƒæ˜¯â€œæ•°å­—å·¥å‚çš„ç®¡ç†è€…â€ã€‚
  - å½“è¿™ä¸¤ä¸ªç»´åº¦ç»“åˆèµ·æ¥æ—¶ï¼ŒçœŸæ­£çš„æ‹ç‚¹å°±å‡ºç°äº†ã€‚LLM ä¸å†åªæ˜¯ä¸€ä¸ªå†·å†°å†°çš„å·¥å…·ï¼Œè€Œæ˜¯é€æ¸å˜æˆäº†å¯ä»¥é•¿æœŸåä½œçš„ä¼™ä¼´ã€‚å®ƒæ—¢èƒ½æ‰¿æ‹…é‡å¤æ€§åŠ³åŠ¨ï¼Œä¹Ÿèƒ½åœ¨å¤æ‚é—®é¢˜ä¸Šæä¾›æ´è§ã€‚å®ƒä¸ä»…ä»…æ˜¯â€œå¸®ä½ åšäº‹â€ï¼Œæ›´æ˜¯â€œå’Œä½ ä¸€èµ·åšäº‹â€ã€‚
  - æœªæ¥çš„å·®è·ï¼Œä¸åœ¨äºè°èƒ½å†™å‡ºæ›´æ¼‚äº®çš„ Promptï¼Œè€Œåœ¨äºè°èƒ½æŠŠ LLM çœŸæ­£èå…¥åˆ°è‡ªå·±çš„æ—¶é—´å’Œç»„ç»‡é‡Œï¼Œå½¢æˆç¨³å®šçš„ç”Ÿäº§æ–¹å¼ã€‚
  - å› æ­¤ï¼Œä¼šä¸ä¼šç”¨ã€ç”¨åˆ°ä»€ä¹ˆæ·±åº¦ã€èƒ½å¦æŒç»­ä¼˜åŒ–ï¼Œè¿™äº›æ‰æ˜¯é•¿æœŸçš„ç«äº‰åŠ›æ¥æºã€‚è°èƒ½æŠŠ AI è¿è¡Œæˆâ€œå·¥å‚â€ï¼Œè®©è‡ªå·±ä»æ‰§è¡Œè€…è½¬ä¸ºç›‘å·¥å’Œç®¡ç†è€…ï¼Œè°å°±èƒ½åœ¨æœªæ¥çš„æ—¥å¸¸å·¥ä½œå’Œä¸šåŠ¡ä¸­ï¼Œè·å¾—çœŸæ­£å¯å¤ç”¨ã€å¯ç´¯ç§¯çš„ä¼˜åŠ¿ã€‚

- ä»£ç æŠ½è±¡çœŸå®ä¸–ç•Œçš„æ—¶ä»£å¿«è¦ç»“æŸäº†ï¼Œæ¬¢è¿æ¥åˆ°token æŠ½è±¡çœŸå®ä¸–ç•Œçš„æ—¶ä»£ã€‚

- å¦‚æœTOKENæŠ•å…¥äº§å‡ºæ˜¯æ­£å‘çš„ï¼Œæ¨ä¸å¾—ä»–ä¸ä¼‘æ¯ 

- é—®é¢˜æ¥äº†ï¼Œæˆ‘è¦åšä»€ä¹ˆï¼ŒAIè¦æ€ä¹ˆå¸®æˆ‘åš

- ## ğŸ“± [ç«¯ä¾§æ¨¡å‹ä¼šæ˜¯ AI æŠ€æœ¯æ¼”è¿›çš„ä¸‹ä¸€ä¸ª ã€Œå¿…äº‰ä¹‹åœ°ã€ å—ï¼Ÿå½“å‰è½åœ°é¢ä¸´å“ªäº›æ ¸å¿ƒç“¶é¢ˆï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/1914319403023032351)
- TO Cåœºæ™¯åº”ç”¨æœ€å¤§ç‰¹ç‚¹æ˜¯ç¡¬ä»¶å‚å·®ä¸é½ã€‚æ€§èƒ½å·®èƒ½å¤§åˆ°10å¤šå€ä»¥ä¸Šã€‚
  - æŠ€æœ¯æ ˆå¿…é¡»æ»¡è¶³æ‰€æœ‰ä¸»æµç¡¬ä»¶ç»“æ„å·®å¼‚å’Œæ€§èƒ½æ€§èƒ½ä¸‹ï¼Œä¿æŒç›¸å¯¹ä¸€è‡´çš„ä½¿ç”¨ä½“éªŒã€‚è¿™æ˜¯ä¸€ä»¶æç´¯çš„æ´»å„¿ã€‚
  - æœåŠ¡ç«¯ä¸‹ä¸ªå¼€æºæ¨¡å‹æ­ä¸ªWEBæœåŠ¡å†™ä¸ªHTMLå°±èƒ½å–æœåŠ¡äº†ã€‚ç«¯ä¾§æƒ³å†…åµŒAIæ¨¡å‹äº§å“åŒ–ï¼ŒåŒæ—¶è¿˜è¦è§£å†³å®æ—¶æ€§ï¼Œè¦è§£å†³çš„å·¥ç¨‹é—®é¢˜è¦å¤š10å€ä¸æ­¢ã€‚è¦è‡ªå·±ä¼˜åŒ–æ¨¡å‹ã€‚ç”šè‡³è¦è‡ªå·±æ­å»ºæ¨¡å‹ç»“æ„ï¼Œè‡ªå·±å‡†å¤‡æ•°æ®è®­ç»ƒã€‚
- ç›®å‰çœ‹ç«¯ä¾§å®Œå…¨è°ˆä¸ä¸ŠAIæ¼”è¿›å¿…äº‰ä¹‹åœ°ã€‚ä¸€æ¥ç¡¬ä»¶è¾ƒå¼±ä¸”å‚å·®ä¸é½ï¼Œå¯¼è‡´TO Cå·¥ç¨‹ä¿éšœä½“éªŒæåº¦å›°éš¾å’Œå¤æ‚ã€‚äºŒæ¥PCé«˜ç®—åŠ›ç¡¬ä»¶å®Œå…¨ä¸æ˜¯å¤§ä¼—æ¶ˆè´¹çº§çš„å”®ä»·å’Œå®šä½ï¼Œæœ€åï¼Œè¿˜æ²¡æœ‰ä»€ä¹ˆç«¯ä¾§AIè½åœ°äº§å“çš„åº”ç”¨èŒƒå¼ã€‚
  - ä»åº”ç”¨ä½“éªŒä¸Šï¼Œé‚£äº›å…è®¸å»¶è¿Ÿ500æ¯«ç§’ä»¥ä¸Šï¼Œå¯¹å¸¦å®½éœ€æ±‚ä¸å¤§çš„åº”ç”¨ï¼Œéƒ½å¯ä»¥æ”¾åˆ°äº‘ç«¯ã€‚
- çœŸéœ€è¦åšåˆ°ç«¯ä¾§çš„ï¼Œå…¶å®æ˜¯é‚£äº›éœ€è¦å³æ—¶å“åº”çš„åº”ç”¨åœºæ™¯ã€‚
  - ä¸ºæ•°ä¸å¤šçš„åº”ç”¨ç±»åˆ«ï¼Œæ¯”å¦‚å³æ—¶äº¤äº’å½±åƒç›¸å…³ï¼Œè¯­éŸ³ç›¸å…³çš„ï¼Œè¯¸å¦‚æ¸¸æˆï¼Œéœ€è¦å³æ—¶äº¤äº’çš„æ•°å­—äººï¼Œ3Dè™šæ‹ŸAIäººï¼Œè™šæ‹Ÿäººç›´æ’­ç­‰ï¼Œå¯èƒ½ä¼šå¯¹ç«¯ä¾§AIæœ‰éƒ¨åˆ†éœ€æ±‚
- è¿™ç±»åº”ç”¨æœ‰3ä¸ªç‰¹å¾ï¼š 
  - 1 å³æ—¶æ€§ç›´æ¥å½±å“ä½“éªŒï¼Œä¸èƒ½ç­‰ã€‚ 
  - 2 å¯¹ç®—åŠ›è¦æ±‚ç›¸å¯¹ä½ï¼Œè¿ç®—æœ¬åœ°ç®—åŠ›èƒ½å¤Ÿæ»¡è¶³ï¼Œå¹¶ä¸ä¸€å®šéè¦äº‘ç®—åŠ›æ”¯æŒã€‚ 
  - 3 å¯¹å¸¦å®½è¦æ±‚è¾ƒå¤§ï¼Œäº‘æœåŠ¡çš„æˆæœ¬è¿‡é«˜ä½¿å¾—å•†ä¸šæ¨¡å¼ä¸æˆç«‹ï¼Œæ‰€ä»¥éœ€è¦æ”¾å…¥æœ¬åœ°ã€‚
- MOBILEç¦»ç«¯ä¾§å¯èƒ½æ›´è¿œã€‚ ç”±äºç®—åŠ›æä¸ºä½ä¸‹ï¼Œåªæœ‰å¾ˆå°‘çš„æƒ…å†µæ‰éœ€è¦æ‰‹æœºä¸Šå³æ—¶å“åº”AIæ¨ç†ç»™å‡ºçš„ç»“æœã€‚å³é‚£äº›æŠŠæ‰‹æœºä½œä¸ºä¿¡å·ä¼ æ„Ÿå™¨ï¼Œå³æ—¶å¯¹ä¿¡å·è¿ç®—çš„åœºåˆï¼šæ¯”å¦‚è¡¨æƒ…æ•æ‰ã€æ‰‹åŠ¿ã€è‚¢ä½“åŠ¨ä½œæ•æ‰å’Œè¯†åˆ«ï¼Œè¯­éŸ³ã€è¿åŠ¨æ•°æ®å¤„ç†å’Œç”Ÿç‰©ä¿¡å·å¤„ç†ç­‰ã€‚ 
  - å¦‚æœæ‰‹æœºæ²¡æœ‰ä¸“ç”¨ç¥ç»/å¼ é‡èŠ¯ç‰‡ï¼ŒGPUè¿˜éœ€è¦æ‰¿æ‹…æ¸²æŸ“3Då›¾å½¢ï¼Œé‚£ä¹ˆèƒ½è·‘çš„AIæ¨¡å‹ä¼šæ›´åŠ å—é™ã€‚
  - ç›®å‰æµ‹è¯•ï¼ŒæŸæ‰‹æœºç”¨NPUè·‘æ¨¡å‹æ¯”ä¸ç”¨æœ€å¤šèƒ½å¿«10å€ã€‚ 
  - æ‰‹æœºèŠ¯ç‰‡å•†æ˜¯å¦æœ‰åŠ¨åŠ›å‘å±•ï¼Œè¦çœ‹æ‰‹æœºä½œä¸ºæ•°æ®ä¼ æ„Ÿå™¨å³æ—¶å¤„ç†æ•°æ®èƒ½å¸¦æ¥å¤šå¤§çš„åº”ç”¨å¸‚åœºã€‚

- æœ¬åœ°éƒ¨ç½²æ¨¡å‹çš„ä¼˜åŠ¿åœ¨äºä½å»¶è¿Ÿï¼Œè¿™åœ¨æˆ‘çœ‹æ¥å…¶å®ä¹Ÿæ˜¯ä¸ªç›¸å¯¹åä¼ªçš„ä¼˜åŠ¿ã€‚
  - äº‹å®ä¸Šï¼Œç°ä»£ç½‘ç»œç¯å¢ƒå®é™…ä¸Šä»¥åŠè¶³å¤Ÿå¿«é€Ÿå’Œç¨³å®šï¼Œæ— è®ºæ˜¯æµé‡è¿˜æ˜¯WiFiï¼Œè¾…ä»¥CDNè¾¹ç¼˜èŠ‚ç‚¹ä¼˜åŒ–åï¼Œç»å¤§å¤šæ•°ä¸»æµAIåº”ç”¨çš„äº‘ç«¯å“åº”éƒ½èƒ½ç¨³å®šåœ¨å‡ åæ¯«ç§’ä»¥å†…ã€‚
- ç«¯ä¾§è®¾å¤‡å—é™äºåŠŸè€—ã€çƒ­é‡ã€ç®—åŠ›ç­‰ç‰©ç†æ¡ä»¶ï¼Œå¾€å¾€åªèƒ½éƒ¨ç½²è½»é‡åŒ–æ¨¡å‹

- ç§»åŠ¨è®¾å¤‡ä¸Šé•¿æœŸè¿è¡Œä¸€ä¸ªæœ‰ç«äº‰åŠ›çš„å¤§æ¨¡å‹ä»ç„¶ä¸ç°å®ï¼Œè´Ÿè½½ã€èµ„æºå ç”¨ã€ç”µé‡æ¶ˆè€—ã€å‘çƒ­ç­‰ç­‰é—®é¢˜ï¼Œå¾ˆéš¾åšåˆ°å¥½çš„ä½“éªŒï¼Œæ›´ä¸è¦è¯´ç‰©è”ç½‘è®¾å¤‡ã€æ™ºèƒ½éŸ³ç®±ç­‰ä½åŠŸè€—äº§å“äº†ã€‚
  - å¾ˆå¤šå£°ç§°è‡ªå·±ç”¨äº†ç«¯ä¾§å¤§æ¨¡å‹çš„ï¼Œå®é™…ä¸Šä»ç„¶ç¦»ä¸å¼€äº‘ç«¯çš„ååŒé…åˆï¼Œæˆ–è€…è¯´ä¸»è¦é äº‘ç«¯ï¼Œå¤‡ç”¨æ–¹æ¡ˆå¯èƒ½æ˜¯ç«¯ä¾§ï¼Œä½†æŠŠç«¯ä¾§æ‹¿å‡ºæ¥å¤§è‚†å®£æ‰¬ï¼Œæ¥åšå¹¿å‘Šè€Œå·²ã€‚

- ğŸ› ç«¯ä¾§æ¨¡å‹çš„åŠ£åŠ¿
  - ç«¯ä¾§æ¨¡å‹ä»ç„¶å—é™äºè®¾å¤‡æœ¬èº«çš„å†…å­˜ã€åŠŸè€—å’Œç®—åŠ›
  - äº‘ç«¯æ¨¡å‹å¯ä»¥éšæ—¶å‡çº§ï¼Œçƒ­æ›´æ–°æœºåˆ¶ä¿è¯æ‰€æœ‰ç”¨æˆ·ç¬¬ä¸€æ—¶é—´äº«å—æ–°ä¸€ä»£æ¨¡å‹
  - ç«¯ä¾§æ¨¡å‹å¯¹äºä¸åŒè®¾å¤‡å’Œç¡¬ä»¶æ¶æ„éœ€è¦åˆ†åˆ«é€‚é…

- ç«¯ä¾§é€‚åˆçš„åœºæ™¯è¿˜æ˜¯éšç§ã€æä½å»¶è¿Ÿï¼Œæˆ–è€…å…¨å¤©å€™ã€‚

- å¦‚æœå®Œå…¨è·‘åœ¨ç«¯ä¾§ï¼Œé‚£å•†ä¸šæ¨¡å¼æ€ä¹ˆåšï¼Ÿç°åœ¨çš„ä¸»æµæ˜¯è®¢é˜…åˆ¶ï¼Œä½†å®Œå…¨æœ¬åœ°çš„è®¢é˜…åˆ¶ï¼Œå¸å¼•åŠ›æ¯”è¾ƒä½ï¼Œç ´è§£é£é™©æ¯”è¾ƒé«˜å§

- ## deepseek 3fs å…¶å®ä¸æ˜¯å¾ˆç†è§£è¿™æ ·çš„æ„ä¹‰æ˜¯å•¥â€¦ å·²ç»è„±ç¦» FS çš„é€šç”¨æ¥å£äº†ï¼Œä¸ºå•¥è¦ç¡¬æŒ‚ä¸€ä¸ª FUSE VFS å±‚â€¦ ç›´æ¥æ‘’å¼ƒ VFS èµ°ä¸ªç§æœ‰çš„ protocol ä¸å¹²å‡€å¤šäº†â€¦
- https://x.com/silsrc/status/1895390926098571505
- > Most applications use FUSE client, which has a low adoption barrier. Performance-critical applications are integrated with the native client.

- æˆ‘çš„ç¬¬ä¸€ååº”æ˜¯ç°æœ‰æ¡†æ¶åº”è¯¥æ²¡åŠæ³•å¤§æ‰¹é‡æ”¹ç”¨ç§æœ‰çš„ APIï¼Œæ¯”å¦‚åœ¨ Python æˆ‘å°±æ˜¯è¦ç”¨ pathlib. Path å¯¹æ–‡ä»¶åšç‚¹ç®€å•æ“ä½œï¼Œé‚£ç¡®å®åªèƒ½æ˜¯æŒ‚ FUSE ä¸Šå»äº†

- ## èŠä¸€èŠå›½å†…å¤§æ¨¡å‹çš„å®‰å…¨æœºåˆ¶ï¼š ä¸€èˆ¬æ˜¯ä¸¤å¥—ï¼Œåˆ†åˆ«ä½œç”¨äºtrain-timeå’Œtest-timeã€‚
- https://x.com/9hills/status/1840786446153921017
  - è®­ç»ƒçš„æ—¶å€™å¢åŠ å®‰å…¨å’Œä»·å€¼è§‚å¯¹é½çš„SFTå’Œåå¥½å¯¹é½æ•°æ®ã€‚æœ€ç»ˆæ•ˆæœç±»ä¼¼å¼€æºçš„Qwen2æ¨¡å‹ï¼Œæœ‰ç‚¹ç”¨ä½†æ˜¯å¾ˆå®¹æ˜“è¢«Jailbreakã€‚
  - æ¨ç†æ—¶å¢åŠ å®‰å…¨ç®—å­ï¼Œå…·ä½“æœ‰å‡ ç§
- æŠ€æœ¯éš¾ç‚¹æœ‰ä¸¤ä¸ªï¼š
  1. è®­ç»ƒåˆ†ç±»å™¨çš„å¤§é‡éå®‰å…¨æ•°æ®ï¼Œæ‰€ä»¥è¯´ä½ å…ˆæˆä¸ºåè´¼æ‰èƒ½è¯†åˆ«åè´¼ã€‚
  2. æ¨¡å‹è¦åšçš„è¶³å¤Ÿå°è¶³å¤Ÿå¿«ï¼Œæœ€å°åŒ–å½±å“æ¨¡å‹ttftå’Œtpsã€‚
  3. æµå¼å¾ˆéš¾ï¼ŒæŸæ¨¡å‹æœ€æ—©æ˜¯ä¸€å¥å¥è¾“å‡ºçš„ï¼Œåæ¥æ‰æ”¹æˆtokençº§æµå¼ã€‚
- è¯·æ•™ä¸€ä¸‹ï¼Œç°åœ¨å¸‚åœºä¸Šçš„å¤§æ¨¡å‹ï¼Œæ€ä¹ˆçŸ¥é“ä»–ä»¬çš„è®­ç»ƒæ•°æ®æ˜¯å¤šä¹…çš„å‘¢ï¼Ÿ
  - å¯ä»¥é—®ä¸€äº›ç‰¹å®šæ—¶é—´çš„æ–°é—»æ¥éªŒè¯ï¼Œä½†æ˜¯å…¶å®æ²¡å…³ç³»ã€‚æ¨¡å‹çš„ç²¾ç¡®çŸ¥è¯†ä¸é‡è¦ï¼Œä¹Ÿå……æ»¡å¹»è§‰ã€‚

- ## Attention is *Not* All You Needï¼Œè¿˜æ˜¯æœ‰äººåœ¨å°è¯• transformer ä»¥å¤–çš„æ¶æ„ï¼Œ
- https://x.com/liumengxinfly/status/1835251398692508114
  - æ¯•ç«Ÿ transfermor æ¨ç†å¤æ‚åº¦åœ¨æ•°å­¦ä¸Šæ˜¯æ— æ³•çº¿æ€§æ‰©å±•çš„ï¼Œæ—©æ™šä¼šèµ°åˆ°ç“¶é¢ˆ

- ## å›½äº§188ä¸ªå¤§æ¨¡å‹çš„excelæ–‡æ¡£ï¼š åŒ—äº¬69 ä¸Šæµ·22 æ­å·15 å¹¿ä¸œ26ä¸ª æ±Ÿè‹15ä¸ª
- https://twitter.com/FinanceYF5/status/1730912502312296935
  - [å›½äº§å¤§æ¨¡å‹188ä¸ªlist - Feishu Docs](https://zw73xyquvv.feishu.cn/wiki/WXLmwBbYuiTobkkJ6Ojc2cxqnj0?sheet=2XjJlJ&table=tblS2Jv7isKtSODz&view=vewfCdOf0U)

# discuss-llm-architecture
- ## 

- ## 

- ## [Your thoughts on "thinking" LLMs? : r/ollama _202602](https://www.reddit.com/r/ollama/comments/1qv1yey/your_thoughts_on_thinking_llms/)
  - almost all of the ollama-ready models released in recent months have been "thinking" or "chain of thought" or "reasoning" models -- you know, the ones that force you to watch the model's simulated thought process before it generates a final answer.

- thinking models are created because they work. RL is used during post-training to train it to think in a way that increases performance.

- Itâ€™s good for long prompts where youâ€™re asking the LLM to complete multiple tasks. The thinking tokens allow the model to work out the intricacies of what you are asking. Simple prompts like â€œhelloâ€ and â€œsummarize this:â€ isnt where these models excel. If the use cases correct, itâ€™s great!

- The smaller models thinking process does work for certain things, like solving puzzles, but it's generally much flimsier than a SotA model's thoughts.

- Thereâ€™s a huge difference between my qwen3 30b thinking vs non thinking models. Itâ€™s the difference between right and wrong answers.

- It's not fake, thinking tokens were created because in a lot of scenarios they help steering the model for a better answer without polluting the contex.

- ## [Files For AI Agents: Context, Search, Skills Guide | LlamaIndex _202601](https://www.llamaindex.ai/blog/files-are-all-you-need)
- 
- 
- 

- ## [[RELEASE] ARC Protocol v2.1: The Parallel Engine. Zero-Hallucination Engineering is here. : r/GoogleAntigravityIDE](https://www.reddit.com/r/GoogleAntigravityIDE/comments/1qmjrjo/release_arc_protocol_v21_the_parallel_engine/)
  - https://github.com/AshishOP/arc-protocol /MIT/python
  - ARC Protocol v2.1 (Analyze. Run. Confirm.) is the complete rebuild. Itâ€™s no longer just a workflow; it's a Parallel Engineering Engine that turns your AI into an autonomous fleet.
  - True Parallelism: Spawn specialized subagents (Alpha-Researcher, Beta-Coder, Gamma-Auditor) via the MCP Bridge. While you architect, they build.
  - Premium TUI Dashboard: A high-fidelity mission control to watch your fleet's heartbeat in real-time.
  - IDE Auto-Linker: A new script to automatically link the ARC bridge to Cursor, Windsurf, ClaudeCode, and VS Code.

- This is a vibe coded mess Like even the instructions to install it and set it up are vibe coded and not how the project actually works

- ## the most capable coding agents don't have hundreds of specialized tools
- https://x.com/vimota/status/2015823598577975453
  - they have three: read a file, write a file, run code
  - filesystem plus code execution is why Cursor and Claude Code can work autonomously for hours. 
  - But it only works when the agent runs on your machine, where you control security through approvals.
  - if you try to build it into products, you need a sandbox VM to avoid getting your files rm -rf'd
  - localsandbox is our exploration into an alternative that uses user-land isolation via a virtual filesystem, a bash interpreter, and WASM-sandboxed python execution.

- ## [I reverse-engineered Microsoft AutoGenâ€™s reasoning loop and cut agent latency by 85% (13.4s â†’ 1.6s). Here is the architecture. : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1qn2n4p/i_reverseengineered_microsoft_autogens_reasoning/)
  - Iâ€™ve been building voice agents using AutoGen, and the "awkward silence" during the Chain-of-Thought (CoT) phase was killing the UX. 
  - The standard sequential loop (Think â†’ Wait â†’ Execute Tool â†’ Wait â†’ Speak) just doesn't work for real-time interaction.
  - Instead of waiting for a v2 update, I dug into the ConversableAgent class and implemented a module for Speculative Reasoning Execution (SRE).
  - Standard Speculative Decoding predicts tokens. I adapted this to predict Tool Calls.

- ## ğŸ“Œ [Unrolling the Codex agent loop | OpenAI _202601](https://openai.com/index/unrolling-the-codex-agent-loop/)

- ## ğŸ§© è¿™ä¸ª Ralph Loop ç°åœ¨è¿™ä¹ˆæµè¡Œå—ï¼Ÿæˆ‘ä»Šå¤©æ‰ä»”ç»†å»äº†è§£ä¸€ä¸‹ï¼ŒåŸæ¥æ˜¯ä¸€ç§æ–¹æ³•è®ºå•Šï¼šç”¨ä¸€ä¸ª bash æ— é™å¾ªç¯åå¤å–‚ prompt ç»™ AI ç¼–ç å·¥å…·ï¼Œç›´åˆ°ä»»åŠ¡å®Œæˆã€‚
- https://x.com/vikingmute/status/2013607905040400520
  - * æ¯æ¬¡è¿­ä»£ AI éƒ½ä»å¹²å‡€ä¸Šä¸‹æ–‡å¼€å§‹ï¼ˆé¿å…ä¸Šä¸‹æ–‡æ±¡æŸ“ï¼‰ã€‚
  - * é€šè¿‡ git å†å²ã€progress.txtã€PRD.json ç­‰æ–‡ä»¶æŒä¹…åŒ–çŠ¶æ€å’Œå­¦ä¹ ï¼ˆæ¯”å¦‚ä¸Šè½®å­¦åˆ°çš„å‘ã€ä¸‹è½®è‡ªåŠ¨é¿å¼€ï¼‰ã€‚
  - * å¼ºè°ƒå°æ­¥è¿­ä»£ï¼šå¤§ä»»åŠ¡æ‹†æˆ 5-10 åˆ†é’Ÿå¯å®Œæˆçš„å­ä»»åŠ¡ï¼Œæ¯æ¬¡å¾ªç¯åªå¤„ç†ä¸€ä¸ªã€‚
  - * ç¨‹åºå‘˜åªç›‘ç£äº†ï¼šå†™å¥½ PRD å’Œ prompt åï¼Œç¡ä¸€è§‰è®© Ralph é€šå®µå¹²æ´»ï¼Œæ—©ä¸Š review PRã€‚

- æˆ‘é€‰æ‹©ç”¨ planning-with-filesï¼Œé™¤äº†ä¸èƒ½è‡ªåŠ¨å¾ªç¯ï¼ŒåŒæ ·èƒ½æŒä¹…åŒ–çŠ¶æ€å’Œç»éªŒï¼Œè‡ªå·±æŠŠå…³å½“å‰æ€æ¥å†³å®šæ˜¯å¦é‡æ‹¾ï¼Œå…³é”®è¿˜æ˜¯SOTAæ¨¡å‹è‡ªå·±çœŸå¾—çƒ§ä¸èµ·token

- æˆ‘è¯•è¿‡ï¼Œæœ‰ä¸¤å›éƒ½æ˜¯å¡åœ¨é‚£ä¸åœåœ°å†™æ–‡æ¡£ï¼Œæˆ‘æ²¡æœ‰ä»»ä½•å‘½ä»¤è®©ä»–å†™ï¼Œä»–å°±æ˜¯å†™ä¸ªä¸åœã€‚å®æ„¿å»ç”¨sddã€‚

- ## The Bitter Lesson of Agent Frameworks
- https://x.com/gregpr07/status/2012052139384979773
  - All the value is in the RL'd model, not your 10, 000 lines of abstractions.
  - An agent is just a for-loop of messages. The only state an agent should have is: keep going until the model stops calling tools. You don't need an agent framework. You don't need anything else. It's just a for-loop of tool calls.
  - Our first Browser Use agents had thousands of lines of abstractions. They worked - until we tried to change anything. Every experiment fought the framework. The agents weren't failing because the model was dumb. They were failing because we were.

- I almost completely agree with this except you don't even need tool calls. Just executable code fences.

- No, it's more than just loop. 
  - 1) Sessions are stateless. Smart framework will keep permanent memory without having to tell agents 101th time the same stuff.
  - 2) They try every possible trick to save tokens, to save costs for those who use API to access tokens
  - 3) Smart multiagent collaboration. Models are not equal, some are better in one specific area than others. That's why having two or three specialized agents working as a team will always beat single agent trying to comp thru massive codebases

- ## [Speculative Decoding: Turning Memory-Bound Inference into Compute-Bound Verification (Step-by-Step) : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1qg2592/speculative_decoding_turning_memorybound/)
  - Most of us assume LLM inference is slow because "matrix multiplication is hard." Thatâ€™s actually false.
  - For a batch size of 1 (which is standard for local inference/chat), your GPU is almost entirely Memory Bandwidth Bound. The bottleneck isn't doing the math; it's moving the 70GB+ of weights from VRAM to the compute units. The Arithmetic Logic Units (ALUs) are spending most of their time idle, waiting for data.
  - Speculative Decoding exploits this idle time to give us a "free lunch"â€”2x-3x speedups with mathematically identical outputs.
  - This converts a memory-bound operation (waiting for weights) into a compute-bound operation (doing more math on loaded weights), maximizing hardware utilization without retraining.

- Welcome to 2 or so years ago (which is eons in LLM). Speculative decoding using draft models looked great back then, but the problem is the limited practical usage. In order to get good speed ups there needs to be a huge difference between the draft model size and the target model size like 1B to 70B. Gains with smaller differences are smaller. In addition the models of that size are MoE now so speculative decoding is not needed. The last setup it did bring something to the table was probably Qwen2.5 where you could use the 0.5B model as draft for the 32B for example. I find limited usage for it nowadays, but it may just be my usage and there may be scenarios where people still use and like it.

- Recently I was reading Nvidia's Nemotron 3 paper. They have used speculative Decoding idea in their latest models. 
  - Except MTP is not the same speculative decoding you wrote about. It is also still only coming with the Super and Ultra models which have not been released yet.

- Works fine with models with a MTP layer

- Sad that vLLM does not fully support smaller drafter models (atleast last I checked) because my gemma-3-270m is screaming to boost my gemma-3-27B token generation speeds
  - Works with llama.cpp though so that's pretty good :). vLLM is next level though. Wish it supported that

- ## ğŸ˜ï¸ [Alternative to Transformer architecture LLMs : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nk58yc/alternative_to_transformer_architecture_llms/)
  - I wanted to ask if there are any other possible LLM architectures instead of this transformer.
- State Space Models (SSMs) are not using standard transformer architecture and they are getting some attention from various research institutes (look for Mamba paper on arxiv).
  - There is also RWKV that (depending on the version) looks more like a standard RNN.
  - Both of them have some advantages and disadvantages compared to transformers. Definitely cool research area

- There is some research towards using diffusion architecture for LLMs. LLaDa is one

- Hybrids are the latest thing, Nemotron Nano is only 8% transformer! Also Qwen3-Next is a hybrid.

- RWKV uses an RNN.

- [GFN v2.5.0: Verified O(1) Memory Inference and 500x Length Extrapolation via Symplectic Geodesic Flows : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1qgogis/gfn_v250_verified_o1_memory_inference_and_500x/)
  - We present GFN (Geodesic Flow Networks), an architecture that reformulates sequence modeling as particle dynamics on a learned Riemannian manifold. Unlike Transformer-based architecturesâ€”which exhibit O(N^2) memory scaling due to the attention mechanismâ€”or standard RNNs that suffer from vanishing gradients, GFN achieves O(1) memory complexity during inference and exhibits infinite-horizon stability through symplectic integration.
  - In our v2.5.0 release, we demonstrate perfect zero-shot generalization on algorithmic tasks with sequences up to 10, 000 tokens while maintaining a strictly bounded memory footprint of ~60MB.

- ## Release the VSC extension design for the local coding model: 
- https://x.com/LiMzba/status/2011106168136220739
  - Reasons for creating a new one instead of using the existing code harness:
  - Make the requests as sequential as possible to avoid overwhelming the local LLM server.
  - Optimize the tools to accommodate the instability of the local model
  - Remove timeout, so you can run it against your slowest model
  - It's fun to build agents
  - mlx-lm server as a first-class citizen, as I can only use mlx-lm to host models

- ## ğŸ“Œ [Ratios of Active Parameters to Total Parameters on major MoE models : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1q401ka/ratios_of_active_parameters_to_total_parameters/)
Model	Total Params	Active Params	% Active
GLM 4.5 Air	106	12	11.3%
GLM 4.6 and 4.7	355	32	9%
GPT OSS 20B	21	3.6	17.1%
GPT OSS 120B	117	5.1	4.4%
Qwen3 30B A3B	30	3	10%
Qwen3 Next 80B A3B	80	3	3.8%
Qwen3 235B A22B	235	22	9.4%
Deepseek 3.2	685	37	5.4%
MiniMax M2.1	230	10	4.3%
Kimi K2	1000	32	3.2%

- having more knowledge memorized, means the model has access to a broader library of reasoning strategies, even if it's not as effective at applying them as a higher active parameter count model. 
  - Pretty much every task will exist on the spectrum of "reasoning" versus "memorization" (even creative writing), and almost everything will be some combination of them. The more it is one end, the closer it'll perform to that parameter count. Ie: a pure reasoning task might depend basically only on active parameter count, whereas a pure memorization task scales basically 1:1 with total parameters.
  - In the end, far more important than the architecture is really the data, and training compute. With good data and compute, the difference between parameter ratios is pretty small, actually (under a fixed compute budget), while with bad data, it doesn't matter if you have a good architecture, it won't perform well.
  - The part you are maybe not taking into account is the the models with lower active parameter ratios are typically trained on more data, effectively, because on the same compute budget you train more tokens than the "dense equivalent" model, so it's not like you would magically have a model better for your usecase just because it's dense.
  - You have to evaluate every individual model, not just from its architecture, but in how it performs for you.

- In my case, granite-4.0-h-small (32B-A9B) is slow(10 t/s) on my 8GB VRAM(Tried to use Q4) due to A9B. Similar size Qwen3-30B-A3B is decent speed(30 t/s) for Q4. GPT-OSS-20B is good with 40 t/s. 

- One interesting thing could be comparing this ratio against dense models having similar benchmarks, across different time periods and empirically find out the formula we used to have to map the dense model size equivalent to MOE.

- ## Context Engineering é¢è¯•é¢˜ï¼šåœ¨ XX ä¸šåŠ¡åœºæ™¯ä¸‹é¢ï¼Œread_file, write_file å¦‚ä½•è®¾è®¡ï¼Ÿ é¢è¯•ä¸­é‡åˆ°è¿™é¢˜æˆ‘ä¼°è®¡ä¸´åœºå‘æŒ¥ä¸ä¼šå¤ªå¥½
- https://x.com/dotey/status/2007524872629387382
  - é¦–å…ˆå¾—åˆ†æåœºæ™¯ï¼Œç„¶åçœ‹åœºæ™¯éœ€è¦çš„ä¸Šä¸‹æ–‡ï¼Œè¿˜è¦çœ‹æ€ä¹ˆç®¡ç†ä¸Šä¸‹æ–‡
- æˆ‘ä¼šè¯´åœ¨æ–°çš„ aiæ—¶ä»£ï¼Œæˆ‘ä¼šå°†è¿™ä¸ªé—®é¢˜é—® opus 4.5ï¼Œè®©ä»–ç»™å‡ ä¸ªè§£å†³æ–¹æ¡ˆæ€è·¯ï¼Œæ¯ä¸ªæ€è·¯åˆ—å‡ºä¼˜ç¼ºç‚¹ï¼Œä¹‹åè®©æˆ‘æ¥åˆ¤æ–­
  - ä¸è¿‡æˆ‘è§‰å¾—è¿™æ€è·¯åœ¨çœŸå®å·¥ä½œåœºæ™¯è¿˜å¯ä»¥, é¢è¯•å½“ç„¶ä¸è¡Œï¼Œé¢è¯•è¦è¾¾åˆ°é€ ç«ç®­çš„æ°´å¹³

- æˆ‘çŒœè¦è€ƒå¯Ÿçš„æ˜¯åº”è˜è€…å¯¹äºä¸€ä¸ªtoolçš„è®¾è®¡ä¼šè€ƒè™‘å“ªäº›äº‹æƒ…ï¼Œæˆ‘ç²—ç•¥æƒ³äº†ä¸‹åº”è¯¥æœ‰ï¼š1. å‚æ•°é¡ºåºå¦‚ä½•æ§åˆ¶æ‰èƒ½é˜²æ­¢å‚æ•°é¡ºåºå¸¦æ¥çš„UIæ¸²æŸ“çš„å¥‡æ€ªé—®é¢˜ ï¼› 2.  é™¤äº†åŠŸèƒ½å­—æ®µæ¯”å¦‚path, contentå­—æ®µä¹‹å¤–æ˜¯å¦éœ€è¦åŠ å…¥ä¸€äº›descriptionå­—æ®µæå‡UIä½“éªŒï¼› 3. é™¤äº†toolåŠŸèƒ½æœ¬èº«ä¹‹å¤–ï¼Œå¯ä»¥æœ‰å“ªäº›tool call å¼‚å¸¸errorå¢å¼ºå’Œç‰µå¼•è®¾è®¡ 4. å¤§æ–‡ä»¶è¯»å†™å¦‚ä½•å¤„ç†ï¼Œæ¯”å¦‚æ˜¯å¦è¦åˆ†å±‚åŠ è½½æˆ–æµå¼è¯»å–  5. ä¸åŒåœºæ™¯ä¸‹çš„read_file, write_fileè€ƒè™‘çš„ç‚¹æ˜¯å¦ä¸ä¸€æ ·ã€‚ 6. å¦‚æœè¦åšcheckpointï¼Œæ˜¯å¦è¦æ”¾åˆ°toolé‡Œè¿˜æ˜¯hooksé‡Œã€‚ 7. ä¸åŒç¯å¢ƒå¦‚ä½•è®¾è®¡ï¼Œæ¯”å¦‚è¿œç¨‹æ²™ç®±ç¯å¢ƒï¼Œæœ¬åœ°ç¯å¢ƒç­‰ 8. æŸäº›åœºæ™¯æ˜¯å¦éœ€è¦åšä¸šåŠ¡æ—è·¯é€»è¾‘

- ä¸åŒä¸šåŠ¡çŠ¶æ€ä¸‹ï¼ŒåŒæ ·æ˜¯â€œæ–‡ä»¶æ“ä½œâ€ï¼Œè¯­ä¹‰ä¸åŒã€‚èƒ½æŠŠè¿™äº›åœºæ™¯ã€ä¸šåŠ¡çŠ¶æ€è¯´å‡ºæ¥ï¼Œè§£é‡Šå¯¹åº”çš„è¯­ä¹‰ä»¥åŠå¯¹åº”çš„æŠ€æœ¯è¯‰æ±‚ã€çº¦æŸï¼Œåº”è¯¥æ˜¯å¥½çš„å›ç­”ã€‚

- è¿™ä¸ªé—®é¢˜è¿˜æœ‰ä¸ªåŠ å€ï¼Œå°±æ˜¯è¿œç¨‹è™šæ‹Ÿç¯å¢ƒçš„read_fileã€write_fileå¦‚ä½•è®¾è®¡ï¼Œæˆ‘è¢«è¿™ä¸ªæŠ˜ç£¨å¥½ä¹…ï¼Œæœ€åæ”¾å¼ƒäº†ï¼Œå’Œæœ¬åœ°å·®ä¸å¤šå¾—äº†

- ## éšæ‰‹å†™å†™æˆ‘å½“å‰è®¤è¯†çš„ coding agent orchestrator /å¤šagentç¼–æ’å™¨ã€‚
- https://x.com/LotusDecoder/status/2007292223864353071
  - é¢„æ„Ÿè¿™ä¸ªä¼šæ˜¯2026å¼€å¹´æœ€ç«çš„æ¦‚å¿µã€‚ç±»ä¼¼äº2024çš„cursorï¼Œ2025å¹´çš„claude codeã€‚
  - é¦–å…ˆè¿™æ˜¯AI agentæ€§èƒ½æå‡å‘å±•åçš„å¿…ç„¶è¶‹åŠ¿ï¼Œä»chatbotæŸ¥æŸ¥æ‰‹å†Œï¼Œåˆ°cursoræ”¹æ–‡ä»¶ï¼Œå†åˆ°claude code cliå¼ä¸€æ¬¡æˆå‹æ•´ä¸ªé¡¹ç›®ï¼Œåˆ°äº†ç°åœ¨å•ä¸ªagentå·²ç»æˆç†Ÿä¹‹åï¼Œè‡ªç„¶å¼€å§‹è¿›å…¥å¤šagentåˆ†å·¥åä½œï¼Œç„¶åå¤šä¸ªagentå¦‚ä½•æ²Ÿé€šã€è¿ç­¹ã€ç®¡ç†ï¼Œè¿™å°±å¾ˆéœ€è¦æ–°çš„èŒƒå¼äº†ã€‚
  - æœ€åŸºç¡€çš„æ˜¯claude codeé‡Œå†…éƒ¨è¯­æ³•å¯åŠ¨subagentï¼Œç”¨bashå‘½ä»¤æˆ–æ–‡æ¡£æ¥ç®¡ç†å„è‡ªçš„ä»»åŠ¡ç›®æ ‡ã€æ—¥å¿—ã€æˆæœã€‚
  - vaguelå…¶æ¬¡ï¼Œæ¯”è¾ƒå‹å¥½çš„æ˜¯ opencode çš„ oh my opencodeï¼Œæœ‰ä¸€ä¸ªåˆå§‹çš„é»˜è®¤ç¼–æ’å™¨é…ç½® è¥¿è¥¿å¼—æ–¯ç³»ç»Ÿï¼Œä¸€æ¬¡æ€§ç¼åˆOpus-4.5 Gemini-3-pro Gpt-5.2ï¼Œå„è‡ªå¹²å„è‡ªæ“…é•¿ã€‚è®¾è®¡äº†æŸ¥æ–‡æ¡£ã€å‰ç«¯ã€åç«¯ã€æ€»æ§å°ç­‰å¤šä¸ªè§’è‰²ï¼ŒæŠŠmodelä¾æ¬¡æ¥å…¥è¿›å»ï¼Œç”±æ€»æ§å°æ¥è‡ªè¡Œåˆ†é…ä»»åŠ¡ã€‚
  - è¿˜æœ‰å£°éŸ³ä¹Ÿæ¯”è¾ƒå¤§çš„ conductorï¼Œè¿™ä¸ªè¿˜æ²¡ç”¨è¿‡ã€‚ ä¼˜ç‚¹æ˜¯å¯è§†åŒ–ç•Œé¢ç®¡ç†å¤šä¸ª agentï¼Œæ¯ä¸ª agent åœ¨ç‹¬ç«‹ git worktree ä¸­å·¥ä½œã€‚è¿˜æœ‰æ”¯æŒcodexã€‚
  - ç°åœ¨æœ€æ¿€è¿›çš„å…ƒæ—¦åˆšå‘å¸ƒçš„ gas townï¼Œä½œè€…è¯´å±•ç¤ºç»™anthropicå…¬å¸å†…éƒ¨çœ‹çš„æ—¶å€™ï¼Œå“åˆ°ä»–ä»¬äº†ã€‚è¿™ä¸ªé¡¹ç›®ç›´æ¥æŠŠæ‰€æœ‰çš„cli å½¢å¼agentä¸€æ½å­åŒ…äº†è¿›æ¥åšç¼–æ’ï¼Œä¸»æµçš„ claude code ã€codexä¹‹å¤– Gemini CLI ã€amp ç­‰ç­‰å…¨éƒ¨å›Šæ‹¬äº†è¿›æ¥ã€‚
  - orchestratorçš„å¤§å‘å±•å°†ä¼šæ˜¯AIæ€§èƒ½æå‡åï¼Œèµ°åˆ°ä¸‹ä¸€æ­¥çš„å¿…ç„¶ï¼Œå¹¶å‘å¹¶è¡Œå¤šä¸ªå¤šç§agentå¹²æ´»ï¼Œå†AIæµ‹è¯•åˆå¹¶ï¼Œå®Œæˆä¸€ä¸ªå¤§å‹é¡¹ç›®ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªæœªæ¥å¾ˆé¥è¿œçš„äº‹æƒ…ï¼Œç”šè‡³å¾ˆå¿«åˆä¼šæˆä¸ºä¸€ç§æ ‡å‡†èŒƒå¼å§ã€‚å½“ç„¶ï¼Œè¿™å¯¹äººçš„è¦æ±‚æ›´é«˜äº†ï¼Œéœ€è¦å¾ˆç³»ç»Ÿçš„è½¯ä»¶å·¥ç¨‹èƒ½åŠ›ï¼Œå¯¹é’±åŒ…çš„è¦æ±‚ä¹Ÿå˜é«˜äº†ï¼Œä»¥å‰ä¸€ä¸ªagentï¼Œorchestratoræ˜¯ä¸€ç¾¤agentçƒ§token

- conductor å€¼å¾—ä¸€è¯•ï¼Œæˆ‘è¿™ä¸¤å¤©åœ¨ç”¨çš„ï¼Œå¤š workspace/åˆ†æ”¯å¹¶è¡Œã€‚ Claudecode/codex è®¢é˜…å¯ä»¥ç›´æ¥ç”¨

- ## [Local model registry to solve duplicate GGUFs across apps? : r/LocalLLM _202512](https://www.reddit.com/r/LocalLLM/comments/1pygyhi/local_model_registry_to_solve_duplicate_ggufs/)
  - I'm running into storage issues with multiple local LLM apps. I downloaded Olmo3-7B through Ollama, then wanted to try Jan.ai's UI and had to download the same 4GB model again. Now multiply this across Dayflow, Monologue, Whispering, and whatever other local AI tools I'm testing.
  - Each app manages its own model directory. No sharing between them. So you end up with duplicate GGUFs eating disk space.
  - Feels like this should be solvable with a shared model registry - something like how package managers work. Download the model once, apps reference it from a common location. Would need buy-in from Ollama, LMStudio, Jan, LibreChat, etc. to adopt a standard, but seems doable if framed as an open spec.
  - I'm guessing the OS vendors will eventually bake something like this in, but that's years away. Could a community-driven library work in the meantime? Or does something like this already exist and I'm just not aware of it?

- They're just files. You can remove duplicates yourself and replace them with symlinks to whichever copy you choose to make the "primary".

- Jan has Import option(to use downloaded GGUF files from any folder).
  - Koboldcpp also does this just with browse GGUF option.
  - For Oobabooga, I used symlinks option.
- I found that buried in the Jan.ai UI under Settings / Model Providers / Llama.cpp / import.

- 
- 
- 
- 
- 
- 
- 
- 

- ## ğŸ¤” [How do you make agents deterministic? : r/AI_Agents](https://www.reddit.com/r/AI_Agents/comments/1pv2gfk/how_do_you_make_agents_deterministic/)
  - I have been talking to many business and a common concern has been lack of reliability of ai agents.
- In my experience, determinism does not come from the model, it comes from the system design around it. You cannot prompt an LLM into behaving like a rule engine. Business rules need to live in code or configuration, not in free text prompts. The agentâ€™s job is to interpret context and propose actions, not decide what is allowed.
  - What works well is separating concerns very clearly. Rules, policies, and exceptions are encoded as deterministic logic or tables.
  - For reliability, many teams also constrain where and how agents can act. When agents need to interact with real systems, running them in predictable environments like hyperbrowser helps keep execution consistent and auditable, which is critical in regulated workflows.

- Typically variance results from ambiguous instructions or situations. Use this method to identify specific points in your conversation and see how you can improve the context.
  - Use something like Langfuse to run experiments and trace execution
  - Run the same experiment multiple times.
  - Use python to scrape the data and compare outputs across runs
  - Compute deltas in your outputs
  - Check if specific inputs prove to have more variance in output
- Correct. In fact you should be focusing more on the sad paths because those are the edge cases you need to test.

- Use regular workflow automation tools instead of agents. Or make your tools highly deterministic and have obvious tool selection criteria.

- 90% of it is prompt engineering and a feedback loop. You have another AI at the end that is fed the last AIs user, systems messages and outputs and feeback from client "like i expected it to do this", this will give you a feedback loop on what went wrong in prompt.

- I have been trying to achieve this with my own framework. Its possible. Not in the sense of 1:1 deterministic responses but the core logic of agent output can be deterministic. People have written some methods which I mostly followed. Plus my framework has agent driven, smart retries, which enforces responds to be constrained. As a result my framework, I can deliver basic crud applications with different domain with exact same gaps and similar bugs. They all look very identical. After I get back from holiday I will announce my work. Looking forward to hear your feedbacks.

- ## ğŸ˜ï¸ğŸ§© [Agents | OpenCode](https://opencode.ai/docs/agents/)
- Agents are specialized AI assistants that can be configured for specific tasks and workflows. 
- There are two types of agents in OpenCode; primary agents and subagents.
- Primary agents are the main assistants you interact with directly. 
  - These agents handle your main conversation and can access all configured tools.
  - OpenCode comes with two built-in primary agents, Build and Plan. 
  - Build is the default primary agent with all tools enabled. This is the standard agent for development work where you need full access to file operations and system commands.
  - Plan is a restricted agent designed for planning and analysis. We use a permission system to give you more control and prevent unintended changes. This agent is useful when you want the LLM to analyze code, suggest changes, or create plans without making any actual modifications to your codebase.
- Subagents are specialized assistants that primary agents can invoke for specific tasks. 
  - OpenCode comes with two built-in subagents, General and Explore. 
  - General is a general-purpose agent for researching complex questions, searching for code, and executing multi-step tasks. Use when searching for keywords or files and youâ€™re not confident youâ€™ll find the right match in the first few tries.
  - Explore is a fast agent specialized for exploring codebases. Use this when you need to quickly find files by patterns, search code for keywords, or answer questions about the codebase.

- ## ä¸ºä»€ä¹ˆç°åœ¨ AI å†™å‰ç«¯ä¸€ä¸‹å­è¿™ä¹ˆå¼ºäº†ï¼Œå°ç±³çš„ MiMo è®ºæ–‡çš„è¿™æ®µä»‹ç»äº†ä»–ä»¬å¦‚ä½•è®­ç»ƒæ¨¡å‹å†™å‰ç«¯çš„ï¼Œå…³é”®æ˜¯è¿™å¥ï¼š
- https://x.com/linexjlin/status/2002383307414409514
  - æˆ‘ä»¬çš„åŸºäºè§†è§‰çš„éªŒè¯å™¨é€šè¿‡å¯¹å½•åˆ¶çš„è§†é¢‘ç‰‡æ®µæ‰§è¡Œæƒ…å†µè¿›è¡Œè¯„åˆ†ï¼Œç»¼åˆè¯„ä¼°è§†è§‰è´¨é‡ã€åŠŸèƒ½å‡†ç¡®æ€§å’Œå¯æ‰§è¡Œæ€§ï¼Œä»è€Œç¡®ä¿å¥–åŠ±æœºåˆ¶èƒ½å¤ŸåŒæ—¶å…¼é¡¾å¤–è§‚è¡¨ç°ä¸è¡Œä¸ºæ•ˆæœã€‚
  - åŸç†ä¸Šå°±æ˜¯æ¨¡å‹æ ¹æ® prompt å†™å¥½ä»£ç åå†ç”¨ Playwright æ“ä½œå½•åˆ¶æˆè§†é¢‘ï¼Œç„¶åï¼Œäº¤ç»™ä¸€ä¸ªè§†è§‰éªŒè¯å™¨ï¼ˆåº”è¯¥æ˜¯ä¸€ä¸ªä¸“é—¨è®­ç»ƒçš„è§†é¢‘ç†è§£æ¨¡å‹ï¼‰ è¿›è¡Œæ‰“åˆ†ï¼Œä»¥æä¾›å¥–åŠ±ä¿¡å·ã€‚

- è¿˜åœ¨æ£é¼“XXPOçš„æˆ‘è§‰å¾—è‡ªå·±æœ›å°˜è«åŠäº†
  - é‡å¤æ€§å·¥ä½œäº¤æ¥ AI  æ˜¯å¯¹çš„ï¼Œäººå¤šå‡ºçš„ç²¾åŠ›å¯ä»¥ç”¨åœ¨å®¡ç¾æŠŠå…³ä¸Šã€‚

- åº”è¯¥æ˜¯è®­äº†ä¸€ä¸ªå¾ˆå°çš„è§†è§‰åˆ¤åˆ«æ¨¡å‹ï¼Œå‰æœŸåŠ äº†å¾ˆå¤šäººå·¥è¯„åˆ¤çš„æ•°æ®ï¼Œè¿˜æ˜¯æŒºæ‰å®çš„

- æ‰€ä»¥ç°åœ¨æ¸…æ¥šäº†ï¼Œå…¶å®è¿˜å¾—ä¾é äººä¸ºåˆ›å»ºå¾ˆå¤šè®­ç»ƒæ•°æ®

- è¿™æ¬¡ç»è¿‡äº†å¤šå°‘å·¥ç¨‹å¸ˆå¤šå°‘å°æ—¶çš„æ‰“ç£¨

- ## [Is direct tool use a trap? Would it be better for LLMs to write tool-calling code instead? : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1px089u/is_direct_tool_use_a_trap_would_it_be_better_for/)
- Look into smolagents from HF, it uses something pretty similar to this.

- "better" is defined by what's model was trained on. Code path is definitely more scalable, but also more unsafe. Currently, most models are primarily trained on direct calls mode, so code calling requires taking context space with explanations and represents another failure mode for the model.
  - Source: agents in my company can decide to call their tools via code, it's pretty hard to make them choose it, unless very specifically targeting to do so.

- Scales much better to give the ai access to a virtual folder structure with code for calling different functions. Just dumping everything into context is silly.

- I use each approach. Function calling APIâ€™s are my approach pretty much 100% of the time if I am building an agent which is integrating with another system with a contract and the job of the agent is to talk to the other system(s).
  - A sandbox is better for broader use cases where the work is all internal.
  - I often define APIs using tool calls that map to python functions running inside the sandbox which is a mix of the two.
  - My opinion is that it completely depends on the situation 

- if you are going to allow a model to write and run arbitrary code, it is riskier and should be sandboxed in most situations. I think smolagents does have the option to constrain to just the functions you are given it or write and run other things as well. I am not 100 percent sure they can guarantee this.

- Yes, this is true. I've had great success providing a restrictive JS environment and typescript definition files and using this instead of JSON tool calls, especially when executing steps which would otherwise require multiple tool calls. However, it does depend on having a model that has been trained extensively on code.

- ## ğŸ’¡ [anthropic blog on code execution for agents. 98.7% token reduction sounds promising for local setups : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/)
  - instead of direct tool calls, model writes code that orchestrates tools
  - basic idea: dont preload all tool definitions. let model explore available tools on demand. data flows through variables not context
  - for local models this could be huge. context limits hit way harder when youre running smaller models
  - the privacy angle is interesting too. sensitive data never enters model context, flows directly between tools
  - cloudflare independently discovered this "code mode" pattern according to the blog
  - main challenge would be sandboxing. running model-generated code locally needs serious isolation
  - tools like cursor and verdent already do basic code generation. this anthropic approach could push that concept way further
  - wondering if anyone has experimented with similar patterns locally

- FYI, this pattern already exists in HFs smolagents, they use model-generated code to execute tools instead of JSON tool calls
  - yep, smolagents is definitely already using this pattern.
- it's up to you how you execute this. The whole approach should depend on strong sandboxing. smolagents can run generated code in a restricted executor, same assumption Anthropic makes in the blog
- The searchable filesystem approach to tool definitions was the most interesting bit for me, very clean way to avoid preloading huge schemas, whether you use code or JSON

- Yes, though in my case I have the model generating a DAG of steps it wants to run instead of arbitrary code, which reduces the sandboxing needed, avoids non-terminating constructs, etc.
  - Token-efficiency is a side-benefit from my perspective. Moving to the plan->execute pattern also makes problems tractable for smaller models, many of which are able to understand instructions and produce "code" of some sort, but which may struggle to pluck details out of even a relatively short context window with the needed accuracy.
- I really like the DAG / planâ†’execute approach , especially for sandboxing and small models. It feels aligned with the same idea of keeping data and state out of the model context, just with tighter structure. Do you generate the full DAG upfront, or refine it during execution?
  - 2 modes. The model can propose a dag using a planning tool and then the user can discuss/iterate it, or auto mode where it just runs.

- if you are writing the function why call an MCP server? Why not just do what the MCP does?
  - MCP is more easily reusable.
- I'd second that; any reasonably shaped API should work really, but this way you avoid installing any packages and browsing for the API docs. It's a way for the model to discover the API instead of being fed how to use it.

- ## Use Anthropic's tool search to give your agent hundreds of tools without filling its context window.
- https://x.com/aisdk/status/2000886249306120473
- Or you could use http://mcpz.it which came out in Feb this year and was the first tool to actually identify and solve the tooling issues with MCPs 

- Game-changer for multi-tool agents  Tool search + deferLoading = hundreds of tools without context explosion? Claude-Sonnet-4.5 agents just scaled massivelyâ€”Vercel AI SDK making production agents real. 

- ## ğŸ¤” [How to make LLM output deterministic? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1plbe8i/how_to_make_llm_output_deterministic/)
  - I am working on a use case where i need to extract some entities from user query and previous user chat history and generate a structured json response from it. The problem i am facing is sometimes it is able to extract the perfect response and sometimes it fails in few entity extraction for the same input ans same prompt due to the probabilistic nature of LLM. I have already tried setting temperature to 0 and setting a seed value to try having a deterministic output.
  - does setting seed value really work? In my case it seems it didn't improve anything.

- Because of certain GPU optimizations, LLMs are technically random even at temperature = 0 IIRC. llama.cpp has a similar issue. And you can run into something similar in training as well for a given training seed unless you configure some knobs if I'm not misremembering.

- Here's a really good blog post around LLM determinism: https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/
  - If you were to host your LLM locally, both vLLM and SGLang have done work on providing deterministic / batch invariant inference

- change your mindset, when you work with llm it is non-deterministic, whatever you do there is still tiny chance that it can't deliver deterministic response. always handling the non-deterministic part is crucial in all llm base application
  - for me personally, try to prompt the model to wrap the anser around xml tag is quite reliable like `<Answer>what ever llm response</Answer>` and going from there

- Literally impossible to make them fully deterministic because input itself affects the inference matrix.

- The problem with using cloud LLM APIs is that your requests will get batched with others which introduces nondeterminism, even with temperature sampling disabled.
  - Itâ€™s relatively easy to achieve this if you run a model yourself and set the batch size = 1, however.

- ## [Claude Skills are just .cursorrules, change my mind : r/ClaudeAI](https://www.reddit.com/r/ClaudeAI/comments/1oj109n/claude_skills_are_just_cursorrules_change_my_mind/)
- It's all just prompt really. Basically an LLM only has prompt and output.

- Cursor rules are like CLAUDE.md files. They are loaded into context on every AI interaction. Claude Skills are executable capabilities that can be invoked _when needed_. 
  - Nope, cursorrules have markdown header thingies with a description and can either be auto-loaded, or invoked by the LLM which will only see their descriptions before invoking them.

- They can run packaged scripts when needed.

- The marketplace/plugin system is the big differentiator, imo

- ğŸ’¡ I think it was presented as revolutionary because it's adaptable for more than just coding. It can execute the scripts and call tools to use for specific purposes. I think the fact that it's accessible to Claude that AI web means that it's accessible to more individuals that may not be using cursor or something else for coding so available for other use cases

- I wish Claude Code could lazy load MCPs as same as skills.

- I'm a Cursor user and I'd love to replicate what I see people getting out of skills.
  - cursor rules are only loaded at the start of a chat. You can have 50 rules, and logic for which ones to load so that cursor only ever loads a few per chat, but it's still always at the start of a chat.
  - My understanding of Claude Skills is that they can be accessed more-or-less at any time. If I want to e.g. file a bug midway through a chat, if the "File a bug" rule wasn't included at the start, Cursor doesn't seem to re-read its Cursor rules and pick up the newly-needed ones, whereas it sounds like Claude Code will.

- ## [å¦‚ä½•çœ‹Anthropicæœ€æ–°å‘å¸ƒçš„Claude Skillsï¼Ÿä¼šæ›¿ä»£MCPå—ï¼Ÿ - çŸ¥ä¹ _202510](https://www.zhihu.com/question/1962512846630941008)
- è¯´ä¸ªæš´è®ºï¼ŒAI Agentæƒ³è¦è½åœ°ï¼Œéœ€è¦çš„åªæœ‰å¼ºå¤§çš„æ¨¡å‹åŸºåº§ï¼Œä»€ä¹ˆSkillã€MCPã€... éƒ½åªæ˜¯æ·»å¤´ï¼Œé€šè¿‡ä»£ç å¾ˆå®¹æ˜“å®ç°ã€‚
- Skillå°±æ˜¯ä¸€ä¸ªæ ‡å‡†åŒ–çš„æ–‡ä»¶å¤¹ï¼Œç”¨æ¥æ‰“åŒ…Agentå®Œæˆç‰¹å®šä»»åŠ¡æ‰€éœ€çš„çŸ¥è¯†å’Œå·¥å…·ã€‚
  - å¯ä»¥æŠŠå®ƒç†è§£æˆç»™æ¨¡å‹çš„è¯´æ˜ä¹¦æˆ–æ ‡å‡†ä½œä¸šç¨‹åºï¼ˆSOPï¼Œæˆ–è€…ä¹‹å‰æ¯”è¾ƒç«çš„æ¦‚å¿µï¼šSPECçš„å¢å¼ºç‰ˆï¼‰ã€‚
  - Anthropicè¿™æ¬¡ä¸ä»…å‘å¸ƒäº†æ¦‚å¿µï¼Œè¿˜ç›´æ¥å¼€æºäº†ä¸€ä¸ªGitHubä»“åº“, é‡Œé¢åŒ…å«äº†æ‰€æœ‰20ä¸ªå·¦å³çš„å®˜æ–¹Skillçš„æºç ç¤ºä¾‹
- ä¸€ä¸ªSkillæ–‡ä»¶å¤¹é€šå¸¸åŒ…å«è¿™å‡ éƒ¨åˆ†ï¼š
  - SKILL.mdï¼šæ ¸å¿ƒæ–‡ä»¶ï¼Œå¿…é¡»å­˜åœ¨ã€‚é‡Œé¢ç”¨YAMLå†™å…ƒæ•°æ®ï¼ˆåå­—ã€æè¿°ï¼‰ï¼Œç”¨Markdownå†™è¯¦ç»†çš„æŒ‡ä»¤ï¼Œå‘Šè¯‰Claudeåœ¨ä»€ä¹ˆæƒ…å†µä¸‹ã€ä»¥åŠå¦‚ä½•ä½¿ç”¨è¿™ä¸ªSkillã€‚
  - scripts/ï¼šå­˜æ”¾å¯æ‰§è¡Œçš„Pythonã€Shellè„šæœ¬ã€‚æ¯”å¦‚PDFå¤„ç†Skillé‡Œï¼Œå°±æœ‰fill_fillable_fields.pyè¿™ç§ç¡®å®šæ€§æå¼ºçš„ä»£ç ã€‚
  - references/ï¼šå­˜æ”¾å‚è€ƒæ–‡æ¡£ã€‚æ¯”å¦‚APIæ–‡æ¡£ã€æ•°æ®åº“Schemaã€å…¬å¸æ”¿ç­–ç­‰ï¼Œè¿™äº›æ˜¯ç»™Claudeçœ‹çš„çŸ¥è¯†åº“ã€‚
  - assets/ï¼šå­˜æ”¾èµ„æºæ–‡ä»¶ã€‚æ¯”å¦‚PPTæ¨¡æ¿ã€å…¬å¸Logoã€Reacté¡¹ç›®è„šæ‰‹æ¶ç­‰ï¼Œè¿™äº›æ˜¯Claudeåœ¨æ‰§è¡Œä»»åŠ¡æ—¶ç›´æ¥ä½¿ç”¨çš„æ–‡ä»¶ï¼Œè€Œä¸æ˜¯é˜…è¯»çš„
- ä¸€ä¸ªSkill = ä»»åŠ¡è¯´æ˜ä¹¦ SKILL.md + å·¥å…·ä»£ç  (scripts) + ä¸“ä¸šçŸ¥è¯† (references) + ç´ æèµ„æº (assets)ã€‚
  - å®ƒæŠŠå®Œæˆä¸€ä¸ªç‰¹å®šä»»åŠ¡æ‰€éœ€çš„ä¸€åˆ‡éƒ½æ‰“åŒ…å¥½äº†ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯ä¸€ç§ä»£ç å’Œèµ„æºçš„ç»„ç»‡æ–¹å¼ï¼Œä¸€ç§çº¦å®šä¼˜äºé…ç½®çš„ç†å¿µã€‚
- ğŸ’¡ Claude Skillsè®¾è®¡çš„ç²¾é«“ï¼Œä¹Ÿæ˜¯å®ƒå’Œç®€å•RAG/MCP/FunctionCallingçš„æœ€å¤§åŒºåˆ«ã€‚å®ƒå°±æ˜¯ä¸€å¥—èªæ˜çš„ï¼Œä¸ºäº†èŠ‚çœä¸Šä¸‹æ–‡çª—å£è€Œè®¾è®¡çš„åˆ†å±‚åŠ è½½ç­–ç•¥ã€‚ 
  - ç¬¬ä¸€å±‚ï¼šå…ƒæ•°æ®ï¼ˆName + Descriptionï¼‰ã€‚è¿™éƒ¨åˆ†ä¿¡æ¯éå¸¸ç®€çŸ­ï¼Œä¼šå¸¸é©»åœ¨Claudeçš„è„‘æµ·é‡Œã€‚å½“ç”¨æˆ·æå‡ºä¸€ä¸ªä»»åŠ¡æ—¶ï¼ŒClaudeä¼šå¿«é€Ÿæ‰«ææ‰€æœ‰å¯ç”¨Skillçš„æè¿°ï¼Œåˆ¤æ–­å“ªä¸ªå¯èƒ½ç›¸å…³ã€‚è¿™æ˜¯ç¬¬ä¸€é“ç­›é€‰ï¼Œæˆæœ¬æä½ã€‚
  - ç¬¬äºŒå±‚ï¼šSKILL.mdã€‚å½“Claudeè®¤ä¸ºæŸä¸ªSkillç›¸å…³æ—¶ï¼Œå®ƒæ‰ä¼šå»åŠ è½½SKILL.mdé‡Œçš„è¯¦ç»†æŒ‡ä»¤ã€‚è¿™éƒ¨åˆ†å†…å®¹å‘Šè¯‰Claudeå®Œæˆä»»åŠ¡çš„å…·ä½“æ­¥éª¤ã€åº”è¯¥éµå¾ªçš„è§„åˆ™ã€ä»¥åŠå¦‚ä½•ä½¿ç”¨æ–‡ä»¶å¤¹é‡Œçš„å…¶ä»–èµ„æºã€‚è¿™æ­¥çš„ä¸Šä¸‹æ–‡æ¶ˆè€—ä¸­ç­‰ã€‚
  - ç¬¬ä¸‰å±‚ï¼šè„šæœ¬å’Œå‚è€ƒæ–‡æ¡£ã€‚åªæœ‰å½“SKILL.mdé‡Œçš„æŒ‡ä»¤æ˜ç¡®è¦æ±‚ï¼Œæˆ–è€…Claudeåœ¨æ‰§è¡Œä¸­åˆ¤æ–­éœ€è¦æ—¶ï¼Œå®ƒæ‰ä¼šå»è¯»å–scripts/é‡Œçš„ä»£ç æˆ–references/é‡Œçš„æ–‡æ¡£ã€‚è¿™æ­¥çš„ä¸Šä¸‹æ–‡æ¶ˆè€—æ˜¯æŒ‰éœ€çš„ï¼Œé¿å…äº†ä¸€æ¬¡æ€§æŠŠæ‰€æœ‰ä¸œè¥¿éƒ½å¡è¿›å»ã€‚
- è¿™ä¸ªæœºåˆ¶çš„å¥½å¤„æ˜¾è€Œæ˜“è§ï¼Œæå¤§åœ°èŠ‚çœäº†å®è´µçš„ä¸Šä¸‹æ–‡çª—å£ã€‚å®ƒå…ˆå‡­ç»éªŒåˆ¤æ–­ç”¨å“ªä¸ªSOPï¼Œç„¶åç¿»å¼€SOPç…§ç€åšï¼Œé‡åˆ°å…·ä½“é—®é¢˜å†æŸ¥é˜…é™„å½•æˆ–å·¥å…·æ‰‹å†Œã€‚è¿™å¥—é€»è¾‘ï¼Œæˆ‘ä»¬ç”¨ä»£ç å½“ç„¶ä¹Ÿèƒ½å®ç°ï¼Œä½†SkillsæŠŠå®ƒæ ‡å‡†åŒ–äº†ã€‚

- å®ƒå’ŒMCPæ˜¯ä»€ä¹ˆå…³ç³»ï¼Œä¼šæ›¿ä»£å—ï¼Ÿç›´æ¥å›ç­”ï¼Œå®Œå…¨ä¸æ˜¯ä¸€å›äº‹ï¼Œä¸ä¼šæ›¿ä»£ï¼Œç”šè‡³æ˜¯äº’è¡¥çš„ã€‚
  - MCPæ˜¯ä¸€ç§é€šä¿¡åè®®ã€‚å®ƒå®šä¹‰äº†Agentï¼ˆå®¢æˆ·ç«¯ï¼‰å¦‚ä½•ä¸ä¸€ä¸ªæš´éœ²äº†å·¥å…·çš„æœåŠ¡ï¼ˆæœåŠ¡ç«¯ï¼‰è¿›è¡Œæ ‡å‡†åŒ–çš„äº¤æµã€‚å®ƒè§£å†³çš„æ˜¯Agentä¸å¤–éƒ¨å·¥å…·å¦‚ä½•å¯¹è¯çš„é—®é¢˜ã€‚
  - Claude Skillsæ˜¯ä¸€ç§èƒ½åŠ›å°è£…æ ¼å¼ã€‚å®ƒå®šä¹‰äº†Agentè‡ªèº«åº”è¯¥å…·å¤‡å“ªäº›çŸ¥è¯†ã€å·¥ä½œæµå’Œå†…éƒ¨å·¥å…·ã€‚å®ƒè§£å†³çš„æ˜¯Agentå¦‚ä½•æ€è€ƒå’Œè¡ŒåŠ¨çš„é—®é¢˜ã€‚
  - Skillé‡Œçš„çŸ¥è¯†å¯ä»¥æŒ‡å¯¼Agentå¦‚ä½•æ›´æœ‰æ•ˆåœ°å»ä½¿ç”¨ä¸€ä¸ªéµå¾ªMCPåè®®çš„å·¥å…·ã€‚ä¸€ä¸ªAgentå®Œå…¨å¯ä»¥åŠ è½½ä¸€ä¸ªSkillï¼Œç„¶åæ ¹æ®Skillé‡Œçš„æŒ‡ä»¤ï¼Œå»è°ƒç”¨ä¸€ä¸ªè¿œç¨‹çš„MCPæœåŠ¡å™¨

- æœ€å¤§çš„ä»·å€¼æ˜¯ï¼šAnthropicæŠŠä»–ä»¬åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ‰“ç£¨å‡ºçš„ä¸€å¥—Agentèƒ½åŠ›ç®¡ç†çš„è®¾è®¡æ¨¡å¼å¼€æºäº†ã€‚æˆ‘ä»¬å®Œå…¨å¯ä»¥æŠŠè¿™ä¸ªæ¨¡å¼å€Ÿé‰´è¿‡æ¥ï¼Œç”¨åœ¨è‡ªå·±çš„Agentä½“ç³»é‡Œï¼Œä¸ç®¡ä½ ç”¨çš„æ˜¯Qwenã€Deepseekï¼Œè¿˜æ˜¯åˆ«çš„æ¨¡å‹
  - å½“ä½ çš„Agentèƒ½åŠ›è¶Šæ¥è¶Šå¤šæ—¶ï¼Œæ€ä¹ˆç®¡ç†ï¼Ÿä¸€ä¸ªå‡ åƒè¡Œçš„System Promptï¼Ÿä¸€ä¸ªåŒ…å«å‡ åä¸ªå·¥å…·å‡½æ•°çš„å¤§æ‚çƒ©æ–‡ä»¶ï¼Ÿè¿™äº›éƒ½å¾ˆéš¾ç»´æŠ¤ã€‚
  - è€ŒSkillsæä¾›äº†ä¸€ç§è§£è€¦çš„ã€æ¨¡å—åŒ–çš„æ–¹æ¡ˆã€‚ä½ å›¢é˜Ÿé‡Œçš„Agentä¸å†æ˜¯ä¾èµ–ä¸€ä¸ªå·¨å¤§çš„ã€éš¾ä»¥ç»´æŠ¤çš„system_prompt.txtï¼Œè€Œæ˜¯ä¸€ä¸ªç”±å‡ åä¸ªæ ‡å‡†åŒ–çš„Skillæ–‡ä»¶å¤¹ç»„æˆçš„èƒ½åŠ›åº“ï¼Œæ¯ä¸ªSkilléƒ½å¯ä»¥ç‹¬ç«‹ç‰ˆæœ¬æ§åˆ¶ã€æµ‹è¯•å’Œè¿­ä»£ã€‚

- åœ¨å­—èŠ‚å®ä¹ åšé€šç”¨Agentç ”å‘çš„æ—¶å€™ï¼ŒAgentéœ€è¦æ”¯æŒå‡ åç§ä¸åŒ å·¥å…·/æ¥å£/å¹³å° æ¥å®Œæˆäº”èŠ±å…«é—¨çš„ä»»åŠ¡ï¼Œå¯¹æ­¤ï¼ŒåŒäº‹ä»¬å¯¹çŸ¥è¯†çš„ç®¡ç†æå‡ºäº†ä¸€ä¸ªknowledgeèŒƒå¼ï¼š
  - æ¯ä¸ªknowledgeå®šä¹‰å¥½title, description, used whenï¼ˆåœ¨ä»€ä¹ˆä»»åŠ¡/å·¥å…·/å¹³å°å‡ºç°æ—¶ï¼Œä½¿ç”¨è¯¥knowledgeï¼‰ã€‚è¿™äº›ä½œä¸ºmetadataï¼Œæ¯ä¸ªknowledgeçš„metadataä¹Ÿå°±ä¸‰è¡Œå­—å·¦å³ã€‚knowledgeçš„æ­£æ–‡æ˜¯ä¸€ä¸ªmarkdownï¼Œä¼šåŒ…å«SOPã€Dosã€Don'tsã€ç”šè‡³ç®€å•çš„è„šæœ¬ã€‚
  - Agentå¯åŠ¨ä¸€æ¬¡ä»»åŠ¡æ—¶ï¼Œå…ˆæ ¹æ®promptè®©LLMæ ¹æ®æä¾›çš„metadataä¸»åŠ¨å¬å›å…¶è®¤ä¸ºç”¨å¾—ä¸Šçš„knowledgeï¼Œå†é€šè¿‡å·¥ç¨‹æ‰‹æ®µæŠŠå®Œæ•´çš„mdæ‹¼è¿›prompté‡Œï¼Œæ”¯æŒå®Œæˆåç»­ä»»åŠ¡ã€‚
  - çœ‹å®ŒAnthropicæå‡ºçš„Claude Skillsï¼Œæ„Ÿå¹å½“æ—¶åŒäº‹ç†å¿µçš„å…ˆè¿›ï¼Œä¹Ÿæ„Ÿå¹è¡Œä¸šéœ¸ä¸»çš„ç”Ÿæ€è¯è¯­æƒâ€”â€”å¦‚æœæ˜¯è±†åŒ…æˆ–è€…seedæå‡ºè¿™æ ·ä¸€ä¸ªèŒƒå¼ï¼Œè‚¯å®šå¾—ä¸åˆ°å¦‚æ­¤å·¨å¤§çš„å…³æ³¨å’Œè·Ÿè¿›ã€‚
  - åŒæ—¶ä¹Ÿæ‰¿è®¤ï¼ŒClaude Skillsçš„å®šä¹‰å†…æ¶µå’Œè§„èŒƒæ¯”å½“æ—¶æˆ‘ä»¬å›¢é˜Ÿæå‡ºçš„knowledgeæ›´åŠ å…¨é¢å’Œå¯å¾ªï¼Œåªä¸è¿‡æœ¬è´¨ä¸Šæ²¡æœ‰å¤ªå¤šè¿›æ­¥ï¼Œä»ç„¶æ˜¯context engineeringçš„ä¸€ç§ã€‚è¦æƒ³è®©æ¨¡å‹å……åˆ†å‘æŒ¥å¥½Skillsçš„èƒ½åŠ›ï¼Œæœ€ç»ˆè¿˜æ˜¯è¦ä¾èµ–æ›´å¥½çš„æ¨¡å‹ï¼Œæ›´å¼ºçš„æ¨ç†ã€‚

- è¿™ç§èŒƒå¼æ˜¯å¼€å‘AGENTçš„å¸¸è§„æ–¹å¼ï¼Œæ²¡å•¥å…ˆè¿›çš„ã€‚å·¥ç¨‹é‡å¤§ä¸€ç‚¹çš„AGENTéƒ½ä¼šæ„å»ºè‡ªå·±çš„çŸ¥è¯†åº“èŒƒå¼ï¼Œæ ¸å¿ƒå°±æ˜¯ç»“æ„åŒ–è‡ªå·±çš„å†…å®¹

- å†™ function call çš„æ—¶å€™å°±ä¼šç”¨åˆ°å‘€ï¼Œä¸åŒçš„æ˜¯åªè€ƒè™‘åˆ°ä»£ç å±‚é¢çš„å°è£…

- ä»skillçš„æ–‡æ¡£æè¿°æ¥çœ‹ å®ƒå°±æ˜¯å•çº¯åœ°æŠŠæè¿°ä¸¢ç»™å¤§æ¨¡å‹ è‡³äºå¤§æ¨¡å‹å®é™…ä¼šä¸ä¼šfollow é‚£å°±å®Œå…¨çœ‹å¿ƒæƒ…äº† ä»è¿™ä¸€ç‚¹æ¥è¯´ skillæ–¹æ¡ˆåœ¨æ€§èƒ½ä¸ç¡®å®šæ€§è¿™å—å¿…ç„¶æ˜¯æ¯”çœŸæ­£çš„toolså·®ä¸å°‘çš„
  - æ€»ä½“æ¥è¯´ skillåŸºæœ¬å¯ä»¥ç®—æ˜¯promptä¹‹ä¸Šçš„åˆçº§è¯­æ³•ç³– è™½ç„¶æ¯”è¾ƒé¸¡è‚‹ ä½†å¯¹ç”¨æˆ·è€Œè¨€ æ€»å½’æ˜¯èƒ½è§£å†³ä¸€äº›åœºæ™¯ä¸‹çš„é—®é¢˜çš„ èµ·ç æœ‰äº†ä¸€å¥—æŒ‡å¯¼å¤§æ¨¡å‹ä½¿ç”¨æ–°å·¥å…·çš„ä¸´æ—¶è§£å†³æ–¹æ¡ˆäº†

- ## [OpenAI è°·æ­Œè”æ‰‹æ¨å‡º AGENTS.mdï¼Œèƒ½å¦æˆä¸ºç¼–ç¨‹ Agent çš„ã€Œå®˜æ–¹è¯´æ˜ä¹¦ã€ï¼Ÿ - çŸ¥ä¹ _202508](https://zhuanlan.zhihu.com/p/1941669020068709122)
- äº‰è®ºä¸€ï¼šAGENTS.md vs. CONTRIBUTING.md
  - README.md æˆ– CONTRIBUTING.md ä¸å¤Ÿç”¨å—ï¼Ÿ
  - è¿™æ•´ä»¶äº‹æœ¬è¯¥åœ¨ CONTRIBUTING.md é‡Œè§£å†³ã€‚AGENTS.md é‡Œçš„å†…å®¹ï¼Œå’Œäººç±»è´¡çŒ®è€…æƒ³äº†è§£çš„ä¸œè¥¿æ²¡ä»€ä¹ˆä¸¤æ ·ã€‚
  - ç»™ Agent çš„æ–‡æ¡£å¿…é¡» é«˜åº¦ç²¾ç‚¼ï¼Œå› ä¸ºè¿‡å¤šçš„å†…å®¹ä¼šæ¶ˆè€—å®è´µçš„ API tokenï¼Œå¢åŠ æˆæœ¬ï¼Œç”šè‡³é™ä½è¾“å‡ºè´¨é‡ã€‚

- äº‰è®ºäºŒï¼šæ–‡ä»¶ vs. æ–‡ä»¶å¤¹ï¼Œå•ä½“ vs. ç»“æ„åŒ–
  - æœ‰ç»éªŒçš„å¼€å‘è€…æå‡ºï¼Œå¯¹äºå¤§å‹å¤æ‚é¡¹ç›®ï¼Œä¸€ä¸ªå·¨å¤§çš„ Markdown æ–‡ä»¶å¾ˆå¿«ä¼šå˜å¾—éš¾ä»¥ç»´æŠ¤ã€‚
  - è®¸å¤šå¼€å‘è€…å»ºè®®é‡‡ç”¨æ›´æœ‰ç»„ç»‡çš„æ–‡ä»¶å¤¹ç»“æ„ï¼Œä¾‹å¦‚ä¸€ä¸ªéšè—çš„ .agents ç›®å½•

- äº‰è®ºä¸‰ï¼šæ ¹ç›®å½•æ±¡æŸ“é—®é¢˜

- äº‰è®ºå››ï¼šç»§æ‰¿è¿˜æ˜¯è¦†ç›–ï¼Ÿ
  - AGENTS.md æ”¯æŒåœ¨å­ç›®å½•ä¸­åµŒå¥—ï¼Œä½†å…¶è§„åˆ™æ˜¯ ã€Œæœ€è¿‘æ–‡ä»¶ä¼˜å…ˆã€ã€‚

- [å‘Šåˆ«æ··ä¹±ï¼Œç”¨ AGENTS.md ç»Ÿä¸€ä½ çš„ AI å¼€å‘å·¥å…·è§„åˆ™ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1951785160124109343)
  - AGENTS.md åªæ˜¯ä¸€ä¸ªå•ç‹¬çš„æ–‡ä»¶ï¼Œæ²¡æ³•åƒ Cursor çš„ `.cursor/rules` ä¸€æ ·æ”¯æŒéå¸¸å¤šçš„ç‹¬ç«‹çš„è§„åˆ™ã€‚
  - æˆ‘çš„å»ºè®®æ˜¯ï¼Œä½ å¯ä»¥å’Œå›¢é˜Ÿå•†å®šä¸€ä¸ªå­˜æ”¾å„ç±»è§„åˆ™çš„å…¬å…±ç›®å½•ï¼Œæ¯”å¦‚ `.ai/rules/`ã€‚ ç„¶åä½ å¯ä»¥åœ¨ AGENTS.md ä¸­è¡¥å…… rules ä¿¡æ¯å†…å®¹ã€‚

- [Switching to AGENTS.md : r/cursor](https://www.reddit.com/r/cursor/comments/1nqwz02/switching_to_agentsmd/)
  - Don't dump all rules into this file. You can link to other rules from this file. Agents should be able to reason about it and read the right documentation.

- [Claude Code and Claude.md: Should you spread your product doumentation and plans and agent instructions over multiple files? : r/ClaudeAI](https://www.reddit.com/r/ClaudeAI/comments/1lr1g0d/claude_code_and_claudemd_should_you_spread_your/)
  - Yes, progressively split the knowledge across multiple md files.
  - Rule of thumb that works for me. Claude.md is for nouns. Slash commands are for verbs. Meaning Claude.md is about where and what things are, and then slash commands are about how to do the thing.

## ğŸŒ° [How to write a great agents.md: Lessons from over 2, 500 repositories - The GitHub Blog _202511](https://github.blog/ai-and-ml/github-copilot/how-to-write-a-great-agents-md-lessons-from-over-2500-repositories/)

- We recently released a new GitHub Copilot feature: custom agents defined in agents.md files. Instead of one general assistant, you can now build a team of specialists: a @docs-agent for technical writing, a @test-agent for quality assurance, and a @security-agent for security analysis

- What works in practice: Lessons from 2, 500+ repos
  - Put commands early: Put relevant executable commands in an early section: npm test, npm run build, pytest -v. Include flags and options, not just tool names.
  - Code examples over explanations: One real code snippet showing your style beats three paragraphs describing it. 
  - Set clear boundaries: Tell AI what it should never touch (e.g., secrets, vendor directories, production configs, or specific folders). â€œNever commit secretsâ€ was the most common helpful constraint.
  - Be specific about your stack: Say â€œReact 18 with TypeScript, Vite, and Tailwind CSSâ€ not â€œReact project.â€ Include versions and key dependencies.
  - Cover six core areas: Hitting these areas puts you in the top tier: commands, testing, project structure, code style, git workflow, and boundaries. 

- ## [Q: When will there be fast and competent SLMs for laptops? : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pcurp8/q_when_will_there_be_fast_and_competent_slms_for/)
  - Qwen3-30B-A3B and GPT-OSS-20B both uses Mixture-of-Experts instead of dense layers for their SLM
  - Kimi-Linear and Qwen3-Next-80B-A3B moved along to use "mixed attention" (majority of layers with linear attention) to speed things up AND have longer contexts
  - Not enough people getting into ternary attention like BitNet a4.8 / BitNet v2 or ternary quantization (PTQ)
  - Whatever layer routing is to reduce the amount of RAM needed, including Ouro-2.6B-Thinking these days and Mixture-of-Depths back in 2024
  - Are all of these different techniques conflicting with one another? If it is just a lack of funding for fine-tuning/modding an existing SLM into something fast (assuming QAFT and RL), how much would it cost to crowdfund a project like this?

- It depends on your standards, I believe that for the average Joe, something like Ling Mini 2.0 would already check those requirements (fast -> 1B active is doable for most modern laptops at 20+ tok/s) (Competent -> 16B total parameters makes it decent enough for 99% of the tasks an average person would likely use it for)

- For most people, Llama3 8B was the moment where their laptop could handle a ton of their work and queries locally.

- I use Ling mini to correctly format the ocr result of screenshots. Its the fastest and adheres well to long system prompt. All on cpu.

- We already have that. Llama 3.2 3B for writing. Gemma 3 for multimodal. Qwen 3 4B for stem. Granite 4 for rag.
  - The issue is that the active/ dense parameter count determines the limit of how intelligent the overall model is. The mixture of experts kinda determines how big the model's encyclopaedia is.
  - Its possible to argue that llama 3.2 3B, gemma 3 4B and Qwen 3 30b are around the same level in writing. But Qwen 3 30b is clearly the one with more knowledge.
  - There is also the requirement for AI to have ethics and emotions, which is frankly way too complex to fit into an SLM without lobotomising it.
  - Laptop ram is just too limited in bandwidth and capacity

- I am tempted to suggest RAG-ing an SLM into being better than just being slowed down by a 32B dense model (8B or 14B on higher-performance laptops), and since dense models are "more intelligent" compared to MoE... Maybe BitNet is a more or flexible layer activation is a workaround then?

- Bitnet and ternary are out as they need specific hardware

- ## [I'm surprised how simple Qwen3 VL's architecture is. : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/)
- Most machine learning architecture isnâ€™t really that complicated when you look at it in code. Plus, in software development simplicity = better.

- training pipeline probably much more complicated

- To be honest... The entire domain of LLMs and even VLMs are fairly simple... Working in self driving for over 5 years exposed to bespoke perception and multi task models, it shocked me how simple LLMs are, especially training it from the model side.
  - The literal loss function for LLMs during pretraining and finetuning is just cross entropy... Compare that to something more complicated like YOLO, it's actually insane in terms of difference of complexity.
  - Really the solution now.... Stack some transformers, use a LM head, chunk input for VLMs into patches... Pretty damn simple I have to say

- The nicest part of Qwen3-VL is that most of the â€œmagicâ€ comes from small, well-chosen inductive biases rather than a baroque stack. Itâ€™s basically ViT â†’ lightweight bridge â†’ plain decoder LLM, with two tasteful upgrades: interleaved 3D positional encoding (i-MRoPE) and DeepStack feature fusion.

- In my experience it is not very good at producing accurate coordinates of items it sees

- ## [Finally DeepSeek supports interleave thinking : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pbal3o/finally_deepseek_supports_interleave_thinking/)
  - If a thinking model supports multi-step tool calls and can incorporate thinking from historical steps during these calls, then this model supports interleaved thinking.
  - So far, among open-source models, only GPT-OSS, Kimi K2 Thinking, and MiniMax M2 support it, and I believe this feature is crucial for agents.
  - Interleave thinking lets an AI agent reason, act, and observe in tight loops, so it can adapt step-by-step to new information instead of blindly following a fixed plan.

- Why is special support needed? Each request to an LLM is whole conversation, and you can eliminate previous thinking blocks at each request. What am I missing here?

- ## [Experimenting with Multiple LLMs at once? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p4pre3/experimenting_with_multiple_llms_at_once/)
- I do mostly coding, so different families of models get wildly different python training data. Having each do the same coding task and then have another model pick the best components of the script for a new third script works really well.
  - Open web UI also has channels, which is a discord style chat room. You can tell the models you have to collaborate with each other on a project and they will take turns with sections of code.

- I had them do a few rounds of back and forth checking each other's work. It produced noticably better results than either model could on their own - but the time it took made it pointless. It was faster to just use a larger/slower model, allow a reasoning model to go nuts with thinking tokens, or just iterate myself. 
  - For tasks that aren't time sensitive there's some value there. 

- I like the idea of this and have experimented. I donâ€™t have a great way to have them collaborate in real time, but using two to check each others work just seems smart to me.

- Andrej Karpathy just posted about a vibe coded project he did called LLM Council that seems pretty cool: https://github.com/karpathy/llm-council

- Here's mine: GitHub.com/irthomasthomas/llm-consortium It's cli based but you can also save a multi-model consortium and use it like a regular model. Then you can use llm-model-gateway to serve that on a openai proxy and use it like a normal model in your tools.

- ## [Instead of either one huge model or one multi-purpose small model, why not have multiple different "small" models all trained for each specific individual use case?   r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1ovsb2x/instead_of_either_one_huge_model_or_one/)
- This is already how MoE (Mixture of Experts) models work. There are plenty of them on huggingface, though I think the separation of the 'experts' often isn't quite so clean. You also have to consider some other things -- your 20B param 'python only' expert model is almost unusable if it doesn't also understand variable names and comments and instructions in natural language, there does have to be some baseline knowledge to make a coding model actually useful, not *just* code in the training data.
- That's kinda what a MOE model is/does. Each of the "experts" are specialists in different trained things and it picks the active ones to use based on the prompt from the user.

- some people mentioned MoE, but MoE works differently. MoE makes decisions for each token separately, not for whole answers. So training many small, specialized models is not the same thing as using MoE.

- Your idea is exactly what Microsoft is attempting with its Phi series of models. They intend to build an ecosystem of models trained on very specific tasks, and then have an agentic system that uses a simple model to process the users request, decide the best model to use, and use that model to answer the question.
  - I believe Google is doing something similar with Gemini, and there are rumors that GPT 5 functions the same way.

- Perfect routing is not a solved problem. In general you have to get part of the way through solving a problem before figuring out exactly what is necessary to solve it.

- ## Every LangGraph user I know is making the same mistake! They all use the popular supervisor pattern to build conversational agents.
- https://x.com/akshay_pachaar/status/1983874390149484881
  - The pattern defines a supervisor agent that analyzes incoming queries and routes them to specialized sub-agents. Each sub-agent handles a specific domain (returns, billing, technical support) with its own system prompt.
  - This works beautifully when there's a clear separation of concerns.
  - The problem is that it always selects just one route.
  - For instance, if a customer asks: "I need to return this laptop. Also, what's your warranty on replacements?"
  - The supervisor routes this to the Returns Agent, which knows returns perfectly but has no idea about warranties.
  - This gets worse as conversations progress because real users don't think categorically. They mix topics, jump between contexts, and still expect the agent to keep up.
  - This isn't a bug you can fix since this is fundamentally how router patterns work.
  - Now, let's see how we can solve this problem.
  - Instead of routing between Agents, first, define some Guidelines.
  - Each guideline has two parts: - Condition: When it gets activated? - Action: What should the agent do?
  - Based on the user's query, relevant guidelines are dynamically loaded into the Agent's context.
  - This approach is actually implemented in Parlant - a recently trending open-source framework (15k+ stars).
  - Instead of routing between specialized agents, Parlant uses dynamic guideline matching. At each turn, it evaluates ALL your guidelines and loads only the relevant ones, maintaining coherent flow across different topics.
  - Another key advantage is that dynamic guidelines keep the system prompt clean, ensuring the agent receives only the right instructions at the right time.

- Hmm wonâ€™t a fully connected sub agent flow with planning enabled work here where one agent can route to any sub agent if it canâ€™t handle the task. So if realize the user needs more than one thing to be done the agent plans the steps and assigns to the required agent , task is executed result return until the final agent and one agent gives a combined answer

- It's written in python, nothing is stopping a string split and route two agents and join intelligently at the end.

- This seems like another form of "skills" that claude just released. Where the different skills are used to solve the flows and problems independently.  Its just making sure that all the parts are handled.
  - Yes the ideas are certainly relatable

- You can have the supervisor select a set of relevant agents instead of just one though
  - I get your point. However, real users don't think categorically. They mix topics, jump between contexts, and still expect the agent to keep up at every turn of the conversation. You need something that can dynamically load context at every turn. That's where @ParlantIO shines!
- I agree that this per turn dynamic context loading for the supervisor is effective however if there are agents added as tools to one of these guidelines then effectively its like dynamically selecting a subset of composite agents and then ReAct on this context. I actually did something very similar in one of my agents about data analysis where the per turn strategy would be selected dynamic (model, prompt and tools). I think as i said subset selection is just one of the possible single route out of all possible subset routes.

- ## [I Built Pocket Flow, an LLM Framework in just 100 Lines â€” Here is Why _202503](https://medium.com/@zh2408/i-built-an-llm-framework-in-just-100-lines-83ff1968014b)
- After a year of struggling with bloated frameworks, I decided to strip away anything unnecessary. The result is Pocket Flow, a minimalist LLM framework in just 100 lines of code.

- After a year of building LLM applications from scratch, I had a revelation: beneath all the complexity, LLM systems are fundamentally just simple directed graphs.
  -  By stripping away the unnecessary layers, I created Pocket Flow â€” a framework with zero bloat, zero dependencies, and zero vendor lock-in, all in just 100 lines of code.
- We also support batch processing, asynchronous execution, and parallel processing for both nodes and flows.

- Unlike other frameworks, Pocket Flow deliberately avoids bundling vendor-specific APIs. 
  - No Vendor Lock-in: Youâ€™re free to use any model you want, including local models like OpenLLaMA, without changing your core architecture.

- [I Built an LLM Framework in 179 Linesâ€”Why Are the Others So Bloated?  : r/LangChain _202502](https://www.reddit.com/r/LangChain/comments/1iwrhuu/i_built_an_llm_framework_in_179_lineswhy_are_the/)
- I had a look at the code. You've built a simple state machine. 
  - State machine are almost aways the worst time of choice when it comes to composition. 
  - In other words organising everything in flows of Nodes while still writing code is worse then for example carefully providing the data structures yourself and deal with the concurrency and dataflow based on the application specific requirements.
- Thanks for the feedback and example. I opted for a state machine for its simplicity! I'm open to finding a way to balance simple state transitions with tailored data structures though

- how is this different to the approach LangGraph took?
  - We think LangGraphâ€™s approach can feel rigid and tends to enforce a strictly linear, single-threaded execution model! We also want to do more than just manage state transitions!!
  - For example, we can handle asynchronous capabilitiesâ€”like node cloning to avoid race conditions and dedicated AsyncParallelBatchNodes and AsyncParallelBatchFlowsâ€”to enable parallel execution. This means you can run I/O-bound tasks concurrently (such as multiple LLM calls) without being restricted to a single-threaded, linear flow.

- ## ğŸ¤” [I built an AI orchestration platform that breaks your promot and runs GPT-5, Claude Opus 4.1, Gemini 2.5 Pro, and 17+ other models together - with an Auto-Router that picks the best approach : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o76n9d/i_built_an_ai_orchestration_platform_that_breaks/)
  - I've been frustrated with choosing between AI models - GPT-5 is great at reasoning, Claude excels at creative writing, Gemini handles data well, Perplexity is best for research - so I built LLM Hub to orchestrate them all intelligently.
  - [LLM HUB - AI Pipeline Orchestration](https://llm-hub.tech/)
  - The Core Problem: Each AI has strengths and weaknesses. Using just one means compromising on quality.
  - ğŸ’¡ The Solution: LLM Hub coordinates 20+ models across 4 execution modes:
- 4 EXECUTION MODES:
  - Single Mode - One model, one response (traditional chat)
  - Sequential Mode - Chain models where each builds on the previous (research â†’ analysis â†’ writing)
  - Parallel Mode - Multiple models tackle the same task, synthesized by a judge model
  - ğŸŒŸ Specialist Mode (the game-changer) - Breaks complex tasks into up to 4 specialized segments, routes each to the expert model, runs them in parallel, then synthesizes everything

- ## Code2Video - é€šè¿‡ä»£ç ç”Ÿæˆæ•™è‚²è§†é¢‘çš„ AI Agent æ¡†æ¶ï¼Œæ¥è‡ªæ–°åŠ å¡å›½ç«‹å¤§å­¦ Show Lab çš„æœ€æ–°ç ”ç©¶ï¼Œè®ºæ–‡å’Œå¼€æºé¡¹ç›®éƒ½å‘å¸ƒäº†ã€‚
- https://x.com/shao__meng/status/1974280130420961548
  - é€šè¿‡ AI Agent çš„æ–¹å¼ç”Ÿæˆç±»ä¼¼ 3Blue1Brown çš„è§†é¢‘ï¼Œå¾ˆæœ‰è¶£ï¼Œå’±ä»¬ä¸€èµ·çœ‹çœ‹ã€‚
  - æ–¹æ³•æ ¸å¿ƒï¼šä¸‰æ™ºèƒ½ä½“åä½œ, è®¾è®¡æ¡†æ¶åˆ†è§£ä»»åŠ¡ä¸ºä¸‰ä¸ªåä½œæ™ºèƒ½ä½“ï¼Œå½¢æˆæ¨¡å—åŒ–ç®¡é“ï¼š
  - Â· Plannerï¼ˆè§„åˆ’è€…ï¼‰ï¼šä»å­¦ä¹ ä¸»é¢˜ï¼ˆå¦‚â€œçº¿æ€§å˜æ¢ä¸çŸ©é˜µâ€ï¼‰ç”Ÿæˆå¤§çº²å’Œæ•…äº‹æ¿ï¼Œç¡®ä¿æ—¶é—´è¿è´¯æ€§ï¼ˆå¦‚æ¦‚å¿µå¼•å…¥ã€æ‰©å±•å’Œå›é¡¾ï¼‰ã€‚å®ƒé›†æˆå¤–éƒ¨æ•°æ®åº“ï¼Œæ£€ç´¢å‚è€ƒå›¾åƒå’Œè§†è§‰èµ„äº§ï¼ˆå¦‚å›¾æ ‡ï¼‰ï¼Œå¹¶è€ƒè™‘å—ä¼—æ°´å¹³ï¼ˆå¦‚é«˜ä¸­ç”Ÿ vs. å¤§å­¦ç”Ÿï¼‰ï¼Œä»¥æå‡äº‹å®å‡†ç¡®æ€§å’Œè§†è§‰ä¸€è‡´æ€§ã€‚
  - Â· Coderï¼ˆç¼–ç è€…ï¼‰ï¼šå°†æ•…äº‹æ¿è½¬æ¢ä¸ºå¯æ‰§è¡Œ Manim åŠ¨ç”»ä»£ç ã€‚é‡‡ç”¨å¹¶è¡Œåˆæˆç­–ç•¥åŠ é€Ÿç”Ÿæˆï¼Œå¹¶å¼•å…¥ ScopeRefineï¼ˆèŒƒå›´å¼•å¯¼ä¿®å¤ï¼‰æœºåˆ¶ï¼šä»è¡Œçº§ï¼ˆå±€éƒ¨ä¿®å¤é”™è¯¯è¡Œï¼‰é€æ­¥æ‰©å±•åˆ°å—çº§å’Œå…¨å±€é‡ç”Ÿï¼Œç¡®ä¿ä»£ç é«˜æ•ˆä¸”æ— è¯­æ³•é”™è¯¯ï¼ŒåŒæ—¶ä¿æŒè·¨èŠ‚ä¸€è‡´æ€§ã€‚
  - Â· Criticï¼ˆæ‰¹è¯„è€…ï¼‰ï¼šä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œé”šç‚¹è§†è§‰æç¤ºï¼ˆå¦‚å ç”¨è¡¨å’Œä½ç½®ç½‘æ ¼ï¼‰è¿­ä»£ç²¾ç‚¼æ¸²æŸ“è§†é¢‘ã€‚é’ˆå¯¹ç©ºé—´é—®é¢˜ï¼ˆå¦‚é‡å æˆ–ç©ºæ—·åŒºåŸŸï¼‰ï¼Œå®ƒæä¾›å…·ä½“è§£å†³æ–¹æ¡ˆï¼ˆå¦‚è°ƒæ•´å¯¹è±¡ä½ç½®æˆ–ç¼©æ”¾ï¼‰ï¼Œæå‡å¸ƒå±€æ¸…æ™°åº¦å’Œæ•™è‚²å¸å¼•åŠ›ã€‚
  - æ•´ä¸ªæµç¨‹ä»ç”¨æˆ·æŸ¥è¯¢è¾“å…¥ï¼Œåˆ°è¾“å‡ºæ•™è‚²è§†é¢‘ï¼Œé€šå¸¸éœ€æ•°åˆ†é’Ÿæ¸²æŸ“ï¼Œè¿œä¼˜äºç«¯åˆ°ç«¯åƒç´ ç”Ÿæˆã€‚

- ## [å¦‚ä½•çœ‹å¾…è§‚ç‚¹ï¼šAI çš„å…³é”®ç‚¹ä¸æ˜¯promptï¼Œè€Œæ˜¯Context Engineeringï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/1923364519964545063)
- Contextè¿œä¸æ­¢æ˜¯ä¸€å¥promptï¼Œå®ƒåŒ…æ‹¬ï¼š
  - æŒ‡ä»¤/ç³»ç»Ÿæç¤ºï¼šå®šä¹‰æ¨¡å‹è¡Œä¸ºçš„åˆå§‹æŒ‡ä»¤
  - ç”¨æˆ·æç¤ºï¼šæ¥è‡ªç”¨æˆ·çš„å³æ—¶ä»»åŠ¡æˆ–é—®é¢˜
  - çŠ¶æ€/å†å²ï¼šå½“å‰å¯¹è¯çš„çŸ­æœŸè®°å¿†
  - é•¿æœŸè®°å¿†ï¼šè·¨å¤šæ¬¡å¯¹è¯æ”¶é›†çš„æŒä¹…
  - çŸ¥è¯†åº“æ£€ç´¢ä¿¡æ¯(RAG)ï¼šæ¥è‡ªæ–‡æ¡£ã€æ•°æ®åº“æˆ–APIçš„å¤–éƒ¨çŸ¥è¯†
  - å¯ç”¨å·¥å…·ï¼šæ¨¡å‹å¯ä»¥è°ƒç”¨çš„æ‰€æœ‰åŠŸèƒ½å®šä¹‰
  - ç»“æ„åŒ–è¾“å‡ºï¼šå¯¹æ¨¡å‹å“åº”æ ¼å¼çš„å®šä¹‰
- è¿™å…¶å®å°±æ˜¯åœ¨æ„å»ºä¸€ä¸ªå®Œæ•´çš„ã€ŒAIå·¥ä½œç¯å¢ƒã€ï¼Œè€Œä¸æ˜¯ç®€å•åœ°ç»™AIä¸‹æŒ‡ä»¤ã€‚

- å³ä½¿ä½ å†™ä¸å¥½promptï¼Œå¸‚é¢ä¸Šä¹Ÿæœ‰å¾ˆå¤šAIå·¥å…·å¯ä»¥å¸®ä½ åšprompt enhancement
  - æœŸå¾…AIèƒ½ã€Œæœ‰è®°å¿†ï¼Œæ‡‚ç”¨æˆ·ã€ã€‚å› æ­¤ï¼ŒContext Engineeringï¼ˆä¸Šä¸‹æ–‡å·¥ç¨‹ï¼‰å°±å¼€å§‹è¢«å…³æ³¨åˆ°ã€‚
  - context engineering, æ˜¯æŒ‡é€šè¿‡ç³»ç»Ÿæ€§æ„å»ºã€ç®¡ç†å’Œä¼˜åŒ– AI æ¨¡å‹çš„è¾“å…¥ä¸Šä¸‹æ–‡ï¼Œä»¥æå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„ç†è§£ä¸è¾“å‡ºèƒ½åŠ›ã€‚
  - ç¬¬ä¸€æ˜¯å†å²äº¤äº’æ•°æ®ã€‚
  - ç¬¬äºŒï¼Œæ˜¯contextçš„æ€»ç»“ã€‚å¯¹è¯å¯ä»¥ä¸€ç›´å»¶ç»­ï¼Œä½†æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£æœ‰é™ï¼Œæ€ä¹ˆè®°ä½ä¹‹å‰çš„å¯¹è¯ï¼Ÿå½“ç„¶æ˜¯å‹ç¼©ï¼Œä¾‹å¦‚æŠŠæœ€è¿‘5æ¡å¯¹è¯çš„å†…å®¹åŸå°ä¿ç•™ï¼Œå†ä¹‹å‰çš„å†…å®¹æ€»ç»“ä¸€ä¸‹ã€‚
  - ä½†æ›´é‡è¦çš„ï¼Œæ˜¯é¢†åŸŸçŸ¥è¯†ä¸èƒŒæ™¯ä¿¡æ¯ã€‚
- AIçš„èƒ½åŠ›æ¥æºäºä¸¤ä¸ªæ–¹é¢ in-weights memoryï¼ˆè®­ç»ƒé›†é‡Œå­¦åˆ°çš„ï¼‰ å’Œ in-context memoryï¼ˆå‚è€ƒèµ„æ–™é‡Œå­¦åˆ°çš„ï¼‰ã€‚ in-weights memoryåŸºæœ¬ä¸Šæ˜¯å¾ˆéš¾ä¿®æ”¹çš„, ä½†in-context memory æ›´å®¹æ˜“ä¿®æ”¹å’Œæ›´æ–°ï¼Œä½ åªè¦æä¾›æ–°çš„èµ„æ–™ï¼Œæ­£ç¡®çš„èµ„æ–™ï¼Œæ¨¡å‹å°±èƒ½æœ‰ã€Œæ–°çŸ¥ã€

- å¤æ‚çš„AIåº”ç”¨ï¼Œä¸æ˜¯é chatè¾“å…¥æç¤ºè¯è¿™ä¹ˆç®€å•ï¼Œè€Œæ˜¯éœ€è¦è‡ªåŠ¨åŒ–å¤šæ¬¡å’Œå¤§æ¨¡å‹çš„äº¤äº’ï¼Œè¿™ä¸ªè‡ªåŠ¨çš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦èƒ½å¤Ÿè‡ªåŠ¨äº§ç”Ÿç»™å¤§æ¨¡å‹çš„æç¤ºè¯ã€‚

- Agent = LLM + Prompt + å·¥å…·è°ƒç”¨ï¼Œä½†ä»å·¥ç¨‹è§†è§’æ¥çœ‹ï¼Œè¿™äº›æœ¬è´¨éƒ½æ˜¯Context Engineeringï¼Œä¹Ÿå°±æ˜¯ä¸Šä¸‹æ–‡å·¥ç¨‹ã€‚
  - Context Engineeringå°±æ˜¯åœ¨æ¯æ¬¡è°ƒç”¨é‡Œï¼ŒæŠŠæ¨¡å‹å®Œæˆä»»åŠ¡æ‰€å¿…éœ€çš„ä¿¡æ¯æŒ‰å¯¹çš„æ ¼å¼ã€åœ¨å¯¹çš„æ—¶æœºå‡†ç¡®æ‰“åŒ…è¿›å»ã€‚

- ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼ŒAIç»˜å›¾ç”¨æˆ·æ—©å°±å¤©å¤©åœ¨ç”¨äº†
  - å½“ä½ ä½¿ç”¨ Inpaintï¼ˆå±€éƒ¨é‡ç»˜ï¼‰ åŠŸèƒ½æ—¶ï¼ŒAIä¸æ˜¯å‡­ç©ºæƒ³è±¡ï¼Œè€Œæ˜¯åŸºäºä½ ç•™ä¸‹çš„ç”»å¸ƒã€è¾¹ç¼˜çº¿æ¡ã€å…‰çº¿æ–¹å‘ã€å·²æœ‰ç”»é£æ¥è¡¥å…¨åŒºåŸŸï¼Œè¿™å°±æ˜¯â€œå›¾åƒä¸Šä¸‹æ–‡â€
  - å½“ä½ è¿›è¡Œ Outpaintï¼ˆå›¾åƒæ‰©å±•ï¼‰ æ—¶ï¼ŒAIå¿…é¡»å‚è€ƒåŸå›¾çš„æ„å›¾é€»è¾‘ã€è‰²å½©æ¸å˜ã€é£æ ¼çº¹ç†ã€äººç‰©é€è§†æ¥ç”Ÿæˆè‡ªç„¶è¡”æ¥çš„éƒ¨åˆ†ã€‚è¿™ç§å¯¹ä¸Šä¸‹æ–‡çš„â€œç†è§£â€å’Œâ€œå»ºæ¨¡â€ï¼Œæ¯”æç¤ºè¯æœ¬èº«æ›´é‡è¦
  - å½“ä½ ç”¨ Photoshopå†…ç½®çš„Fireflyè¿›è¡Œå±€éƒ¨ä¿®æ”¹åˆ›æˆå¼å¡«å……æ—¶ï¼ŒçœŸæ­£èµ·å†³å®šä½œç”¨çš„ä¸æ˜¯ä½ è¯´äº†â€œç»™æˆ‘åŠ ä¸€ä¸ªç¯å¡”â€ï¼Œè€Œæ˜¯AIæ˜¯å¦å‡†ç¡®è¯»å–äº†å½“å‰ç”»é¢æ˜¯å¤œæ™šã€æ˜¯æ°´è¾¹ã€æœ‰å…‰æºæŠ•å°„ã€æœ‰é€è§†æ¶ˆå¤±ç‚¹ã€‚
  - ä¸Šé¢è¿™äº›åŠŸèƒ½ï¼Œå“ªæ€•ä½ æ ¹æœ¬ä¸å†™æç¤ºè¯ï¼Œå¥½çš„å·¥å…·ä¹Ÿèƒ½ç»™ä½ è„‘è¡¥å¥½ï¼Œæ¯”å¦‚æˆ‘ç°åœ¨æ ¹æœ¬æ”¾ä¸å¼€è®¢é˜…çš„Photoshop AIã€‚

- 
- 
- 
- 
- 

- ## DeepSeekæœ€å¤§çš„åˆ›æ–°ï¼Œæ˜¯ä¸éœ€è¦å¤§é‡çš„äººå·¥æ ‡æ³¨ï¼Œè€Œæ˜¯ç›´æ¥ä»å…¶ä»–å¤§æ¨¡å‹è’¸é¦æˆ–è€…ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼ˆGRPOï¼‰ã€CoTï¼ˆè‡ªæˆ‘åæ€ï¼‰æ¥ç»™å¤§æ¨¡å‹åé¦ˆï¼Œ
- https://x.com/seclink/status/1888011462008005030
  - å°±ç›¸å½“äºå®Œå…¨ä½¿ç”¨RLï¼ˆæˆ–è€…å¦ä¸€ä¸ªåŸºç¡€å¤§æ¨¡å‹ï¼‰æ¥æ›¿ä»£äººå·¥æ ‡æ³¨äº†ã€‚
  - è¿™å®é™…ä¸Šæ˜¯æŠ¢äº†Scale AI è¿™ç§å…¬å¸çš„è›‹ç³•ï¼ŒDeepSeekç‰›Xä¹‹å¤„åœ¨äºï¼Œå¾ˆå¤šè€å¤–ä¸€å¼€å§‹ä¸ä¿¡ï¼Œç„¶åç…§ç€è®ºæ–‡é‡Œçš„æ–¹æ³•å¿«é€Ÿï¼ˆå±€ä¿ƒåœ°ï¼‰å¤ç°ï¼Œå´å‘ç°ç«Ÿç„¶ä¹Ÿèƒ½å¤ç°æˆåŠŸã€‚

- cotå’Œè’¸é¦ä¹‹å‰ï¼Œå°±é€šè¿‡grpoè¿›è¡Œrlè·å¾—äº†ç›¸å½“ä¸é”™çš„æ¨ç†èƒ½åŠ›ã€‚ã€‚ä½ è¿™æ•´ç†çš„ä¸æ¸…æ™°ã€‚

- è¿™æ²¡æœ‰ä»»ä½•åˆ›æ–°ï¼ŒGPT1å°±ç”¨äº†åŒæ ·çš„æ–¹æ³•ï¼Œè€Œä¸”ç°åœ¨ä¸ä½¿ç”¨æ˜¯æœ‰ç†ç”±çš„ã€‚å› ä¸ºdeepseekæ¨¡å‹æœ¬èº«ä¸è¡Œï¼Œä¸€ä¸ªqueryéœ€è¦RL chainæ‰èƒ½è¾¾åˆ°å¯æ¥å—çš„ç­”æ¡ˆï¼Œè‡´ä½¿inference æ•ˆç‡ä½åˆ°å¯æ€•ï¼Œè™½ç„¶trainingä¾¿å®œï¼Œä½†æ˜¯operation costè¦å¤šå¥½å‡ å€ï¼Œå¾—ä¸å¿å¤±

- ä½ è¿™ä¸ªè¯´çš„å®Œå…¨ä¸å¯¹ï¼ŒCoTæ˜¯GPTå‘æ˜çš„ï¼Œè’¸é¦ä¹Ÿæ—©å°±æœ‰äº†ï¼ŒRLä¹Ÿæ—©å°±æœ‰äº†ï¼Œdeepseekæ˜¯å‘æ˜äº†RLé‡Œçš„GRPO

- ## I read up on DeepSeekâ€™s learning algo, GRPO. GRPO: group relative policy optimization
- https://x.com/virattt/status/1885102056546910672
- How GRPO works:
  1 â€¢ model generates a group of answers
  2 â€¢ compute score for each answer
  3 â€¢ compute avg score for entire group
  4 â€¢ compare each answer score to avg score
  5 â€¢ reinforce model to favor higher scores
  - This process is repeated, allowing the model to learn and improve over time.
- Other methods like PPO, use a value function model to do reinforcement learning.
  - GRPO does not, which reduces memory and computational overhead when training.

- GRPO was proposed for the first time in Feb 2024 in DeepSeekMath paper. It is not new.
  - Back then they used Neural Networks for Rewards (PRM/ORM). The magic happened when DeepSeek  replaced PRM/ORM with the exact reward (Verified Reward).

- ## ğŸ”¡ OpenAI's Deep Research is just a search+read+reasoning in a while-loop, right? here is my replicate of it in nodejs, using gemini-flash and jina reader
- https://x.com/hxiao/status/1886250705415229627
- If it includes evaluating js driven websites, including images Then yes 
  - To really replicate that, you'd probably wanna just use puppeteer, save the whole page as an image, extract the info from that, and then crunch that data

# discuss-multi-agents ğŸ˜ï¸
- ## 

- ## 

- ## 

- ## ğŸ†š ä»Šå¤©å¾ˆæœ‰è¶£ï¼Œä¸¤å®¶çŸ¥åçš„å…¬å¸å„å‡ºäº†ä¸€ç¯‡æ–‡ç« ï¼Œäº‰è®ºè¦ä¸è¦ä½¿ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚
- https://x.com/oran_ge/status/1933754019010539923
  - Claude çš„å®˜æ–¹ Anthropic ï¼šå¦‚ä½•æ„å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
  - Devin çš„å®˜æ–¹ Cognition ï¼šä¸è¦æ„å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
- è¿™æ ¸å¿ƒçš„äº‰è®®ç‚¹åœ¨äºï¼šContext ä¸Šä¸‹æ–‡åˆ°åº•åº”è¯¥å…±äº«è¿˜æ˜¯åˆ†å¼€ï¼Ÿ
  - Claude è¿™è¾¹çš„è§‚ç‚¹æ˜¯ï¼Œæœç´¢ä¿¡æ¯çš„æœ¬è´¨æ˜¯å‹ç¼©ï¼Œå•ä¸ªæ™ºèƒ½ä½“çš„ä¸Šä¸‹æ–‡æœ‰é™ï¼Œé¢å¯¹æ— é™çš„ä¿¡æ¯ï¼Œå‹ç¼©æ¯”å¤ªå¤§å°±ä¼šå¤±çœŸã€‚è¿™æ˜¯é›†ä½“æ™ºæ…§ï¼Œä¸€èµ·åä½œè·å¾—çš„èƒœåˆ©ã€‚
  - Devin è¿™è¾¹çš„è§‚ç‚¹æ˜¯ï¼Œå¤šä¸ªæ™ºèƒ½ä½“çš„ä¸Šä¸‹æ–‡ä¸ä¸€è‡´ï¼Œä¼šå¯¼è‡´ä¿¡æ¯å‰²è£‚ã€è¯¯è§£ã€ä»–ä»¬æ±‡æŠ¥ç»™è€æ¿çš„ä¿¡æ¯ç»å¸¸å……æ»¡äº†çŸ›ç›¾ã€‚
  - è¿™è®©æˆ‘æƒ³åˆ°ï¼Œè½¯ä»¶å·¥ç¨‹ä»æ¥ä¸æ˜¯è¿½æ±‚å®Œç¾ï¼Œè€Œæ˜¯æŒç»­è¿­ä»£ã€‚

- æ²¡å•¥çŸ›ç›¾çš„ã€‚çœ‹éœ€æ±‚ã€‚open-ended çš„é—®é¢˜æ¯”è¾ƒé€‚åˆ multi-agentï¼›ç›®æ ‡å¾ˆå…·ä½“çš„è¯ï¼Œå°±æ¯”è¾ƒé€‚åˆå•ä¸ª agent

- æ„Ÿè§‰å’Œç°å®ä¸­çš„ä¸¤å®¶ä¸åŒç­–ç•¥è¿è¥çš„å…¬å¸å·®ä¸å¤šï¼Œæ²¡æœ‰ç»å¯¹çš„å¯¹å’Œé”™ï¼Œéƒ½èƒ½èµ°å‡ºæ¥çš„å¯èƒ½æ€§ä¹Ÿè¶…å¤§ã€‚

- ## @KuraAIAgents é€šè¿‡åˆ›æ–°çš„äº”é‡ Agent æ¶æ„ï¼ˆè§„åˆ’ã€æ‰§è¡Œã€è¯„ä¼°ï¼‰å®ç°äº† 87% çš„æµè§ˆå™¨è‡ªåŠ¨åŒ–å‡†ç¡®ç‡ï¼Œè¶…è¶Š Claude è®¡ç®—æœºæ“ä½œ 28 ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶æ”¯æŒä½æˆæœ¬æ¨¡å‹æ›¿æ¢æ–¹æ¡ˆ
- https://x.com/shao__meng/status/1857586562588094918
  - åŒ…å«5ä¸ªä¸“é—¨çš„ Agentï¼Œå…¶ä¸­3ä¸ªæ ¸å¿ƒ Agent å½¢æˆä¸€ä¸ªå¾ªç¯ç³»ç»Ÿ
  - åœ¨ WebVoyager åŸºå‡†æµ‹è¯•ä¸­å–å¾— 87% çš„æˆç»©
  - æ¯” Claude çš„è®¡ç®—æœºæ“ä½œé«˜å‡º 28%
- äº”ä¸ªæ ¸å¿ƒ Agentï¼š
a) åˆå§‹è§„åˆ’è€…(Initial Planner)
- è´Ÿè´£åˆ¶å®šé«˜å±‚æ¬¡è®¡åˆ’
- ä½¿ç”¨ OpenAI o1 æ¨¡å‹è¿›è¡Œæ¨ç†
b) å¾ªç¯è§„åˆ’è€…(Agent Loop Planner)
- è¯„ä¼°ä»»åŠ¡æ˜¯å¦å®Œæˆæˆ–ä¸å¯èƒ½å®Œæˆ
- ä¸ºæ‰§è¡Œè€…æä¾›ä¸‹ä¸€æ­¥æŒ‡ä»¤
- æ ¹æ®éœ€è¦ä¿®æ”¹è®¡åˆ’
c) æ‰§è¡Œè€…(Executor)å…·å¤‡ä¸‰é¡¹æ ¸å¿ƒæŠ€èƒ½ï¼š
- ç½‘å€å¯¼èˆªå’Œè¿”å›
- è¯»å–å½“å‰é¡µé¢æ•°æ®
- æ‰§è¡Œå±å¹•æ“ä½œ(ç‚¹å‡»ã€æ»šåŠ¨ã€è¾“å…¥)
d) å¾ªç¯è¯„è®ºè€…(Agent Loop Critic)
- è¯„ä¼°æ‰§è¡Œè€…çš„è¡¨ç°
- ç‰¹åˆ«åœ¨å¤æ‚ç•Œé¢æ“ä½œä¸­èµ·å…³é”®ä½œç”¨
e) æœ€ç»ˆè¯„è®ºè€…(Final Critic)
- è¯„ä¼°æ•´ä¸ªä»»åŠ¡è½¨è¿¹
- å¿…è¦æ—¶æä¾›åé¦ˆå¹¶å¯åŠ¨æ–°çš„å¾ªç¯

- ## OpenAI å‘å¸ƒå¤š Agent ç¼–æ’æ¡†æ¶èƒŒåçš„æ€è€ƒä»¥åŠå®è·µè¿‡ç¨‹
- https://x.com/tuturetom/status/1845634978530693494
  - æ ¸å¿ƒæ˜¯ OpenAI çš„å·¥ç¨‹å¸ˆåœ¨æ€è€ƒ  Agent çš„ã€Œè·¯ç”±ã€ + ã€Œç§»äº¤ã€ç­‰èƒ½åŠ›æ—¶æ‹“å±•å‡ºæ¥çš„ä¸€ä¸ªç¤ºä¾‹ï¼Œè¿›è€Œå‘ç°è¿™ä¸ªç¤ºä¾‹åŸè¯­å¾ˆæ™®é€‚ï¼Œæ‰€ä»¥å¼€å‘äº† Swarm æ¡†æ¶
  - æ‰€æœ‰ä¼Ÿå¤§çš„æ€è€ƒéƒ½æºäºå‘¨æœ«ä¸šä½™å·¥ä½œ

- ## OpenAI æ‚„æ‚„å¼€æºäº†æ„å»ºå¤šä»£ç†æ™ºèƒ½ä½“ååŒæ¡†æ¶ï¼šSwarm
- https://x.com/aigclink/status/1844936446416912628
  - ç”¨äºæ„å»ºã€ç¼–æ’å’Œéƒ¨ç½²å¤šä»£ç†

# discuss-ai-format/interop
- ## 

- ## 

- ## 

- ## [Use YAML over JSON when dumping into prompts for ~2x token saving : r/ChatGPTCoding _202509](https://www.reddit.com/r/ChatGPTCoding/comments/1nl7xux/use_yaml_over_json_when_dumping_into_prompts_for/)
  - [YAML vs. JSON: Which Is More Efficient for Language Models? _202307](https://medium.com/better-programming/yaml-vs-json-which-is-more-efficient-for-language-models-5bc11dd0f6df)
  - It's been pointed out in the comments (with sass) that minifying your JSON is another, perhaps even better, alternative than transforming to YAML. So now there's two options for saving tokens.

- Does the guy who wrote the article know that you don't need to use whitepaces in JSON and you can minify it to consume less space than YAML? Generally speaking, JSON is more space-efficient and compact than YAML.

- Just remove the spaces and condence the JSON into a single line. LLMs don't care about spaces, it's a visual thing for us.

- Thought LLM's don't count white space as context... or if they did, it would be incredibly minimal
  - They kind of have to, if only to correctly write Python
  - ASCII art too

- Another point is accuracy... some like XML more as well - and there is BAML. If i just wanna save money I could get a cheaper model too.
- xml is also what Claude officially recommends for better accuracy.

- I use YAML and JSON, because i use the CMS Drupal since 2006 - so this fits quite well in my workflow

- TOML is actually more verbose when it comes to complex data structures.
  - Which makes sense since it was designed to be a JSON/YAML mappable language for better human readability.
# discuss-local-llm-usecases
- resources
  - [Use Cases | Claude](https://claude.com/resources/use-cases)

- ## 

- ## 

- ## [Getting OpenClaw to work with Qwen3:14b including tool calling and MCP support : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1qrywko/getting_openclaw_to_work_with_qwen314b_including/)
  - I got local tool calling working on Qwen3:14b with ~40 tools, accessible through WhatsApp. 
  - OpenClaw is an AI assistant framework that supports multiple messaging channels. It talks to its LLM backend via an OpenAI-compatible API (/v1/chat/completions).
  - Why a bridge instead of adding tools directly in OpenClaw? OpenClaw supports custom tools natively. You could write each MCP tool as an OpenClaw extension. But I have multiple apps that need the same tools: OpenClaw for WhatsApp, Engram (my personal knowledge system), Jan.ai, etc. Writing each tool as a per-app extension means duplicating everything. With the bridge as a shared MCP layer, you configure your tools once, and any OpenAI-compatible client gets them. Just point it at :11435 instead of :11434.
  - I tested both qwen3:8b and qwen3:14b on an M4-series Mac Studio with 64GB of RAM
  - The 8b model is 3-5x faster but basically treats every message as a new conversation when there are 40 tool schemas in the context. OpenClaw sends the full message history (confirmed via logging: messages=16), so the problem isn't missing context. The model just can't follow it alongside those massive tool definitions.
  - Verdict(è£å®šï¼Œåˆ¤å†³): qwen3:14b. Quality over speed for now.
  - The patch code is available as a GitHub Gist. Running this as a daily driver via WhatsApp and it's surprisingly capable for a 14b model.

- I am having looping tool call issues with glm 4.7 flash 8bit running on lm studio with mlx. I just ask open claw â€˜use browser to open google.comâ€™ and it goes into a loop. I have tried all sampling combinations but nothing works. Please help

- ## ğŸ¤” [Why we went desktop and local-first for agents 6 months ago : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1qr5v9d/why_we_went_desktop_and_localfirst_for_agents_6/)
  - Weâ€™ve been thinking a lot about first principles when building agent project, and one conclusion we keep coming back to is this: The first thing you should optimize for is the agentâ€™s capability ceiling.
  - From that perspective, a desktop-first agent architecture makes a lot of sense. A few reasons why:
- Context access
  - If you want agents to be genuinely useful, they need real user context. On desktop, an agent can natively and seamlessly access local files, folders, running apps, logs, configs, and other artifacts that are either impossible or extremely awkward to reach from a purely web-based agent.
- Permissions equal intelligence
  - Powerful agents need powerful permissions. Desktop agents can read and write the local file system, control native software like IDEs, terminals, browsers, or design tools, and make system-level calls or interact with hardware. This isnâ€™t about being invasive, but about enabling workflows that simply donâ€™t fit inside a web sandbox.
- Web parity without web limitations
  - A desktop agent can still do everything a web agent can do, whether through an embedded Chromium environment or via browser-extension-style control. The reverse is not true: web agents canâ€™t escape their sandbox.
- An often overlooked point is that desktop agents run on user-owned compute. Browsers, terminals, and local tools all execute locally, which significantly reduces backend costs and makes high-frequency, long-running agents much more viable.
- This line of thinking is what led us to build Eigent, the opensource alternative to cowork

- ## [ç°åœ¨é«˜æ™ºå•†äººç¾¤éƒ½æ˜¯å¦‚ä½•åˆ©ç”¨AIçš„?(å’¨è¯¢è´´, ç–‘é—®å¥) _202507](https://linux.do/t/topic/779052)
- æˆ‘æœ‰æ—¶å€™å†™ä¸œè¥¿æ²¡æ€è·¯äº†ï¼Œä¼šè®©AIç»™æˆ‘å¤šåˆ—å‡ ç»„æçº²ï¼Œç„¶åä»ä¸­é€‰ä¸€ç»„æ»¡æ„çš„ï¼Œè‡ªå·±å¼€å§‹æ‹“å±•å†™
  - å¾ˆå°‘ç”¨å®ƒç”Ÿæˆé•¿æ–‡ï¼Œå¤ªé•¿äº†å®ƒå°±ä¼šæœ‰å¹»è§‰ï¼Œç”šè‡³è·‘é¢˜ï¼Œè€Œä¸”ä¹Ÿä¸ç¬¦åˆè¦æ±‚å®ƒåšçš„åŸæ„

- å’Œç½‘é¡µç‰ˆçš„åŒºåˆ«æ˜¯ä½ å¯ä»¥ç”¨ä¸åŒçš„æç¤ºè¯è®©aiå»åšä¸åŒçš„äº‹æƒ…ï¼Œæ¯”å¦‚ç¿»è¯‘ï¼Œå½’çº³ï¼Œæ¶¦è‰²é‚®ä»¶ã€‚

- çŸ¥è¯†åº“å‘æ›´å¤§â€¦â€¦æ–°æŠ€æœ¯å±‚å‡ºä¸ç©·ä¸è¯´ï¼Œéƒ½ä¸æˆç†Ÿã€‚å•ä¸€ä¸ªragå°±å¤Ÿç ”ç©¶å¥½ä¹…, ç„¶åé«˜è´¨é‡çš„æ•°æ®æºåˆä»å“ªæ¥, æ¥äº†ä»¥ååˆå¦‚ä½•å¤„ç†è¿™äº›æ•°æ®ç‰‡, æ–­æœ€åæ‰åˆ°é«˜è´¨é‡çš„aièƒ½ä¸èƒ½ç»„åˆè¿™äº›æ•°æ®ï¼Œå¾—åˆ°ä½ æƒ³è¦çš„ä¸œè¥¿å‡ ä¸ªéƒ½æ˜¯å¤§å‘ï¼Œéƒ½ä¸å®Œç¾ï¼Œè¦è¾¾åˆ°ç”Ÿäº§åŠ›æ„Ÿè§‰ç”šè‡³å·®å¾—è¿œã€‚

- å¸®ä½ æå‡æ•ˆç‡ï¼Œè‡ªå·±æ ¸å¿ƒç«äº‰åŠ›ä¿æŒä½å°±å¯ä»¥äº†ï¼Œæé’±æ‰æ˜¯ç‹é“ å…¶ä»–éƒ½æ˜¯è™šçš„

- ## [What are you building with sub-4B LLMs in early 2025? Real-world use wins? : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1qewdza/what_are_you_building_with_sub4b_llms_in_early/)
- Been running a Qwen 2.5 3B on my Pi 4 for basic home automation - it parses voice commands from my cheap USB mic and controls lights/temp through Home Assistant API. Nothing fancy but beats yelling at Alexa when the internet's down
  - Works surprisingly well for stuff like "turn off bedroom lights" or "what's the temp upstairs" - just had to keep the vocab simple and train it on my specific device names

- a Raspberry Pi monitors IP cameras and sensors. A tiny LLM processes live feeds in real-time to detect intruders, leaks, or hacks alerting you via push notifications or automated responses (e.g., locking doors).

- We used https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF for an on premise Graph-RAG System and it worked pretty well. The Model showed promising results in not hallucinating with the right prompt and context.

- I previously looked at them for simple classification tasks, I am now using FunctionGemma in a Mobile App I am building as a function caller/info extractor where a LLM call is way overkill.

- ## [Why aren't more people using local models? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1oj7ts7/why_arent_more_people_using_local_models/)
- Two reasons that I could think of:
  - Average consumer HW isn't just there yet to run something universally useful
  - Consumer mindset forbids to start doing things on your own
  - free time
  - Most people probably don't have the time, willingness or patience to get into the matter just far enough for using local models. The average user has no clue about how to compile llama.cpp and is also not intrigued to get into this. Things have changed a lot since the 80's.

- The problem with edge devices is simple. Battery life. Yeah, I can use a local model on my phone, and yeah, it works mostly relatively well. But I'm not willing to lose 20% of my battery on a short conversation

- Because graphics cards cost a kazillion dollars and you need 10 of them to get claude quality which is good enough but nothing less is..

- Because it's a huge pain in the ass and requires decently expensive hardware to do well.

- I love using local models and it's a great feeling having control over my privacy but honestly it's a fuckton of work. If you just wanna chat and load up LM Studio it's fine but if you want more you basically have to build everything yourself. Research mode, STT and TTS, RAG, MCP or tool use. All these are small building blocks you have to integrate yourself. It really gets messed-up if you also want these in your native language other than english. When showing my setup to my normie friends everyone quickly comes to the same conclusion.

- ## [We tried to automate product labeling in one prompt. It failed. 27 steps later, we've processed 10, 000+ products. : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1qcsmww/we_tried_to_automate_product_labeling_in_one/)
  - We built an AI agent to localize imported food products for a retail client. The task sounds simple: extract product info, translate it contextually (not Google Translate), calculate nutritional values for local formats, check compliance with local regulations.
  - First attempt: one detailed prompt. Let the AI figure out the workflow.
  - Result: chaos. The AI would hallucinate numbers even with clean images. It would skip steps randomly. At scale, we had no idea where things broke. Every error was a mystery to debug.
- So we broke it down. Way down. 27 steps.
  - Each column in our system handles one thing: Extract name/weight/desc/...
- What changed:
  - 1. Traceability. When something fails, we know exactly which step. No more guessing.
  - 2. Fixability. Client corrects a number extraction error once, we build a formula that prevents it downstream. Errors get fixed permanently, not repeatedly.
  - 3. Consistency at scale. The AI isn't "deciding" what to do. It's executing a defined process. Same input, same process, predictable output.
  - 4. Human oversight actually works. The person reviewing outputs learns where the AI struggles. Step 14 always needs checking. Step 22 is solid. They get faster over time.
- The counterintuitive part: making the AI "dumber" per step made the overall system smarter. One prompt trying to do everything is one prompt that can fail in infinite ways. 27 simple steps means 27 places where you can inspect, correct, and improve.
  - We've processed over 10, 000 products this way. The manual process used to take 20 minutes per product. Now it's 3 minutes, mostly human review.
  - The boring truth about reliable AI agents: it's not about prompt engineering magic. It's about architecture that assumes AI will fail and makes failure easy to find and fix.

- This is case with 90% of real world AI use for non coding currently. It always requires human quailty control and intervention.

- Good example of human in the loop. I've been talking with my CEO about how the best use of LLM's at least for the foreseeable future will be wtih humans in the loop. The problem will always be getting humans to actually do the human in the loop process as the error rate begins to plummet.

- same attempt and result I had. Found Nemotron nano solved any output quality issues once it was all broken down.

- ## [Llama 3.2 1B Instruct â€“ What Are the Best Use Cases for Small LLMs? : r/LocalLLaMA _202501](https://www.reddit.com/r/LocalLLaMA/comments/1i4cfpz/llama_32_1b_instruct_what_are_the_best_use_cases/)
- Llama 3.2 1B Instruct can work as speculative decoding model for Llama 3.2-11B/90B or 3.3-70B.

- Code completion and autocomplete

- With a bit of fine tuning they can be really good at task specific things, including structured output (do not try llama 1b for structured output without fine tuning).
  - Long term my hope is local models built into the OS, with small task specific Lora adapters. iOS is doing it, but not open to 3rd parties yet.

- For research it's nice to have a dirt cheap model to prototype datasets when evaluating LLMs I usually use 8B for that though

- ## [Real world use cases for small LLM on edge devices : r/LocalLLaMA _202409](https://www.reddit.com/r/LocalLLaMA/comments/1ffzsy0/real_world_use_cases_for_small_llm_on_edge_devices/)
- Small local models can make many factual mistakes, because it's impossible to compress the entire world model into 2 GB. However, they can be great analyzers (for their size) of the existing text.
  - Summarizing news articles, extracting insights from long web pages, finding a fact that you're searching for in a long text (a book?) and so on. Imagine it as a personal assistant in daily activities.

- Function calling. If it can do basic language to function translation, this is a perfect use. So it allows human voice or text interaction with complicated systems that would otherwise require custom coding. Also, text to structured responses, like using embeds or RAG to return well-defined SQL queries that might result from a wide variety of human specifications.

- All of these tasks involve LLM now, and are simple enough that they can be done on edge. In fact, most of them already do. Sure a cloud service can also do them, sometime better, but with a recurring cost that scale with usage. Most business that sell hardware, and user that buy it, will prefer the fixed upfront cost for R&D that small model.
  - OCR
  - Live transcription and translation
  - Summarization
  - Autocorrect
  - Basic voice control

- Analysis of secured data in offline mode . Maybe finance data , healthcare data , research data. Embed it on drone and check for suitable usecases

- ## [The curious case of Qwen3-4B (or; are <8b models *actually* good?) : r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1p76wtf/the_curious_case_of_qwen34b_or_are_8b_models/)
  - how good are the smaller models at answering some of the sort of questions I might ask of them, chatting, instruction following etc?
  - I ran each model's output against the "council of AI elders", then got GPT 5.1 (my paid account craps out today, so as you can see I am putting it to good use) to run a tally and provide final meta-commentary.
  - The results, per GPT 5.1: GPT-OSS 20B and Qwen 3-4B emerged as the strongest overall performers. Mid-tier models like DeepThink 7B and Qwen 2.5 7B produced competent technical content but struggled severely with the style transform, while Phi-Mini 4B showed the weakest combination of accuracy, coherence, and instruction adherence.
  - The results align closely with real-world use cases: larger or better-trained models excel at technical clarity and instruction-following, whereas smaller models require caution for detail-sensitive or persona-driven tasks, underscoring that the most reliable workflow continues to be â€œstrong model for substance, optional model for vibe.â€

- Iâ€™m using qwen3-4b 2507 instruct for simple tasks such as meeting summarization, keeping track of to-doâ€™s and the likes. So far, it has performed quite well

- ## [My Journey to finding a Use Case for Local LLMs : r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1p1xy0q/my_journey_to_finding_a_use_case_for_local_llms/)
  - I have a lot of phone pictures of recipes, and a lot of inherited cookbooks. The thought of gathering the ones I really liked into one place was daunting. The recipes would get buried in mountains of photos of cats (yes, it happens), planes, landscapes etc. Google photos is pretty good at identifying recipe images, but not the greatest.
  - I landed on qwen3-vl:8b. It was able to take the image (with very strict prompting) and output the exact text from the image. I did have to verify and do some editing here and there. I was happy
  - I then turned to GPT-OSS:20b since it is newer, and asked it to convert the recipe text to json-ld recipe schema compatible format.
  - Now I can take a pic of any recipe I want, run it through the qwen-vl:8b model for OCR, verify the text, then have GPT-OSS:20b spit out json-ld recipe schema text that can be imported into the mealie database. (And verify the json-ld text again, of course).
  - I'm not using any system prompts at this time. Here is the prompt

- You could now take this even further and query it with a prompt on your cell phone.

- My use case is that I use local vlm to the content of the download file, then rename following the format I define and use bash script to route it to the its dedicated folder. Now every time a file is downloaded, it is renamed and route to its location automatically.
# discuss-local-llm-xp/tips
- ## 

- ## 

- ## We've been exploring the value of letting agents use CLIs vs just navigating a REST API directly. 
- https://x.com/dhh/status/2012543705161326941
  - The smartest models can do without a CLI, but take longer and cost more. Even small models can succeed when given CLIs. But the puck keeps moving!
- AI does better with CLIs because it's a human interface. It's designed with self-discovery in a logical way. 

- We realized this very early on. Our first party integrations with Ci providers are clis we purpose built for our vertical agent workflows. Mcps are terribly inefficient even if tools are lazy loaded for multi turn agentic conversations

- Everyone whoâ€™s been early to using coding agents, figured this out a few months ago

- Yeh my personal assistant is all CLI driven (a cli for open code to use, so I say "Plan out my day" and it goes and uses it to do that).  Super useful.  At @agentuity , we've really had to make sure our sdk, api's, and CLI are FOR agents as much as developers. Mindshift change.

- Using the same CLI for both humans and agents turned out to be surprisingly practical.

- The CLI abstraction buys you something else too: versioning. REST APIs change endpoints, parameters, auth flows. A stable CLI surface means your agent skills don't break when the underlying API shifts. That durability matters more as you scale agent count.

- This is why I recently converted my Basecamp MCP connector to a Basecamp CLI.

- ## [I benchmarked 7 Small LLMs on a 16GB Laptop. Here is what is actually usable. : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pzxtnr/i_benchmarked_7_small_llms_on_a_16gb_laptop_here/)
  - I tested Qwen 2.5 (14B), Mistral Small (12B), Llama 3 (8B), and Gemma 3 (all 4-bit quants) to see which ones I could actually run without crashing my laptop.
  - Qwen 2.5 (14B): The smartest for coding, but it eats 11GB System RAM + Context. On a 16GB laptop, if I opened 3 Chrome tabs, it crashed immediately (OOM).

- They are all ancient models by llm age so its kinda an invalid test. The king of low spec ram is mamba2 and sliding window attention. Try Granite 4, or Nemotron Nano 2

- I'm not considering 1-2 tokens/s "usable". Even 10 tokens/s is barely usable for the most part.

- ## [I'm curious whether people ask for the model's name in their prompts when testing on LMArena (ChatBot Arena). : r/LocalLLaMA _202504](https://www.reddit.com/r/LocalLLaMA/comments/1jtfzci/im_curious_whether_people_ask_for_the_models_name/)
  - After all, by doing this, users can know the names of the models being A/B tested beforehand, which could bias the ongoing test to some extent.

- Models never know a thing about themselves; no serious LLM user in 2025 will ask model about itself.

- well, the model's output might not be correct at all: deepseek sometimes call itself gpt. so you never now who these models actually are.

- ## [Hard lesson learned after a year of running large models locally : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)
  - Iâ€™m running everything off a workstation with a single RTXâ€¯3090, Ubuntu 22.04, llama.cpp for smaller models and vLLM for anything above 30â€¯B parameters.
  - My goal has always been to avoid cloud dependencies and keep as much computation offline as possible, so Iâ€™ve tried every quantization trick and caching tweak I could find.
  - The biggest friction point has been scaling beyond 13â€¯B models.
  - My takeaway so far is that local first inference is viable for small to medium models, but thereâ€™s a hard ceiling unless you invest in server grade hardware or cluster multiple GPUs.
  - Quantization helps, but you trade some quality and run into new bugs.
  - For privacy sensitive tasks, the tradeâ€‘off is worth it; for fast iteration, itâ€™s been painful compared to cloud based runners.

- vLLM works great if model + context fit into VRAM, but it doesn't do CPU offloading well - use llama.cpp for anything that spills over to RAM.

- instead of trying to use "smarter bigger" models to achieve whatever youre trying to achieve, its more reliable to use multiple parallel instances (via vllm for example) of smaller model that can communicate with each other in distinct roles to create a system that produces accurate results.

- You can also rent a card in the cloud for near local inference.

- ## [Useful patterns for building HTML tools _202512](https://simonwillison.net/2025/Dec/10/html-tools/)
- https://x.com/vista8/status/2003803470449787263
  - ç”¨å¤§æ¨¡å‹ä¸¤å¹´åšäº†150ä¸ªå·¥å…·ã€‚è¿™ä¸å¥‡æ€ªï¼Œå¼ºçš„æ˜¯ä»–åªç”¨å•é¡µHTMLåšï¼
  - ä¸€ä¸ª HTML æ–‡ä»¶ï¼Œæ‰“å¼€å°±èƒ½ç”¨ï¼Œä¸éœ€è¦å®‰è£…ï¼Œä¸éœ€è¦æ³¨å†Œã€‚
  - ä»–ç‰¹åˆ«å¼ºè°ƒ "ä¸è¦ç”¨ React"ã€‚ å› ä¸º JSX éœ€è¦æ„å»ºï¼Œè¿™ä¼šè®©æ•´ä¸ªæµç¨‹å˜å¾—éº»çƒ¦ã€‚ ä¿æŒç®€å•ï¼Œå°±æ˜¯ä¿æŒå¯ç»´æŠ¤ã€‚
- ä¼˜åŠ¿ä¹Ÿå¾ˆæ˜æ˜¾ï¼š
  - å¯ä»¥ç›´æ¥ä» ChatGPT æˆ– Claude é‡Œå¤åˆ¶ç²˜è´´å‡ºæ¥
  - æ‰”åˆ° GitHub Pages ä¸Šå‡ ç§’é’Ÿå°±èƒ½ç”¨
  - ä¸éœ€è¦ npmï¼Œä¸éœ€è¦æ„å»ºæ­¥éª¤
  - ä¸¤å¹´åè¿˜èƒ½æ‰“å¼€ï¼Œä¸ä¼šå› ä¸ºä¾èµ–è¿‡æœŸå´©æºƒ
- çŠ¶æ€å­˜åœ¨å“ªé‡Œï¼Ÿ æ²¡æœ‰åç«¯æ•°æ®åº“æ€ä¹ˆåŠï¼ŸSimon æœ‰ä¸¤ä¸ªåŠæ³•ï¼š
  - æŠŠçŠ¶æ€å­˜åœ¨ URL é‡Œã€‚ æ¯”å¦‚ï¼Œä»–åšäº†ä¸€ä¸ª 24x24 çš„å›¾æ ‡ç¼–è¾‘å™¨ï¼Œä½ ç”»çš„å›¾æ ‡ç›´æ¥ç¼–ç åœ¨ URL é‡Œã€‚è¿™æ ·ä½ å¯ä»¥æ”¶è—ï¼Œå¯ä»¥åˆ†äº«ï¼Œæ‰“å¼€é“¾æ¥å°±èƒ½ç»§ç»­ç¼–è¾‘ã€‚
  - æŠŠå…³é”®çš„ä¿¡æ¯å­˜åœ¨ localStorage é‡Œã€‚ æ¯”å¦‚ API key è¿™ç§æ•æ„Ÿä¿¡æ¯ï¼Œä¸èƒ½å‡ºç°åœ¨ URL é‡Œï¼Œä¹Ÿä¸èƒ½å‘åˆ°æœåŠ¡å™¨ã€‚ localStorage è®©æ•°æ®åªå­˜åœ¨ç”¨æˆ·çš„æµè§ˆå™¨é‡Œã€‚ ä»–è¿˜ç”¨ localStorage åšè‡ªåŠ¨ä¿å­˜ã€‚ å†™å­—æ•°ç»Ÿè®¡å·¥å…·æ—¶ï¼Œæ¯æ¬¡è¾“å…¥éƒ½ä¼šä¿å­˜ï¼Œè¿™æ ·ä¸å°å¿ƒå…³æ‰æ ‡ç­¾é¡µä¹Ÿä¸ä¼šä¸¢å†…å®¹ã€‚
- CORSï¼ˆè·¨åŸŸèµ„æºå…±äº«ï¼‰å¬èµ·æ¥å¾ˆæŠ€æœ¯ï¼Œä½†ç†è§£å®ƒå¾ˆé‡è¦ã€‚
  - iNaturalist å¯ä»¥æŸ¥åŠ¨ç‰©çš„è§‚å¯Ÿè®°å½•
  - PyPI å¯ä»¥è·å– Python åŒ…çš„ä¿¡æ¯
  - GitHub çš„å…¬å¼€ä»“åº“å†…å®¹éƒ½å¯ä»¥ç›´æ¥è·å–
  - Bluesky å’Œ Mastodon çš„ API ä¹Ÿå¾ˆå¼€æ”¾
  - Simon ç”šè‡³ç”¨ GitHub Gists æ¥æŒä¹…åŒ–æ•°æ®ã€‚ å› ä¸º Gist çš„ API æ”¯æŒ CORSï¼Œä½ å¯ä»¥åšä¸€ä¸ªçº¯å‰ç«¯å·¥å…·ï¼ŒæŠŠç”¨æˆ·çš„æ•°æ®ä¿å­˜åˆ°ä»–ä»¬è‡ªå·±çš„ Gist é‡Œã€‚
- æ–‡ä»¶ä¸éœ€è¦ä¸Šä¼  `<input type="file">` ä¸åªæ˜¯ç”¨æ¥ä¸Šä¼ æ–‡ä»¶ã€‚ JavaScript å¯ä»¥ç›´æ¥è¯»å–æ–‡ä»¶å†…å®¹ï¼Œåœ¨æµè§ˆå™¨é‡Œå¤„ç†ã€‚
- Python å’Œ WebAssembly è¿™æ˜¯æœ€è®©äººå…´å¥‹çš„éƒ¨åˆ†ã€‚ Pyodide è®© Python å¯ä»¥åœ¨æµè§ˆå™¨é‡Œè¿è¡Œã€‚
  - ä¸æ˜¯ç©å…·çº§åˆ«çš„è¿è¡Œï¼Œæ˜¯çœŸæ­£çš„ Pythonï¼ŒåŒ…æ‹¬ Pandas å’Œ matplotlibã€‚
- Tesseract OCRã€SQLiteã€å›¾ç‰‡å‹ç¼©åº“ï¼Œè¿™äº›åŸæœ¬ç”¨ C æˆ– C++ å†™çš„è½¯ä»¶ï¼Œç°åœ¨éƒ½èƒ½åœ¨æµè§ˆå™¨é‡Œè·‘ã€‚
- è¿™äº›å·¥å…·çœŸå¯ä»¥è§£å†³é—®é¢˜ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¿™ç§æ–¹å¼è®©ä½ èƒ½å¿«é€Ÿå®éªŒã€‚æƒ³æ³•åˆ°åŸå‹å¯èƒ½åªéœ€è¦å‡ åˆ†é’Ÿã€‚ä¸ç”¨æ‹…å¿ƒéƒ¨ç½²ï¼Œä¸ç”¨æ‹…å¿ƒç»´æŠ¤ï¼Œä¸ç”¨æ‹…å¿ƒæˆæœ¬ã€‚
- æ€ä¹ˆèµ·æ­¥å°è¯•ï¼Ÿ ä¸€ä¸ª GitHub ä»“åº“ï¼Œå¼€å¯ GitHub Pagesã€‚ ç„¶åå°±å¯ä»¥å¼€å§‹äº†ã€‚ ç”¨ ChatGPT æˆ– Claude ç”Ÿæˆä¸€ä¸ªå·¥å…·ï¼Œå¤åˆ¶ç²˜è´´åˆ°ä»“åº“é‡Œï¼Œå‡ ç§’é’Ÿåå°±èƒ½åœ¨ç½‘ä¸Šè®¿é—®ã€‚ ä¸éœ€è¦å®Œç¾ï¼Œä¸éœ€è¦å¤æ‚ã€‚ ä»ä¸€ä¸ªè§£å†³ä½ è‡ªå·±é—®é¢˜çš„å°å·¥å…·å¼€å§‹ã€‚ ç„¶åæ…¢æ…¢ç§¯ç´¯ï¼Œæ…¢æ…¢æ”¹è¿›ã€‚ ä¸¤å¹´åä½ å¯èƒ½ä¹Ÿä¼šæœ‰å‡ åä¸ªç”šè‡³ä¸Šç™¾ä¸ªè¿™æ ·çš„å·¥å…·ã€‚ æ¯ä¸€ä¸ªéƒ½åœ¨æŸä¸ªæ—¶åˆ»å¸®åˆ°ä½ ï¼Œæ¯ä¸€ä¸ªéƒ½è®©ä¸‹ä¸€ä¸ªæ›´å®¹æ˜“åšå‡ºæ¥ã€‚
- è¿™å°±æ˜¯ HTML å·¥å…·çš„é­…åŠ›ï¼šç®€å•ã€å®ç”¨ã€å¯æŒç»­ã€‚

- ## [Senior engineer struggles with learning LLMs foundations : r/LLMDevs _202512](https://www.reddit.com/r/LLMDevs/comments/1plxnq0/senior_engineer_struggles_with_learning_llms/)
  - I've been using ollama and openai to create some interesting side projects and to learn more about LLMs, but I think I'm hugely lacking solid foundations. Please provide me with a structure learning material for a senior engineer with some knowledge of LLMs, thanks
- Andrej Karpathyâ€™s series on YouTube and Stanfordâ€™s CME course on LLMs and Transformers which is published to YouTube for free.
  - I had a quick look at the Stanford's course and I think it's exactly what I needed, thanks

- You need to learn four critical things
  - Your agent's core product logic is the prompt/instructions you send to an LLM. You will be spending time here with domain experts too to construct good instructions so that the model aligns to your policy. There is no magic bullet. You iterate and evaluate until you are satisfied. Making investments in evals is worth it.
  - If you want to build an agentic application, then you need to expose tools to different models. These are essentially APIs that you have today, both internal or external which the model will instruct you to run and return its results as string. 
  - There are two agentic loops, one is called the inner loop where you agent interacts with an LLM until the LLM is done (stop_reason=finish). There is an outerloop which runs to route traffic to/from agents (if you have a multi-agent architecture), ensure that only good traffic is reaching your agents and that if multiple agents need to be engaged it would be handled outside your core product logic.
  - You need exceptional observability to know what happens, how things fail, etc. And you need to account for different models in your stack so that you can easily improve performance, and/or latency and/or cost.

- ## [å’ŒAIå¯¹è¯ï¼Œä¸è¦ä½¿ç”¨â€œä½ â€ ](https://linux.do/t/topic/1293395)
- AI å¤§ç¥ Karpathy åˆ†äº«äº†ä¸€ä¸ªåç›´è§‰çš„è§‚ç‚¹ï¼šè·Ÿå¤§æ¨¡å‹èŠå¤©æ—¶ï¼Œä¸è¦ä½¿ç”¨ â€œä½ â€ è¿™ä¸ªå­—ï¼Œä¹Ÿå°±æ˜¯ä¸è¦æŠŠå®ƒä»¬å½“ â€œäººâ€ã€‚
  - Karpathy æŒ‡å‡ºï¼Œè¿™ç§é—®æ³•å…¶å®æ˜¯åœ¨ç»™ AI é™æ™ºã€‚å› ä¸º LLM æœ¬è´¨ä¸Šå¹¶æ²¡æœ‰è‡ªæˆ‘æ„è¯†ï¼Œå®ƒæ›´åƒæ˜¯ä¸€ä¸ªæ‹¥æœ‰æµ·é‡çŸ¥è¯†çš„æ¨¡æ‹Ÿå™¨ã€‚
  - å½“ä½¿ç”¨ â€œä½ â€ è¿™ä¸ªå­—çš„æ—¶å€™ï¼Œæ¯”å¦‚é—® â€œä½ çš„çœ‹æ³•â€ï¼Œæ¨¡å‹å°±ä¼šç«‹åˆ»è¢«è§¦å‘ä¸€ç§ç‰¹å®šçš„äººæ ¼åµŒå…¥ã€‚
  - å®ƒä¼šå¼ºè¡Œè®©è‡ªå·±æ‰®æ¼”ä¸€ä¸ªç¤¼è²Œã€å®‰å…¨ä½†æœ‰ç‚¹æ— èŠçš„ AI åŠ©æ‰‹ã€‚è¿™æ—¶å€™å®ƒåå‡ºæ¥çš„å¾€å¾€æ˜¯é‚£äº›å››å¹³å…«ç¨³ã€æ»´æ°´ä¸æ¼ï¼Œä½†å®é™…ä¸Šæ²¡å•¥æ·±åº¦çš„è½¦è½±è¾˜è¯ï¼Œä¹Ÿå°±æ˜¯å¤§å®¶å¸¸è¯´çš„ â€œAl å‘³â€ã€‚
  - ç›¸åï¼ŒKarpathy å»ºè®®æˆ‘ä»¬è¦è½¬å˜æ€è·¯ï¼Œè¦æƒ³è§£é” AI çœŸæ­£çš„å®åŠ›ï¼Œå°±æŠŠ AI å½“æˆæ¨¡æ‹Ÿå™¨ç”¨ã€‚
  - ä¸¾ä¸ªä¾‹å­ï¼Œä¸è¦é—® â€œä½ æ€ä¹ˆçœ‹â€ï¼Œè€Œæ˜¯è¦è®¾å®šå…·ä½“çš„æƒ…å¢ƒå’Œè§’è‰²ã€‚ä½ å¯ä»¥é—® â€œå¦‚æœæ˜¯ä¸‰ä½èµ„æ·±äº§å“ç»ç†ååœ¨ä¸€èµ·è®¨è®ºè¿™ä¸ªåŠŸèƒ½ï¼Œä»–ä»¬ä¼šæå‡ºå“ªäº›å°–é”çš„æ‰¹è¯„â€ï¼Œæˆ–è€… â€œè¯·ä»¥ä¸€ä½è¯ºè´å°”ç»æµå­¦å¥–å¾—ä¸»çš„è§†è§’åˆ†æè¿™ä¸ªç°è±¡â€ã€‚
- è¯„è®ºåŒºä¹Ÿæœ‰ä¸å°‘é«˜æ‰‹è¡¥å……ï¼Œä¸ä»…è¦å°‘ç”¨ â€œä½ â€ï¼Œä¹Ÿè¦å°‘ç”¨ â€œæˆ‘â€ã€‚
  - å› ä¸ºå½“ä½ è¡¨è¾¾ â€œæˆ‘è§‰å¾—.â€ çš„æ—¶å€™ï¼ŒAI ä¸ºäº†è®¨å¥½ç”¨æˆ·ï¼Œå¾€å¾€ä¼šé¡ºç€ä½ çš„è¯è¯´ï¼Œè¿™å°±å¯¼è‡´äº†æ‰€è°“çš„é˜¿è°€å¥‰æ‰¿ç°è±¡ï¼Œè€Œä¸æ˜¯åŸºäºå®¢è§‚äº‹å®ç»™å‡ºç­”æ¡ˆã€‚
- ä¸‹æ¬¡å†™æç¤ºè¯çš„æ—¶å€™ï¼Œè¯•è¯•æˆ’æ‰ â€œä½ â€ å’Œ â€œæˆ‘â€ è¿™ä¸¤ä¸ªå­—ã€‚è®© AI å»æ¨¡æ‹Ÿé‚£äº›å…·ä½“çš„ä¸“å®¶ã€å›¢é˜Ÿç”šè‡³åæ–¹è¾©æ‰‹ï¼Œä½ ä¼šå‘ç°å®ƒçš„å›ç­”è´¨é‡èƒ½æå‡å¥½å‡ ä¸ªæ¡£æ¬¡ã€‚

- é‚£ä¹ˆæ¯”å¦‚ â€œä½ æ˜¯ä¸€ä¸ªç‰©ç†å­¦é¢†åŸŸä¸“ä¸šäººå£«â€ã€â€œä½ æ˜¯ä¸€ä¸ªç©¿å°è£™å­çš„ç®—æ³•é«˜æ‰‹â€ã€â€œä½ æ˜¯ä¸€ä¸ªçŒ«å¨˜â€ çš„ promptï¼Œåˆ°åº•è¯¥ä¸è¯¥ç”¨å‘¢ï¼Ÿä»¥å‰æˆ‘çœ‹å¾ˆå¤šäººé¼“åŠ±åœ¨éœ€è¦ç‰¹å®šå·¥ä½œåœºæ™¯æ—¶è¿™ä¹ˆåšï¼Œè¿™é‡Œé¢æœ‰ä½ ï¼Œä½†æ˜¯æŒ‡å®šäº†äººæ ¼çš„ç±»å‹
  - æˆ‘ä¹Ÿåœ¨æƒ³è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥æ”¹å†™æˆ â€œä½œä¸ºä¸€ä¸ª xxxâ€ï¼Œâ€œæ‰®æ¼”ä¸€å xxxâ€ï¼Œç„¶ååé¢å…¨éƒ¨ç”¨ç¥ˆä½¿å¥è¯•ä¸€ä¸‹
- â€œä½ â€ çš„è¯æ„Ÿè§‰æ˜¯å·²ç»æŒ‡å®šäº†ä¸€ä¸ªäººï¼Œè€Œç”¨ â€œè¯·ä»¥â€ çš„è¯æ„Ÿè§‰ä¼šä»è¯¥é¢†åŸŸä¸­æ¯ä¸ªäººä¸åŒè§†è§’æ¥æ€è€ƒï¼Œå¯èƒ½ä¼šæ›´å…¨é¢ä¸€ç‚¹ï¼Ÿ

- çœæµï¼Œå¤šç”¨è§’è‰²æ‰®æ¼”æ³•ã€‚ æ­¤äº‹åœ¨è„‘ç­‹æ€¥è½¬å¼¯ä¸­äº¦æœ‰è®°å½•ã€‚ éæ€è€ƒæ¨¡å‹ä½ ç›´æ¥æé—® è„‘ç­‹æ€¥è½¬å¼¯ç±»å‹çš„é¢˜ç›® å¤§å¤šæ¨¡å‹å¾ˆå¯èƒ½è¢«æ¬ºéª—ã€‚ ä½†ä½ å¼ºè°ƒè¿™æ˜¯è„‘ç­‹æ€¥è½¬å¼¯ï¼Œ è®©ä»–å»æ‰®æ¼”ç±»ä¼¼é¢†åŸŸçš„é«˜æ‰‹ï¼Œ å³ä½¿æ˜¯éæ€è€ƒæ¨¡å‹ä¹Ÿèƒ½ç ´è§£é™·é˜±

- â€œä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ swift ç¨‹åºå‘˜â€  â†’  â€œè¯·ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„ swift ç¨‹åºå‘˜ï¼Œxxxxâ€ è¿™ä¹ˆç†è§£å¯¹å—

- è¿™äº›æŠ€å·§ç›´æ¥è°ƒç”¨ api æ•ˆæœæœ€æ˜æ˜¾å§ï¼Ÿç½‘é¡µç‰ˆçš„å¤§æ¨¡å‹å’Œå…¶ä»–å·¥å…·é‡Œè°ƒ APIï¼ŒåŸºæœ¬éƒ½é¢„è®¾äº†è§’è‰²ã€‚å³ä½¿ä¸ç”¨ç¬¬ä¸€äºŒäººç§°ï¼Œæ•ˆæœæå‡ä¹Ÿä¸å¤§å§ï¼Ÿ

- ## [8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/)
  - https://github.com/lemonade-sdk/lemonade
  - Lemonade v9.0.6 came out today, making it really easy to run many models at the same time... and this is the best demo I could think of. Hope it makes someone laugh today!
  - This is a Ryzen AI MAX 395+ (aka Strix Halo). The models are between 3 and 8B parameters (size on disk is visible at the start of the video).
  - Lemonade is a free and open local LLM server made by AMD to make sure we have something optimized for AMD PCs. Today we released a new version that lets many LLMs run at the same time.
  - I made this quick HTML/CSS/JS app to demo the capability. It loads 8 LLMs, has them share a chat history, and then keeps prompting them until they vote yes or no on the user's question.
  - In the backend, there are 8 llama-server processes running on the Strix Halo's GPU. The web app talks to lemonade server at http://localhost:8000, and then lemonade routes the request to the right llama-server process based on the model ID in the request.
  - I thought it was funny the LLMs couldn't come to a final decision on this question. Just like people!

- A tie? That's no good. There should have been 9 LLMs, like the Supreme Court.

- Are they debating each other? Seems like they donâ€™t spend much time disagreeing. I want to see one where they are forced into consensus or it doesnâ€™t end (or maybe time it out and score it then)
  - Thereâ€™s 5 rounds of debate. First round they are supposed to give a hot take. Rounds 2 and 3 theyâ€™re supposed to react to each other (shared chat history). Rounds 4 and 5 theyâ€™re supposed to vote.

- ## ["We're in an LLM bubble, not an AI bubble" - Here's what's actually getting downloaded on HuggingFace and how you can start to really use AI. : r/LlamaFarm _202512](https://www.reddit.com/r/LlamaFarm/comments/1pb2wr2/were_in_an_llm_bubble_not_an_ai_bubble_heres/)
  - Encoder-only models (BERT family) account for 45% of HuggingFace downloads, nearly 5x more than decoder-only LLMs at 9.5%. 
  - Every one of these model families exists because someone realized the "one model to rule them all" approach was failing for their use case
  - This is "Mixture of Experts" at the application level. Many small, specialized models working together instead of one massive model trying to do everything.

- ## [What broke when you tried to take local LLMs to production? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p91p4k/what_broke_when_you_tried_to_take_local_llms_to/)
- Ollama broke a lot of the time because we devâ€™d on a Mac but pushed production to nvidia. Switching to vllm has largely solved this and pushed it to more of an interface rather than model problem.

- Hosting several Models on one GPU is always really annoying with vLLM. You basically have to fiddle with the memory utilisation until all models fit. There is no clean way I found to calculate memory usage. You just have two tweak, boot, look at the logs and repeat. And even if you got all models to fit, vLLM sometimes crashes with cuda out of memory exceptions when loading the models since there seems to be a peak in memory consumption on boot up.

- Why doesn't anyone mention tabbyAPI/exllamav3? It's much more memory-efficient than AWQ, also it loads quicker, and the quantization is more optimized. Also K/V cache can be quantized between 2 and 8 bits seperately (=more room for context)

- ## [Why do you use open-source LLMs ? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p16kxx/why_do_you_use_opensource_llms/)
- Freedom. I can do whatever I want without having my data stolen.

- The benefit of open source LLMs for me is that you can access the internal layers to do things like attention injection, intermediate layer feature extraction, attention map extraction, token-wise early stoppage, adaptive layer skips, conversion to latent attention, conversion to mamba or gated delta net hybrid etc
  - This is it. Intervening mathematically in something that can talk like a human is an otherworldly experience. I do it almost every day and it makes me feel like an alchemist tinkering with a homunculus. Thereâ€™s nothing else like it. Most other intellectual activities feel shallow by comparison.

- I just think it's very cool to have a summary of the vast wealth of human text and knowledge in a 30GB file on my computer.

- I want to own my capabilities. I donâ€™t want my coding skills to be tied to some external service that I have zero control over.

- ## [What do you use local LLMs for? What is your use case? : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1ms4gmz/what_do_you_use_local_llms_for_what_is_your_use/)
- They are best used for small context tasks like classification/sentiment, asking it summarize, pull out keywords, grammar, etc.
  - Not for long conversations or complex tasks, but system prompt, user request, and then get a single response.
  - For example, I've been experimenting with a local LLM checking chat messages for rule-breaking in a Discord server, flag the suspicious once, and then I checked the flagged messages and take action myself.

- I used it to generate a synthetic dataset for training my first neural network, with successful results.

- Structured data extraction from private documents (millions of documents)
  - We have produced an annotated dataset of a few thousand records. We used this to fine tune a model and evaluate performance. We are getting >0.98 F1 score which is a margin of error we are willing to tolerate given the scale and time saved... It's for a very specific type of short documents. Our extraction pipeline has multiple extraction and validation steps.

- Data extraction and classification.

- ## [What are you using your local models for ? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1oz6k5j/what_are_you_using_your_local_models_for/)
- Confidential research. In SillyTavern. Just yet with a quantized Skyfall-31b from Drummer, apparently some blend of Mistral Small or whatever. It's fun.

- Qwen3-VL-30B-A3B-Thinking is heating my home by processing video. I've posted about it before but llm-ffmpeg-edit.bash handles the logic & llm-python-vision-multi-images.py handles sending the images/frames to the LLM backend.
  - I was using Mistral 3.2 (24B dense) before Qwen3-VL got support. The speed increase from 24B -> 30B-A3B has been incredible, while maintaining accuracy.

- I'm bound by the legal terms of my employment to not discuss the technologies we use there, but am free to talk about my personal use-cases.
- STEM research assistant -- I give it my technical notes (usually physics and/or math) and a question, and get back helpful replies. My go-to is Phi-4-25B, and when it's not smart enough I escalate to Tulu3-70B, sometimes Qwen3-235B pipelined with Tulu3-70B.
- Creative writing -- Cthulhu-24B, Big-Tiger-Gemma-27B-v3, or Valkyrie-49B-v2. Mostly sci-fi (space opera or Murderbot fanfic).
- Evol-Instruct and synthetic dataset generation or augmentation -- again, mostly Phi-4-25B or Tulu3-70B, though recently I have been using Valkyrie-49B-v2 to bulk up a RAG database of technical troubleshooting advice/solutions. To my surprise Valkyrie is a lot better at this than Tulu3-70B, even though they are derived from similar models (Tulu3 from Llama-3.1, Valkyrie from Llama-3.3-Nemotron-Super-49B-v1.5 which in turn is based on Llama-3.3).
- Persuasion research -- studying the capacity for LLM inference to change people's minds. Big-Tiger-Gemma-27B-v3 is excellent at this.
- Wikipedia-backed RAG for general question-and-answer. I use Big-Tiger-Gemma-27B-v3 for this as well.
- Describing images so I can index them in a locally hosted search engine. Qwen2.5-VL-72B is still the best vision model I've yet used, but I haven't had a chance yet to compare it against Qwen3-VL-32B. I am hoping Qwen3 is better, despite having fewer than half as many parameters.
- I also run an IRC bot for a technical support channel, which is mostly GOFAI-driven but I've been working on a plugin for it to be RAG/LLM-driven too. That, too, uses Big-Tiger-Gemma-27B-v3.
- Recently I've been trying to use Phi-4 (14B) as a synthetic dataset rewriter, to salvage low-quality inferred data I would normally prune from the dataset. I read a paper suggesting even very small models (4B) are effective at this. So far my results have been mixed. I've been meaning to try Tiger-Gemma-12B-v3 as well; possibly Phi-4 just isn't the right model for this.
- GLM-4.5-Air for slow inference of entire programming projects (which I don't do much, since I don't want my coding skills to atrophy) or to find bugs in my own code.
- Qwen3-Coder-REAP-25B-A3B for fast FIM code inference. The model doesn't have to be smart to figure out what my "for"-statement is going to look like, but it does need to be fast enough that it can suggest a completion before I've finished typing the "for"-statement myself. I use the REAPed version of this model so that it fits in my GPU's VRAM (at Q4_K_M); the original 30B-A3B didn't quite fit.
- I'm also tentatively using Phi-4 as a judge, comparing two replies to the same prompt and telling me which is better. It's early days yet, for this project, and it might not be the right model for this. We will see.
- Sometimes I use Big-Tiger-Gemma-27B-v3 or Phi-4 (14B) for language translation (mostly Spanish to English, but sometimes German or Russian to English). Overall Big Tiger is better at this, though Phi-4 does surprisingly well, and is better than Big Tiger at taking situational context into account with its replies. It's also a lot faster than Big Tiger, which is sometimes important for translation tasks.

- ## [Are any of you using local llms for "real" work? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1otnj2k/are_any_of_you_using_local_llms_for_real_work/)
- The most "real" work I've done is that Qwen3-VL-30B-A3B-Thinking is currently going through videos 10-seconds at a time. 
  - Based on the bot's True/False boolean output a wrapping program keeps track of what segments `<thing I'm looking for>` is within. At the end, we're done using Qwen3-VL and the wrapping program uses the segment information to use FFMPEG to make a clipped version where `<thing I'm looking for>` should always be present.
  - A general example is you could have the bot look for every explosion in an action film. Maybe it considers muzzle flash from a gun to be an explosion, not technically wrong as far as I know. So you could prompt that it should only be explosions not from gunfire and try that.

- I use an LLM, a scraper to fetch security vulnerabilities (CVEs), and an internal API that lists my running services, and the LLM generates a daily report for me about whether any of the software I'm running might have been mentioned.

- Using qwen3 vl to parse vids to extract car plate numbers snd other vehicle markings

- I used a local LLM and a vector database to help me with my corporate taxes. I first fed all past 3 or 4 years of expenses along with their categorizations (meals, fuel, office supplies etc.) into a Postgres PG Vector database and embedded the data using the Nomic model. I then classified several hundred transactions for the past tax year using this vector set of data to categorized the transactions.

- I use nvidia/Llama3-ChatQA-1.5-8B to index 2M similar insurance docs using ollama. I load the index into Meilisearch and sell access to it. I did this after a trip to micro center and a little over $3k.

- I use them as zeroshot NER/Classification models. They're pretty decent at it out of the box. Training isn't too complicated, but for small repetitive tasks, they're often good enough.

- ## [Why we shifted to Spec-Driven Development (and how we did it) : r/ClaudeCode _202511](https://www.reddit.com/r/ClaudeCode/comments/1op8b6i/why_we_shifted_to_specdriven_development_and_how/)
  - Over the last few months we came up with our own Spec-Driven Development (SDD) flow that we feel has some benefits over other approaches out there. 
  - Specifically, using a structured execution workflow and including the results of the agent work. 
- In short: you design your docs/specs first, then use them as input into implementation. And then you capture what happens during the implementation (research, agent discussion, review etc.) as output specs for future reference. 
- The cycle is:
  - Input specs: product brief, technical brief, user stories, task requirements.
  - Workflow: research â†’ plan â†’ code â†’ review â†’ revisions.
  - Output specs: research logs, coding plan, code notes, review results, findings.
- By making the docs (both input and output) first-class artifacts, you force understanding, and traceability. The goal isnâ€™t to create a mountain of docs. 
  - The goal is to create just enough structure so your decisions are traceable and the agent has context for the next iteration of a given feature area.
- First, worth mentioning this approach really only applies to a decent sized feature. Bug fixes, small tweaks or clean up items are better served just by giving a brief explanation and letting the agent do its thing.
- How we implemented it (step-by-step)
  - Define your prd.md**:** goals for the feature, user journey, basic requirements.
  - Define your tech_brief.md: high-level architecture, constraints, tech-stack, definitions.
  - For each feature/user story, write a requirements.md file: what the story is, acceptance criteria, dependencies.
  - For each task under the story, write an instructions.md: detailed task instructions
  - To start implementation, create a custom set of commands that do the following for each task
  - Commit these spec files alongside code so future folks (agents, humans) have full context.
  - Use folder conventions: e.g., project/story/task/requirements.md, â€¦/instructions.md etc. So itâ€™s intuitive.
- Bonus: If you want a tool that automates this kind of workflow opposed to doing it yourself (input specs creation, task management, output specs), Iâ€™m working on one called Devplan that might be interesting for you.

- Bmad does exactly this: https://github.com/bmad-code-org/BMAD-METHOD

- have you tried any of BMAD, GitHub/Spec-kit or Privacy-AI/spec-kitty for a community fork with extensive git worktree support

- ## [Which model do you wish could run locally but still canâ€™t? : r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1omoodc/which_model_do_you_wish_could_run_locally_but/)
- It's not really about particular models at this point. I'm way more interested in the local infrastructure around it. The main thing I'm still missing is simple knowledge-base integration for something like Open-WebUI. I would really like to just point the front-end at my local Kiwix-server and a 1TB-eBook collection, let it index to its heart's content for a few weeks and then have any model be able to reference and integrate all those information.

- Qwen3 Omni, how come no one is talking about a 30B model that can do video and speech?

- Something which I can install with just a dmg or exe file please
  - I tried a couple but I am not tech savvy and had to install a lot of other stuff to my laptop

- For me, itâ€™s not that itâ€™s a single model I canâ€™t runâ€¦ itâ€™s the fact that I canâ€™t run multiple models.

- ## [What's the missing piece in the LLaMA ecosystem right now? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o5dh3v/whats_the_missing_piece_in_the_llama_ecosystem/)
  - For me, it's the data prep and annotation tools. The models are getting powerful, but cleaning and structuring quality training data for fine-tuning is still a major, manual bottleneck.

- Training-Data is the biggest issue for local ecosystem right now i think. There is so many datasets, but who knows about their real quality.
  - For me personally, finetuning an LLM is like 500x harder than a diffusion model, simply due to the lack of tooling. Unsloth is nice and all, but i dont want to run fucking Jupyter Notebooks, i want something akin to kohya_ss with as many of the relevant hyperparameters exposed.
- Hardware accessibility is only secondary. If you have a small Model, e.g. the Qwen3 0.6B full finetune should be possible on local hardware. If that proves to be effective, renting a GPU machine somewhere for a few bucks shouldnt be the issue.

- llms are very bad at image recognition, give it a civ or other strategy game screenshot and it gets nearly everything wrong.

- Benchmarks. There has been little to no progress in the past two years regarding how LLMs are evaluated. Itâ€™s still mostly huge catalogues of questions with predetermined answers. Thatâ€™s a very poor system for testing intelligence.
# discuss
- ## 

- ## 

- ## 

- ## 

- ## [ä½ å¯¹AIæœ€å¸¸æ•²çš„æŒ‡ä»¤æ˜¯å•¥ ](https://linux.do/t/topic/1536540)
- è¿˜æ˜¯æ— æ³•è¿è¡Œ
- è¿˜æ˜¯æœ‰é—®é¢˜
- continue

- ## ç°åœ¨æ‰€æœ‰çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œæ— è®ºå®ƒå·ç§°ä¸Šä¸‹æ–‡çª—å£å¯ä»¥å¤šå°‘ï¼Œè¾“å…¥æ˜¯çœŸçš„å¯ä»¥å¾ˆé•¿ï¼Œä½†æ˜¯è¾“å‡ºä¸èƒ½å¤ªé•¿ï¼Œè¾“å‡ºé•¿äº†å°±å¹»è§‰ä¸¥é‡ï¼Œç›¸å¯¹å¥½ä¸€äº›çš„æ˜¯ Geminiï¼Œ
- https://x.com/dotey/status/1995667479377707123
  - æ‰€ä»¥ä½¿ç”¨æ—¶ï¼Œä½ å¯ä»¥è¾“å…¥å¾ˆå¤šèµ„æ–™ç»™å®ƒå‚è€ƒï¼Œä½†æ˜¯æ¯æ¬¡ä¸è¦è¾“å‡ºå¤ªå¤šï¼Œæ¯”å¦‚ä¸€æ¬¡æœ€å¤šè¾“å‡ºå‡ åƒå­—ï¼Œå¤šäº†å°±è¦åˆ†é¡µã€‚
- å¯ä»¥é‡‡ç”¨åˆ†å‡ æ­¥ï¼Œç„¶åæä¾›ä¸€ä¸ªchecklistç¡®ä¿ä»–è‡ªå·±å®¡æ ¸
  - å¯¹ï¼Œtodo listæœ‰åŠ©äºæ‹‰å›æ³¨æ„åŠ›å¥½ä¸€äº›ï¼Œä½†ä¸€æ ·è¾“å‡ºå†…å®¹ä¸èƒ½å¤ªé•¿

- è¯´ä¸‹æˆ‘çš„ç†è§£ï¼šæ¯ä¸ªtokenè¾“å‡ºéƒ½æ˜¯ä¸€ä¸ªæ¦‚ç‡ï¼Œä¸¾ä¸ªä¾‹å­ï¼šæ¯”å¦‚è¯´â€œæˆ‘â€è¿™ä¸ªå­—åé¢è·Ÿç€â€œä»¬â€è¿˜æ˜¯â€œçš„â€ï¼Œå¯¹llmæ¥è¯´å°±æ˜¯å–æ¦‚ç‡é«˜çš„ã€‚ä½†æ˜¯æ¦‚ç‡æœ‰2ä¸ªé—®é¢˜ï¼š
  1. å°±ç®—æ¯æ¬¡éƒ½å–æœ€é«˜çš„ï¼Œ0.9*0.9*0.9â€¦ï¼Œæ•´ä½“æ¥çœ‹ï¼Œç¬¦åˆé¢„æœŸçš„æ¦‚ç‡è¶Šæ¥è¶Šä½ï¼Œæ‰€ä»¥è¾“å‡ºå¤ªå¤šä¹‹åæ²¡æ³•ä¿è¯ä¸èµ°å
  2. ä¸­é—´æŸä¸€ä¸ªä½ç½®ï¼Œä¸¤ä¸ªtokençš„æ¦‚ç‡ç›¸è¿‘ï¼Œllmé€‰æ‹©ä¸å°å¿ƒèµ°åäº†ä¹‹åï¼Œåé¢è¾“å‡ºçš„tokenä¼šä¸€ç›´åœ¨è¿™ä¸ªèµ°åçš„tokenä¸Šç»§ç»­è¾“å‡º
  - è¡¥å……ä¸€ç‚¹ï¼Œç°åœ¨LLMæœ‰æ¸©åº¦è®¾ç½®ï¼Œä¸æ˜¯çº¯è´ªå¿ƒå–æœ€å¤§æ¦‚ç‡çš„ã€‚è€Œä¸”ç®—åŠ›æœ‰é™ï¼Œæ— æ³•åšåˆ°å…¨å±€æœ€ä¼˜ã€‚åªèƒ½è¯´ç¯‡å¹…è¶ŠçŸ­ï¼Œè¶Šå‡†ç¡®ã€‚ ç°åœ¨å¤§å®¶åšçš„CoTï¼Œä»¥åŠåˆ©ç”¨Agentic workflowä¸æ–­é‡å†™ï¼Œå…¶å®éƒ½æ˜¯åœ¨é€¼è¿‘å…¨å±€æœ€ä¼˜ã€‚

- ä¸ªäººæ„Ÿè§‰ï¼šGemini å·ç§°æœ‰ 100 ä¸‡çš„ä¸Šä¸‹æ–‡ï¼Œå…¶å®ä¸€èˆ¬è·‘åˆ° 20-30 ä¸‡å­—å·¦å³å°±ä¸è¡Œäº†

- è¾“å…¥å’Œè¾“å‡ºå¯¹æ¨¡å‹æ¥è¯´éƒ½æ˜¯ä¸€æ ·çš„ï¼Œåªè¦ä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œæ¨¡å‹æˆ–å¤šæˆ–å°‘éƒ½ä¼šå‡ºç°æŒ‡ä»¤ä¸è·Ÿéšã€‚

- ## â³ [A Tribute to MetaAI and Stability AI - 2 Giants Who Brought us so Much Joy... And, 2025 is the Year they Die... So Sad! : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1p9ah2v/a_tribute_to_metaai_and_stability_ai_2_giants_who/)
- People think the trillion dollar spend is for the r&d and scale. Research is a fraction of the opex, and the scale is not about breaking through new model features but to suck in as much actual human usage data as possible to train exclusively better models.

- ## [China just passed the US in open model downloads for the first time : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p7alka/china_just_passed_the_us_in_open_model_downloads/)
- This analysis includes all models on HuggingFace, not just LLMs. The most downloaded models (by far) are smaller stuff like embedding models, classifiers, VAD, etc., and small base models like BERT and GPT-2. The most popular properly large instruction-tuned LLM is Qwen2.5-VL-3B-Instruct in 24th place.
  - I suspect a lot of downloads are coming from poorly configured CI/CD pipelines and stuff, rather than individual users

- I'm surprised this hasn't happened earlier, the only good open weight models from the US in the past like 6 months has been Gpt-oss.

- How tf Germany is third place?
  - They have Black Forest Labs
  - stable diffusion guys (now bfl aka flux) are from germany
- Sentence Transformers (SBERT) is originally German. Gets an ungodly number of downloads, and because the models are small people just grab it directly from HF instead of running a local mirror.
  - (Also LAION (CLAP, some popular CLIP-ViT models) and Stable Diffusion/Black Forest Labs (FLUX), though I don't think those get nearly as many downloads.)

- Chinese users predominantly go to modelscope

- ## ğŸ’° [How are Chinese AI models claiming such low training costs? Did some research : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p6cf2p/how_are_chinese_ai_models_claiming_such_low/)
  - deepseek claims $6M training cost. Everyones losing their minds cause ChatGPT-4 cost $40-80M and Gemini Ultra hit $190M.
  - glm-4.6: $8-12M estimated, 357B parameters (thats model size)
  - Kimi K2-0905: $25-35M estimated, 1T parameters total (MoE architecture, only ~32B active at once)
  - MiniMax: $15-20M estimated, Mid-range model, mid-range cost
  - ğŸ§® Training cost = GPU hours Ã— GPU price + electricity + data costs.
  - çœŸå®çš„æˆæœ¬æ•°å­—éƒ½åº”è¯¥æ¥è¿‘ä¸Šé¢çš„å…¬å¼
  - deepseeks $6M feels like marketing. You cant rent enough H100s for months and only spend $6M unless youre getting massive subsidies or cutting major corners.
  - glms $8-12M is more realistic. Still cheap compared to Western models but not suspiciously fake-cheap.
  - Kimi at $25-35M shows you CAN build competitive models for less than $100M+ but probably not for $6M.

- One difference could be the accounting methodology. I can for sure guarantee that not every training attempt is successful, and companies spend a fortune of gpu-hours on practice runs training smaller models; and then there might be a rollback or two to earlier checkpoints in the big run. Then imagine one company counting the entire cost, while the other accounts only for the end run, and boom - you got drastically different reported figures while effectively the same amount of spent money.
- In the paper for Deepseek they are actually never claiming 6 million - there saying at an assumed price per GPU hour (canâ€™t remember from the top of my head) the final run would be around 6 million.
  - OpenAI also estimated GPT-OSS final training cost like this too iirc. They just didn't for other models.

- They were very transparent about this, and have stated multiple times that it was just the final training run in that estimate and explicitly did not include prior incremental runs.

- The costs listed are likely just the literal hardware cost for the final training run for the model. Every other aspect of the model training process is ignored.

- metas new ai team are getting 9 figure salaries lol makes it hard to profit when youâ€™re paying people such insane salaries.

- Because they're leveraging Western frontier models. Let's be clear, the Chinese labs aren't doing any hard training. All they're really doing is distilling the hard work done by Western labs.
  - I remember it being reported that Western frontier models trained on copyrighted data (even pirated material).

- [Are Chinese AI models really that cheap to train? Did some research. : r/LLMDevs](https://www.reddit.com/r/LLMDevs/comments/1p77x5k/are_chinese_ai_models_really_that_cheap_to_train/)
  - Itâ€™s a combo of all those things, especially lying, plus they used a ton of distillation techniques - i.e. they trained a ton on the output of ChatGPT and other established models. 

- ## åšäº§å“çº§æœç´¢ä»¥åå‘ç° 3.7 æ•ˆæœç‰¹å¥½ä½†æ˜¯ç‰¹åˆ«è´µï¼Œæ‰€ä»¥å¿…é¡»å¾—åš content cache é™ä½æˆæœ¬ã€‚
- https://x.com/arvin17x/status/1896922111505285484
  - ä½†ä¸ºäº†åš context cache ï¼Œå‘ç°å¿…é¡»å¾—è®°å½• token usageï¼Œä¸ç„¶æ„ŸçŸ¥ä¸åˆ° cache çš„æ•ˆæœã€‚

- å¯ä»¥åˆ†äº«ä¸‹ Context Cache çš„åŸç†ä¹ˆï¼Œæˆ‘è¿˜ä»¥ä¸ºè¿™ç§ä¸œè¥¿åªèƒ½åœ¨æ¨¡å‹ä¾›åº”å•†ä¾§åšã€‚
  - å°±æ˜¯åœ¨æ¨¡å‹ä¾›åº”å•†ä¾§åšã€‚åªæ˜¯ OpenAI å’Œ DeepSeek åšçš„æ˜¯é™é»˜æ–¹æ¡ˆï¼Œè€Œ Anthropic å’Œ Google æ˜¯éœ€è¦å¼€å‘è€…æ‰‹åŠ¨å¼€çš„
- åº”è¯¥æ˜¯ prompt caching å§
  - Anthropicå®¶è‡ªå·±çš„è¯´æ³•æ˜¯ prompt Caching. ä½†æˆ‘æ„Ÿè§‰è¡Œä¸šé‡Œæ„Ÿè§‰è¿˜æ˜¯ä¹ æƒ¯å« context caching çš„ã€‚
  - ä¸è¿‡ OpenAI å®˜æ–¹å’Œ Anthropic å®˜æ–¹éƒ½ç§°ä¹‹ä¸º prompt caching

- å¾ˆå¥‡æ€ªï¼Œè¿™ç©æ„ä¸ºå•¥ä¸é»˜è®¤å¼€å¯ã€‚
  - æŸäº›åœºæ™¯ä¸‹çš„ç¡®ä¸ä¸€å®šé€‚åˆï¼Œå› ä¸ºå†™å…¥ cache çš„æˆæœ¬æ˜¯åŸä»·çš„ 1.25å€

- ## It is common to generate train and validation sets using random splitting.
- https://x.com/_avichawla/status/1898622288737767785
  - However, in many situations, it can be fatal for model building.
  - Consider building a model that generates captions for images.
  - Group shuffle split solves this.

- ## ç”±äºDeepSeek-R1 çˆ†ç«ï¼Œæ‰€ä»¥ä¸ºå¤§å®¶å¸¦æ¥ä»€ä¹ˆæ˜¯LLMè’¸é¦æŠ€æœ¯çš„ç¬”è®°ã€‚
- https://x.com/karminski3/status/1882233538042597423
  - å‡ ä¸ªåŠ©è®°è¯ï¼šæ•™å¸ˆæ¨¡å‹ï¼Œå­¦ç”Ÿæ¨¡å‹ï¼Œè½¯ç›®æ ‡ï¼Œç¡¬ç›®æ ‡ã€‚

- https://x.com/ShanghaoJin/status/1882679738789216456
  - ä½ å¬è¯´è¿‡ä»€ä¹ˆå«â€œè’¸é¦â€ä¹ˆï¼Ÿè¯´ä¸ªå¤§ç™½è¯ï¼šå°±æ˜¯æ‹¿äººå®¶ç®—å‡ºæ¥çš„æ¨¡å‹å‚æ•°ï¼Œè·³è¿‡æ‰€æœ‰æ•°æ®æ¸…æ´—ã€è®­ç»ƒï¼Œåšæœ€åä¸€ç¨‹ã€‚å…¶å®æ²¡æœ‰ä»»ä½•åˆ›æ–°
  - å¥½åƒäººå®¶è¯æ˜äº†Ï€=3.14ï¼Œä»–æ‹¿ç»“æœå»ç®—äº†åœ†é¢ç§¯ã€‚è®©ä»–å†è‡ªå·±å»è¯æ˜ç®—ä¸€ä¸ªeï¼Œä»–åˆæŠ“çäº†
- åƒä¸‡ä¸è¦ç”¨â€œå¤§ç™½è¯â€æ¥è§£é‡Šè‡ªå·±éƒ½æ²¡å®Œå…¨ç†è§£çš„æ¦‚å¿µï¼Œ åªèƒ½è®©å¤–è¡Œæ‹æ‰‹ï¼Œæ‡‚çš„äººåªä¼šç¬‘è¯ä½ ã€‚ ä½ å®Œå…¨ä¸çŸ¥é“è’¸é¦æ˜¯åœ¨å¹²ä»€ä¹ˆã€‚å¦‚æœä½ çŸ¥é“çš„è¯ï¼Œé‚£å°±æ˜¯å®Œå…¨ä¸çŸ¥é“Deepseek åœ¨åšä»€ä¹ˆã€‚
- â€œè’¸é¦â€è¯´æ³•ä¸æ­£ç¡®ã€‚1. è’¸é¦æ•ˆæœä¸€èˆ¬ä¸ä¼šè¶…è¿‡åŸæ¨¡å‹ 2. deepseekçš„ reasoningè¡Œä¸ºå’Œå¸‚é¢ä¸Šå…¶ä»–æ¨¡å‹ä¸ä¸€è‡´(æœ‰è¶…è¶Šäººç±»æ ‡æ³¨çš„å¥‡å¦™è¡Œä¸º) 3. å¼€æ™ºå¯¹å†è®­ç»ƒæ¨¡å‹æœ‰ç¦æ­¢å¹¶ä¼šç›‘æ§ APIæ»¥ç”¨

- ## æˆ‘æ—¥å¸¸ç”¨ Cursor å†™ä»£ç çš„åœºæ™¯ä¹‹ä¸€ï¼šâ€œè¯·å‚è€ƒä»£ç  @ XXX1 @ XXXn åš YYY äº‹ã€‚â€
- https://x.com/dotey/status/1869436413600731146
  - ç®€å•æ¥è¯´å°±æ˜¯è®© AI ç…§è‘«èŠ¦ç”»ç“¢ï¼Œé‡è¦çš„æ˜¯ç»™å‡ºå……è¶³çš„ä¸Šä¸‹æ–‡ï¼Œè®© AI å¯ä»¥å­¦ä¹ å’Œæ¨¡ä»¿ã€‚å‰©ä¸‹çš„å°±æ˜¯ Review + Acceptï¼Œå¾ˆç®€å•é«˜æ•ˆã€‚
  - ç‰¹åˆ«è¦æ³¨æ„çš„æ˜¯ç¬¬ä¸€ä¸ªâ€œè‘«èŠ¦â€è¦æ‰“ç£¨å¥½ï¼Œè¿™æ ·åç»­çš„â€œç“¢â€æ‰ä¸ä¼šç”»æ­ªã€‚

- æ›´ç®€å•çš„åšæ³•æœ‰æ—¶å€™å¯ä»¥ç›´æ¥ @ git æŸæ¬¡æäº¤

- æˆ‘ç°åœ¨æ˜¯æ–°é¡¹ç›®é‡Œåˆ›å»ºä¸€ä¸ªtxtæ–‡ä»¶ï¼Œé‡Œé¢å†™ä¸Šæƒ³æ³•å’Œgptå¯¹æˆ‘æƒ³æ³•çš„å»ºè®®ï¼Œç„¶åè®©cursor å‚è€ƒè¿™ä¸ªæ–‡ä»¶æ¥å¼€å‘ï¼Œé‡Œé¢æˆ‘ä¹Ÿæœ‰æ—¶ä¼šå†™ä¸Šæ­¥éª¤ï¼Œé¦–å…ˆå®ç°ä»€ä¹ˆï¼Œç„¶åå®ç°ä»€ä¹ˆï¼Œåšä¸€æ®µäº†ï¼Œè®©cursoræ ¹æ®è¿™ä¸ªæ–‡ä»¶æ£€æŸ¥ä¸€ä¸‹é¡¹ç›®å®Œæˆåº¦ï¼Œåˆ—å‡ºæ¥å“ªäº›æ²¡åšï¼Œè¿™æ ·åå¤è¿­ä»£å‘å‰

- LM å……åˆ†è¯æ˜äº†äººç±»çš„æœ¬è´¨å°±æ˜¯å¤è¯»æœºã€‚ å“ªé‡Œæœ‰ä»€ä¹ˆæŒ‡ä»¤éµå¾ªï¼Œæ¨ç†ï¼Œå¤§å®¶éƒ½æ˜¯ä»ä¸åŒçš„ç»´åº¦ç”¨ä¸åŒçš„æ–¹å¼åœ¨å¤è¯»ä¸€äº›ä¸œè¥¿è€Œå·²

- ## å¦‚æœæƒ³è¦è®© LLM ç¨³å®šç”Ÿæˆ JSON å¯¹è±¡ï¼Œæœ€ç®€å•çš„æ–¹å¼å°±æ˜¯ä½¿ç”¨ zod å®šä¹‰ schema å¹¶é…åˆ @vercel ai sdkçš„ generateObjectä½¿ç”¨ï¼Œæ¯”å¦‚è¿™é‡Œæˆ‘æƒ³è¦ä»ç½‘é¡µæ–‡æœ¬å†…å®¹æå–ç»“æ„åŒ–çš„ä¿¡æ¯ã€‚
- https://x.com/FeigelC35583/status/1819558128297648412
  - è¿™ç§æ–¹å¼å’Œå½“åˆ langchain åœ¨ prompt é‡Œå†™ä¸€å¤§å †json å®šä¹‰æœ‰æœ¬è´¨åŒºåˆ«ï¼Œåœ¨äºä½¿ç”¨äº† function call çš„èƒ½åŠ›
  - ä»è¯·æ±‚ä¸­å¯ä»¥çœ‹åˆ°ï¼Œæœ¬è´¨ä¸Šæ˜¯åœ¨è°ƒç”¨æ¨¡å‹çš„æ—¶å€™ï¼Œæ„å»ºäº†ä¸€ä¸ªåä¸º json çš„ å‡½æ•°, æè¿°æ˜¯ respond with a json object, å…¶ä¸­å‚æ•°æ˜¯è‡ªå·±å®šä¹‰çš„ schemaï¼Œç„¶ååœ¨ tool_choice ä¸­é™åˆ¶å¿…é¡»è¦ä½¿ç”¨è¿™ä¸ª json å‡½æ•°ï¼Œé‚£ä¹ˆæ¨¡å‹å°±ä¼šè¿”å›è°ƒç”¨json å‡½æ•°çš„å‚æ•°ï¼Œå³ä½ å®šä¹‰çš„ schema
  - ç¤ºä¾‹ä»£ç æ¥è‡ªäºhttps://github.com/DiscovAI/DiscovAI-crawl æˆ‘æ­£åœ¨ building çš„ä¸€ä¸ªé¢å‘ RAG åº”ç”¨çš„çˆ¬è™«å¹³å°
- åº”è¯¥åªæœ‰GPTç³»åˆ—èƒ½ç”¨å§
  - æ”¯æŒfunction callå°±å¯ä»¥ï¼Œdeepseekåº”è¯¥ä¹Ÿå¯ä»¥çš„
- åœ¨è¿™åŸºç¡€ä¸Šã€‚æˆ‘ä¼šè€ƒè™‘ä½¿ç”¨jsonrepairè¿™ä¸ªåŒ…ï¼Œæ‰‹åŠ¨ä¿®å¤ä¸‹ï¼Œå¢åŠ å®¹é”™
- å¦‚æœå¤§æ¨¡å‹æ²¡æœ‰æ²¡æœ‰è¿”å›å¯¹åº”è¦æ±‚çš„å­—æ®µæ•°æ®ï¼Œæˆ–è€…è¿”å›é”™äº†ç±»å‹ï¼Œå®ƒä¼šæ€ä¹ˆæ ·ï¼Œä¼šè‡ªå·±è¡¥å……ç©ºçš„ï¼Œæˆ–è€…è‡ªåŠ¨è½¬æ¢ç±»å‹å—ï¼Ÿ
  - ä¸ä¼šè¡¥å……ï¼Œä¼šthrow errorï¼Œä¹Ÿå¯ä»¥ç”¨ä¸Šé¢æ¨å‹æ¨èçš„jsonrepairæ‰‹åŠ¨fix

- èƒ½æ”¯æŒå¼€æºæ¨¡å‹å—
  - å–å†³äºæ¨¡å‹æ”¯ä¸æ”¯æŒfunction callï¼Œæ”¯æŒçš„è¯å°±å¯ä»¥ï¼Œæ•ˆæœçš„è¯è¦çœ‹æ¨¡å‹çš„èƒ½åŠ›
- ç”¨ function call æ„Ÿè§‰æ¨¡å‹çš„èƒ½åŠ›é™äº†ä¸€ä¸ªç»´åº¦ï¼Œä¸å¦‚ç›´æ¥ç»™æ–‡æœ¬ï¼Œæˆ‘è¿˜æ˜¯æ›´å–œæ¬¢ç”¨xmlè‡ªå·±æå–ã€‚

- æˆ‘æ˜¯ç”¨ä¼ªä»£ç â•ç±»å‹å£°æ˜, ä¹Ÿæ˜¯ä¸€æ ·çš„ç¨³å®šè¾“å‡º json
- langchainæ¡†æ¶ä¸­æœ‰Pydantic json è§£æå™¨å¯ä»¥ç›´æ¥ç”¨ï¼Œæœ¬è´¨ä¹Ÿæ˜¯ç”Ÿæˆschemaï¼Œå†é…åˆé‡è¯•è§£æå™¨ä¹Ÿå¯ä»¥ç¨³å®šç”Ÿæˆjsonæ ¼å¼

- ## ğŸ’¡ LLMs are literally the most unreliable technology of all time (followed by **ing bluetooth)
- https://x.com/Steve8708/status/1819448686424084892
  - After an absurd amount of trial and error, we've internally created a set of rules for make LLMs considerably more reliable
  - our secrets: restrict the llm to only what rag provides

- what's your stance on AI for no-code? Do people prefer drag-and-drop vs prompting?
  - i think the winning move is combining both

- Bluetooth is hell and causes frustration daily.

- ## ğŸŒ° Firefox will use Transformers.js to power on-device features
- https://x.com/osanseviero/status/1797291569348751848
  - In their PDF Editor to generate alt text for images
  - Improve translations
  - Fully offline, open-source and with <200M models
- [Experimenting with local alt text generation in Firefox Nightly - Mozilla Hacks - the Web developer blog _202405](https://hacks.mozilla.org/2024/05/experimenting-with-local-alt-text-generation-in-firefox-nightly/)
  - https://huggingface.co/Mozilla

    - æä¾›äº†æ•°æ®é›†å’Œæ¨¡å‹

- Offline and open-source is a big win for privacy-focused tools

- ## [langchainåˆ°åº•è¯¥æ€ä¹ˆä½¿ç”¨ï¼Œå¤§å®¶åœ¨é¡¹ç›®ä¸­å®è·µæœ‰æˆåŠŸçš„æ¡ˆä¾‹å—? - çŸ¥ä¹](https://www.zhihu.com/question/609483833)
- LangChainä¹‹æ‰€ä»¥å¤§ç«ï¼Œæ˜¯å› ä¸ºå®ƒæä¾›äº†ä¸€ç³»åˆ—æ–¹ä¾¿çš„å·¥å…·ã€ç»„ä»¶å’Œæ¥å£ï¼Œå¤§å¤§é™ä½äº† AI åº”ç”¨å¼€å‘çš„é—¨æ§›ï¼Œä¹Ÿæå¤§ç®€åŒ–äº†å¤§æ¨¡å‹åº”ç”¨ç¨‹åºçš„å¼€å‘è¿‡ç¨‹ã€‚
  - LangChainæ¡†æ¶èƒŒåçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†è‡ªç„¶è¯­è¨€å¤„ç†åºåˆ—åˆ†è§£ä¸ºå„ä¸ªéƒ¨åˆ†ï¼Œå…è®¸å¼€å‘äººå‘˜æ ¹æ®è‡ªå·±çš„éœ€æ±‚é«˜æ•ˆåœ°å®šåˆ¶å·¥ä½œæµç¨‹ã€‚
- Langchainæœ‰6å¤§æ ¸å¿ƒæ¨¡å—ï¼š
  - Modelsï¼šæ¨¡å‹ï¼Œæ˜¯å„ç§ç±»å‹çš„æ¨¡å‹å’Œæ¨¡å‹é›†æˆã€‚
  - Promptsï¼šæç¤ºï¼ŒåŒ…æ‹¬æç¤ºç®¡ç†ã€æç¤ºä¼˜åŒ–å’Œæç¤ºåºåˆ—åŒ–ã€‚
  - Memoryï¼šè®°å¿†ï¼Œç”¨æ¥ä¿å­˜å’Œæ¨¡å‹äº¤äº’æ—¶çš„ä¸Šä¸‹æ–‡çŠ¶æ€ã€‚
  - Indexesï¼šç´¢å¼•ï¼Œç”¨æ¥ç»“æ„åŒ–æ–‡æ¡£ï¼Œä»¥ä¾¿å’Œæ¨¡å‹äº¤äº’ã€‚åŒ…æ‹¬æ–‡æ¡£åŠ è½½ç¨‹åºã€å‘é‡å­˜å‚¨å™¨ã€æ–‡æœ¬åˆ†å‰²å™¨å’Œæ£€ç´¢å™¨ç­‰ã€‚
  - Agentsï¼šä»£ç†ï¼Œå†³å®šæ¨¡å‹é‡‡å–å“ªäº›è¡ŒåŠ¨ï¼Œæ‰§è¡Œå¹¶ä¸”è§‚å¯Ÿæµç¨‹ï¼Œç›´åˆ°å®Œæˆä¸ºæ­¢ã€‚
  - Chainsï¼šé“¾ï¼Œä¸€ç³»åˆ—å¯¹å„ç§ç»„ä»¶çš„è°ƒç”¨ã€‚
- LangChain é€šå¸¸è¢«ç”¨ä½œã€Œç²˜åˆå‰‚ã€ï¼Œå°†æ„å»º LLM åº”ç”¨æ‰€éœ€çš„å„ä¸ªæ¨¡å—è¿æ¥åœ¨ä¸€èµ·ã€‚ä½¿ç”¨Langchainä¸­ä¸åŒç»„ä»¶çš„ç‰¹æ€§å’Œèƒ½åŠ›ï¼Œå¯ä»¥æ„å»ºä¸åŒåœºæ™¯ä¸‹çš„åº”ç”¨ï¼Œå¦‚èŠå¤©æœºå™¨äººã€åŸºäºæ–‡æ¡£çš„é—®ç­”ã€çŸ¥è¯†ç®¡ç†ã€ä¸ªäººåŠ©ç†ã€Agentæ™ºèƒ½ä½“ç­‰ç­‰ã€‚

- ä½ çš„è¿™ä¸ªè®¤è¯†å­˜åœ¨ä¸€äº›åå·®ï¼Œé¦–å…ˆï¼Œä¾èµ–API key æ˜¯ä¸ºäº†ä½ ä½¿ç”¨å¤§æ¨¡å‹å‚å•†çš„æœåŠ¡å’Œé‰´æƒï¼Œè¿™æ²¡æœ‰ä»€ä¹ˆæ‹‰è·¨çš„ã€‚å¾ˆå¤šç¬¬ä¸‰æ–¹çš„æœåŠ¡éƒ½éœ€è¦é‰´æƒéªŒè¯ï¼Œè¿™æ˜¯æ¯”è¾ƒä¸»æµçš„æ–¹å¼ã€‚
- å¯ä»¥ä¼ä¸šè‡ªå·±éƒ¨ç½²å¤§æ¨¡å‹ï¼Œè¿™ç§æˆæœ¬æ˜¯å¾ˆé«˜çš„ã€‚ä»æˆ‘ä»¬è‡ªå·±çš„å®éªŒæ•ˆæœæ¥çœ‹ï¼Œ13B ä»¥ä¸‹çš„å¤§æ¨¡å‹åŸºæœ¬å°±æ˜¯ç©å…·ï¼Œä¼˜åŒ–åŠå¤©è´¹æ—¶è´¹åŠ›ï¼Œè€Œ 34B æˆ–è€…æ›´å¤§çš„æ¨¡å‹ï¼Œå…¬å¸éƒ¨ç½²æˆæœ¬åˆå¾ˆé«˜ã€‚
- langchain ä¸­çš„ç‰¹è‰²æ˜¯å®ƒçš„ langchain expression language (LCELï¼‰ï¼Œæ˜¯ä¸€ç§ç±»ä¼¼ linux ç®¡é“å½¢å¼çš„è°ƒç”¨æ–¹å¼ï¼Œå¯ä»¥å¾ˆç®€å•çš„å®ç°å®ƒçš„ chain ç›¸å…³çš„åŠŸèƒ½ã€‚è¿™ä¸ªï¼Œåœ¨æˆ‘å®é™…ä½¿ç”¨çš„æ—¶å€™ï¼Œæ²¡æœ‰æƒ³è±¡çš„é‚£ä¹ˆå¥½ç”¨ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µå»å­¦ä¹ ã€‚
- æœ€åï¼Œlangchain ä¸­è¿˜æœ‰ä¸€ä¸ªå«åš langgraph çš„ç»„ä»¶ï¼Œèƒ½å¤Ÿå’Œ pytorch ä¸€æ ·ç”¨æ­ç§¯æœ¨çš„æ–¹å¼å»æ„é€ ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ã€å¾ªç¯çš„é“¾ï¼Œæ¯” LCEL æ›´é«˜çº§ã€‚

- 
- 

- ## LLMæåç¼–è¯‘ï¼Œ.not careå’ŒJvavç”¨æˆ·å†ä¹Ÿä¸ç”¨æŠ˜è…¾ä»€ä¹ˆæ··æ·†äº†ï¼Œéƒ½æ²¡ç”¨äº†
- https://twitter.com/geniusvczh/status/1774053196039962758
  - æ–‡ç« é‡Œåç¼–è¯‘çš„æ˜¯x86, x86éƒ½å¯ä»¥ï¼ŒILéš¾åº¦åªä¼šæ›´ä½
- å¤§æ¦‚çœ‹äº†ä¸€ä¸‹ï¼Œå°±æ˜¯æŠŠç¼–è¯‘å‡ºçš„æ±‡ç¼–è·Ÿæºä»£ç åšäº†ä¸€ä¸ªç®€å•çš„seq2seqçš„fine tuneï¼Œè®­ç»ƒé›†è¿æ··æ·†éƒ½æ²¡æœ‰ï¼Œç¦»è®©æ‰€æœ‰æ··æ·†éƒ½æ²¡ç”¨é‚£æ›´æ˜¯è¿˜å·®å¾—è¿œã€‚
- 17å¹´googleé‚£ç¯‡transformerçš„è®ºæ–‡å°±é è¿™æ ·å®Œæˆäº†è‡ªç„¶è¯­è¨€çš„ç¿»è¯‘ï¼Œè¿™äº›éƒ½æ˜¯è¿Ÿæ—©çš„äº‹ï¼Œåç¼–è¯‘å’Œåæ··æ·†çš„è®­ç»ƒæ•°æ®éƒ½æ˜¯å¯ä»¥æ‰¹é‡ç”Ÿæˆçš„ï¼Œåšèµ·æ¥ç®€å•å¤šäº†
  - æˆ‘è§‰å¾—LLMå¯¹äºåç¼–è¯‘å’Œåæ··æ·†ï¼Œå¯èƒ½æ›´å¤§çš„ä½œç”¨åœ¨äºç”Ÿæˆäººç±»å‹å¥½çš„å˜é‡/ç¨‹åºç»“æ„ã€‚æ¯•ç«Ÿåç¼–è¯‘å’Œåæ··æ·†æ˜¯çŒ«é¼ æ¸¸æˆï¼Œæ€»å¯ä»¥æƒ³å‡ºæ–°ç‚¹å­ï¼Œäººç±»çš„å¹²é¢„è¿˜æ˜¯å¿…ä¸å¯å°‘çš„ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼ŒåŸºäºè§„åˆ™/ç¨‹åºåˆ†æçš„ä¼ ç»Ÿæ–¹æ³•å¯èƒ½æ›´å¥½ï¼Œç„¶åå†ç”¨LLMçŒœå˜é‡å
- ä¸ºäº†æ‹‰èµ„é‡‘è€Œå·²ï¼Œé’±ç”³è¯·åˆ°äº†è®ºæ–‡å°±æ²¡å•¥ç”¨äº†â€¦â€¦å¤„ç†å±å±±ç•™ç»™å·¨å¤´çš„ç¨‹åºå‘˜å°±è¡Œäº†ï¼Œè¿˜è½®ä¸åˆ°å­¦æœ¯åœˆæ¥æŒ‡ç‚¹æ±Ÿå±±
- è¿™ç§å±€é™äºå‡½æ•°çš„åæ··æ·†å•¥ç”¨éƒ½æ²¡æœ‰ï¼Œå¯¹ä»˜ç‚¹ä¸‰è„šçŒ«åŠŸå¤«çš„æ··æ·†è¿˜å·®ä¸å¤š

- ## ğŸª§ ç ”ç©¶äº†ä¸€ä¸‹æœ¬åœ°å¤§æ¨¡å‹çš„åœºæ™¯ï¼š
- https://twitter.com/changmingY/status/1773336179296887162
  1. ä¸èƒ½è”ç½‘çš„å›½å†…ç”¨æˆ·
  2. ä¸€èˆ¬ç”¨æˆ·æœºå™¨é…ç½®è¾¾ä¸åˆ°ï¼Œæ•ˆç‡å¤ªå·®
  3. æœ¬åœ°çŸ¥è¯†åº“ç®—æ˜¯ä¸€ä¸ªåˆšæ€§éœ€æ±‚
  4. å‚ç›´é¢†åŸŸæ¨¡å‹è¶Šæ¥è¶Šå¤š, ä¸€ä¸ªhubé›†ä¸­ä½¿ç”¨
  5. å°è€Œç¾çš„æ¨¡å‹ä¼šè¶Šæ¥è¶Šå¤šï¼Œå®Œæˆä¸€ä¸ªç‰¹å®šåŠŸèƒ½

- ## ollama çš„ç¼–è¯‘ç©çš„å¤ªèŠ±äº†ï¼Œå…ˆæ˜¯å§ llama.cpp åœ¨ä¸åŒ cpu å’Œ gpu çš„åŠ¨æ€é“¾æ¥åº“éƒ½ç¼–è¯‘äº†å‡ºæ¥é¿å…ç”¨æˆ·åœ¨è¿è¡Œæ—¶å†å»ç¼–è¯‘ï¼Œ
- https://twitter.com/liumengxinfly/status/1767073319956971891
  - ç„¶åç”¨ go çš„ embed ç‰¹æ€§ç›´æ¥æŠŠè¿™äº›åŠ¨æ€åº“å…¨éƒ½æ‰“åŒ…åˆ° go çš„äºŒè¿›åˆ¶é‡Œï¼Œç„¶ååœ¨ç”¨ cgo å’Œ dlfcn åŠ è½½å’Œè°ƒç”¨ llama.cppï¼Œå®ç°äº†ä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶å…ç¼–è¯‘ï¼Œå…å®‰è£…çš„è§£å†³æ‰€æœ‰é—®é¢˜

- https://twitter.com/holegots/status/1767427148506431665
  - ä¸è¿‡è¿™ä¸ªæœ¬è´¨ä¹Ÿæ˜¯ llama.cpp å¥—å£³å§ , åº•å±‚è¿˜æ˜¯ cpp, golang å¹¶ä¸å‚ä¸å®é™…çš„æ¨ç†.

- ## æœ€æ–°ç‰ˆçš„ OpenAI Translator å·²ç»æ— ç¼æ”¯æŒæœ¬åœ°å¤§æ¨¡å‹äº†ï¼ˆOllamaï¼‰ï¼Œæ— éœ€è”ç½‘ï¼Œå¿«é€Ÿä¾¿æ·ï¼Œå®‰å…¨ç¨³å®šï¼å†ä¹Ÿä¸æ€• OpenAI è´¦å·è¢«å°äº†ï¼ç¿»è¯‘æ•ˆæœå¯¹æ¯”å¤§å®¶å¯ä»¥çœ‹ä¸€ä¸‹æˆªå›¾ï¼Œå¤§å®¶å¿«æ¥ä¸‹è½½ä½“éªŒä¸€ä¸‹å§ï¼ _202402
- https://twitter.com/yetone/status/1761607398819840511
- ç°åœ¨å¥½åƒè¿˜ä¸æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹ï¼Ÿåªæœ‰æœ‰é™çš„å‡ ä¸ªæ¨¡å‹å¯ä¾›é€‰æ‹©ï¼Œæœ€å¥½æ˜¯æœ‰ä¸€ä¸ªæ–‡æœ¬æ¡†å¯ä»¥è‡ªå®šä¹‰è¾“å…¥
- è¿™æ˜¯Mistralå¤šå¤§çš„æ¨¡å‹ï¼Œ7Bçš„å—ï¼Ÿ
  - æ˜¯çš„

- ä¸çŸ¥é“è¿™äº›7b 13bçš„å°æ¨¡å‹å“ªä¸ªç¿»è¯‘è´¨é‡æ›´é«˜

- ## é˜¿é‡Œäº‘ç«Ÿç„¶æ”¯æŒè¿™ä¹ˆå¤šæ¨¡å‹äº†
- https://twitter.com/yihong0618/status/1746745371441967540
- http://ai.azureä¹ŸåŒ…å«äº†å¥½å¤šæ¨¡å‹ï¼Œæ˜¨å¤©æƒŠåˆ°äº†

- ## è¶Šæ¥è¶Šè§‰å¾— RAG è¿™ä¸œè¥¿æœ‰æ„æ€ã€‚
- https://twitter.com/wwwgoubuli/status/1737471851654160548
  - åŠå¹´å‰æ¥è§¦åˆ°è¿™ä¸ªè¯çš„æ—¶å€™å¼€å§‹æˆ‘è¿˜æœ‰äº›ä¸å±‘ï¼Œæœç´¢å†…å®¹æ’å…¥åˆ°æç¤ºè¯ç®—ä»€ä¹ˆå˜›ï¼Œå°å­¦äºŒå¹´çº§éƒ½èƒ½æ˜ç™½ã€‚å°¤å…¶æ˜¯çœ‹åˆ°éšä¾¿ä¸¢å‘é‡åº“éƒ½èƒ½è·‘å‡ºä¸ªä¸ƒä¸ƒå…«å…«ï¼Œè¶Šå‘è§‰å¾—è¿™ä¸ªç®€å•ã€‚
  - ä½†ç°åœ¨çœŸçš„æäº†åŠå¹´ï¼Œæˆ‘è¶Šå‘çš„è§‰å¾—è¿™æ‰æ˜¯ä¸‹ä¸€ä¸ªå¤§å¤šæ•°äººå¯ä»¥å‚ä¸çš„é£å£ã€‚å®ƒæœ‰é—¨æ§›ã€‚
- æŠ€å·§å¾ˆå¤š æ‰€ä»¥å¥½ç© ä½†é£é™©æ˜¯å¤§éƒ¨ä»½æŠ€å·§éƒ½è¢«æ¨¡å‹æä¾›å•†ç©è¿‡ï¼Œ80%éœ€æ±‚éƒ½å¯èƒ½è¢«ä»–ä»¬ç›´æ¥è¦†ç›–
  - RAGä¸å°±æ˜¯query transformation/rewrite/expanding, hybrid search, reranking, etcå—ï¼Ÿå½“ç„¶è¿˜æœ‰äº›å…¶ä»–æŠ€å·§å•¥IAGä¹‹ç±»çš„ã€‚æ•°æ®ingestionä¹Ÿæœ‰äº›æŠ€å·§ï¼Œä¸è¿‡æˆ‘çœ‹ä¸»è¦è¿˜æ˜¯åœ¨queryä¸Šã€‚ è¿™äº›å¤§éƒ¨åˆ†OAI, Baichuan, æœˆä¹‹æš—é¢å†…éƒ¨éƒ½æ¢ç´¢è¿‡äº†å§
- RAGä¸€çœ‹å°±æ˜¯ä¸€ä¸ªæœ‰é—®é¢˜çš„åŒºåŸŸï¼Œå¤§æ¨¡å‹éšæ—¶ä¸‹ä¸€æ¬¡å‡çº§å¯èƒ½å°±ä¼šæ”¹å˜æ•´ä¸ªæ¡†æ¶ï¼Œ3.5è¿˜èƒ¡è¯´å…«é“ï¼Œ4å·²ç»å¾ˆå¤šéƒ½æ˜¯æœ‰æ ¹æœ‰æ®çš„äº†
- æåˆ°æœ€åï¼Œè¿˜æ˜¯æ¸…æ´—æ•°æ®ï¼ŒRAGåªç”¨ç®€å•ç­–ç•¥è§£å†³å¤§å¤šæ•°é—®é¢˜ï¼Œå¯è§‚æµ‹ã€‚å‰ææ˜¯æ‰€æœ‰å¤æ‚ç­–ç•¥éƒ½è¦è¯•è¿‡æ‰çŸ¥é“ã€‚

- ## LangChainå¼€æºäº†AnythingLLMï¼šå¯ä»¥ä¸ä»»ä½•å†…å®¹èŠå¤©çš„ç§äºº ChatGPTï¼Œåº”è¯¥å°±æ˜¯ä»–ä»¬è‡ªå·±æ–‡æ¡£ç³»ç»Ÿç”¨çš„é‚£ä¸€å¥—ã€‚
- https://twitter.com/op7418/status/1733893368974073873
  - An efficient, customizable, and open-source enterprise-ready document chatbot solution.
  - https://github.com/Mintplex-Labs/anything-llm /MIT/js/python

- æœ‰æ²¡æœ‰è¯¦ç»†è¯´æ˜ï¼Ÿæœ€å¤§å¯ä»¥æ”¯æ’‘å¤šå¤§çš„æ–‡æ¡£ï¼Ÿ
  - åº”è¯¥æ˜¯ä¸é™å¤§å°çš„ï¼Œæ‹†å¼€å°±å¥½äº†
- è¯´æ²¡è¯´ç¡¬ä»¶éœ€æ±‚ï¼Ÿ

- ## å¤§æ¨¡å‹çš„è¿™äº› benchmark åº”è¯¥æ˜¯å…¨å®‡å®™æœ€æ²¡ç”¨çš„ benchmark äº†å§ï¼Ÿ
- https://twitter.com/yihong0618/status/1721401347533324688
- ä¹Ÿä¸æ˜¯å…¨æ²¡ç”¨ï¼Œä¹Ÿæœ‰ä¸€äº›æœ‰ç”¨çš„, å°¤å…¶ç»†åˆ†ä»»åŠ¡ä¸Šçš„ï¼Œè¿˜æ˜¯æŒºæœ‰ç”¨çš„ã€‚å½“å‰ç›¸æ¯”å…¶ä»–benchmarkï¼Œå¯æ“ä½œç©ºé—´ç¡®å®å¤§
- å…¬å¼€çš„åªèƒ½å…¨çœ‹è‡ªè§‰

- ## ä¸­æ–‡å¼€æºæ¨¡å‹è™½å¤šï¼Œæ•°æ®é›†å´å¾ˆå°‘å¼€æºã€‚
- https://twitter.com/9hills/status/1718828132046942218
  - ç›®å‰è‹±æ–‡ 7B è§„æ¨¡çš„ SOTA æ¨¡å‹æ˜¯ zephyr-7b-betaã€‚å®ƒæ”¾å¼ƒäº†è´¨é‡å‚å·®ä¸é½çš„å¼€æºæ•°æ®é›†ï¼Œä½¿ç”¨ChatGPTå’ŒGPT-4 å…¨æ–°æ ‡æ³¨äº† UltraChat å’Œ UltraFeedback æ•°æ®é›†ï¼ˆå·²å¼€æºï¼‰ã€‚æ˜¯ llama-index é¡¹ç›®å®æµ‹å‡ºæ¥å”¯ä¸€èƒ½å¤Ÿæ”¯æŒ Agent çš„å°å‚æ•°æ¨¡å‹ã€‚
- ä¸­æ–‡æ•°æ®é›†éƒ½æ˜¯æ‹¿æ¥å–é’±çš„
