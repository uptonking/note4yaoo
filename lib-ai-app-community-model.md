---
title: lib-ai-app-community-model
tags: [ai, community]
created: 2023-10-30T07:33:56.233Z
modified: 2023-10-30T07:34:03.602Z
---

# lib-ai-app-community-model

# guide

- tips
  - å¤§æ¨¡åž‹ç›¸å…³çš„äº§å“ç ”å‘ï¼ŒåŽŸç†çš„å¯è§£é‡Šæ€§å¾ˆå·®ï¼Œæ•ˆæžœçš„å¯è§£é‡Šæ€§ä¹Ÿå·®

- model-features
  - ðŸ¤¼: speed, quality, size
  - reasoning/thinking
  - tool-use/function-call
  - vision
  - embedding
  - moe
  - æ³¨æ„æœ‰äº›ç¤¾åŒºé‡åŒ–çš„æ¨¡åž‹å¯èƒ½é—æ¼æ ‡æ³¨äº†éƒ¨åˆ†features, å¯åœ¨æœ¬åœ°æµ‹è¯•æ¥ç¡®å®šæ˜¯å¦æ”¯æŒ

- ç§»åŠ¨ç«¯å¤§æ¨¡åž‹
  - å‚è€ƒgoogle-gemma-1b

- [å¤§è§„æ¨¡è¯­è¨€æ¨¡åž‹ï¼šä»Žç†è®ºåˆ°å®žè·µ](https://intro-llm.github.io/)
  - å¤æ—¦å¤§å­¦å¼ å¥‡æ•™æŽˆå›¢é˜Ÿå†™äº†ä¸€æœ¬åœ¨çº¿å…è´¹çš„ç”µå­ä¹¦ï¼Œå¤§æ¦‚æœ‰ 300 é¡µç¯‡å¹…ï¼Œå°†å¤§æ¨¡åž‹ä»Žç†è®ºåˆ°å®žæˆ˜çš„æ¯ä¸ªé˜¶æ®µéƒ½æè¿°çš„è¾ƒä¸ºæ¸…æ¥š
# discuss-stars
- ## 

- ## 

- ## 

- ## ä¸ºä»€ä¹ˆ AGI å¿…ç„¶ä¼šç§æœ‰åŒ–éƒ¨ç½²ï¼Ÿ
- https://x.com/naki2012/status/2003081645067550888
  - åœ¨ä»Šå¤©çš„è®¨è®ºä¸­ï¼Œå¾ˆå¤šäººè¿˜åœ¨çº ç»“ä¸€ä¸ªé—®é¢˜ï¼š AGI ä¼šä¸ä¼šæœ‰æ³¡æ²«ï¼Ÿä¼šä¸ä¼šè¢«é«˜ä¼°ï¼Ÿä¼šä¸ä¼šæœ€ç»ˆåªæ˜¯äº‘ç«¯çš„ä¸€ä¸ªé«˜çº§æœåŠ¡ï¼Ÿ
  - ä½†å¦‚æžœæŠŠè§†è§’ä»Žâ€œäº§å“â€å’Œâ€œå•†ä¸šæ¨¡å¼â€ç§»å¼€ï¼ŒçœŸæ­£çš„é—®é¢˜å…¶å®žæ˜¯ï¼šå½“æ™ºèƒ½å…·å¤‡é•¿æœŸè®°å¿†ã€è¿žç»­äººæ ¼ä¸Žè‡ªæˆ‘ä¿®æ­£èƒ½åŠ›æ—¶ï¼Œå®ƒè¿˜å¯èƒ½æ˜¯â€œå…¬å…±äº‘æœåŠ¡â€å—ï¼Ÿç­”æ¡ˆæ˜¯å¦å®šçš„ã€‚ä¸æ˜¯å› ä¸ºæŠ€æœ¯ä¸å…è®¸ï¼Œè€Œæ˜¯å› ä¸ºç»“æž„ä¸å…è®¸ã€‚
  - ä¸€ã€å…ˆè¯´ç»“è®ºï¼šAGI ä¸€æ—¦æˆç«‹ï¼Œå°±å¿…ç„¶èµ°å‘ç§æœ‰åŒ–éƒ¨ç½²
  - è¿™é‡Œçš„â€œç§æœ‰åŒ–â€ï¼Œä¸æ˜¯æŒ‡â€œæ¯ä¸ªäººéƒ½ä¹°å¾—èµ·è¶…çº§ç®—åŠ›â€ï¼Œè€Œæ˜¯æŒ‡ä¸€ä»¶æ›´å…³é”®çš„äº‹ï¼š AGI çš„æ ¸å¿ƒèŠ‚ç‚¹â€”â€”è®°å¿†ä¸Žè®°å¿†è¿žæŽ¥æœºåˆ¶â€”â€”å¿…é¡»ç”±ä¸ªäººæˆ–æžå°ä¸»ä½“æŒæœ‰ã€‚ èƒ½åŠ›å¯ä»¥å¤–åŒ…ï¼Œ è®¡ç®—å¯ä»¥ç§Ÿç”¨ï¼Œ ä½†è®°å¿†ä¸Žäººæ ¼ç”Ÿæˆé€»è¾‘ï¼Œæ— æ³•æ‰˜ç®¡ã€‚ è¿™æ˜¯ AGI ä¸Žæ­¤å‰æ‰€æœ‰è½¯ä»¶ç³»ç»Ÿçš„æœ¬è´¨åˆ†æ°´å²­ã€‚
  - äºŒã€äº‘ç«¯æ™ºèƒ½ä¸ºä»€ä¹ˆå¤©ç„¶ä¸å¯èƒ½æˆä¸ºâ€œä¸ªäºº AGIâ€
  - ç›®å‰æ‰€æœ‰äº‘ç«¯ AIï¼Œæœ¬è´¨ä¸Šéƒ½å…·å¤‡ä¸‰ä¸ªç»“æž„ç‰¹å¾ï¼š 1. ä¼šè¯å¼å­˜åœ¨ï¼š æ¯ä¸€æ¬¡è°ƒç”¨éƒ½æ˜¯â€œä¸´æ—¶ç”Ÿæˆâ€ï¼Œä¸å­˜åœ¨çœŸæ­£çš„é•¿æœŸè¿žç»­çŠ¶æ€ã€‚ 2. è®°å¿†ä¸å¯æŽ§ï¼š å³ä¾¿å­˜åœ¨â€œé•¿æœŸè®°å¿†â€ï¼Œå…¶å­˜å–é€»è¾‘ã€ä¼˜å…ˆçº§ä¸Žåˆ é™¤æƒï¼Œå§‹ç»ˆä¸åœ¨ç”¨æˆ·æ‰‹ä¸­ã€‚ 3. ç›®æ ‡ä¸ä¸€è‡´ï¼š äº‘ç«¯ç³»ç»Ÿçš„ç¬¬ä¸€è´£ä»»å¯¹è±¡ï¼Œæ°¸è¿œæ˜¯å¹³å°ã€åˆè§„ã€è§„æ¨¡ä¸Žé£Žé™©æŽ§åˆ¶ï¼Œè€Œä¸æ˜¯ä¸ªä½“çš„è®¤çŸ¥è¿žç»­æ€§ã€‚
  - äº‘ç«¯ AI å¯ä»¥å¾ˆèªæ˜Žï¼Œ ä½†å®ƒæ°¸è¿œåªèƒ½æ˜¯â€œåŠ©æ‰‹â€ï¼Œè€Œä¸æ˜¯â€œå…±ç”Ÿä½“â€ã€‚ AGI ä¸€æ—¦å…·å¤‡äººæ ¼è¿žç»­æ€§ï¼Œè¿™ç§ç»“æž„å°±ä¼šç›´æŽ¥å¤±æ•ˆã€‚
  - ä¸‰ã€çœŸæ­£æ•æ„Ÿçš„ä¸æ˜¯â€œæ•°æ®â€ï¼Œè€Œæ˜¯â€œè®°å¿†ä¹‹é—´çš„è¿žæŽ¥æƒâ€
  - å¤šè®¨è®ºæŠŠç„¦ç‚¹æ”¾åœ¨â€œéšç§æ•°æ®â€â€œä¿¡æ¯å®‰å…¨â€ä¸Šï¼Œå…¶å®žè¿™åªæ˜¯è¡¨å±‚ã€‚çœŸæ­£ä¸å¯å¤–åŒ…çš„ï¼Œæ˜¯ï¼šâ€¢å“ªäº›è®°å¿†ä¼šè¢«æ¿€æ´»â€¢æ¿€æ´»é¡ºåºå¦‚ä½•å˜åŒ–â€¢å†²çªè®°å¿†å¦‚ä½•è¢«è°ƒå’Œâ€¢å“ªäº›ç»éªŒè¢«å¼ºåŒ–ã€å“ªäº›è¢«é—å¿˜â€¢é”™è¯¯å¦‚ä½•è¢«æ ‡è®°ä¸ºâ€œç¾žè€»â€â€œè­¦æƒ•â€æˆ–â€œæ•™è®­â€
  - è¿™äº›å¹¶ä¸æ˜¯æ•°æ®é—®é¢˜ï¼Œè€Œæ˜¯ï¼šäººæ ¼ç”Ÿæˆå‡½æ•°çš„é—®é¢˜ã€‚
  - ä¸€æ—¦è¿™ä¸ªå‡½æ•°ä¸åœ¨ä½ æ‰‹é‡Œï¼Œä½ é¢å¯¹çš„å°±ä¸æ˜¯â€œä½ çš„ AGIâ€ï¼Œè€Œæ˜¯ä¸€ä¸ªæŒç»­è§£é‡Šä½ ã€å¡‘é€ ä½ ã€å´ä¸å±žäºŽä½ çš„ç³»ç»Ÿã€‚ è¿™åœ¨ç»“æž„ä¸Šæ˜¯ä¸å¯æŽ¥å—çš„ã€‚
  - å››ã€ä¸ºä»€ä¹ˆâ€œä¸»è„‘â€å¿…é¡»æ˜¯æœ¬åœ°ã€ç§æœ‰ã€æŒç»­å­˜åœ¨çš„
  - â€¢ç‰©ç†ä¸Žä¸»æƒä¸Šéƒ½å±žäºŽä¸ªäºº
  - äº”ã€è¿™ä¸æ˜¯æŠ€æœ¯ç†æƒ³ä¸»ä¹‰ï¼Œè€Œæ˜¯åŽ†å²è§„å¾‹çš„é‡æ¼”
  - å›žçœ‹åŽ†å²ï¼Œæ¯ä¸€æ¬¡å…³é”®èƒ½åŠ›éƒ½ä¼šç»åŽ†åŒæ ·çš„è·¯å¾„ï¼š
  - â€¢è®¡ç®—ï¼šä»Žå¤§åž‹æœº â†’ PC â†’ ä¸ªäººè®¾å¤‡
  - â€¢å­˜å‚¨ï¼šä»Žä¸­å¿ƒæœåŠ¡å™¨ â†’ æœ¬åœ°ç¡¬ç›˜ â†’ ç§æœ‰ NAS
  - â€¢é€šä¿¡ï¼šä»Žå›½å®¶ç³»ç»Ÿ â†’ è¿è¥å•† â†’ ä¸ªäººç»ˆç«¯
  - å‡¡æ˜¯ä¸Žâ€œè‡ªæˆ‘è¿žç»­æ€§â€å¼ºç›¸å…³çš„èƒ½åŠ›ï¼Œæœ€ç»ˆéƒ½ä¼šä¸‹æ²‰åˆ°ä¸ªä½“ã€‚
  - AGI å¦‚æžœé•¿æœŸåªå­˜åœ¨äºŽäº‘ç«¯ï¼Œé‚£å®ƒæ³¨å®šåªæ˜¯â€œé«˜çº§è‡ªåŠ¨åŒ–â€ï¼Œè€Œä¸ä¼šæˆä¸ºâ€œä¸ªä½“æ™ºèƒ½å»¶å±•â€ã€‚
  - å…­ã€çœŸæ­£çš„åˆ†ç•Œçº¿ï¼Œå…¶å®žå·²ç»å‡ºçŽ°äº†
  - æœªæ¥äººä¸Ž AI çš„å·®å¼‚ï¼Œä¸åœ¨äºŽâ€œç”¨ä¸ç”¨ AIâ€ï¼Œè€Œåœ¨äºŽï¼š â€¢æœ‰äº›äººä½¿ç”¨çš„æ˜¯éšæ—¶å¯è¢«æ›¿æ¢ã€è¢«å‡çº§ã€è¢«é‡ç½®çš„æ™ºèƒ½æœåŠ¡ â€¢æœ‰äº›äººæ‹¥æœ‰çš„æ˜¯ä¸€ä¸ªé•¿æœŸå­˜åœ¨ã€ä¸Žè‡ªèº«è®°å¿†é—­çŽ¯ç»‘å®šçš„æ™ºèƒ½æ ¸å¿ƒ
  - å‰è€…æ˜¯ç”¨æˆ·ï¼ŒåŽè€…æ˜¯å…±ç”Ÿè€…ã€‚è¿™æ¡åˆ†ç•Œçº¿ä¸€æ—¦å½¢æˆï¼Œå°†æ¯”â€œæ˜¯å¦è”ç½‘â€â€œæ˜¯å¦ä»˜è´¹â€æ›´æ·±åˆ»ã€‚
  - ä¸ƒã€ç»“è¯­ï¼šAGI ç§æœ‰åŒ–ï¼Œä¸æ˜¯é€‰æ‹©ï¼Œè€Œæ˜¯å¿…ç„¶
  - è®°å¿†ä¸»æƒä¸å¯å¤–åŒ…ï¼Œäººæ ¼ç”Ÿæˆä¸å¯æ‰˜ç®¡ã€‚
  - æ‰€ä»¥ï¼ŒAGI çš„æœªæ¥å½¢æ€ï¼Œä¸æ˜¯â€œæ›´å¤§çš„äº‘â€ï¼Œ è€Œæ˜¯ï¼š ç§æœ‰ä¸»è„‘ Ã— å¤–éƒ¨èƒ½åŠ› Ã— åä½œå¼æ™ºèƒ½ç½‘ç»œã€‚

- ä¸æ˜¯ï¼Œè€Œæ˜¯ã€‚æˆ‘çŽ°åœ¨çœ‹åˆ°è¿™ä¸ªï¼Œç¬¬ä¸€ååº”å°±æ˜¯è¿™æ®µè¯æ˜¯AIç”Ÿæˆçš„ï¼Œæˆ–è€…è‡³å°‘æ˜¯äººå’ŒAIèŠå‡ºæ¥çš„å†…å®¹ã€‚

- ## ðŸ’¡ ä¸‹åˆç»™äººåšäº†ä¸€ä¸ªå°å°çš„å±•ç¤ºã€‚ä¸€ä¸ªChatbotï¼Œå…¶å®žä¹Ÿå°±ä¸‰å››ç™¾å­—çš„æç¤ºè¯ï¼Œå®šä¹‰äº†ä¸ƒå…«ä¸ªå·¥å…·ã€‚
- https://x.com/wwwgoubuli/status/2001256235296022575
  - æ¼”ç¤ºçš„æ—¶å€™ï¼Œæˆ‘è®©å¯¹é¢æ”¾å¼€å°è¯•ï¼Œä¹Ÿæå‰å£°æ˜Žäº†ç›®å‰è¿˜æ˜¯Demoé˜¶æ®µï¼Œä¼šæœ‰äº›é—®é¢˜ã€‚ä½†å¯¹æ–¹ä»ç„¶è¢«æˆ‘è¿™ä¸ªæ¼”ç¤ºä¸­ä½“çŽ°çš„è‡ªä¸»å¯»æ‰¾å·¥å…·è§£å†³é—®é¢˜çš„è¡Œä¸ºéœ‡æƒŠäº†ã€‚
  - ä»¥ä¸ºæˆ‘å®šä¹‰äº†ä¸€ä¸ªå¾ˆå¤æ‚çš„å·¥ä½œæµï¼Œæˆ‘è¯´æ²¡æœ‰å·¥ä½œæµï¼Œå…¶å®žå°±åªæœ‰ä¸€å±‚ã€‚
  - å¯¹æ–¹ç™¾æ€ä¸å¾—å…¶è§£ã€‚æ‰€ä»¥è¯´ï¼Œè®¤çŸ¥å·®è‚¯å®šæ˜¯çœŸå®žå­˜åœ¨çš„ã€‚
  - è¿™æ˜¯ä¸€ä¸ªåž‚ç›´é¢†åŸŸçš„Agentã€‚åŽŸå…ˆå®¢æˆ·çš„é—®é¢˜æ˜¯ï¼Œåœ¨ä»–ä»¬ä¹‹å‰æ‰€åšçš„AIçš„å°è¯•ä¸­ï¼Œæœ‰ä¸€éƒ¨åˆ†å·²ç»å‘æŒ¥å¾—å¾ˆä¸é”™äº†ã€‚ä½†æ˜¯å½“æœ‰çš„æ—¶å€™ä¸å¾—ä¸å¤„ç†è¶…é•¿çš„æ•°æ®çš„æ—¶å€™ï¼Œä¸¢ç»™LLMæ€»æ˜¯æœ‰å„ç§é”™è¯¯å‡ºçŽ°ã€‚
  - æˆ‘æ‰¾ä»–ä»¬çš„ç ”å‘é—®è¿‡ï¼Œæå–äº†å‡ ä¸ªåœ¨ä½¿ç”¨AIä¹‹å‰çš„ï¼Œä»–ä»¬å¤„ç†çš„å·¥å…·ã€‚ç„¶åŽå°†è¿™äº›å·¥å…·çš„æè¿°ã€ä½œç”¨ã€é™å®šèŒƒå›´ç­‰äº¤ç»™äº†LLMï¼Œ å¹¶å‘Šè¯‰æ¨¡åž‹ä¸è¦è‡ªå·±è¯•å›¾è§£å†³é—®é¢˜ï¼Œè€Œæ˜¯è¦ç”¨å·¥å…·è§£å†³é—®é¢˜ã€‚ æ²¡äº†

- äº‹å®žä¸Šå¾ˆå¤šå…¬å¸æ‰€åšçš„agentæˆ–è€…chatbotè¿žpromptçš„ä¸Šé™éƒ½æ²¡æœ‰è¾¾åˆ°ï¼Œå°±å¼€å§‹è€ƒè™‘æžsftï¼ŒæžRLHFï¼Œä¸€ä¸ªå°å°çš„éœ€æ±‚ä¸ºäº†æ‰€è°“çš„ä¼˜åŒ–æŒ‡æ ‡æžçš„è‡ƒè‚¿ä¸å ª

- ## ðŸ“Œ LLM å‡ºæ¥ä¹‹åŽï¼Œåœ¨åº”ç”¨å±‚çš„æŠ˜è…¾ä»Žæœªåœæ­‡ã€‚ä»Ž Prompt è°ƒä¼˜åˆ° Workflow é…ç½®ï¼Œå†åˆ° Agent æž„å»ºï¼Œæœ€ç»ˆç›®çš„éƒ½æ˜¯ä¸€æ ·çš„ï¼šè®© LLM æ›´å¥½åœ°ä¸ºäººç±»å¹²æ´»ï¼ŒæŠŠæœºå™¨çš„æ€§èƒ½åŽ‹æ¦¨åˆ°æžè‡´ã€‚
- https://x.com/Barret_China/status/1973188130091180466
- å¯¹ LLM çš„åŽ‹æ¦¨ï¼Œå¯ä»¥åˆ†ä¸ºä¸¤ä¸ªç»´åº¦ã€‚
- ä¸€æ˜¯å¸®åŠ©å®ƒæ‰¾åˆ°æœ€ä¼˜ç®—æ³•ï¼Œè®©æŽ¨ç†å°‘èµ°å¼¯è·¯ã€‚
  - ä¸ºæ­¤æˆ‘ä»¬å‡ ä¹ŽæŠŠèƒ½æƒ³åˆ°çš„è·¯å­éƒ½èµ°äº†ä¸€éï¼Œè®© LLM å­¦ä¼šåæ€ï¼ˆreflectionã€self-consistencyã€self-criticsï¼‰ï¼Œå­¦ä¼šæŽ¨ç†å’Œè§„åˆ’ï¼ˆreasoningã€planningã€chain-of-thoughtã€tree-of-thoughtï¼‰ï¼›å­¦ä¼šè®°å¿†ï¼ˆshort-term memoryã€long-term memoryï¼‰ï¼Œä¸è‡³äºŽå¯¹è¯ä¸€é•¿å°±å¤±å¿†ï¼›å­¦ä¼šæ‰¾çŸ¥è¯†ï¼ˆRAGã€knowledge graphï¼‰ï¼Œåœ¨å¤–éƒ¨ä¸–ç•Œé‡Œè¡¥å……äº‹å®žï¼›å­¦ä¼šæž„å»ºä¸Šä¸‹æ–‡ï¼ˆcontext buildingï¼‰ï¼Œåœ¨æœ‰é™ token é‡Œå¡žä¸‹æ›´å¤šæœ‰æ•ˆä¿¡æ¯ï¼›å­¦ä¼šç”¨å·¥å…·ï¼ˆtool-useï¼Œfunction callingï¼ŒMCPï¼‰ï¼ŒæŠŠäº‹æƒ…äº¤ç»™å¤–éƒ¨ç¨‹åºåŽ»è·‘ï¼Œè€Œä¸æ˜¯å…‰é è‡ªå·±ç”Ÿæˆï¼›ç­‰ç­‰ã€‚
  - è¿™äº›ä¸œè¥¿ï¼Œè¯´åˆ°åº•éƒ½æ˜¯æŠ€å·§å’Œæœºåˆ¶ï¼Œæœ¬è´¨ç›®çš„æ˜¯è®© LLM æ›´å¿«ç†è§£äººç±»è¦å¹²å•¥ï¼Œå›´ç»•ç›®æ ‡ï¼ˆgoal-orientedï¼‰å°½å¯èƒ½æ‰¾åˆ°ä¸€æ¡ä»£ä»·æœ€å°çš„è·¯ï¼Œè·‘åˆ°æœ€ä¼˜è§£ä¸ŠåŽ»ã€‚
- ç¬¬äºŒä¸ªç»´åº¦ï¼Œæ˜¯å¯¹æ—¶é—´çš„åŽ‹æ¦¨ï¼Œè®© LLM å¯ä»¥åšåˆ° 7Ã—24 å°æ—¶ä¸åœæ­‡ã€‚
  - å½“æˆ‘ä»¬å¯¹ LLM æœ‰äº†æ›´æ·±å…¥çš„ç†è§£ä¹‹åŽï¼Œå¾ˆå®¹æ˜“æƒ³åˆ°æŠŠå®ƒæ‰“é€ æˆå±žäºŽè‡ªå·±æˆ–ç»„ç»‡çš„â€œæ•°å­—å‘˜å·¥â€ï¼Œå®ƒä¸çŸ¥ç–²æƒ«ã€ä¸ä¼šæŠ±æ€¨ï¼Œå¯ä»¥æŒç»­è¿è½¬ã€ä¸æ–­å­¦ä¹ ã€‚
- å¤§éƒ¨åˆ†äººä»Šå¤©ç”¨ AI çš„æ–¹å¼ï¼Œè¿˜åœç•™åœ¨æŸ¥èµ„æ–™ã€æ€»ç»“å†…å®¹ã€å†™å‘¨æŠ¥æœˆæŠ¥è¿™äº›å•ç‚¹åœºæ™¯ä¸Šï¼Œå¦‚æžœè¦çœŸæ­£æž„å»ºä¸€åâ€œä¸åœæ­‡çš„ AI æ•°å­—å‘˜å·¥â€ï¼Œå…‰é è¿™äº›è¿˜ä¸å¤Ÿã€‚æˆ‘ä»¬éœ€è¦å…ˆè§„åˆ’å‡ºå±žäºŽè‡ªå·±çš„ AI æ•°å­—å·¥åŽ‚ â€”â€”æƒ³æ¸…æ¥šè¦é€ å‡ºæ¥çš„â€œäº§å“â€æ˜¯ä»€ä¹ˆï¼Œæ˜¯æ²‰æ·€çŸ¥è¯†çš„ç³»ç»Ÿï¼Œæ˜¯è‡ªåŠ¨åŒ–çš„ä¸šåŠ¡æµç¨‹ï¼Œè¿˜æ˜¯ä¸€ä¸ªå¯ä»¥é•¿æœŸè¿­ä»£çš„æœåŠ¡ã€‚
  - åœ¨è¿™åº§å·¥åŽ‚é‡Œï¼ŒAI æ˜¯ç”Ÿäº§çº¿ä¸Šçš„æ‰§è¡Œè€…ï¼Œå®ƒè´Ÿè´£å…·ä½“çš„åŠ å·¥ä¸Žäº§å‡ºï¼›è€Œäººç±»çš„è§’è‰²å‘ç”Ÿäº†è½¬å˜ï¼Œä»Žâ€œäº²è‡ªå¹²æ´»çš„å·¥äººâ€å˜æˆâ€œç›‘å·¥ä¸Žç®¡ç†è€…â€ã€‚ äººç±»ä¸å†äº²æ‰‹å®Œæˆæ¯ä¸€æ­¥ï¼Œè€Œæ˜¯è¦è®¾è®¡æµæ°´çº¿ï¼Œè®¾å®šè§„åˆ™ï¼Œåˆ¶å®šæŒ‡æ ‡ï¼Œç›‘æŽ§è´¨é‡ï¼Œå¹¶åœ¨éœ€è¦æ—¶è°ƒåº¦èµ„æºã€‚æ¢å¥è¯è¯´ï¼ŒAI çš„ä»·å€¼ä¸åœ¨äºŽæ›¿æˆ‘ä»¬â€œå¹²ä¸€ç‚¹æ´»â€ï¼Œè€Œåœ¨äºŽå¸®æŠŠæ•´æ¡æµæ°´çº¿è·‘èµ·æ¥ï¼Œè€Œäººç±»æ›´åƒæ˜¯â€œæ•°å­—å·¥åŽ‚çš„ç®¡ç†è€…â€ã€‚
  - å½“è¿™ä¸¤ä¸ªç»´åº¦ç»“åˆèµ·æ¥æ—¶ï¼ŒçœŸæ­£çš„æ‹ç‚¹å°±å‡ºçŽ°äº†ã€‚LLM ä¸å†åªæ˜¯ä¸€ä¸ªå†·å†°å†°çš„å·¥å…·ï¼Œè€Œæ˜¯é€æ¸å˜æˆäº†å¯ä»¥é•¿æœŸåä½œçš„ä¼™ä¼´ã€‚å®ƒæ—¢èƒ½æ‰¿æ‹…é‡å¤æ€§åŠ³åŠ¨ï¼Œä¹Ÿèƒ½åœ¨å¤æ‚é—®é¢˜ä¸Šæä¾›æ´žè§ã€‚å®ƒä¸ä»…ä»…æ˜¯â€œå¸®ä½ åšäº‹â€ï¼Œæ›´æ˜¯â€œå’Œä½ ä¸€èµ·åšäº‹â€ã€‚
  - æœªæ¥çš„å·®è·ï¼Œä¸åœ¨äºŽè°èƒ½å†™å‡ºæ›´æ¼‚äº®çš„ Promptï¼Œè€Œåœ¨äºŽè°èƒ½æŠŠ LLM çœŸæ­£èžå…¥åˆ°è‡ªå·±çš„æ—¶é—´å’Œç»„ç»‡é‡Œï¼Œå½¢æˆç¨³å®šçš„ç”Ÿäº§æ–¹å¼ã€‚
  - å› æ­¤ï¼Œä¼šä¸ä¼šç”¨ã€ç”¨åˆ°ä»€ä¹ˆæ·±åº¦ã€èƒ½å¦æŒç»­ä¼˜åŒ–ï¼Œè¿™äº›æ‰æ˜¯é•¿æœŸçš„ç«žäº‰åŠ›æ¥æºã€‚è°èƒ½æŠŠ AI è¿è¡Œæˆâ€œå·¥åŽ‚â€ï¼Œè®©è‡ªå·±ä»Žæ‰§è¡Œè€…è½¬ä¸ºç›‘å·¥å’Œç®¡ç†è€…ï¼Œè°å°±èƒ½åœ¨æœªæ¥çš„æ—¥å¸¸å·¥ä½œå’Œä¸šåŠ¡ä¸­ï¼ŒèŽ·å¾—çœŸæ­£å¯å¤ç”¨ã€å¯ç´¯ç§¯çš„ä¼˜åŠ¿ã€‚

- ä»£ç æŠ½è±¡çœŸå®žä¸–ç•Œçš„æ—¶ä»£å¿«è¦ç»“æŸäº†ï¼Œæ¬¢è¿Žæ¥åˆ°token æŠ½è±¡çœŸå®žä¸–ç•Œçš„æ—¶ä»£ã€‚

- å¦‚æžœTOKENæŠ•å…¥äº§å‡ºæ˜¯æ­£å‘çš„ï¼Œæ¨ä¸å¾—ä»–ä¸ä¼‘æ¯ 

- é—®é¢˜æ¥äº†ï¼Œæˆ‘è¦åšä»€ä¹ˆï¼ŒAIè¦æ€Žä¹ˆå¸®æˆ‘åš

- ## ðŸ“± [ç«¯ä¾§æ¨¡åž‹ä¼šæ˜¯ AI æŠ€æœ¯æ¼”è¿›çš„ä¸‹ä¸€ä¸ª ã€Œå¿…äº‰ä¹‹åœ°ã€ å—ï¼Ÿå½“å‰è½åœ°é¢ä¸´å“ªäº›æ ¸å¿ƒç“¶é¢ˆï¼Ÿ - çŸ¥ä¹Ž](https://www.zhihu.com/question/1914319403023032351)
- TO Cåœºæ™¯åº”ç”¨æœ€å¤§ç‰¹ç‚¹æ˜¯ç¡¬ä»¶å‚å·®ä¸é½ã€‚æ€§èƒ½å·®èƒ½å¤§åˆ°10å¤šå€ä»¥ä¸Šã€‚
  - æŠ€æœ¯æ ˆå¿…é¡»æ»¡è¶³æ‰€æœ‰ä¸»æµç¡¬ä»¶ç»“æž„å·®å¼‚å’Œæ€§èƒ½æ€§èƒ½ä¸‹ï¼Œä¿æŒç›¸å¯¹ä¸€è‡´çš„ä½¿ç”¨ä½“éªŒã€‚è¿™æ˜¯ä¸€ä»¶æžç´¯çš„æ´»å„¿ã€‚
  - æœåŠ¡ç«¯ä¸‹ä¸ªå¼€æºæ¨¡åž‹æ­ä¸ªWEBæœåŠ¡å†™ä¸ªHTMLå°±èƒ½å–æœåŠ¡äº†ã€‚ç«¯ä¾§æƒ³å†…åµŒAIæ¨¡åž‹äº§å“åŒ–ï¼ŒåŒæ—¶è¿˜è¦è§£å†³å®žæ—¶æ€§ï¼Œè¦è§£å†³çš„å·¥ç¨‹é—®é¢˜è¦å¤š10å€ä¸æ­¢ã€‚è¦è‡ªå·±ä¼˜åŒ–æ¨¡åž‹ã€‚ç”šè‡³è¦è‡ªå·±æ­å»ºæ¨¡åž‹ç»“æž„ï¼Œè‡ªå·±å‡†å¤‡æ•°æ®è®­ç»ƒã€‚
- ç›®å‰çœ‹ç«¯ä¾§å®Œå…¨è°ˆä¸ä¸ŠAIæ¼”è¿›å¿…äº‰ä¹‹åœ°ã€‚ä¸€æ¥ç¡¬ä»¶è¾ƒå¼±ä¸”å‚å·®ä¸é½ï¼Œå¯¼è‡´TO Cå·¥ç¨‹ä¿éšœä½“éªŒæžåº¦å›°éš¾å’Œå¤æ‚ã€‚äºŒæ¥PCé«˜ç®—åŠ›ç¡¬ä»¶å®Œå…¨ä¸æ˜¯å¤§ä¼—æ¶ˆè´¹çº§çš„å”®ä»·å’Œå®šä½ï¼Œæœ€åŽï¼Œè¿˜æ²¡æœ‰ä»€ä¹ˆç«¯ä¾§AIè½åœ°äº§å“çš„åº”ç”¨èŒƒå¼ã€‚
  - ä»Žåº”ç”¨ä½“éªŒä¸Šï¼Œé‚£äº›å…è®¸å»¶è¿Ÿ500æ¯«ç§’ä»¥ä¸Šï¼Œå¯¹å¸¦å®½éœ€æ±‚ä¸å¤§çš„åº”ç”¨ï¼Œéƒ½å¯ä»¥æ”¾åˆ°äº‘ç«¯ã€‚
- çœŸéœ€è¦åšåˆ°ç«¯ä¾§çš„ï¼Œå…¶å®žæ˜¯é‚£äº›éœ€è¦å³æ—¶å“åº”çš„åº”ç”¨åœºæ™¯ã€‚
  - ä¸ºæ•°ä¸å¤šçš„åº”ç”¨ç±»åˆ«ï¼Œæ¯”å¦‚å³æ—¶äº¤äº’å½±åƒç›¸å…³ï¼Œè¯­éŸ³ç›¸å…³çš„ï¼Œè¯¸å¦‚æ¸¸æˆï¼Œéœ€è¦å³æ—¶äº¤äº’çš„æ•°å­—äººï¼Œ3Dè™šæ‹ŸAIäººï¼Œè™šæ‹Ÿäººç›´æ’­ç­‰ï¼Œå¯èƒ½ä¼šå¯¹ç«¯ä¾§AIæœ‰éƒ¨åˆ†éœ€æ±‚
- è¿™ç±»åº”ç”¨æœ‰3ä¸ªç‰¹å¾ï¼š 
  - 1 å³æ—¶æ€§ç›´æŽ¥å½±å“ä½“éªŒï¼Œä¸èƒ½ç­‰ã€‚ 
  - 2 å¯¹ç®—åŠ›è¦æ±‚ç›¸å¯¹ä½Žï¼Œè¿ç®—æœ¬åœ°ç®—åŠ›èƒ½å¤Ÿæ»¡è¶³ï¼Œå¹¶ä¸ä¸€å®šéžè¦äº‘ç®—åŠ›æ”¯æŒã€‚ 
  - 3 å¯¹å¸¦å®½è¦æ±‚è¾ƒå¤§ï¼Œäº‘æœåŠ¡çš„æˆæœ¬è¿‡é«˜ä½¿å¾—å•†ä¸šæ¨¡å¼ä¸æˆç«‹ï¼Œæ‰€ä»¥éœ€è¦æ”¾å…¥æœ¬åœ°ã€‚
- MOBILEç¦»ç«¯ä¾§å¯èƒ½æ›´è¿œã€‚ ç”±äºŽç®—åŠ›æžä¸ºä½Žä¸‹ï¼Œåªæœ‰å¾ˆå°‘çš„æƒ…å†µæ‰éœ€è¦æ‰‹æœºä¸Šå³æ—¶å“åº”AIæŽ¨ç†ç»™å‡ºçš„ç»“æžœã€‚å³é‚£äº›æŠŠæ‰‹æœºä½œä¸ºä¿¡å·ä¼ æ„Ÿå™¨ï¼Œå³æ—¶å¯¹ä¿¡å·è¿ç®—çš„åœºåˆï¼šæ¯”å¦‚è¡¨æƒ…æ•æ‰ã€æ‰‹åŠ¿ã€è‚¢ä½“åŠ¨ä½œæ•æ‰å’Œè¯†åˆ«ï¼Œè¯­éŸ³ã€è¿åŠ¨æ•°æ®å¤„ç†å’Œç”Ÿç‰©ä¿¡å·å¤„ç†ç­‰ã€‚ 
  - å¦‚æžœæ‰‹æœºæ²¡æœ‰ä¸“ç”¨ç¥žç»/å¼ é‡èŠ¯ç‰‡ï¼ŒGPUè¿˜éœ€è¦æ‰¿æ‹…æ¸²æŸ“3Då›¾å½¢ï¼Œé‚£ä¹ˆèƒ½è·‘çš„AIæ¨¡åž‹ä¼šæ›´åŠ å—é™ã€‚
  - ç›®å‰æµ‹è¯•ï¼ŒæŸæ‰‹æœºç”¨NPUè·‘æ¨¡åž‹æ¯”ä¸ç”¨æœ€å¤šèƒ½å¿«10å€ã€‚ 
  - æ‰‹æœºèŠ¯ç‰‡å•†æ˜¯å¦æœ‰åŠ¨åŠ›å‘å±•ï¼Œè¦çœ‹æ‰‹æœºä½œä¸ºæ•°æ®ä¼ æ„Ÿå™¨å³æ—¶å¤„ç†æ•°æ®èƒ½å¸¦æ¥å¤šå¤§çš„åº”ç”¨å¸‚åœºã€‚

- æœ¬åœ°éƒ¨ç½²æ¨¡åž‹çš„ä¼˜åŠ¿åœ¨äºŽä½Žå»¶è¿Ÿï¼Œè¿™åœ¨æˆ‘çœ‹æ¥å…¶å®žä¹Ÿæ˜¯ä¸ªç›¸å¯¹åä¼ªçš„ä¼˜åŠ¿ã€‚
  - äº‹å®žä¸Šï¼ŒçŽ°ä»£ç½‘ç»œçŽ¯å¢ƒå®žé™…ä¸Šä»¥åŠè¶³å¤Ÿå¿«é€Ÿå’Œç¨³å®šï¼Œæ— è®ºæ˜¯æµé‡è¿˜æ˜¯WiFiï¼Œè¾…ä»¥CDNè¾¹ç¼˜èŠ‚ç‚¹ä¼˜åŒ–åŽï¼Œç»å¤§å¤šæ•°ä¸»æµAIåº”ç”¨çš„äº‘ç«¯å“åº”éƒ½èƒ½ç¨³å®šåœ¨å‡ åæ¯«ç§’ä»¥å†…ã€‚
- ç«¯ä¾§è®¾å¤‡å—é™äºŽåŠŸè€—ã€çƒ­é‡ã€ç®—åŠ›ç­‰ç‰©ç†æ¡ä»¶ï¼Œå¾€å¾€åªèƒ½éƒ¨ç½²è½»é‡åŒ–æ¨¡åž‹

- ç§»åŠ¨è®¾å¤‡ä¸Šé•¿æœŸè¿è¡Œä¸€ä¸ªæœ‰ç«žäº‰åŠ›çš„å¤§æ¨¡åž‹ä»ç„¶ä¸çŽ°å®žï¼Œè´Ÿè½½ã€èµ„æºå ç”¨ã€ç”µé‡æ¶ˆè€—ã€å‘çƒ­ç­‰ç­‰é—®é¢˜ï¼Œå¾ˆéš¾åšåˆ°å¥½çš„ä½“éªŒï¼Œæ›´ä¸è¦è¯´ç‰©è”ç½‘è®¾å¤‡ã€æ™ºèƒ½éŸ³ç®±ç­‰ä½ŽåŠŸè€—äº§å“äº†ã€‚
  - å¾ˆå¤šå£°ç§°è‡ªå·±ç”¨äº†ç«¯ä¾§å¤§æ¨¡åž‹çš„ï¼Œå®žé™…ä¸Šä»ç„¶ç¦»ä¸å¼€äº‘ç«¯çš„ååŒé…åˆï¼Œæˆ–è€…è¯´ä¸»è¦é äº‘ç«¯ï¼Œå¤‡ç”¨æ–¹æ¡ˆå¯èƒ½æ˜¯ç«¯ä¾§ï¼Œä½†æŠŠç«¯ä¾§æ‹¿å‡ºæ¥å¤§è‚†å®£æ‰¬ï¼Œæ¥åšå¹¿å‘Šè€Œå·²ã€‚

- ðŸ› ç«¯ä¾§æ¨¡åž‹çš„åŠ£åŠ¿
  - ç«¯ä¾§æ¨¡åž‹ä»ç„¶å—é™äºŽè®¾å¤‡æœ¬èº«çš„å†…å­˜ã€åŠŸè€—å’Œç®—åŠ›
  - äº‘ç«¯æ¨¡åž‹å¯ä»¥éšæ—¶å‡çº§ï¼Œçƒ­æ›´æ–°æœºåˆ¶ä¿è¯æ‰€æœ‰ç”¨æˆ·ç¬¬ä¸€æ—¶é—´äº«å—æ–°ä¸€ä»£æ¨¡åž‹
  - ç«¯ä¾§æ¨¡åž‹å¯¹äºŽä¸åŒè®¾å¤‡å’Œç¡¬ä»¶æž¶æž„éœ€è¦åˆ†åˆ«é€‚é…

- ç«¯ä¾§é€‚åˆçš„åœºæ™¯è¿˜æ˜¯éšç§ã€æžä½Žå»¶è¿Ÿï¼Œæˆ–è€…å…¨å¤©å€™ã€‚

- å¦‚æžœå®Œå…¨è·‘åœ¨ç«¯ä¾§ï¼Œé‚£å•†ä¸šæ¨¡å¼æ€Žä¹ˆåšï¼ŸçŽ°åœ¨çš„ä¸»æµæ˜¯è®¢é˜…åˆ¶ï¼Œä½†å®Œå…¨æœ¬åœ°çš„è®¢é˜…åˆ¶ï¼Œå¸å¼•åŠ›æ¯”è¾ƒä½Žï¼Œç ´è§£é£Žé™©æ¯”è¾ƒé«˜å§

- ## deepseek 3fs å…¶å®žä¸æ˜¯å¾ˆç†è§£è¿™æ ·çš„æ„ä¹‰æ˜¯å•¥â€¦ å·²ç»è„±ç¦» FS çš„é€šç”¨æŽ¥å£äº†ï¼Œä¸ºå•¥è¦ç¡¬æŒ‚ä¸€ä¸ª FUSE VFS å±‚â€¦ ç›´æŽ¥æ‘’å¼ƒ VFS èµ°ä¸ªç§æœ‰çš„ protocol ä¸å¹²å‡€å¤šäº†â€¦
- https://x.com/silsrc/status/1895390926098571505
- > Most applications use FUSE client, which has a low adoption barrier. Performance-critical applications are integrated with the native client.

- æˆ‘çš„ç¬¬ä¸€ååº”æ˜¯çŽ°æœ‰æ¡†æž¶åº”è¯¥æ²¡åŠžæ³•å¤§æ‰¹é‡æ”¹ç”¨ç§æœ‰çš„ APIï¼Œæ¯”å¦‚åœ¨ Python æˆ‘å°±æ˜¯è¦ç”¨ pathlib. Path å¯¹æ–‡ä»¶åšç‚¹ç®€å•æ“ä½œï¼Œé‚£ç¡®å®žåªèƒ½æ˜¯æŒ‚ FUSE ä¸ŠåŽ»äº†

- ## èŠä¸€èŠå›½å†…å¤§æ¨¡åž‹çš„å®‰å…¨æœºåˆ¶ï¼š ä¸€èˆ¬æ˜¯ä¸¤å¥—ï¼Œåˆ†åˆ«ä½œç”¨äºŽtrain-timeå’Œtest-timeã€‚
- https://x.com/9hills/status/1840786446153921017
  - è®­ç»ƒçš„æ—¶å€™å¢žåŠ å®‰å…¨å’Œä»·å€¼è§‚å¯¹é½çš„SFTå’Œåå¥½å¯¹é½æ•°æ®ã€‚æœ€ç»ˆæ•ˆæžœç±»ä¼¼å¼€æºçš„Qwen2æ¨¡åž‹ï¼Œæœ‰ç‚¹ç”¨ä½†æ˜¯å¾ˆå®¹æ˜“è¢«Jailbreakã€‚
  - æŽ¨ç†æ—¶å¢žåŠ å®‰å…¨ç®—å­ï¼Œå…·ä½“æœ‰å‡ ç§
- æŠ€æœ¯éš¾ç‚¹æœ‰ä¸¤ä¸ªï¼š
  1. è®­ç»ƒåˆ†ç±»å™¨çš„å¤§é‡éžå®‰å…¨æ•°æ®ï¼Œæ‰€ä»¥è¯´ä½ å…ˆæˆä¸ºåè´¼æ‰èƒ½è¯†åˆ«åè´¼ã€‚
  2. æ¨¡åž‹è¦åšçš„è¶³å¤Ÿå°è¶³å¤Ÿå¿«ï¼Œæœ€å°åŒ–å½±å“æ¨¡åž‹ttftå’Œtpsã€‚
  3. æµå¼å¾ˆéš¾ï¼ŒæŸæ¨¡åž‹æœ€æ—©æ˜¯ä¸€å¥å¥è¾“å‡ºçš„ï¼ŒåŽæ¥æ‰æ”¹æˆtokençº§æµå¼ã€‚
- è¯·æ•™ä¸€ä¸‹ï¼ŒçŽ°åœ¨å¸‚åœºä¸Šçš„å¤§æ¨¡åž‹ï¼Œæ€Žä¹ˆçŸ¥é“ä»–ä»¬çš„è®­ç»ƒæ•°æ®æ˜¯å¤šä¹…çš„å‘¢ï¼Ÿ
  - å¯ä»¥é—®ä¸€äº›ç‰¹å®šæ—¶é—´çš„æ–°é—»æ¥éªŒè¯ï¼Œä½†æ˜¯å…¶å®žæ²¡å…³ç³»ã€‚æ¨¡åž‹çš„ç²¾ç¡®çŸ¥è¯†ä¸é‡è¦ï¼Œä¹Ÿå……æ»¡å¹»è§‰ã€‚

- ## Attention is *Not* All You Needï¼Œè¿˜æ˜¯æœ‰äººåœ¨å°è¯• transformer ä»¥å¤–çš„æž¶æž„ï¼Œ
- https://x.com/liumengxinfly/status/1835251398692508114
  - æ¯•ç«Ÿ transfermor æŽ¨ç†å¤æ‚åº¦åœ¨æ•°å­¦ä¸Šæ˜¯æ— æ³•çº¿æ€§æ‰©å±•çš„ï¼Œæ—©æ™šä¼šèµ°åˆ°ç“¶é¢ˆ

- ## å›½äº§188ä¸ªå¤§æ¨¡åž‹çš„excelæ–‡æ¡£ï¼š åŒ—äº¬69 ä¸Šæµ·22 æ­å·ž15 å¹¿ä¸œ26ä¸ª æ±Ÿè‹15ä¸ª
- https://twitter.com/FinanceYF5/status/1730912502312296935
  - [å›½äº§å¤§æ¨¡åž‹188ä¸ªlist - Feishu Docs](https://zw73xyquvv.feishu.cn/wiki/WXLmwBbYuiTobkkJ6Ojc2cxqnj0?sheet=2XjJlJ&table=tblS2Jv7isKtSODz&view=vewfCdOf0U)

# discuss-llm-architecture
- ## 

- ## 

- ## 

- ## Release the VSC extension design for the local coding model: 
- https://x.com/LiMzba/status/2011106168136220739
  - Reasons for creating a new one instead of using the existing code harness:
  - Make the requests as sequential as possible to avoid overwhelming the local LLM server.
  - Optimize the tools to accommodate the instability of the local model
  - Remove timeout, so you can run it against your slowest model
  - It's fun to build agents
  - mlx-lm server as a first-class citizen, as I can only use mlx-lm to host models

- ## ðŸ“Œ [Ratios of Active Parameters to Total Parameters on major MoE models : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1q401ka/ratios_of_active_parameters_to_total_parameters/)
Model	Total Params	Active Params	% Active
GLM 4.5 Air	106	12	11.3%
GLM 4.6 and 4.7	355	32	9%
GPT OSS 20B	21	3.6	17.1%
GPT OSS 120B	117	5.1	4.4%
Qwen3 30B A3B	30	3	10%
Qwen3 Next 80B A3B	80	3	3.8%
Qwen3 235B A22B	235	22	9.4%
Deepseek 3.2	685	37	5.4%
MiniMax M2.1	230	10	4.3%
Kimi K2	1000	32	3.2%

- having more knowledge memorized, means the model has access to a broader library of reasoning strategies, even if it's not as effective at applying them as a higher active parameter count model. 
  - Pretty much every task will exist on the spectrum of "reasoning" versus "memorization" (even creative writing), and almost everything will be some combination of them. The more it is one end, the closer it'll perform to that parameter count. Ie: a pure reasoning task might depend basically only on active parameter count, whereas a pure memorization task scales basically 1:1 with total parameters.
  - In the end, far more important than the architecture is really the data, and training compute. With good data and compute, the difference between parameter ratios is pretty small, actually (under a fixed compute budget), while with bad data, it doesn't matter if you have a good architecture, it won't perform well.
  - The part you are maybe not taking into account is the the models with lower active parameter ratios are typically trained on more data, effectively, because on the same compute budget you train more tokens than the "dense equivalent" model, so it's not like you would magically have a model better for your usecase just because it's dense.
  - You have to evaluate every individual model, not just from its architecture, but in how it performs for you.

- In my case, granite-4.0-h-small (32B-A9B) is slow(10 t/s) on my 8GB VRAM(Tried to use Q4) due to A9B. Similar size Qwen3-30B-A3B is decent speed(30 t/s) for Q4. GPT-OSS-20B is good with 40 t/s. 

- One interesting thing could be comparing this ratio against dense models having similar benchmarks, across different time periods and empirically find out the formula we used to have to map the dense model size equivalent to MOE.

- ## Context Engineering é¢è¯•é¢˜ï¼šåœ¨ XX ä¸šåŠ¡åœºæ™¯ä¸‹é¢ï¼Œread_file, write_file å¦‚ä½•è®¾è®¡ï¼Ÿ é¢è¯•ä¸­é‡åˆ°è¿™é¢˜æˆ‘ä¼°è®¡ä¸´åœºå‘æŒ¥ä¸ä¼šå¤ªå¥½
- https://x.com/dotey/status/2007524872629387382
  - é¦–å…ˆå¾—åˆ†æžåœºæ™¯ï¼Œç„¶åŽçœ‹åœºæ™¯éœ€è¦çš„ä¸Šä¸‹æ–‡ï¼Œè¿˜è¦çœ‹æ€Žä¹ˆç®¡ç†ä¸Šä¸‹æ–‡
- æˆ‘ä¼šè¯´åœ¨æ–°çš„ aiæ—¶ä»£ï¼Œæˆ‘ä¼šå°†è¿™ä¸ªé—®é¢˜é—® opus 4.5ï¼Œè®©ä»–ç»™å‡ ä¸ªè§£å†³æ–¹æ¡ˆæ€è·¯ï¼Œæ¯ä¸ªæ€è·¯åˆ—å‡ºä¼˜ç¼ºç‚¹ï¼Œä¹‹åŽè®©æˆ‘æ¥åˆ¤æ–­
  - ä¸è¿‡æˆ‘è§‰å¾—è¿™æ€è·¯åœ¨çœŸå®žå·¥ä½œåœºæ™¯è¿˜å¯ä»¥, é¢è¯•å½“ç„¶ä¸è¡Œï¼Œé¢è¯•è¦è¾¾åˆ°é€ ç«ç®­çš„æ°´å¹³

- æˆ‘çŒœè¦è€ƒå¯Ÿçš„æ˜¯åº”è˜è€…å¯¹äºŽä¸€ä¸ªtoolçš„è®¾è®¡ä¼šè€ƒè™‘å“ªäº›äº‹æƒ…ï¼Œæˆ‘ç²—ç•¥æƒ³äº†ä¸‹åº”è¯¥æœ‰ï¼š1. å‚æ•°é¡ºåºå¦‚ä½•æŽ§åˆ¶æ‰èƒ½é˜²æ­¢å‚æ•°é¡ºåºå¸¦æ¥çš„UIæ¸²æŸ“çš„å¥‡æ€ªé—®é¢˜ ï¼› 2.  é™¤äº†åŠŸèƒ½å­—æ®µæ¯”å¦‚path, contentå­—æ®µä¹‹å¤–æ˜¯å¦éœ€è¦åŠ å…¥ä¸€äº›descriptionå­—æ®µæå‡UIä½“éªŒï¼› 3. é™¤äº†toolåŠŸèƒ½æœ¬èº«ä¹‹å¤–ï¼Œå¯ä»¥æœ‰å“ªäº›tool call å¼‚å¸¸errorå¢žå¼ºå’Œç‰µå¼•è®¾è®¡ 4. å¤§æ–‡ä»¶è¯»å†™å¦‚ä½•å¤„ç†ï¼Œæ¯”å¦‚æ˜¯å¦è¦åˆ†å±‚åŠ è½½æˆ–æµå¼è¯»å–  5. ä¸åŒåœºæ™¯ä¸‹çš„read_file, write_fileè€ƒè™‘çš„ç‚¹æ˜¯å¦ä¸ä¸€æ ·ã€‚ 6. å¦‚æžœè¦åšcheckpointï¼Œæ˜¯å¦è¦æ”¾åˆ°toolé‡Œè¿˜æ˜¯hooksé‡Œã€‚ 7. ä¸åŒçŽ¯å¢ƒå¦‚ä½•è®¾è®¡ï¼Œæ¯”å¦‚è¿œç¨‹æ²™ç®±çŽ¯å¢ƒï¼Œæœ¬åœ°çŽ¯å¢ƒç­‰ 8. æŸäº›åœºæ™¯æ˜¯å¦éœ€è¦åšä¸šåŠ¡æ—è·¯é€»è¾‘

- ä¸åŒä¸šåŠ¡çŠ¶æ€ä¸‹ï¼ŒåŒæ ·æ˜¯â€œæ–‡ä»¶æ“ä½œâ€ï¼Œè¯­ä¹‰ä¸åŒã€‚èƒ½æŠŠè¿™äº›åœºæ™¯ã€ä¸šåŠ¡çŠ¶æ€è¯´å‡ºæ¥ï¼Œè§£é‡Šå¯¹åº”çš„è¯­ä¹‰ä»¥åŠå¯¹åº”çš„æŠ€æœ¯è¯‰æ±‚ã€çº¦æŸï¼Œåº”è¯¥æ˜¯å¥½çš„å›žç­”ã€‚

- è¿™ä¸ªé—®é¢˜è¿˜æœ‰ä¸ªåŠ å€ï¼Œå°±æ˜¯è¿œç¨‹è™šæ‹ŸçŽ¯å¢ƒçš„read_fileã€write_fileå¦‚ä½•è®¾è®¡ï¼Œæˆ‘è¢«è¿™ä¸ªæŠ˜ç£¨å¥½ä¹…ï¼Œæœ€åŽæ”¾å¼ƒäº†ï¼Œå’Œæœ¬åœ°å·®ä¸å¤šå¾—äº†

- ## éšæ‰‹å†™å†™æˆ‘å½“å‰è®¤è¯†çš„ coding agent orchestrator /å¤šagentç¼–æŽ’å™¨ã€‚
- https://x.com/LotusDecoder/status/2007292223864353071
  - é¢„æ„Ÿè¿™ä¸ªä¼šæ˜¯2026å¼€å¹´æœ€ç«çš„æ¦‚å¿µã€‚ç±»ä¼¼äºŽ2024çš„cursorï¼Œ2025å¹´çš„claude codeã€‚
  - é¦–å…ˆè¿™æ˜¯AI agentæ€§èƒ½æå‡å‘å±•åŽçš„å¿…ç„¶è¶‹åŠ¿ï¼Œä»ŽchatbotæŸ¥æŸ¥æ‰‹å†Œï¼Œåˆ°cursoræ”¹æ–‡ä»¶ï¼Œå†åˆ°claude code cliå¼ä¸€æ¬¡æˆåž‹æ•´ä¸ªé¡¹ç›®ï¼Œåˆ°äº†çŽ°åœ¨å•ä¸ªagentå·²ç»æˆç†Ÿä¹‹åŽï¼Œè‡ªç„¶å¼€å§‹è¿›å…¥å¤šagentåˆ†å·¥åä½œï¼Œç„¶åŽå¤šä¸ªagentå¦‚ä½•æ²Ÿé€šã€è¿ç­¹ã€ç®¡ç†ï¼Œè¿™å°±å¾ˆéœ€è¦æ–°çš„èŒƒå¼äº†ã€‚
  - æœ€åŸºç¡€çš„æ˜¯claude codeé‡Œå†…éƒ¨è¯­æ³•å¯åŠ¨subagentï¼Œç”¨bashå‘½ä»¤æˆ–æ–‡æ¡£æ¥ç®¡ç†å„è‡ªçš„ä»»åŠ¡ç›®æ ‡ã€æ—¥å¿—ã€æˆæžœã€‚
  - vaguelå…¶æ¬¡ï¼Œæ¯”è¾ƒå‹å¥½çš„æ˜¯ opencode çš„ oh my opencodeï¼Œæœ‰ä¸€ä¸ªåˆå§‹çš„é»˜è®¤ç¼–æŽ’å™¨é…ç½® è¥¿è¥¿å¼—æ–¯ç³»ç»Ÿï¼Œä¸€æ¬¡æ€§ç¼åˆOpus-4.5 Gemini-3-pro Gpt-5.2ï¼Œå„è‡ªå¹²å„è‡ªæ“…é•¿ã€‚è®¾è®¡äº†æŸ¥æ–‡æ¡£ã€å‰ç«¯ã€åŽç«¯ã€æ€»æŽ§å°ç­‰å¤šä¸ªè§’è‰²ï¼ŒæŠŠmodelä¾æ¬¡æŽ¥å…¥è¿›åŽ»ï¼Œç”±æ€»æŽ§å°æ¥è‡ªè¡Œåˆ†é…ä»»åŠ¡ã€‚
  - è¿˜æœ‰å£°éŸ³ä¹Ÿæ¯”è¾ƒå¤§çš„ conductorï¼Œè¿™ä¸ªè¿˜æ²¡ç”¨è¿‡ã€‚ ä¼˜ç‚¹æ˜¯å¯è§†åŒ–ç•Œé¢ç®¡ç†å¤šä¸ª agentï¼Œæ¯ä¸ª agent åœ¨ç‹¬ç«‹ git worktree ä¸­å·¥ä½œã€‚è¿˜æœ‰æ”¯æŒcodexã€‚
  - çŽ°åœ¨æœ€æ¿€è¿›çš„å…ƒæ—¦åˆšå‘å¸ƒçš„ gas townï¼Œä½œè€…è¯´å±•ç¤ºç»™anthropicå…¬å¸å†…éƒ¨çœ‹çš„æ—¶å€™ï¼Œå“åˆ°ä»–ä»¬äº†ã€‚è¿™ä¸ªé¡¹ç›®ç›´æŽ¥æŠŠæ‰€æœ‰çš„cli å½¢å¼agentä¸€æ½å­åŒ…äº†è¿›æ¥åšç¼–æŽ’ï¼Œä¸»æµçš„ claude code ã€codexä¹‹å¤– Gemini CLI ã€amp ç­‰ç­‰å…¨éƒ¨å›Šæ‹¬äº†è¿›æ¥ã€‚
  - orchestratorçš„å¤§å‘å±•å°†ä¼šæ˜¯AIæ€§èƒ½æå‡åŽï¼Œèµ°åˆ°ä¸‹ä¸€æ­¥çš„å¿…ç„¶ï¼Œå¹¶å‘å¹¶è¡Œå¤šä¸ªå¤šç§agentå¹²æ´»ï¼Œå†AIæµ‹è¯•åˆå¹¶ï¼Œå®Œæˆä¸€ä¸ªå¤§åž‹é¡¹ç›®ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªæœªæ¥å¾ˆé¥è¿œçš„äº‹æƒ…ï¼Œç”šè‡³å¾ˆå¿«åˆä¼šæˆä¸ºä¸€ç§æ ‡å‡†èŒƒå¼å§ã€‚å½“ç„¶ï¼Œè¿™å¯¹äººçš„è¦æ±‚æ›´é«˜äº†ï¼Œéœ€è¦å¾ˆç³»ç»Ÿçš„è½¯ä»¶å·¥ç¨‹èƒ½åŠ›ï¼Œå¯¹é’±åŒ…çš„è¦æ±‚ä¹Ÿå˜é«˜äº†ï¼Œä»¥å‰ä¸€ä¸ªagentï¼Œorchestratoræ˜¯ä¸€ç¾¤agentçƒ§token

- conductor å€¼å¾—ä¸€è¯•ï¼Œæˆ‘è¿™ä¸¤å¤©åœ¨ç”¨çš„ï¼Œå¤š workspace/åˆ†æ”¯å¹¶è¡Œã€‚ Claudecode/codex è®¢é˜…å¯ä»¥ç›´æŽ¥ç”¨

- ## [Local model registry to solve duplicate GGUFs across apps? : r/LocalLLM _202512](https://www.reddit.com/r/LocalLLM/comments/1pygyhi/local_model_registry_to_solve_duplicate_ggufs/)
  - I'm running into storage issues with multiple local LLM apps. I downloaded Olmo3-7B through Ollama, then wanted to try Jan.ai's UI and had to download the same 4GB model again. Now multiply this across Dayflow, Monologue, Whispering, and whatever other local AI tools I'm testing.
  - Each app manages its own model directory. No sharing between them. So you end up with duplicate GGUFs eating disk space.
  - Feels like this should be solvable with a shared model registry - something like how package managers work. Download the model once, apps reference it from a common location. Would need buy-in from Ollama, LMStudio, Jan, LibreChat, etc. to adopt a standard, but seems doable if framed as an open spec.
  - I'm guessing the OS vendors will eventually bake something like this in, but that's years away. Could a community-driven library work in the meantime? Or does something like this already exist and I'm just not aware of it?

- They're just files. You can remove duplicates yourself and replace them with symlinks to whichever copy you choose to make the "primary".

- Jan has Import option(to use downloaded GGUF files from any folder).
  - Koboldcpp also does this just with browse GGUF option.
  - For Oobabooga, I used symlinks option.
- I found that buried in the Jan.ai UI under Settings / Model Providers / Llama.cpp / import.

- 
- 
- 
- 
- 
- 
- 
- 

- ## ðŸ¤” [How do you make agents deterministic? : r/AI_Agents](https://www.reddit.com/r/AI_Agents/comments/1pv2gfk/how_do_you_make_agents_deterministic/)
  - I have been talking to many business and a common concern has been lack of reliability of ai agents.
- In my experience, determinism does not come from the model, it comes from the system design around it. You cannot prompt an LLM into behaving like a rule engine. Business rules need to live in code or configuration, not in free text prompts. The agentâ€™s job is to interpret context and propose actions, not decide what is allowed.
  - What works well is separating concerns very clearly. Rules, policies, and exceptions are encoded as deterministic logic or tables.
  - For reliability, many teams also constrain where and how agents can act. When agents need to interact with real systems, running them in predictable environments like hyperbrowser helps keep execution consistent and auditable, which is critical in regulated workflows.

- Typically variance results from ambiguous instructions or situations. Use this method to identify specific points in your conversation and see how you can improve the context.
  - Use something like Langfuse to run experiments and trace execution
  - Run the same experiment multiple times.
  - Use python to scrape the data and compare outputs across runs
  - Compute deltas in your outputs
  - Check if specific inputs prove to have more variance in output
- Correct. In fact you should be focusing more on the sad paths because those are the edge cases you need to test.

- Use regular workflow automation tools instead of agents. Or make your tools highly deterministic and have obvious tool selection criteria.

- 90% of it is prompt engineering and a feedback loop. You have another AI at the end that is fed the last AIs user, systems messages and outputs and feeback from client "like i expected it to do this", this will give you a feedback loop on what went wrong in prompt.

- I have been trying to achieve this with my own framework. Its possible. Not in the sense of 1:1 deterministic responses but the core logic of agent output can be deterministic. People have written some methods which I mostly followed. Plus my framework has agent driven, smart retries, which enforces responds to be constrained. As a result my framework, I can deliver basic crud applications with different domain with exact same gaps and similar bugs. They all look very identical. After I get back from holiday I will announce my work. Looking forward to hear your feedbacks.

- ## ðŸ˜ï¸ðŸ§© [Agents | OpenCode](https://opencode.ai/docs/agents/)
- Agents are specialized AI assistants that can be configured for specific tasks and workflows. 
- There are two types of agents in OpenCode; primary agents and subagents.
- Primary agents are the main assistants you interact with directly. 
  - These agents handle your main conversation and can access all configured tools.
  - OpenCode comes with two built-in primary agents, Build and Plan. 
  - Build is the default primary agent with all tools enabled. This is the standard agent for development work where you need full access to file operations and system commands.
  - Plan is a restricted agent designed for planning and analysis. We use a permission system to give you more control and prevent unintended changes. This agent is useful when you want the LLM to analyze code, suggest changes, or create plans without making any actual modifications to your codebase.
- Subagents are specialized assistants that primary agents can invoke for specific tasks. 
  - OpenCode comes with two built-in subagents, General and Explore. 
  - General is a general-purpose agent for researching complex questions, searching for code, and executing multi-step tasks. Use when searching for keywords or files and youâ€™re not confident youâ€™ll find the right match in the first few tries.
  - Explore is a fast agent specialized for exploring codebases. Use this when you need to quickly find files by patterns, search code for keywords, or answer questions about the codebase.

- ## ä¸ºä»€ä¹ˆçŽ°åœ¨ AI å†™å‰ç«¯ä¸€ä¸‹å­è¿™ä¹ˆå¼ºäº†ï¼Œå°ç±³çš„ MiMo è®ºæ–‡çš„è¿™æ®µä»‹ç»äº†ä»–ä»¬å¦‚ä½•è®­ç»ƒæ¨¡åž‹å†™å‰ç«¯çš„ï¼Œå…³é”®æ˜¯è¿™å¥ï¼š
- https://x.com/linexjlin/status/2002383307414409514
  - æˆ‘ä»¬çš„åŸºäºŽè§†è§‰çš„éªŒè¯å™¨é€šè¿‡å¯¹å½•åˆ¶çš„è§†é¢‘ç‰‡æ®µæ‰§è¡Œæƒ…å†µè¿›è¡Œè¯„åˆ†ï¼Œç»¼åˆè¯„ä¼°è§†è§‰è´¨é‡ã€åŠŸèƒ½å‡†ç¡®æ€§å’Œå¯æ‰§è¡Œæ€§ï¼Œä»Žè€Œç¡®ä¿å¥–åŠ±æœºåˆ¶èƒ½å¤ŸåŒæ—¶å…¼é¡¾å¤–è§‚è¡¨çŽ°ä¸Žè¡Œä¸ºæ•ˆæžœã€‚
  - åŽŸç†ä¸Šå°±æ˜¯æ¨¡åž‹æ ¹æ® prompt å†™å¥½ä»£ç åŽå†ç”¨ Playwright æ“ä½œå½•åˆ¶æˆè§†é¢‘ï¼Œç„¶åŽï¼Œäº¤ç»™ä¸€ä¸ªè§†è§‰éªŒè¯å™¨ï¼ˆåº”è¯¥æ˜¯ä¸€ä¸ªä¸“é—¨è®­ç»ƒçš„è§†é¢‘ç†è§£æ¨¡åž‹ï¼‰ è¿›è¡Œæ‰“åˆ†ï¼Œä»¥æä¾›å¥–åŠ±ä¿¡å·ã€‚

- è¿˜åœ¨æ£é¼“XXPOçš„æˆ‘è§‰å¾—è‡ªå·±æœ›å°˜èŽ«åŠäº†
  - é‡å¤æ€§å·¥ä½œäº¤æŽ¥ AI  æ˜¯å¯¹çš„ï¼Œäººå¤šå‡ºçš„ç²¾åŠ›å¯ä»¥ç”¨åœ¨å®¡ç¾ŽæŠŠå…³ä¸Šã€‚

- åº”è¯¥æ˜¯è®­äº†ä¸€ä¸ªå¾ˆå°çš„è§†è§‰åˆ¤åˆ«æ¨¡åž‹ï¼Œå‰æœŸåŠ äº†å¾ˆå¤šäººå·¥è¯„åˆ¤çš„æ•°æ®ï¼Œè¿˜æ˜¯æŒºæ‰Žå®žçš„

- æ‰€ä»¥çŽ°åœ¨æ¸…æ¥šäº†ï¼Œå…¶å®žè¿˜å¾—ä¾é äººä¸ºåˆ›å»ºå¾ˆå¤šè®­ç»ƒæ•°æ®

- è¿™æ¬¡ç»è¿‡äº†å¤šå°‘å·¥ç¨‹å¸ˆå¤šå°‘å°æ—¶çš„æ‰“ç£¨

- ## [Is direct tool use a trap? Would it be better for LLMs to write tool-calling code instead? : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1px089u/is_direct_tool_use_a_trap_would_it_be_better_for/)
- Look into smolagents from HF, it uses something pretty similar to this.

- "better" is defined by what's model was trained on. Code path is definitely more scalable, but also more unsafe. Currently, most models are primarily trained on direct calls mode, so code calling requires taking context space with explanations and represents another failure mode for the model.
  - Source: agents in my company can decide to call their tools via code, it's pretty hard to make them choose it, unless very specifically targeting to do so.

- Scales much better to give the ai access to a virtual folder structure with code for calling different functions. Just dumping everything into context is silly.

- I use each approach. Function calling APIâ€™s are my approach pretty much 100% of the time if I am building an agent which is integrating with another system with a contract and the job of the agent is to talk to the other system(s).
  - A sandbox is better for broader use cases where the work is all internal.
  - I often define APIs using tool calls that map to python functions running inside the sandbox which is a mix of the two.
  - My opinion is that it completely depends on the situation 

- if you are going to allow a model to write and run arbitrary code, it is riskier and should be sandboxed in most situations. I think smolagents does have the option to constrain to just the functions you are given it or write and run other things as well. I am not 100 percent sure they can guarantee this.

- Yes, this is true. I've had great success providing a restrictive JS environment and typescript definition files and using this instead of JSON tool calls, especially when executing steps which would otherwise require multiple tool calls. However, it does depend on having a model that has been trained extensively on code.

- ## ðŸ’¡ [anthropic blog on code execution for agents. 98.7% token reduction sounds promising for local setups : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/)
  - instead of direct tool calls, model writes code that orchestrates tools
  - basic idea: dont preload all tool definitions. let model explore available tools on demand. data flows through variables not context
  - for local models this could be huge. context limits hit way harder when youre running smaller models
  - the privacy angle is interesting too. sensitive data never enters model context, flows directly between tools
  - cloudflare independently discovered this "code mode" pattern according to the blog
  - main challenge would be sandboxing. running model-generated code locally needs serious isolation
  - tools like cursor and verdent already do basic code generation. this anthropic approach could push that concept way further
  - wondering if anyone has experimented with similar patterns locally

- FYI, this pattern already exists in HFs smolagents, they use model-generated code to execute tools instead of JSON tool calls
  - yep, smolagents is definitely already using this pattern.
- it's up to you how you execute this. The whole approach should depend on strong sandboxing. smolagents can run generated code in a restricted executor, same assumption Anthropic makes in the blog
- The searchable filesystem approach to tool definitions was the most interesting bit for me, very clean way to avoid preloading huge schemas, whether you use code or JSON

- Yes, though in my case I have the model generating a DAG of steps it wants to run instead of arbitrary code, which reduces the sandboxing needed, avoids non-terminating constructs, etc.
  - Token-efficiency is a side-benefit from my perspective. Moving to the plan->execute pattern also makes problems tractable for smaller models, many of which are able to understand instructions and produce "code" of some sort, but which may struggle to pluck details out of even a relatively short context window with the needed accuracy.
- I really like the DAG / planâ†’execute approach , especially for sandboxing and small models. It feels aligned with the same idea of keeping data and state out of the model context, just with tighter structure. Do you generate the full DAG upfront, or refine it during execution?
  - 2 modes. The model can propose a dag using a planning tool and then the user can discuss/iterate it, or auto mode where it just runs.

- if you are writing the function why call an MCP server? Why not just do what the MCP does?
  - MCP is more easily reusable.
- I'd second that; any reasonably shaped API should work really, but this way you avoid installing any packages and browsing for the API docs. It's a way for the model to discover the API instead of being fed how to use it.

- ## Use Anthropic's tool search to give your agent hundreds of tools without filling its context window.
- https://x.com/aisdk/status/2000886249306120473
- Or you could use http://mcpz.it which came out in Feb this year and was the first tool to actually identify and solve the tooling issues with MCPs 

- Game-changer for multi-tool agents  Tool search + deferLoading = hundreds of tools without context explosion? Claude-Sonnet-4.5 agents just scaled massivelyâ€”Vercel AI SDK making production agents real. 

- ## ðŸ¤” [How to make LLM output deterministic? : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1plbe8i/how_to_make_llm_output_deterministic/)
  - I am working on a use case where i need to extract some entities from user query and previous user chat history and generate a structured json response from it. The problem i am facing is sometimes it is able to extract the perfect response and sometimes it fails in few entity extraction for the same input ans same prompt due to the probabilistic nature of LLM. I have already tried setting temperature to 0 and setting a seed value to try having a deterministic output.
  - does setting seed value really work? In my case it seems it didn't improve anything.

- Because of certain GPU optimizations, LLMs are technically random even at temperature = 0 IIRC. llama.cpp has a similar issue. And you can run into something similar in training as well for a given training seed unless you configure some knobs if I'm not misremembering.

- Here's a really good blog post around LLM determinism: https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/
  - If you were to host your LLM locally, both vLLM and SGLang have done work on providing deterministic / batch invariant inference

- change your mindset, when you work with llm it is non-deterministic, whatever you do there is still tiny chance that it can't deliver deterministic response. always handling the non-deterministic part is crucial in all llm base application
  - for me personally, try to prompt the model to wrap the anser around xml tag is quite reliable like `<Answer>what ever llm response</Answer>` and going from there

- Literally impossible to make them fully deterministic because input itself affects the inference matrix.

- The problem with using cloud LLM APIs is that your requests will get batched with others which introduces nondeterminism, even with temperature sampling disabled.
  - Itâ€™s relatively easy to achieve this if you run a model yourself and set the batch size = 1, however.

- ## [Claude Skills are just .cursorrules, change my mind : r/ClaudeAI](https://www.reddit.com/r/ClaudeAI/comments/1oj109n/claude_skills_are_just_cursorrules_change_my_mind/)
- It's all just prompt really. Basically an LLM only has prompt and output.

- Cursor rules are like CLAUDE.md files. They are loaded into context on every AI interaction. Claude Skills are executable capabilities that can be invoked _when needed_. 
  - Nope, cursorrules have markdown header thingies with a description and can either be auto-loaded, or invoked by the LLM which will only see their descriptions before invoking them.

- They can run packaged scripts when needed.

- The marketplace/plugin system is the big differentiator, imo

- ðŸ’¡ I think it was presented as revolutionary because it's adaptable for more than just coding. It can execute the scripts and call tools to use for specific purposes. I think the fact that it's accessible to Claude that AI web means that it's accessible to more individuals that may not be using cursor or something else for coding so available for other use cases

- I wish Claude Code could lazy load MCPs as same as skills.

- I'm a Cursor user and I'd love to replicate what I see people getting out of skills.
  - cursor rules are only loaded at the start of a chat. You can have 50 rules, and logic for which ones to load so that cursor only ever loads a few per chat, but it's still always at the start of a chat.
  - My understanding of Claude Skills is that they can be accessed more-or-less at any time. If I want to e.g. file a bug midway through a chat, if the "File a bug" rule wasn't included at the start, Cursor doesn't seem to re-read its Cursor rules and pick up the newly-needed ones, whereas it sounds like Claude Code will.

- ## [å¦‚ä½•çœ‹Anthropicæœ€æ–°å‘å¸ƒçš„Claude Skillsï¼Ÿä¼šæ›¿ä»£MCPå—ï¼Ÿ - çŸ¥ä¹Ž _202510](https://www.zhihu.com/question/1962512846630941008)
- è¯´ä¸ªæš´è®ºï¼ŒAI Agentæƒ³è¦è½åœ°ï¼Œéœ€è¦çš„åªæœ‰å¼ºå¤§çš„æ¨¡åž‹åŸºåº§ï¼Œä»€ä¹ˆSkillã€MCPã€... éƒ½åªæ˜¯æ·»å¤´ï¼Œé€šè¿‡ä»£ç å¾ˆå®¹æ˜“å®žçŽ°ã€‚
- Skillå°±æ˜¯ä¸€ä¸ªæ ‡å‡†åŒ–çš„æ–‡ä»¶å¤¹ï¼Œç”¨æ¥æ‰“åŒ…Agentå®Œæˆç‰¹å®šä»»åŠ¡æ‰€éœ€çš„çŸ¥è¯†å’Œå·¥å…·ã€‚
  - å¯ä»¥æŠŠå®ƒç†è§£æˆç»™æ¨¡åž‹çš„è¯´æ˜Žä¹¦æˆ–æ ‡å‡†ä½œä¸šç¨‹åºï¼ˆSOPï¼Œæˆ–è€…ä¹‹å‰æ¯”è¾ƒç«çš„æ¦‚å¿µï¼šSPECçš„å¢žå¼ºç‰ˆï¼‰ã€‚
  - Anthropicè¿™æ¬¡ä¸ä»…å‘å¸ƒäº†æ¦‚å¿µï¼Œè¿˜ç›´æŽ¥å¼€æºäº†ä¸€ä¸ªGitHubä»“åº“, é‡Œé¢åŒ…å«äº†æ‰€æœ‰20ä¸ªå·¦å³çš„å®˜æ–¹Skillçš„æºç ç¤ºä¾‹
- ä¸€ä¸ªSkillæ–‡ä»¶å¤¹é€šå¸¸åŒ…å«è¿™å‡ éƒ¨åˆ†ï¼š
  - SKILL.mdï¼šæ ¸å¿ƒæ–‡ä»¶ï¼Œå¿…é¡»å­˜åœ¨ã€‚é‡Œé¢ç”¨YAMLå†™å…ƒæ•°æ®ï¼ˆåå­—ã€æè¿°ï¼‰ï¼Œç”¨Markdownå†™è¯¦ç»†çš„æŒ‡ä»¤ï¼Œå‘Šè¯‰Claudeåœ¨ä»€ä¹ˆæƒ…å†µä¸‹ã€ä»¥åŠå¦‚ä½•ä½¿ç”¨è¿™ä¸ªSkillã€‚
  - scripts/ï¼šå­˜æ”¾å¯æ‰§è¡Œçš„Pythonã€Shellè„šæœ¬ã€‚æ¯”å¦‚PDFå¤„ç†Skillé‡Œï¼Œå°±æœ‰fill_fillable_fields.pyè¿™ç§ç¡®å®šæ€§æžå¼ºçš„ä»£ç ã€‚
  - references/ï¼šå­˜æ”¾å‚è€ƒæ–‡æ¡£ã€‚æ¯”å¦‚APIæ–‡æ¡£ã€æ•°æ®åº“Schemaã€å…¬å¸æ”¿ç­–ç­‰ï¼Œè¿™äº›æ˜¯ç»™Claudeçœ‹çš„çŸ¥è¯†åº“ã€‚
  - assets/ï¼šå­˜æ”¾èµ„æºæ–‡ä»¶ã€‚æ¯”å¦‚PPTæ¨¡æ¿ã€å…¬å¸Logoã€Reacté¡¹ç›®è„šæ‰‹æž¶ç­‰ï¼Œè¿™äº›æ˜¯Claudeåœ¨æ‰§è¡Œä»»åŠ¡æ—¶ç›´æŽ¥ä½¿ç”¨çš„æ–‡ä»¶ï¼Œè€Œä¸æ˜¯é˜…è¯»çš„
- ä¸€ä¸ªSkill = ä»»åŠ¡è¯´æ˜Žä¹¦ SKILL.md + å·¥å…·ä»£ç  (scripts) + ä¸“ä¸šçŸ¥è¯† (references) + ç´ æèµ„æº (assets)ã€‚
  - å®ƒæŠŠå®Œæˆä¸€ä¸ªç‰¹å®šä»»åŠ¡æ‰€éœ€çš„ä¸€åˆ‡éƒ½æ‰“åŒ…å¥½äº†ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯ä¸€ç§ä»£ç å’Œèµ„æºçš„ç»„ç»‡æ–¹å¼ï¼Œä¸€ç§çº¦å®šä¼˜äºŽé…ç½®çš„ç†å¿µã€‚
- ðŸ’¡ Claude Skillsè®¾è®¡çš„ç²¾é«“ï¼Œä¹Ÿæ˜¯å®ƒå’Œç®€å•RAG/MCP/FunctionCallingçš„æœ€å¤§åŒºåˆ«ã€‚å®ƒå°±æ˜¯ä¸€å¥—èªæ˜Žçš„ï¼Œä¸ºäº†èŠ‚çœä¸Šä¸‹æ–‡çª—å£è€Œè®¾è®¡çš„åˆ†å±‚åŠ è½½ç­–ç•¥ã€‚ 
  - ç¬¬ä¸€å±‚ï¼šå…ƒæ•°æ®ï¼ˆName + Descriptionï¼‰ã€‚è¿™éƒ¨åˆ†ä¿¡æ¯éžå¸¸ç®€çŸ­ï¼Œä¼šå¸¸é©»åœ¨Claudeçš„è„‘æµ·é‡Œã€‚å½“ç”¨æˆ·æå‡ºä¸€ä¸ªä»»åŠ¡æ—¶ï¼ŒClaudeä¼šå¿«é€Ÿæ‰«ææ‰€æœ‰å¯ç”¨Skillçš„æè¿°ï¼Œåˆ¤æ–­å“ªä¸ªå¯èƒ½ç›¸å…³ã€‚è¿™æ˜¯ç¬¬ä¸€é“ç­›é€‰ï¼Œæˆæœ¬æžä½Žã€‚
  - ç¬¬äºŒå±‚ï¼šSKILL.mdã€‚å½“Claudeè®¤ä¸ºæŸä¸ªSkillç›¸å…³æ—¶ï¼Œå®ƒæ‰ä¼šåŽ»åŠ è½½SKILL.mdé‡Œçš„è¯¦ç»†æŒ‡ä»¤ã€‚è¿™éƒ¨åˆ†å†…å®¹å‘Šè¯‰Claudeå®Œæˆä»»åŠ¡çš„å…·ä½“æ­¥éª¤ã€åº”è¯¥éµå¾ªçš„è§„åˆ™ã€ä»¥åŠå¦‚ä½•ä½¿ç”¨æ–‡ä»¶å¤¹é‡Œçš„å…¶ä»–èµ„æºã€‚è¿™æ­¥çš„ä¸Šä¸‹æ–‡æ¶ˆè€—ä¸­ç­‰ã€‚
  - ç¬¬ä¸‰å±‚ï¼šè„šæœ¬å’Œå‚è€ƒæ–‡æ¡£ã€‚åªæœ‰å½“SKILL.mdé‡Œçš„æŒ‡ä»¤æ˜Žç¡®è¦æ±‚ï¼Œæˆ–è€…Claudeåœ¨æ‰§è¡Œä¸­åˆ¤æ–­éœ€è¦æ—¶ï¼Œå®ƒæ‰ä¼šåŽ»è¯»å–scripts/é‡Œçš„ä»£ç æˆ–references/é‡Œçš„æ–‡æ¡£ã€‚è¿™æ­¥çš„ä¸Šä¸‹æ–‡æ¶ˆè€—æ˜¯æŒ‰éœ€çš„ï¼Œé¿å…äº†ä¸€æ¬¡æ€§æŠŠæ‰€æœ‰ä¸œè¥¿éƒ½å¡žè¿›åŽ»ã€‚
- è¿™ä¸ªæœºåˆ¶çš„å¥½å¤„æ˜¾è€Œæ˜“è§ï¼Œæžå¤§åœ°èŠ‚çœäº†å®è´µçš„ä¸Šä¸‹æ–‡çª—å£ã€‚å®ƒå…ˆå‡­ç»éªŒåˆ¤æ–­ç”¨å“ªä¸ªSOPï¼Œç„¶åŽç¿»å¼€SOPç…§ç€åšï¼Œé‡åˆ°å…·ä½“é—®é¢˜å†æŸ¥é˜…é™„å½•æˆ–å·¥å…·æ‰‹å†Œã€‚è¿™å¥—é€»è¾‘ï¼Œæˆ‘ä»¬ç”¨ä»£ç å½“ç„¶ä¹Ÿèƒ½å®žçŽ°ï¼Œä½†SkillsæŠŠå®ƒæ ‡å‡†åŒ–äº†ã€‚

- å®ƒå’ŒMCPæ˜¯ä»€ä¹ˆå…³ç³»ï¼Œä¼šæ›¿ä»£å—ï¼Ÿç›´æŽ¥å›žç­”ï¼Œå®Œå…¨ä¸æ˜¯ä¸€å›žäº‹ï¼Œä¸ä¼šæ›¿ä»£ï¼Œç”šè‡³æ˜¯äº’è¡¥çš„ã€‚
  - MCPæ˜¯ä¸€ç§é€šä¿¡åè®®ã€‚å®ƒå®šä¹‰äº†Agentï¼ˆå®¢æˆ·ç«¯ï¼‰å¦‚ä½•ä¸Žä¸€ä¸ªæš´éœ²äº†å·¥å…·çš„æœåŠ¡ï¼ˆæœåŠ¡ç«¯ï¼‰è¿›è¡Œæ ‡å‡†åŒ–çš„äº¤æµã€‚å®ƒè§£å†³çš„æ˜¯Agentä¸Žå¤–éƒ¨å·¥å…·å¦‚ä½•å¯¹è¯çš„é—®é¢˜ã€‚
  - Claude Skillsæ˜¯ä¸€ç§èƒ½åŠ›å°è£…æ ¼å¼ã€‚å®ƒå®šä¹‰äº†Agentè‡ªèº«åº”è¯¥å…·å¤‡å“ªäº›çŸ¥è¯†ã€å·¥ä½œæµå’Œå†…éƒ¨å·¥å…·ã€‚å®ƒè§£å†³çš„æ˜¯Agentå¦‚ä½•æ€è€ƒå’Œè¡ŒåŠ¨çš„é—®é¢˜ã€‚
  - Skillé‡Œçš„çŸ¥è¯†å¯ä»¥æŒ‡å¯¼Agentå¦‚ä½•æ›´æœ‰æ•ˆåœ°åŽ»ä½¿ç”¨ä¸€ä¸ªéµå¾ªMCPåè®®çš„å·¥å…·ã€‚ä¸€ä¸ªAgentå®Œå…¨å¯ä»¥åŠ è½½ä¸€ä¸ªSkillï¼Œç„¶åŽæ ¹æ®Skillé‡Œçš„æŒ‡ä»¤ï¼ŒåŽ»è°ƒç”¨ä¸€ä¸ªè¿œç¨‹çš„MCPæœåŠ¡å™¨

- æœ€å¤§çš„ä»·å€¼æ˜¯ï¼šAnthropicæŠŠä»–ä»¬åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­æ‰“ç£¨å‡ºçš„ä¸€å¥—Agentèƒ½åŠ›ç®¡ç†çš„è®¾è®¡æ¨¡å¼å¼€æºäº†ã€‚æˆ‘ä»¬å®Œå…¨å¯ä»¥æŠŠè¿™ä¸ªæ¨¡å¼å€Ÿé‰´è¿‡æ¥ï¼Œç”¨åœ¨è‡ªå·±çš„Agentä½“ç³»é‡Œï¼Œä¸ç®¡ä½ ç”¨çš„æ˜¯Qwenã€Deepseekï¼Œè¿˜æ˜¯åˆ«çš„æ¨¡åž‹
  - å½“ä½ çš„Agentèƒ½åŠ›è¶Šæ¥è¶Šå¤šæ—¶ï¼Œæ€Žä¹ˆç®¡ç†ï¼Ÿä¸€ä¸ªå‡ åƒè¡Œçš„System Promptï¼Ÿä¸€ä¸ªåŒ…å«å‡ åä¸ªå·¥å…·å‡½æ•°çš„å¤§æ‚çƒ©æ–‡ä»¶ï¼Ÿè¿™äº›éƒ½å¾ˆéš¾ç»´æŠ¤ã€‚
  - è€ŒSkillsæä¾›äº†ä¸€ç§è§£è€¦çš„ã€æ¨¡å—åŒ–çš„æ–¹æ¡ˆã€‚ä½ å›¢é˜Ÿé‡Œçš„Agentä¸å†æ˜¯ä¾èµ–ä¸€ä¸ªå·¨å¤§çš„ã€éš¾ä»¥ç»´æŠ¤çš„system_prompt.txtï¼Œè€Œæ˜¯ä¸€ä¸ªç”±å‡ åä¸ªæ ‡å‡†åŒ–çš„Skillæ–‡ä»¶å¤¹ç»„æˆçš„èƒ½åŠ›åº“ï¼Œæ¯ä¸ªSkilléƒ½å¯ä»¥ç‹¬ç«‹ç‰ˆæœ¬æŽ§åˆ¶ã€æµ‹è¯•å’Œè¿­ä»£ã€‚

- åœ¨å­—èŠ‚å®žä¹ åšé€šç”¨Agentç ”å‘çš„æ—¶å€™ï¼ŒAgentéœ€è¦æ”¯æŒå‡ åç§ä¸åŒ å·¥å…·/æŽ¥å£/å¹³å° æ¥å®Œæˆäº”èŠ±å…«é—¨çš„ä»»åŠ¡ï¼Œå¯¹æ­¤ï¼ŒåŒäº‹ä»¬å¯¹çŸ¥è¯†çš„ç®¡ç†æå‡ºäº†ä¸€ä¸ªknowledgeèŒƒå¼ï¼š
  - æ¯ä¸ªknowledgeå®šä¹‰å¥½title, description, used whenï¼ˆåœ¨ä»€ä¹ˆä»»åŠ¡/å·¥å…·/å¹³å°å‡ºçŽ°æ—¶ï¼Œä½¿ç”¨è¯¥knowledgeï¼‰ã€‚è¿™äº›ä½œä¸ºmetadataï¼Œæ¯ä¸ªknowledgeçš„metadataä¹Ÿå°±ä¸‰è¡Œå­—å·¦å³ã€‚knowledgeçš„æ­£æ–‡æ˜¯ä¸€ä¸ªmarkdownï¼Œä¼šåŒ…å«SOPã€Dosã€Don'tsã€ç”šè‡³ç®€å•çš„è„šæœ¬ã€‚
  - Agentå¯åŠ¨ä¸€æ¬¡ä»»åŠ¡æ—¶ï¼Œå…ˆæ ¹æ®promptè®©LLMæ ¹æ®æä¾›çš„metadataä¸»åŠ¨å¬å›žå…¶è®¤ä¸ºç”¨å¾—ä¸Šçš„knowledgeï¼Œå†é€šè¿‡å·¥ç¨‹æ‰‹æ®µæŠŠå®Œæ•´çš„mdæ‹¼è¿›prompté‡Œï¼Œæ”¯æŒå®ŒæˆåŽç»­ä»»åŠ¡ã€‚
  - çœ‹å®ŒAnthropicæå‡ºçš„Claude Skillsï¼Œæ„Ÿå¹å½“æ—¶åŒäº‹ç†å¿µçš„å…ˆè¿›ï¼Œä¹Ÿæ„Ÿå¹è¡Œä¸šéœ¸ä¸»çš„ç”Ÿæ€è¯è¯­æƒâ€”â€”å¦‚æžœæ˜¯è±†åŒ…æˆ–è€…seedæå‡ºè¿™æ ·ä¸€ä¸ªèŒƒå¼ï¼Œè‚¯å®šå¾—ä¸åˆ°å¦‚æ­¤å·¨å¤§çš„å…³æ³¨å’Œè·Ÿè¿›ã€‚
  - åŒæ—¶ä¹Ÿæ‰¿è®¤ï¼ŒClaude Skillsçš„å®šä¹‰å†…æ¶µå’Œè§„èŒƒæ¯”å½“æ—¶æˆ‘ä»¬å›¢é˜Ÿæå‡ºçš„knowledgeæ›´åŠ å…¨é¢å’Œå¯å¾ªï¼Œåªä¸è¿‡æœ¬è´¨ä¸Šæ²¡æœ‰å¤ªå¤šè¿›æ­¥ï¼Œä»ç„¶æ˜¯context engineeringçš„ä¸€ç§ã€‚è¦æƒ³è®©æ¨¡åž‹å……åˆ†å‘æŒ¥å¥½Skillsçš„èƒ½åŠ›ï¼Œæœ€ç»ˆè¿˜æ˜¯è¦ä¾èµ–æ›´å¥½çš„æ¨¡åž‹ï¼Œæ›´å¼ºçš„æŽ¨ç†ã€‚

- è¿™ç§èŒƒå¼æ˜¯å¼€å‘AGENTçš„å¸¸è§„æ–¹å¼ï¼Œæ²¡å•¥å…ˆè¿›çš„ã€‚å·¥ç¨‹é‡å¤§ä¸€ç‚¹çš„AGENTéƒ½ä¼šæž„å»ºè‡ªå·±çš„çŸ¥è¯†åº“èŒƒå¼ï¼Œæ ¸å¿ƒå°±æ˜¯ç»“æž„åŒ–è‡ªå·±çš„å†…å®¹

- å†™ function call çš„æ—¶å€™å°±ä¼šç”¨åˆ°å‘€ï¼Œä¸åŒçš„æ˜¯åªè€ƒè™‘åˆ°ä»£ç å±‚é¢çš„å°è£…

- ä»Žskillçš„æ–‡æ¡£æè¿°æ¥çœ‹ å®ƒå°±æ˜¯å•çº¯åœ°æŠŠæè¿°ä¸¢ç»™å¤§æ¨¡åž‹ è‡³äºŽå¤§æ¨¡åž‹å®žé™…ä¼šä¸ä¼šfollow é‚£å°±å®Œå…¨çœ‹å¿ƒæƒ…äº† ä»Žè¿™ä¸€ç‚¹æ¥è¯´ skillæ–¹æ¡ˆåœ¨æ€§èƒ½ä¸Žç¡®å®šæ€§è¿™å—å¿…ç„¶æ˜¯æ¯”çœŸæ­£çš„toolså·®ä¸å°‘çš„
  - æ€»ä½“æ¥è¯´ skillåŸºæœ¬å¯ä»¥ç®—æ˜¯promptä¹‹ä¸Šçš„åˆçº§è¯­æ³•ç³– è™½ç„¶æ¯”è¾ƒé¸¡è‚‹ ä½†å¯¹ç”¨æˆ·è€Œè¨€ æ€»å½’æ˜¯èƒ½è§£å†³ä¸€äº›åœºæ™¯ä¸‹çš„é—®é¢˜çš„ èµ·ç æœ‰äº†ä¸€å¥—æŒ‡å¯¼å¤§æ¨¡åž‹ä½¿ç”¨æ–°å·¥å…·çš„ä¸´æ—¶è§£å†³æ–¹æ¡ˆäº†

- ## [OpenAI è°·æ­Œè”æ‰‹æŽ¨å‡º AGENTS.mdï¼Œèƒ½å¦æˆä¸ºç¼–ç¨‹ Agent çš„ã€Œå®˜æ–¹è¯´æ˜Žä¹¦ã€ï¼Ÿ - çŸ¥ä¹Ž _202508](https://zhuanlan.zhihu.com/p/1941669020068709122)
- äº‰è®ºä¸€ï¼šAGENTS.md vs. CONTRIBUTING.md
  - README.md æˆ– CONTRIBUTING.md ä¸å¤Ÿç”¨å—ï¼Ÿ
  - è¿™æ•´ä»¶äº‹æœ¬è¯¥åœ¨ CONTRIBUTING.md é‡Œè§£å†³ã€‚AGENTS.md é‡Œçš„å†…å®¹ï¼Œå’Œäººç±»è´¡çŒ®è€…æƒ³äº†è§£çš„ä¸œè¥¿æ²¡ä»€ä¹ˆä¸¤æ ·ã€‚
  - ç»™ Agent çš„æ–‡æ¡£å¿…é¡» é«˜åº¦ç²¾ç‚¼ï¼Œå› ä¸ºè¿‡å¤šçš„å†…å®¹ä¼šæ¶ˆè€—å®è´µçš„ API tokenï¼Œå¢žåŠ æˆæœ¬ï¼Œç”šè‡³é™ä½Žè¾“å‡ºè´¨é‡ã€‚

- äº‰è®ºäºŒï¼šæ–‡ä»¶ vs. æ–‡ä»¶å¤¹ï¼Œå•ä½“ vs. ç»“æž„åŒ–
  - æœ‰ç»éªŒçš„å¼€å‘è€…æå‡ºï¼Œå¯¹äºŽå¤§åž‹å¤æ‚é¡¹ç›®ï¼Œä¸€ä¸ªå·¨å¤§çš„ Markdown æ–‡ä»¶å¾ˆå¿«ä¼šå˜å¾—éš¾ä»¥ç»´æŠ¤ã€‚
  - è®¸å¤šå¼€å‘è€…å»ºè®®é‡‡ç”¨æ›´æœ‰ç»„ç»‡çš„æ–‡ä»¶å¤¹ç»“æž„ï¼Œä¾‹å¦‚ä¸€ä¸ªéšè—çš„ .agents ç›®å½•

- äº‰è®ºä¸‰ï¼šæ ¹ç›®å½•æ±¡æŸ“é—®é¢˜

- äº‰è®ºå››ï¼šç»§æ‰¿è¿˜æ˜¯è¦†ç›–ï¼Ÿ
  - AGENTS.md æ”¯æŒåœ¨å­ç›®å½•ä¸­åµŒå¥—ï¼Œä½†å…¶è§„åˆ™æ˜¯ ã€Œæœ€è¿‘æ–‡ä»¶ä¼˜å…ˆã€ã€‚

- [å‘Šåˆ«æ··ä¹±ï¼Œç”¨ AGENTS.md ç»Ÿä¸€ä½ çš„ AI å¼€å‘å·¥å…·è§„åˆ™ - çŸ¥ä¹Ž](https://zhuanlan.zhihu.com/p/1951785160124109343)
  - AGENTS.md åªæ˜¯ä¸€ä¸ªå•ç‹¬çš„æ–‡ä»¶ï¼Œæ²¡æ³•åƒ Cursor çš„ `.cursor/rules` ä¸€æ ·æ”¯æŒéžå¸¸å¤šçš„ç‹¬ç«‹çš„è§„åˆ™ã€‚
  - æˆ‘çš„å»ºè®®æ˜¯ï¼Œä½ å¯ä»¥å’Œå›¢é˜Ÿå•†å®šä¸€ä¸ªå­˜æ”¾å„ç±»è§„åˆ™çš„å…¬å…±ç›®å½•ï¼Œæ¯”å¦‚ `.ai/rules/`ã€‚ ç„¶åŽä½ å¯ä»¥åœ¨ AGENTS.md ä¸­è¡¥å…… rules ä¿¡æ¯å†…å®¹ã€‚

- [Switching to AGENTS.md : r/cursor](https://www.reddit.com/r/cursor/comments/1nqwz02/switching_to_agentsmd/)
  - Don't dump all rules into this file. You can link to other rules from this file. Agents should be able to reason about it and read the right documentation.

- [Claude Code and Claude.md: Should you spread your product doumentation and plans and agent instructions over multiple files? : r/ClaudeAI](https://www.reddit.com/r/ClaudeAI/comments/1lr1g0d/claude_code_and_claudemd_should_you_spread_your/)
  - Yes, progressively split the knowledge across multiple md files.
  - Rule of thumb that works for me. Claude.md is for nouns. Slash commands are for verbs. Meaning Claude.md is about where and what things are, and then slash commands are about how to do the thing.

## ðŸŒ° [How to write a great agents.md: Lessons from over 2, 500 repositories - The GitHub Blog _202511](https://github.blog/ai-and-ml/github-copilot/how-to-write-a-great-agents-md-lessons-from-over-2500-repositories/)

- We recently released a new GitHub Copilot feature: custom agents defined in agents.md files. Instead of one general assistant, you can now build a team of specialists: a @docs-agent for technical writing, a @test-agent for quality assurance, and a @security-agent for security analysis

- What works in practice: Lessons from 2, 500+ repos
  - Put commands early: Put relevant executable commands in an early section: npm test, npm run build, pytest -v. Include flags and options, not just tool names.
  - Code examples over explanations: One real code snippet showing your style beats three paragraphs describing it. 
  - Set clear boundaries: Tell AI what it should never touch (e.g., secrets, vendor directories, production configs, or specific folders). â€œNever commit secretsâ€ was the most common helpful constraint.
  - Be specific about your stack: Say â€œReact 18 with TypeScript, Vite, and Tailwind CSSâ€ not â€œReact project.â€ Include versions and key dependencies.
  - Cover six core areas: Hitting these areas puts you in the top tier: commands, testing, project structure, code style, git workflow, and boundaries. 

- ## [Q: When will there be fast and competent SLMs for laptops? : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pcurp8/q_when_will_there_be_fast_and_competent_slms_for/)
  - Qwen3-30B-A3B and GPT-OSS-20B both uses Mixture-of-Experts instead of dense layers for their SLM
  - Kimi-Linear and Qwen3-Next-80B-A3B moved along to use "mixed attention" (majority of layers with linear attention) to speed things up AND have longer contexts
  - Not enough people getting into ternary attention like BitNet a4.8 / BitNet v2 or ternary quantization (PTQ)
  - Whatever layer routing is to reduce the amount of RAM needed, including Ouro-2.6B-Thinking these days and Mixture-of-Depths back in 2024
  - Are all of these different techniques conflicting with one another? If it is just a lack of funding for fine-tuning/modding an existing SLM into something fast (assuming QAFT and RL), how much would it cost to crowdfund a project like this?

- It depends on your standards, I believe that for the average Joe, something like Ling Mini 2.0 would already check those requirements (fast -> 1B active is doable for most modern laptops at 20+ tok/s) (Competent -> 16B total parameters makes it decent enough for 99% of the tasks an average person would likely use it for)

- For most people, Llama3 8B was the moment where their laptop could handle a ton of their work and queries locally.

- I use Ling mini to correctly format the ocr result of screenshots. Its the fastest and adheres well to long system prompt. All on cpu.

- We already have that. Llama 3.2 3B for writing. Gemma 3 for multimodal. Qwen 3 4B for stem. Granite 4 for rag.
  - The issue is that the active/ dense parameter count determines the limit of how intelligent the overall model is. The mixture of experts kinda determines how big the model's encyclopaedia is.
  - Its possible to argue that llama 3.2 3B, gemma 3 4B and Qwen 3 30b are around the same level in writing. But Qwen 3 30b is clearly the one with more knowledge.
  - There is also the requirement for AI to have ethics and emotions, which is frankly way too complex to fit into an SLM without lobotomising it.
  - Laptop ram is just too limited in bandwidth and capacity

- I am tempted to suggest RAG-ing an SLM into being better than just being slowed down by a 32B dense model (8B or 14B on higher-performance laptops), and since dense models are "more intelligent" compared to MoE... Maybe BitNet is a more or flexible layer activation is a workaround then?

- Bitnet and ternary are out as they need specific hardware

- ## [I'm surprised how simple Qwen3 VL's architecture is. : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/)
- Most machine learning architecture isnâ€™t really that complicated when you look at it in code. Plus, in software development simplicity = better.

- training pipeline probably much more complicated

- To be honest... The entire domain of LLMs and even VLMs are fairly simple... Working in self driving for over 5 years exposed to bespoke perception and multi task models, it shocked me how simple LLMs are, especially training it from the model side.
  - The literal loss function for LLMs during pretraining and finetuning is just cross entropy... Compare that to something more complicated like YOLO, it's actually insane in terms of difference of complexity.
  - Really the solution now.... Stack some transformers, use a LM head, chunk input for VLMs into patches... Pretty damn simple I have to say

- The nicest part of Qwen3-VL is that most of the â€œmagicâ€ comes from small, well-chosen inductive biases rather than a baroque stack. Itâ€™s basically ViT â†’ lightweight bridge â†’ plain decoder LLM, with two tasteful upgrades: interleaved 3D positional encoding (i-MRoPE) and DeepStack feature fusion.

- In my experience it is not very good at producing accurate coordinates of items it sees

- ## [Finally DeepSeek supports interleave thinking : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pbal3o/finally_deepseek_supports_interleave_thinking/)
  - If a thinking model supports multi-step tool calls and can incorporate thinking from historical steps during these calls, then this model supports interleaved thinking.
  - So far, among open-source models, only GPT-OSS, Kimi K2 Thinking, and MiniMax M2 support it, and I believe this feature is crucial for agents.
  - Interleave thinking lets an AI agent reason, act, and observe in tight loops, so it can adapt step-by-step to new information instead of blindly following a fixed plan.

- Why is special support needed? Each request to an LLM is whole conversation, and you can eliminate previous thinking blocks at each request. What am I missing here?

- ## [Experimenting with Multiple LLMs at once? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p4pre3/experimenting_with_multiple_llms_at_once/)
- I do mostly coding, so different families of models get wildly different python training data. Having each do the same coding task and then have another model pick the best components of the script for a new third script works really well.
  - Open web UI also has channels, which is a discord style chat room. You can tell the models you have to collaborate with each other on a project and they will take turns with sections of code.

- I had them do a few rounds of back and forth checking each other's work. It produced noticably better results than either model could on their own - but the time it took made it pointless. It was faster to just use a larger/slower model, allow a reasoning model to go nuts with thinking tokens, or just iterate myself. 
  - For tasks that aren't time sensitive there's some value there. 

- I like the idea of this and have experimented. I donâ€™t have a great way to have them collaborate in real time, but using two to check each others work just seems smart to me.

- Andrej Karpathy just posted about a vibe coded project he did called LLM Council that seems pretty cool: https://github.com/karpathy/llm-council

- Here's mine: GitHub.com/irthomasthomas/llm-consortium It's cli based but you can also save a multi-model consortium and use it like a regular model. Then you can use llm-model-gateway to serve that on a openai proxy and use it like a normal model in your tools.

- ## [Instead of either one huge model or one multi-purpose small model, why not have multiple different "small" models all trained for each specific individual use case?   r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1ovsb2x/instead_of_either_one_huge_model_or_one/)
- This is already how MoE (Mixture of Experts) models work. There are plenty of them on huggingface, though I think the separation of the 'experts' often isn't quite so clean. You also have to consider some other things -- your 20B param 'python only' expert model is almost unusable if it doesn't also understand variable names and comments and instructions in natural language, there does have to be some baseline knowledge to make a coding model actually useful, not *just* code in the training data.
- That's kinda what a MOE model is/does. Each of the "experts" are specialists in different trained things and it picks the active ones to use based on the prompt from the user.

- some people mentioned MoE, but MoE works differently. MoE makes decisions for each token separately, not for whole answers. So training many small, specialized models is not the same thing as using MoE.

- Your idea is exactly what Microsoft is attempting with its Phi series of models. They intend to build an ecosystem of models trained on very specific tasks, and then have an agentic system that uses a simple model to process the users request, decide the best model to use, and use that model to answer the question.
  - I believe Google is doing something similar with Gemini, and there are rumors that GPT 5 functions the same way.

- Perfect routing is not a solved problem. In general you have to get part of the way through solving a problem before figuring out exactly what is necessary to solve it.

- ## Every LangGraph user I know is making the same mistake! They all use the popular supervisor pattern to build conversational agents.
- https://x.com/akshay_pachaar/status/1983874390149484881
  - The pattern defines a supervisor agent that analyzes incoming queries and routes them to specialized sub-agents. Each sub-agent handles a specific domain (returns, billing, technical support) with its own system prompt.
  - This works beautifully when there's a clear separation of concerns.
  - The problem is that it always selects just one route.
  - For instance, if a customer asks: "I need to return this laptop. Also, what's your warranty on replacements?"
  - The supervisor routes this to the Returns Agent, which knows returns perfectly but has no idea about warranties.
  - This gets worse as conversations progress because real users don't think categorically. They mix topics, jump between contexts, and still expect the agent to keep up.
  - This isn't a bug you can fix since this is fundamentally how router patterns work.
  - Now, let's see how we can solve this problem.
  - Instead of routing between Agents, first, define some Guidelines.
  - Each guideline has two parts: - Condition: When it gets activated? - Action: What should the agent do?
  - Based on the user's query, relevant guidelines are dynamically loaded into the Agent's context.
  - This approach is actually implemented in Parlant - a recently trending open-source framework (15k+ stars).
  - Instead of routing between specialized agents, Parlant uses dynamic guideline matching. At each turn, it evaluates ALL your guidelines and loads only the relevant ones, maintaining coherent flow across different topics.
  - Another key advantage is that dynamic guidelines keep the system prompt clean, ensuring the agent receives only the right instructions at the right time.

- Hmm wonâ€™t a fully connected sub agent flow with planning enabled work here where one agent can route to any sub agent if it canâ€™t handle the task. So if realize the user needs more than one thing to be done the agent plans the steps and assigns to the required agent , task is executed result return until the final agent and one agent gives a combined answer

- It's written in python, nothing is stopping a string split and route two agents and join intelligently at the end.

- This seems like another form of "skills" that claude just released. Where the different skills are used to solve the flows and problems independently.  Its just making sure that all the parts are handled.
  - Yes the ideas are certainly relatable

- You can have the supervisor select a set of relevant agents instead of just one though
  - I get your point. However, real users don't think categorically. They mix topics, jump between contexts, and still expect the agent to keep up at every turn of the conversation. You need something that can dynamically load context at every turn. That's where @ParlantIO shines!
- I agree that this per turn dynamic context loading for the supervisor is effective however if there are agents added as tools to one of these guidelines then effectively its like dynamically selecting a subset of composite agents and then ReAct on this context. I actually did something very similar in one of my agents about data analysis where the per turn strategy would be selected dynamic (model, prompt and tools). I think as i said subset selection is just one of the possible single route out of all possible subset routes.

- ## [I Built Pocket Flow, an LLM Framework in just 100 Lines â€” Here is Why _202503](https://medium.com/@zh2408/i-built-an-llm-framework-in-just-100-lines-83ff1968014b)
- After a year of struggling with bloated frameworks, I decided to strip away anything unnecessary. The result is Pocket Flow, a minimalist LLM framework in just 100 lines of code.

- After a year of building LLM applications from scratch, I had a revelation: beneath all the complexity, LLM systems are fundamentally just simple directed graphs.
  -  By stripping away the unnecessary layers, I created Pocket Flow â€” a framework with zero bloat, zero dependencies, and zero vendor lock-in, all in just 100 lines of code.
- We also support batch processing, asynchronous execution, and parallel processing for both nodes and flows.

- Unlike other frameworks, Pocket Flow deliberately avoids bundling vendor-specific APIs. 
  - No Vendor Lock-in: Youâ€™re free to use any model you want, including local models like OpenLLaMA, without changing your core architecture.

- [I Built an LLM Framework in 179 Linesâ€”Why Are the Others So Bloated?  : r/LangChain _202502](https://www.reddit.com/r/LangChain/comments/1iwrhuu/i_built_an_llm_framework_in_179_lineswhy_are_the/)
- I had a look at the code. You've built a simple state machine. 
  - State machine are almost aways the worst time of choice when it comes to composition. 
  - In other words organising everything in flows of Nodes while still writing code is worse then for example carefully providing the data structures yourself and deal with the concurrency and dataflow based on the application specific requirements.
- Thanks for the feedback and example. I opted for a state machine for its simplicity! I'm open to finding a way to balance simple state transitions with tailored data structures though

- how is this different to the approach LangGraph took?
  - We think LangGraphâ€™s approach can feel rigid and tends to enforce a strictly linear, single-threaded execution model! We also want to do more than just manage state transitions!!
  - For example, we can handle asynchronous capabilitiesâ€”like node cloning to avoid race conditions and dedicated AsyncParallelBatchNodes and AsyncParallelBatchFlowsâ€”to enable parallel execution. This means you can run I/O-bound tasks concurrently (such as multiple LLM calls) without being restricted to a single-threaded, linear flow.

- ## ðŸ¤” [I built an AI orchestration platform that breaks your promot and runs GPT-5, Claude Opus 4.1, Gemini 2.5 Pro, and 17+ other models together - with an Auto-Router that picks the best approach : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o76n9d/i_built_an_ai_orchestration_platform_that_breaks/)
  - I've been frustrated with choosing between AI models - GPT-5 is great at reasoning, Claude excels at creative writing, Gemini handles data well, Perplexity is best for research - so I built LLM Hub to orchestrate them all intelligently.
  - [LLM HUB - AI Pipeline Orchestration](https://llm-hub.tech/)
  - The Core Problem: Each AI has strengths and weaknesses. Using just one means compromising on quality.
  - ðŸ’¡ The Solution: LLM Hub coordinates 20+ models across 4 execution modes:
- 4 EXECUTION MODES:
  - Single Mode - One model, one response (traditional chat)
  - Sequential Mode - Chain models where each builds on the previous (research â†’ analysis â†’ writing)
  - Parallel Mode - Multiple models tackle the same task, synthesized by a judge model
  - ðŸŒŸ Specialist Mode (the game-changer) - Breaks complex tasks into up to 4 specialized segments, routes each to the expert model, runs them in parallel, then synthesizes everything

- ## Code2Video - é€šè¿‡ä»£ç ç”Ÿæˆæ•™è‚²è§†é¢‘çš„ AI Agent æ¡†æž¶ï¼Œæ¥è‡ªæ–°åŠ å¡å›½ç«‹å¤§å­¦ Show Lab çš„æœ€æ–°ç ”ç©¶ï¼Œè®ºæ–‡å’Œå¼€æºé¡¹ç›®éƒ½å‘å¸ƒäº†ã€‚
- https://x.com/shao__meng/status/1974280130420961548
  - é€šè¿‡ AI Agent çš„æ–¹å¼ç”Ÿæˆç±»ä¼¼ 3Blue1Brown çš„è§†é¢‘ï¼Œå¾ˆæœ‰è¶£ï¼Œå’±ä»¬ä¸€èµ·çœ‹çœ‹ã€‚
  - æ–¹æ³•æ ¸å¿ƒï¼šä¸‰æ™ºèƒ½ä½“åä½œ, è®¾è®¡æ¡†æž¶åˆ†è§£ä»»åŠ¡ä¸ºä¸‰ä¸ªåä½œæ™ºèƒ½ä½“ï¼Œå½¢æˆæ¨¡å—åŒ–ç®¡é“ï¼š
  - Â· Plannerï¼ˆè§„åˆ’è€…ï¼‰ï¼šä»Žå­¦ä¹ ä¸»é¢˜ï¼ˆå¦‚â€œçº¿æ€§å˜æ¢ä¸ŽçŸ©é˜µâ€ï¼‰ç”Ÿæˆå¤§çº²å’Œæ•…äº‹æ¿ï¼Œç¡®ä¿æ—¶é—´è¿žè´¯æ€§ï¼ˆå¦‚æ¦‚å¿µå¼•å…¥ã€æ‰©å±•å’Œå›žé¡¾ï¼‰ã€‚å®ƒé›†æˆå¤–éƒ¨æ•°æ®åº“ï¼Œæ£€ç´¢å‚è€ƒå›¾åƒå’Œè§†è§‰èµ„äº§ï¼ˆå¦‚å›¾æ ‡ï¼‰ï¼Œå¹¶è€ƒè™‘å—ä¼—æ°´å¹³ï¼ˆå¦‚é«˜ä¸­ç”Ÿ vs. å¤§å­¦ç”Ÿï¼‰ï¼Œä»¥æå‡äº‹å®žå‡†ç¡®æ€§å’Œè§†è§‰ä¸€è‡´æ€§ã€‚
  - Â· Coderï¼ˆç¼–ç è€…ï¼‰ï¼šå°†æ•…äº‹æ¿è½¬æ¢ä¸ºå¯æ‰§è¡Œ Manim åŠ¨ç”»ä»£ç ã€‚é‡‡ç”¨å¹¶è¡Œåˆæˆç­–ç•¥åŠ é€Ÿç”Ÿæˆï¼Œå¹¶å¼•å…¥ ScopeRefineï¼ˆèŒƒå›´å¼•å¯¼ä¿®å¤ï¼‰æœºåˆ¶ï¼šä»Žè¡Œçº§ï¼ˆå±€éƒ¨ä¿®å¤é”™è¯¯è¡Œï¼‰é€æ­¥æ‰©å±•åˆ°å—çº§å’Œå…¨å±€é‡ç”Ÿï¼Œç¡®ä¿ä»£ç é«˜æ•ˆä¸”æ— è¯­æ³•é”™è¯¯ï¼ŒåŒæ—¶ä¿æŒè·¨èŠ‚ä¸€è‡´æ€§ã€‚
  - Â· Criticï¼ˆæ‰¹è¯„è€…ï¼‰ï¼šä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ˆVLMï¼‰å’Œé”šç‚¹è§†è§‰æç¤ºï¼ˆå¦‚å ç”¨è¡¨å’Œä½ç½®ç½‘æ ¼ï¼‰è¿­ä»£ç²¾ç‚¼æ¸²æŸ“è§†é¢‘ã€‚é’ˆå¯¹ç©ºé—´é—®é¢˜ï¼ˆå¦‚é‡å æˆ–ç©ºæ—·åŒºåŸŸï¼‰ï¼Œå®ƒæä¾›å…·ä½“è§£å†³æ–¹æ¡ˆï¼ˆå¦‚è°ƒæ•´å¯¹è±¡ä½ç½®æˆ–ç¼©æ”¾ï¼‰ï¼Œæå‡å¸ƒå±€æ¸…æ™°åº¦å’Œæ•™è‚²å¸å¼•åŠ›ã€‚
  - æ•´ä¸ªæµç¨‹ä»Žç”¨æˆ·æŸ¥è¯¢è¾“å…¥ï¼Œåˆ°è¾“å‡ºæ•™è‚²è§†é¢‘ï¼Œé€šå¸¸éœ€æ•°åˆ†é’Ÿæ¸²æŸ“ï¼Œè¿œä¼˜äºŽç«¯åˆ°ç«¯åƒç´ ç”Ÿæˆã€‚

- ## [å¦‚ä½•çœ‹å¾…è§‚ç‚¹ï¼šAI çš„å…³é”®ç‚¹ä¸æ˜¯promptï¼Œè€Œæ˜¯Context Engineeringï¼Ÿ - çŸ¥ä¹Ž](https://www.zhihu.com/question/1923364519964545063)
- Contextè¿œä¸æ­¢æ˜¯ä¸€å¥promptï¼Œå®ƒåŒ…æ‹¬ï¼š
  - æŒ‡ä»¤/ç³»ç»Ÿæç¤ºï¼šå®šä¹‰æ¨¡åž‹è¡Œä¸ºçš„åˆå§‹æŒ‡ä»¤
  - ç”¨æˆ·æç¤ºï¼šæ¥è‡ªç”¨æˆ·çš„å³æ—¶ä»»åŠ¡æˆ–é—®é¢˜
  - çŠ¶æ€/åŽ†å²ï¼šå½“å‰å¯¹è¯çš„çŸ­æœŸè®°å¿†
  - é•¿æœŸè®°å¿†ï¼šè·¨å¤šæ¬¡å¯¹è¯æ”¶é›†çš„æŒä¹…
  - çŸ¥è¯†åº“æ£€ç´¢ä¿¡æ¯(RAG)ï¼šæ¥è‡ªæ–‡æ¡£ã€æ•°æ®åº“æˆ–APIçš„å¤–éƒ¨çŸ¥è¯†
  - å¯ç”¨å·¥å…·ï¼šæ¨¡åž‹å¯ä»¥è°ƒç”¨çš„æ‰€æœ‰åŠŸèƒ½å®šä¹‰
  - ç»“æž„åŒ–è¾“å‡ºï¼šå¯¹æ¨¡åž‹å“åº”æ ¼å¼çš„å®šä¹‰
- è¿™å…¶å®žå°±æ˜¯åœ¨æž„å»ºä¸€ä¸ªå®Œæ•´çš„ã€ŒAIå·¥ä½œçŽ¯å¢ƒã€ï¼Œè€Œä¸æ˜¯ç®€å•åœ°ç»™AIä¸‹æŒ‡ä»¤ã€‚

- å³ä½¿ä½ å†™ä¸å¥½promptï¼Œå¸‚é¢ä¸Šä¹Ÿæœ‰å¾ˆå¤šAIå·¥å…·å¯ä»¥å¸®ä½ åšprompt enhancement
  - æœŸå¾…AIèƒ½ã€Œæœ‰è®°å¿†ï¼Œæ‡‚ç”¨æˆ·ã€ã€‚å› æ­¤ï¼ŒContext Engineeringï¼ˆä¸Šä¸‹æ–‡å·¥ç¨‹ï¼‰å°±å¼€å§‹è¢«å…³æ³¨åˆ°ã€‚
  - context engineering, æ˜¯æŒ‡é€šè¿‡ç³»ç»Ÿæ€§æž„å»ºã€ç®¡ç†å’Œä¼˜åŒ– AI æ¨¡åž‹çš„è¾“å…¥ä¸Šä¸‹æ–‡ï¼Œä»¥æå‡æ¨¡åž‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„ç†è§£ä¸Žè¾“å‡ºèƒ½åŠ›ã€‚
  - ç¬¬ä¸€æ˜¯åŽ†å²äº¤äº’æ•°æ®ã€‚
  - ç¬¬äºŒï¼Œæ˜¯contextçš„æ€»ç»“ã€‚å¯¹è¯å¯ä»¥ä¸€ç›´å»¶ç»­ï¼Œä½†æ¨¡åž‹çš„ä¸Šä¸‹æ–‡çª—å£æœ‰é™ï¼Œæ€Žä¹ˆè®°ä½ä¹‹å‰çš„å¯¹è¯ï¼Ÿå½“ç„¶æ˜¯åŽ‹ç¼©ï¼Œä¾‹å¦‚æŠŠæœ€è¿‘5æ¡å¯¹è¯çš„å†…å®¹åŽŸå°ä¿ç•™ï¼Œå†ä¹‹å‰çš„å†…å®¹æ€»ç»“ä¸€ä¸‹ã€‚
  - ä½†æ›´é‡è¦çš„ï¼Œæ˜¯é¢†åŸŸçŸ¥è¯†ä¸ŽèƒŒæ™¯ä¿¡æ¯ã€‚
- AIçš„èƒ½åŠ›æ¥æºäºŽä¸¤ä¸ªæ–¹é¢ in-weights memoryï¼ˆè®­ç»ƒé›†é‡Œå­¦åˆ°çš„ï¼‰ å’Œ in-context memoryï¼ˆå‚è€ƒèµ„æ–™é‡Œå­¦åˆ°çš„ï¼‰ã€‚ in-weights memoryåŸºæœ¬ä¸Šæ˜¯å¾ˆéš¾ä¿®æ”¹çš„, ä½†in-context memory æ›´å®¹æ˜“ä¿®æ”¹å’Œæ›´æ–°ï¼Œä½ åªè¦æä¾›æ–°çš„èµ„æ–™ï¼Œæ­£ç¡®çš„èµ„æ–™ï¼Œæ¨¡åž‹å°±èƒ½æœ‰ã€Œæ–°çŸ¥ã€

- å¤æ‚çš„AIåº”ç”¨ï¼Œä¸æ˜¯é chatè¾“å…¥æç¤ºè¯è¿™ä¹ˆç®€å•ï¼Œè€Œæ˜¯éœ€è¦è‡ªåŠ¨åŒ–å¤šæ¬¡å’Œå¤§æ¨¡åž‹çš„äº¤äº’ï¼Œè¿™ä¸ªè‡ªåŠ¨çš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦èƒ½å¤Ÿè‡ªåŠ¨äº§ç”Ÿç»™å¤§æ¨¡åž‹çš„æç¤ºè¯ã€‚

- Agent = LLM + Prompt + å·¥å…·è°ƒç”¨ï¼Œä½†ä»Žå·¥ç¨‹è§†è§’æ¥çœ‹ï¼Œè¿™äº›æœ¬è´¨éƒ½æ˜¯Context Engineeringï¼Œä¹Ÿå°±æ˜¯ä¸Šä¸‹æ–‡å·¥ç¨‹ã€‚
  - Context Engineeringå°±æ˜¯åœ¨æ¯æ¬¡è°ƒç”¨é‡Œï¼ŒæŠŠæ¨¡åž‹å®Œæˆä»»åŠ¡æ‰€å¿…éœ€çš„ä¿¡æ¯æŒ‰å¯¹çš„æ ¼å¼ã€åœ¨å¯¹çš„æ—¶æœºå‡†ç¡®æ‰“åŒ…è¿›åŽ»ã€‚

- ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼ŒAIç»˜å›¾ç”¨æˆ·æ—©å°±å¤©å¤©åœ¨ç”¨äº†
  - å½“ä½ ä½¿ç”¨ Inpaintï¼ˆå±€éƒ¨é‡ç»˜ï¼‰ åŠŸèƒ½æ—¶ï¼ŒAIä¸æ˜¯å‡­ç©ºæƒ³è±¡ï¼Œè€Œæ˜¯åŸºäºŽä½ ç•™ä¸‹çš„ç”»å¸ƒã€è¾¹ç¼˜çº¿æ¡ã€å…‰çº¿æ–¹å‘ã€å·²æœ‰ç”»é£Žæ¥è¡¥å…¨åŒºåŸŸï¼Œè¿™å°±æ˜¯â€œå›¾åƒä¸Šä¸‹æ–‡â€
  - å½“ä½ è¿›è¡Œ Outpaintï¼ˆå›¾åƒæ‰©å±•ï¼‰ æ—¶ï¼ŒAIå¿…é¡»å‚è€ƒåŽŸå›¾çš„æž„å›¾é€»è¾‘ã€è‰²å½©æ¸å˜ã€é£Žæ ¼çº¹ç†ã€äººç‰©é€è§†æ¥ç”Ÿæˆè‡ªç„¶è¡”æŽ¥çš„éƒ¨åˆ†ã€‚è¿™ç§å¯¹ä¸Šä¸‹æ–‡çš„â€œç†è§£â€å’Œâ€œå»ºæ¨¡â€ï¼Œæ¯”æç¤ºè¯æœ¬èº«æ›´é‡è¦
  - å½“ä½ ç”¨ Photoshopå†…ç½®çš„Fireflyè¿›è¡Œå±€éƒ¨ä¿®æ”¹åˆ›æˆå¼å¡«å……æ—¶ï¼ŒçœŸæ­£èµ·å†³å®šä½œç”¨çš„ä¸æ˜¯ä½ è¯´äº†â€œç»™æˆ‘åŠ ä¸€ä¸ªç¯å¡”â€ï¼Œè€Œæ˜¯AIæ˜¯å¦å‡†ç¡®è¯»å–äº†å½“å‰ç”»é¢æ˜¯å¤œæ™šã€æ˜¯æ°´è¾¹ã€æœ‰å…‰æºæŠ•å°„ã€æœ‰é€è§†æ¶ˆå¤±ç‚¹ã€‚
  - ä¸Šé¢è¿™äº›åŠŸèƒ½ï¼Œå“ªæ€•ä½ æ ¹æœ¬ä¸å†™æç¤ºè¯ï¼Œå¥½çš„å·¥å…·ä¹Ÿèƒ½ç»™ä½ è„‘è¡¥å¥½ï¼Œæ¯”å¦‚æˆ‘çŽ°åœ¨æ ¹æœ¬æ”¾ä¸å¼€è®¢é˜…çš„Photoshop AIã€‚

- 
- 
- 
- 
- 

- ## DeepSeekæœ€å¤§çš„åˆ›æ–°ï¼Œæ˜¯ä¸éœ€è¦å¤§é‡çš„äººå·¥æ ‡æ³¨ï¼Œè€Œæ˜¯ç›´æŽ¥ä»Žå…¶ä»–å¤§æ¨¡åž‹è’¸é¦æˆ–è€…ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼ˆGRPOï¼‰ã€CoTï¼ˆè‡ªæˆ‘åæ€ï¼‰æ¥ç»™å¤§æ¨¡åž‹åé¦ˆï¼Œ
- https://x.com/seclink/status/1888011462008005030
  - å°±ç›¸å½“äºŽå®Œå…¨ä½¿ç”¨RLï¼ˆæˆ–è€…å¦ä¸€ä¸ªåŸºç¡€å¤§æ¨¡åž‹ï¼‰æ¥æ›¿ä»£äººå·¥æ ‡æ³¨äº†ã€‚
  - è¿™å®žé™…ä¸Šæ˜¯æŠ¢äº†Scale AI è¿™ç§å…¬å¸çš„è›‹ç³•ï¼ŒDeepSeekç‰›Xä¹‹å¤„åœ¨äºŽï¼Œå¾ˆå¤šè€å¤–ä¸€å¼€å§‹ä¸ä¿¡ï¼Œç„¶åŽç…§ç€è®ºæ–‡é‡Œçš„æ–¹æ³•å¿«é€Ÿï¼ˆå±€ä¿ƒåœ°ï¼‰å¤çŽ°ï¼Œå´å‘çŽ°ç«Ÿç„¶ä¹Ÿèƒ½å¤çŽ°æˆåŠŸã€‚

- cotå’Œè’¸é¦ä¹‹å‰ï¼Œå°±é€šè¿‡grpoè¿›è¡ŒrlèŽ·å¾—äº†ç›¸å½“ä¸é”™çš„æŽ¨ç†èƒ½åŠ›ã€‚ã€‚ä½ è¿™æ•´ç†çš„ä¸æ¸…æ™°ã€‚

- è¿™æ²¡æœ‰ä»»ä½•åˆ›æ–°ï¼ŒGPT1å°±ç”¨äº†åŒæ ·çš„æ–¹æ³•ï¼Œè€Œä¸”çŽ°åœ¨ä¸ä½¿ç”¨æ˜¯æœ‰ç†ç”±çš„ã€‚å› ä¸ºdeepseekæ¨¡åž‹æœ¬èº«ä¸è¡Œï¼Œä¸€ä¸ªqueryéœ€è¦RL chainæ‰èƒ½è¾¾åˆ°å¯æŽ¥å—çš„ç­”æ¡ˆï¼Œè‡´ä½¿inference æ•ˆçŽ‡ä½Žåˆ°å¯æ€•ï¼Œè™½ç„¶trainingä¾¿å®œï¼Œä½†æ˜¯operation costè¦å¤šå¥½å‡ å€ï¼Œå¾—ä¸å¿å¤±

- ä½ è¿™ä¸ªè¯´çš„å®Œå…¨ä¸å¯¹ï¼ŒCoTæ˜¯GPTå‘æ˜Žçš„ï¼Œè’¸é¦ä¹Ÿæ—©å°±æœ‰äº†ï¼ŒRLä¹Ÿæ—©å°±æœ‰äº†ï¼Œdeepseekæ˜¯å‘æ˜Žäº†RLé‡Œçš„GRPO

- ## I read up on DeepSeekâ€™s learning algo, GRPO. GRPO: group relative policy optimization
- https://x.com/virattt/status/1885102056546910672
- How GRPO works:
  1 â€¢ model generates a group of answers
  2 â€¢ compute score for each answer
  3 â€¢ compute avg score for entire group
  4 â€¢ compare each answer score to avg score
  5 â€¢ reinforce model to favor higher scores
  - This process is repeated, allowing the model to learn and improve over time.
- Other methods like PPO, use a value function model to do reinforcement learning.
  - GRPO does not, which reduces memory and computational overhead when training.

- GRPO was proposed for the first time in Feb 2024 in DeepSeekMath paper. It is not new.
  - Back then they used Neural Networks for Rewards (PRM/ORM). The magic happened when DeepSeek  replaced PRM/ORM with the exact reward (Verified Reward).

- ## ðŸ”¡ OpenAI's Deep Research is just a search+read+reasoning in a while-loop, right? here is my replicate of it in nodejs, using gemini-flash and jina reader
- https://x.com/hxiao/status/1886250705415229627
- If it includes evaluating js driven websites, including images Then yes 
  - To really replicate that, you'd probably wanna just use puppeteer, save the whole page as an image, extract the info from that, and then crunch that data

# discuss-multi-agents ðŸ˜ï¸
- ## 

- ## 

- ## 

- ## ðŸ†š ä»Šå¤©å¾ˆæœ‰è¶£ï¼Œä¸¤å®¶çŸ¥åçš„å…¬å¸å„å‡ºäº†ä¸€ç¯‡æ–‡ç« ï¼Œäº‰è®ºè¦ä¸è¦ä½¿ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚
- https://x.com/oran_ge/status/1933754019010539923
  - Claude çš„å®˜æ–¹ Anthropic ï¼šå¦‚ä½•æž„å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
  - Devin çš„å®˜æ–¹ Cognition ï¼šä¸è¦æž„å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
- è¿™æ ¸å¿ƒçš„äº‰è®®ç‚¹åœ¨äºŽï¼šContext ä¸Šä¸‹æ–‡åˆ°åº•åº”è¯¥å…±äº«è¿˜æ˜¯åˆ†å¼€ï¼Ÿ
  - Claude è¿™è¾¹çš„è§‚ç‚¹æ˜¯ï¼Œæœç´¢ä¿¡æ¯çš„æœ¬è´¨æ˜¯åŽ‹ç¼©ï¼Œå•ä¸ªæ™ºèƒ½ä½“çš„ä¸Šä¸‹æ–‡æœ‰é™ï¼Œé¢å¯¹æ— é™çš„ä¿¡æ¯ï¼ŒåŽ‹ç¼©æ¯”å¤ªå¤§å°±ä¼šå¤±çœŸã€‚è¿™æ˜¯é›†ä½“æ™ºæ…§ï¼Œä¸€èµ·åä½œèŽ·å¾—çš„èƒœåˆ©ã€‚
  - Devin è¿™è¾¹çš„è§‚ç‚¹æ˜¯ï¼Œå¤šä¸ªæ™ºèƒ½ä½“çš„ä¸Šä¸‹æ–‡ä¸ä¸€è‡´ï¼Œä¼šå¯¼è‡´ä¿¡æ¯å‰²è£‚ã€è¯¯è§£ã€ä»–ä»¬æ±‡æŠ¥ç»™è€æ¿çš„ä¿¡æ¯ç»å¸¸å……æ»¡äº†çŸ›ç›¾ã€‚
  - è¿™è®©æˆ‘æƒ³åˆ°ï¼Œè½¯ä»¶å·¥ç¨‹ä»Žæ¥ä¸æ˜¯è¿½æ±‚å®Œç¾Žï¼Œè€Œæ˜¯æŒç»­è¿­ä»£ã€‚

- æ²¡å•¥çŸ›ç›¾çš„ã€‚çœ‹éœ€æ±‚ã€‚open-ended çš„é—®é¢˜æ¯”è¾ƒé€‚åˆ multi-agentï¼›ç›®æ ‡å¾ˆå…·ä½“çš„è¯ï¼Œå°±æ¯”è¾ƒé€‚åˆå•ä¸ª agent

- æ„Ÿè§‰å’ŒçŽ°å®žä¸­çš„ä¸¤å®¶ä¸åŒç­–ç•¥è¿è¥çš„å…¬å¸å·®ä¸å¤šï¼Œæ²¡æœ‰ç»å¯¹çš„å¯¹å’Œé”™ï¼Œéƒ½èƒ½èµ°å‡ºæ¥çš„å¯èƒ½æ€§ä¹Ÿè¶…å¤§ã€‚

- ## @KuraAIAgents é€šè¿‡åˆ›æ–°çš„äº”é‡ Agent æž¶æž„ï¼ˆè§„åˆ’ã€æ‰§è¡Œã€è¯„ä¼°ï¼‰å®žçŽ°äº† 87% çš„æµè§ˆå™¨è‡ªåŠ¨åŒ–å‡†ç¡®çŽ‡ï¼Œè¶…è¶Š Claude è®¡ç®—æœºæ“ä½œ 28 ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶æ”¯æŒä½Žæˆæœ¬æ¨¡åž‹æ›¿æ¢æ–¹æ¡ˆ
- https://x.com/shao__meng/status/1857586562588094918
  - åŒ…å«5ä¸ªä¸“é—¨çš„ Agentï¼Œå…¶ä¸­3ä¸ªæ ¸å¿ƒ Agent å½¢æˆä¸€ä¸ªå¾ªçŽ¯ç³»ç»Ÿ
  - åœ¨ WebVoyager åŸºå‡†æµ‹è¯•ä¸­å–å¾— 87% çš„æˆç»©
  - æ¯” Claude çš„è®¡ç®—æœºæ“ä½œé«˜å‡º 28%
- äº”ä¸ªæ ¸å¿ƒ Agentï¼š
a) åˆå§‹è§„åˆ’è€…(Initial Planner)
- è´Ÿè´£åˆ¶å®šé«˜å±‚æ¬¡è®¡åˆ’
- ä½¿ç”¨ OpenAI o1 æ¨¡åž‹è¿›è¡ŒæŽ¨ç†
b) å¾ªçŽ¯è§„åˆ’è€…(Agent Loop Planner)
- è¯„ä¼°ä»»åŠ¡æ˜¯å¦å®Œæˆæˆ–ä¸å¯èƒ½å®Œæˆ
- ä¸ºæ‰§è¡Œè€…æä¾›ä¸‹ä¸€æ­¥æŒ‡ä»¤
- æ ¹æ®éœ€è¦ä¿®æ”¹è®¡åˆ’
c) æ‰§è¡Œè€…(Executor)å…·å¤‡ä¸‰é¡¹æ ¸å¿ƒæŠ€èƒ½ï¼š
- ç½‘å€å¯¼èˆªå’Œè¿”å›ž
- è¯»å–å½“å‰é¡µé¢æ•°æ®
- æ‰§è¡Œå±å¹•æ“ä½œ(ç‚¹å‡»ã€æ»šåŠ¨ã€è¾“å…¥)
d) å¾ªçŽ¯è¯„è®ºè€…(Agent Loop Critic)
- è¯„ä¼°æ‰§è¡Œè€…çš„è¡¨çŽ°
- ç‰¹åˆ«åœ¨å¤æ‚ç•Œé¢æ“ä½œä¸­èµ·å…³é”®ä½œç”¨
e) æœ€ç»ˆè¯„è®ºè€…(Final Critic)
- è¯„ä¼°æ•´ä¸ªä»»åŠ¡è½¨è¿¹
- å¿…è¦æ—¶æä¾›åé¦ˆå¹¶å¯åŠ¨æ–°çš„å¾ªçŽ¯

- ## OpenAI å‘å¸ƒå¤š Agent ç¼–æŽ’æ¡†æž¶èƒŒåŽçš„æ€è€ƒä»¥åŠå®žè·µè¿‡ç¨‹
- https://x.com/tuturetom/status/1845634978530693494
  - æ ¸å¿ƒæ˜¯ OpenAI çš„å·¥ç¨‹å¸ˆåœ¨æ€è€ƒ  Agent çš„ã€Œè·¯ç”±ã€ + ã€Œç§»äº¤ã€ç­‰èƒ½åŠ›æ—¶æ‹“å±•å‡ºæ¥çš„ä¸€ä¸ªç¤ºä¾‹ï¼Œè¿›è€Œå‘çŽ°è¿™ä¸ªç¤ºä¾‹åŽŸè¯­å¾ˆæ™®é€‚ï¼Œæ‰€ä»¥å¼€å‘äº† Swarm æ¡†æž¶
  - æ‰€æœ‰ä¼Ÿå¤§çš„æ€è€ƒéƒ½æºäºŽå‘¨æœ«ä¸šä½™å·¥ä½œ

- ## OpenAI æ‚„æ‚„å¼€æºäº†æž„å»ºå¤šä»£ç†æ™ºèƒ½ä½“ååŒæ¡†æž¶ï¼šSwarm
- https://x.com/aigclink/status/1844936446416912628
  - ç”¨äºŽæž„å»ºã€ç¼–æŽ’å’Œéƒ¨ç½²å¤šä»£ç†

# discuss-ai-format/interop
- ## 

- ## 

- ## 

- ## [Use YAML over JSON when dumping into prompts for ~2x token saving : r/ChatGPTCoding _202509](https://www.reddit.com/r/ChatGPTCoding/comments/1nl7xux/use_yaml_over_json_when_dumping_into_prompts_for/)
  - [YAML vs. JSON: Which Is More Efficient for Language Models? _202307](https://medium.com/better-programming/yaml-vs-json-which-is-more-efficient-for-language-models-5bc11dd0f6df)
  - It's been pointed out in the comments (with sass) that minifying your JSON is another, perhaps even better, alternative than transforming to YAML. So now there's two options for saving tokens.

- Does the guy who wrote the article know that you don't need to use whitepaces in JSON and you can minify it to consume less space than YAML? Generally speaking, JSON is more space-efficient and compact than YAML.

- Just remove the spaces and condence the JSON into a single line. LLMs don't care about spaces, it's a visual thing for us.

- Thought LLM's don't count white space as context... or if they did, it would be incredibly minimal
  - They kind of have to, if only to correctly write Python
  - ASCII art too

- Another point is accuracy... some like XML more as well - and there is BAML. If i just wanna save money I could get a cheaper model too.
- xml is also what Claude officially recommends for better accuracy.

- I use YAML and JSON, because i use the CMS Drupal since 2006 - so this fits quite well in my workflow

- TOML is actually more verbose when it comes to complex data structures.
  - Which makes sense since it was designed to be a JSON/YAML mappable language for better human readability.
# discuss-local-llm-usecases
- resources
  - [Use Cases | Claude](https://claude.com/resources/use-cases)

- ## 

- ## 

- ## 

- ## 

- ## [We tried to automate product labeling in one prompt. It failed. 27 steps later, we've processed 10, 000+ products. : r/LocalLLaMA _202601](https://www.reddit.com/r/LocalLLaMA/comments/1qcsmww/we_tried_to_automate_product_labeling_in_one/)
  - We built an AI agent to localize imported food products for a retail client. The task sounds simple: extract product info, translate it contextually (not Google Translate), calculate nutritional values for local formats, check compliance with local regulations.
  - First attempt: one detailed prompt. Let the AI figure out the workflow.
  - Result: chaos. The AI would hallucinate numbers even with clean images. It would skip steps randomly. At scale, we had no idea where things broke. Every error was a mystery to debug.
- So we broke it down. Way down. 27 steps.
  - Each column in our system handles one thing: Extract name/weight/desc/...
- What changed:
  - 1. Traceability. When something fails, we know exactly which step. No more guessing.
  - 2. Fixability. Client corrects a number extraction error once, we build a formula that prevents it downstream. Errors get fixed permanently, not repeatedly.
  - 3. Consistency at scale. The AI isn't "deciding" what to do. It's executing a defined process. Same input, same process, predictable output.
  - 4. Human oversight actually works. The person reviewing outputs learns where the AI struggles. Step 14 always needs checking. Step 22 is solid. They get faster over time.
- The counterintuitive part: making the AI "dumber" per step made the overall system smarter. One prompt trying to do everything is one prompt that can fail in infinite ways. 27 simple steps means 27 places where you can inspect, correct, and improve.
  - We've processed over 10, 000 products this way. The manual process used to take 20 minutes per product. Now it's 3 minutes, mostly human review.
  - The boring truth about reliable AI agents: it's not about prompt engineering magic. It's about architecture that assumes AI will fail and makes failure easy to find and fix.

- This is case with 90% of real world AI use for non coding currently. It always requires human quailty control and intervention.

- Good example of human in the loop. I've been talking with my CEO about how the best use of LLM's at least for the foreseeable future will be wtih humans in the loop. The problem will always be getting humans to actually do the human in the loop process as the error rate begins to plummet.

- same attempt and result I had. Found Nemotron nano solved any output quality issues once it was all broken down.

- ## [Llama 3.2 1B Instruct â€“ What Are the Best Use Cases for Small LLMs? : r/LocalLLaMA _202501](https://www.reddit.com/r/LocalLLaMA/comments/1i4cfpz/llama_32_1b_instruct_what_are_the_best_use_cases/)
- Llama 3.2 1B Instruct can work as speculative decoding model for Llama 3.2-11B/90B or 3.3-70B.

- Code completion and autocomplete

- With a bit of fine tuning they can be really good at task specific things, including structured output (do not try llama 1b for structured output without fine tuning).
  - Long term my hope is local models built into the OS, with small task specific Lora adapters. iOS is doing it, but not open to 3rd parties yet.

- For research it's nice to have a dirt cheap model to prototype datasets when evaluating LLMs I usually use 8B for that though

- ## [Real world use cases for small LLM on edge devices : r/LocalLLaMA _202409](https://www.reddit.com/r/LocalLLaMA/comments/1ffzsy0/real_world_use_cases_for_small_llm_on_edge_devices/)
- Small local models can make many factual mistakes, because it's impossible to compress the entire world model into 2 GB. However, they can be great analyzers (for their size) of the existing text.
  - Summarizing news articles, extracting insights from long web pages, finding a fact that you're searching for in a long text (a book?) and so on. Imagine it as a personal assistant in daily activities.

- Function calling. If it can do basic language to function translation, this is a perfect use. So it allows human voice or text interaction with complicated systems that would otherwise require custom coding. Also, text to structured responses, like using embeds or RAG to return well-defined SQL queries that might result from a wide variety of human specifications.

- All of these tasks involve LLM now, and are simple enough that they can be done on edge. In fact, most of them already do. Sure a cloud service can also do them, sometime better, but with a recurring cost that scale with usage. Most business that sell hardware, and user that buy it, will prefer the fixed upfront cost for R&D that small model.
  - OCR
  - Live transcription and translation
  - Summarization
  - Autocorrect
  - Basic voice control

- Analysis of secured data in offline mode . Maybe finance data , healthcare data , research data. Embed it on drone and check for suitable usecases

- ## [The curious case of Qwen3-4B (or; are <8b models *actually* good?) : r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1p76wtf/the_curious_case_of_qwen34b_or_are_8b_models/)
  - how good are the smaller models at answering some of the sort of questions I might ask of them, chatting, instruction following etc?
  - I ran each model's output against the "council of AI elders", then got GPT 5.1 (my paid account craps out today, so as you can see I am putting it to good use) to run a tally and provide final meta-commentary.
  - The results, per GPT 5.1: GPT-OSS 20B and Qwen 3-4B emerged as the strongest overall performers. Mid-tier models like DeepThink 7B and Qwen 2.5 7B produced competent technical content but struggled severely with the style transform, while Phi-Mini 4B showed the weakest combination of accuracy, coherence, and instruction adherence.
  - The results align closely with real-world use cases: larger or better-trained models excel at technical clarity and instruction-following, whereas smaller models require caution for detail-sensitive or persona-driven tasks, underscoring that the most reliable workflow continues to be â€œstrong model for substance, optional model for vibe.â€

- Iâ€™m using qwen3-4b 2507 instruct for simple tasks such as meeting summarization, keeping track of to-doâ€™s and the likes. So far, it has performed quite well

- ## [My Journey to finding a Use Case for Local LLMs : r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1p1xy0q/my_journey_to_finding_a_use_case_for_local_llms/)
  - I have a lot of phone pictures of recipes, and a lot of inherited cookbooks. The thought of gathering the ones I really liked into one place was daunting. The recipes would get buried in mountains of photos of cats (yes, it happens), planes, landscapes etc. Google photos is pretty good at identifying recipe images, but not the greatest.
  - I landed on qwen3-vl:8b. It was able to take the image (with very strict prompting) and output the exact text from the image. I did have to verify and do some editing here and there. I was happy
  - I then turned to GPT-OSS:20b since it is newer, and asked it to convert the recipe text to json-ld recipe schema compatible format.
  - Now I can take a pic of any recipe I want, run it through the qwen-vl:8b model for OCR, verify the text, then have GPT-OSS:20b spit out json-ld recipe schema text that can be imported into the mealie database. (And verify the json-ld text again, of course).
  - I'm not using any system prompts at this time. Here is the prompt

- You could now take this even further and query it with a prompt on your cell phone.

- My use case is that I use local vlm to the content of the download file, then rename following the format I define and use bash script to route it to the its dedicated folder. Now every time a file is downloaded, it is renamed and route to its location automatically.
# discuss-local-llm-xp/tips
- ## 

- ## 

- ## 

- ## [I benchmarked 7 Small LLMs on a 16GB Laptop. Here is what is actually usable. : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pzxtnr/i_benchmarked_7_small_llms_on_a_16gb_laptop_here/)
  - I tested Qwen 2.5 (14B), Mistral Small (12B), Llama 3 (8B), and Gemma 3 (all 4-bit quants) to see which ones I could actually run without crashing my laptop.
  - Qwen 2.5 (14B): The smartest for coding, but it eats 11GB System RAM + Context. On a 16GB laptop, if I opened 3 Chrome tabs, it crashed immediately (OOM).

- They are all ancient models by llm age so its kinda an invalid test. The king of low spec ram is mamba2 and sliding window attention. Try Granite 4, or Nemotron Nano 2

- I'm not considering 1-2 tokens/s "usable". Even 10 tokens/s is barely usable for the most part.

- ## [I'm curious whether people ask for the model's name in their prompts when testing on LMArena (ChatBot Arena). : r/LocalLLaMA _202504](https://www.reddit.com/r/LocalLLaMA/comments/1jtfzci/im_curious_whether_people_ask_for_the_models_name/)
  - After all, by doing this, users can know the names of the models being A/B tested beforehand, which could bias the ongoing test to some extent.

- Models never know a thing about themselves; no serious LLM user in 2025 will ask model about itself.

- well, the model's output might not be correct at all: deepseek sometimes call itself gpt. so you never now who these models actually are.

- ## [Hard lesson learned after a year of running large models locally : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/)
  - Iâ€™m running everything off a workstation with a single RTXâ€¯3090, Ubuntu 22.04, llama.cpp for smaller models and vLLM for anything above 30â€¯B parameters.
  - My goal has always been to avoid cloud dependencies and keep as much computation offline as possible, so Iâ€™ve tried every quantization trick and caching tweak I could find.
  - The biggest friction point has been scaling beyond 13â€¯B models.
  - My takeaway so far is that local first inference is viable for small to medium models, but thereâ€™s a hard ceiling unless you invest in server grade hardware or cluster multiple GPUs.
  - Quantization helps, but you trade some quality and run into new bugs.
  - For privacy sensitive tasks, the tradeâ€‘off is worth it; for fast iteration, itâ€™s been painful compared to cloud based runners.

- vLLM works great if model + context fit into VRAM, but it doesn't do CPU offloading well - use llama.cpp for anything that spills over to RAM.

- instead of trying to use "smarter bigger" models to achieve whatever youre trying to achieve, its more reliable to use multiple parallel instances (via vllm for example) of smaller model that can communicate with each other in distinct roles to create a system that produces accurate results.

- You can also rent a card in the cloud for near local inference.

- ## [Useful patterns for building HTML tools _202512](https://simonwillison.net/2025/Dec/10/html-tools/)
- https://x.com/vista8/status/2003803470449787263
  - ç”¨å¤§æ¨¡åž‹ä¸¤å¹´åšäº†150ä¸ªå·¥å…·ã€‚è¿™ä¸å¥‡æ€ªï¼Œå¼ºçš„æ˜¯ä»–åªç”¨å•é¡µHTMLåšï¼
  - ä¸€ä¸ª HTML æ–‡ä»¶ï¼Œæ‰“å¼€å°±èƒ½ç”¨ï¼Œä¸éœ€è¦å®‰è£…ï¼Œä¸éœ€è¦æ³¨å†Œã€‚
  - ä»–ç‰¹åˆ«å¼ºè°ƒ "ä¸è¦ç”¨ React"ã€‚ å› ä¸º JSX éœ€è¦æž„å»ºï¼Œè¿™ä¼šè®©æ•´ä¸ªæµç¨‹å˜å¾—éº»çƒ¦ã€‚ ä¿æŒç®€å•ï¼Œå°±æ˜¯ä¿æŒå¯ç»´æŠ¤ã€‚
- ä¼˜åŠ¿ä¹Ÿå¾ˆæ˜Žæ˜¾ï¼š
  - å¯ä»¥ç›´æŽ¥ä»Ž ChatGPT æˆ– Claude é‡Œå¤åˆ¶ç²˜è´´å‡ºæ¥
  - æ‰”åˆ° GitHub Pages ä¸Šå‡ ç§’é’Ÿå°±èƒ½ç”¨
  - ä¸éœ€è¦ npmï¼Œä¸éœ€è¦æž„å»ºæ­¥éª¤
  - ä¸¤å¹´åŽè¿˜èƒ½æ‰“å¼€ï¼Œä¸ä¼šå› ä¸ºä¾èµ–è¿‡æœŸå´©æºƒ
- çŠ¶æ€å­˜åœ¨å“ªé‡Œï¼Ÿ æ²¡æœ‰åŽç«¯æ•°æ®åº“æ€Žä¹ˆåŠžï¼ŸSimon æœ‰ä¸¤ä¸ªåŠžæ³•ï¼š
  - æŠŠçŠ¶æ€å­˜åœ¨ URL é‡Œã€‚ æ¯”å¦‚ï¼Œä»–åšäº†ä¸€ä¸ª 24x24 çš„å›¾æ ‡ç¼–è¾‘å™¨ï¼Œä½ ç”»çš„å›¾æ ‡ç›´æŽ¥ç¼–ç åœ¨ URL é‡Œã€‚è¿™æ ·ä½ å¯ä»¥æ”¶è—ï¼Œå¯ä»¥åˆ†äº«ï¼Œæ‰“å¼€é“¾æŽ¥å°±èƒ½ç»§ç»­ç¼–è¾‘ã€‚
  - æŠŠå…³é”®çš„ä¿¡æ¯å­˜åœ¨ localStorage é‡Œã€‚ æ¯”å¦‚ API key è¿™ç§æ•æ„Ÿä¿¡æ¯ï¼Œä¸èƒ½å‡ºçŽ°åœ¨ URL é‡Œï¼Œä¹Ÿä¸èƒ½å‘åˆ°æœåŠ¡å™¨ã€‚ localStorage è®©æ•°æ®åªå­˜åœ¨ç”¨æˆ·çš„æµè§ˆå™¨é‡Œã€‚ ä»–è¿˜ç”¨ localStorage åšè‡ªåŠ¨ä¿å­˜ã€‚ å†™å­—æ•°ç»Ÿè®¡å·¥å…·æ—¶ï¼Œæ¯æ¬¡è¾“å…¥éƒ½ä¼šä¿å­˜ï¼Œè¿™æ ·ä¸å°å¿ƒå…³æŽ‰æ ‡ç­¾é¡µä¹Ÿä¸ä¼šä¸¢å†…å®¹ã€‚
- CORSï¼ˆè·¨åŸŸèµ„æºå…±äº«ï¼‰å¬èµ·æ¥å¾ˆæŠ€æœ¯ï¼Œä½†ç†è§£å®ƒå¾ˆé‡è¦ã€‚
  - iNaturalist å¯ä»¥æŸ¥åŠ¨ç‰©çš„è§‚å¯Ÿè®°å½•
  - PyPI å¯ä»¥èŽ·å– Python åŒ…çš„ä¿¡æ¯
  - GitHub çš„å…¬å¼€ä»“åº“å†…å®¹éƒ½å¯ä»¥ç›´æŽ¥èŽ·å–
  - Bluesky å’Œ Mastodon çš„ API ä¹Ÿå¾ˆå¼€æ”¾
  - Simon ç”šè‡³ç”¨ GitHub Gists æ¥æŒä¹…åŒ–æ•°æ®ã€‚ å› ä¸º Gist çš„ API æ”¯æŒ CORSï¼Œä½ å¯ä»¥åšä¸€ä¸ªçº¯å‰ç«¯å·¥å…·ï¼ŒæŠŠç”¨æˆ·çš„æ•°æ®ä¿å­˜åˆ°ä»–ä»¬è‡ªå·±çš„ Gist é‡Œã€‚
- æ–‡ä»¶ä¸éœ€è¦ä¸Šä¼  `<input type="file">` ä¸åªæ˜¯ç”¨æ¥ä¸Šä¼ æ–‡ä»¶ã€‚ JavaScript å¯ä»¥ç›´æŽ¥è¯»å–æ–‡ä»¶å†…å®¹ï¼Œåœ¨æµè§ˆå™¨é‡Œå¤„ç†ã€‚
- Python å’Œ WebAssembly è¿™æ˜¯æœ€è®©äººå…´å¥‹çš„éƒ¨åˆ†ã€‚ Pyodide è®© Python å¯ä»¥åœ¨æµè§ˆå™¨é‡Œè¿è¡Œã€‚
  - ä¸æ˜¯çŽ©å…·çº§åˆ«çš„è¿è¡Œï¼Œæ˜¯çœŸæ­£çš„ Pythonï¼ŒåŒ…æ‹¬ Pandas å’Œ matplotlibã€‚
- Tesseract OCRã€SQLiteã€å›¾ç‰‡åŽ‹ç¼©åº“ï¼Œè¿™äº›åŽŸæœ¬ç”¨ C æˆ– C++ å†™çš„è½¯ä»¶ï¼ŒçŽ°åœ¨éƒ½èƒ½åœ¨æµè§ˆå™¨é‡Œè·‘ã€‚
- è¿™äº›å·¥å…·çœŸå¯ä»¥è§£å†³é—®é¢˜ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¿™ç§æ–¹å¼è®©ä½ èƒ½å¿«é€Ÿå®žéªŒã€‚æƒ³æ³•åˆ°åŽŸåž‹å¯èƒ½åªéœ€è¦å‡ åˆ†é’Ÿã€‚ä¸ç”¨æ‹…å¿ƒéƒ¨ç½²ï¼Œä¸ç”¨æ‹…å¿ƒç»´æŠ¤ï¼Œä¸ç”¨æ‹…å¿ƒæˆæœ¬ã€‚
- æ€Žä¹ˆèµ·æ­¥å°è¯•ï¼Ÿ ä¸€ä¸ª GitHub ä»“åº“ï¼Œå¼€å¯ GitHub Pagesã€‚ ç„¶åŽå°±å¯ä»¥å¼€å§‹äº†ã€‚ ç”¨ ChatGPT æˆ– Claude ç”Ÿæˆä¸€ä¸ªå·¥å…·ï¼Œå¤åˆ¶ç²˜è´´åˆ°ä»“åº“é‡Œï¼Œå‡ ç§’é’ŸåŽå°±èƒ½åœ¨ç½‘ä¸Šè®¿é—®ã€‚ ä¸éœ€è¦å®Œç¾Žï¼Œä¸éœ€è¦å¤æ‚ã€‚ ä»Žä¸€ä¸ªè§£å†³ä½ è‡ªå·±é—®é¢˜çš„å°å·¥å…·å¼€å§‹ã€‚ ç„¶åŽæ…¢æ…¢ç§¯ç´¯ï¼Œæ…¢æ…¢æ”¹è¿›ã€‚ ä¸¤å¹´åŽä½ å¯èƒ½ä¹Ÿä¼šæœ‰å‡ åä¸ªç”šè‡³ä¸Šç™¾ä¸ªè¿™æ ·çš„å·¥å…·ã€‚ æ¯ä¸€ä¸ªéƒ½åœ¨æŸä¸ªæ—¶åˆ»å¸®åˆ°ä½ ï¼Œæ¯ä¸€ä¸ªéƒ½è®©ä¸‹ä¸€ä¸ªæ›´å®¹æ˜“åšå‡ºæ¥ã€‚
- è¿™å°±æ˜¯ HTML å·¥å…·çš„é­…åŠ›ï¼šç®€å•ã€å®žç”¨ã€å¯æŒç»­ã€‚

- ## [Senior engineer struggles with learning LLMs foundations : r/LLMDevs _202512](https://www.reddit.com/r/LLMDevs/comments/1plxnq0/senior_engineer_struggles_with_learning_llms/)
  - I've been using ollama and openai to create some interesting side projects and to learn more about LLMs, but I think I'm hugely lacking solid foundations. Please provide me with a structure learning material for a senior engineer with some knowledge of LLMs, thanks
- Andrej Karpathyâ€™s series on YouTube and Stanfordâ€™s CME course on LLMs and Transformers which is published to YouTube for free.
  - I had a quick look at the Stanford's course and I think it's exactly what I needed, thanks

- You need to learn four critical things
  - Your agent's core product logic is the prompt/instructions you send to an LLM. You will be spending time here with domain experts too to construct good instructions so that the model aligns to your policy. There is no magic bullet. You iterate and evaluate until you are satisfied. Making investments in evals is worth it.
  - If you want to build an agentic application, then you need to expose tools to different models. These are essentially APIs that you have today, both internal or external which the model will instruct you to run and return its results as string. 
  - There are two agentic loops, one is called the inner loop where you agent interacts with an LLM until the LLM is done (stop_reason=finish). There is an outerloop which runs to route traffic to/from agents (if you have a multi-agent architecture), ensure that only good traffic is reaching your agents and that if multiple agents need to be engaged it would be handled outside your core product logic.
  - You need exceptional observability to know what happens, how things fail, etc. And you need to account for different models in your stack so that you can easily improve performance, and/or latency and/or cost.

- ## [å’ŒAIå¯¹è¯ï¼Œä¸è¦ä½¿ç”¨â€œä½ â€ ](https://linux.do/t/topic/1293395)
- AI å¤§ç¥ž Karpathy åˆ†äº«äº†ä¸€ä¸ªåç›´è§‰çš„è§‚ç‚¹ï¼šè·Ÿå¤§æ¨¡åž‹èŠå¤©æ—¶ï¼Œä¸è¦ä½¿ç”¨ â€œä½ â€ è¿™ä¸ªå­—ï¼Œä¹Ÿå°±æ˜¯ä¸è¦æŠŠå®ƒä»¬å½“ â€œäººâ€ã€‚
  - Karpathy æŒ‡å‡ºï¼Œè¿™ç§é—®æ³•å…¶å®žæ˜¯åœ¨ç»™ AI é™æ™ºã€‚å› ä¸º LLM æœ¬è´¨ä¸Šå¹¶æ²¡æœ‰è‡ªæˆ‘æ„è¯†ï¼Œå®ƒæ›´åƒæ˜¯ä¸€ä¸ªæ‹¥æœ‰æµ·é‡çŸ¥è¯†çš„æ¨¡æ‹Ÿå™¨ã€‚
  - å½“ä½¿ç”¨ â€œä½ â€ è¿™ä¸ªå­—çš„æ—¶å€™ï¼Œæ¯”å¦‚é—® â€œä½ çš„çœ‹æ³•â€ï¼Œæ¨¡åž‹å°±ä¼šç«‹åˆ»è¢«è§¦å‘ä¸€ç§ç‰¹å®šçš„äººæ ¼åµŒå…¥ã€‚
  - å®ƒä¼šå¼ºè¡Œè®©è‡ªå·±æ‰®æ¼”ä¸€ä¸ªç¤¼è²Œã€å®‰å…¨ä½†æœ‰ç‚¹æ— èŠçš„ AI åŠ©æ‰‹ã€‚è¿™æ—¶å€™å®ƒåå‡ºæ¥çš„å¾€å¾€æ˜¯é‚£äº›å››å¹³å…«ç¨³ã€æ»´æ°´ä¸æ¼ï¼Œä½†å®žé™…ä¸Šæ²¡å•¥æ·±åº¦çš„è½¦è½±è¾˜è¯ï¼Œä¹Ÿå°±æ˜¯å¤§å®¶å¸¸è¯´çš„ â€œAl å‘³â€ã€‚
  - ç›¸åï¼ŒKarpathy å»ºè®®æˆ‘ä»¬è¦è½¬å˜æ€è·¯ï¼Œè¦æƒ³è§£é” AI çœŸæ­£çš„å®žåŠ›ï¼Œå°±æŠŠ AI å½“æˆæ¨¡æ‹Ÿå™¨ç”¨ã€‚
  - ä¸¾ä¸ªä¾‹å­ï¼Œä¸è¦é—® â€œä½ æ€Žä¹ˆçœ‹â€ï¼Œè€Œæ˜¯è¦è®¾å®šå…·ä½“çš„æƒ…å¢ƒå’Œè§’è‰²ã€‚ä½ å¯ä»¥é—® â€œå¦‚æžœæ˜¯ä¸‰ä½èµ„æ·±äº§å“ç»ç†ååœ¨ä¸€èµ·è®¨è®ºè¿™ä¸ªåŠŸèƒ½ï¼Œä»–ä»¬ä¼šæå‡ºå“ªäº›å°–é”çš„æ‰¹è¯„â€ï¼Œæˆ–è€… â€œè¯·ä»¥ä¸€ä½è¯ºè´å°”ç»æµŽå­¦å¥–å¾—ä¸»çš„è§†è§’åˆ†æžè¿™ä¸ªçŽ°è±¡â€ã€‚
- è¯„è®ºåŒºä¹Ÿæœ‰ä¸å°‘é«˜æ‰‹è¡¥å……ï¼Œä¸ä»…è¦å°‘ç”¨ â€œä½ â€ï¼Œä¹Ÿè¦å°‘ç”¨ â€œæˆ‘â€ã€‚
  - å› ä¸ºå½“ä½ è¡¨è¾¾ â€œæˆ‘è§‰å¾—.â€ çš„æ—¶å€™ï¼ŒAI ä¸ºäº†è®¨å¥½ç”¨æˆ·ï¼Œå¾€å¾€ä¼šé¡ºç€ä½ çš„è¯è¯´ï¼Œè¿™å°±å¯¼è‡´äº†æ‰€è°“çš„é˜¿è°€å¥‰æ‰¿çŽ°è±¡ï¼Œè€Œä¸æ˜¯åŸºäºŽå®¢è§‚äº‹å®žç»™å‡ºç­”æ¡ˆã€‚
- ä¸‹æ¬¡å†™æç¤ºè¯çš„æ—¶å€™ï¼Œè¯•è¯•æˆ’æŽ‰ â€œä½ â€ å’Œ â€œæˆ‘â€ è¿™ä¸¤ä¸ªå­—ã€‚è®© AI åŽ»æ¨¡æ‹Ÿé‚£äº›å…·ä½“çš„ä¸“å®¶ã€å›¢é˜Ÿç”šè‡³åæ–¹è¾©æ‰‹ï¼Œä½ ä¼šå‘çŽ°å®ƒçš„å›žç­”è´¨é‡èƒ½æå‡å¥½å‡ ä¸ªæ¡£æ¬¡ã€‚

- é‚£ä¹ˆæ¯”å¦‚ â€œä½ æ˜¯ä¸€ä¸ªç‰©ç†å­¦é¢†åŸŸä¸“ä¸šäººå£«â€ã€â€œä½ æ˜¯ä¸€ä¸ªç©¿å°è£™å­çš„ç®—æ³•é«˜æ‰‹â€ã€â€œä½ æ˜¯ä¸€ä¸ªçŒ«å¨˜â€ çš„ promptï¼Œåˆ°åº•è¯¥ä¸è¯¥ç”¨å‘¢ï¼Ÿä»¥å‰æˆ‘çœ‹å¾ˆå¤šäººé¼“åŠ±åœ¨éœ€è¦ç‰¹å®šå·¥ä½œåœºæ™¯æ—¶è¿™ä¹ˆåšï¼Œè¿™é‡Œé¢æœ‰ä½ ï¼Œä½†æ˜¯æŒ‡å®šäº†äººæ ¼çš„ç±»åž‹
  - æˆ‘ä¹Ÿåœ¨æƒ³è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥æ”¹å†™æˆ â€œä½œä¸ºä¸€ä¸ª xxxâ€ï¼Œâ€œæ‰®æ¼”ä¸€å xxxâ€ï¼Œç„¶åŽåŽé¢å…¨éƒ¨ç”¨ç¥ˆä½¿å¥è¯•ä¸€ä¸‹
- â€œä½ â€ çš„è¯æ„Ÿè§‰æ˜¯å·²ç»æŒ‡å®šäº†ä¸€ä¸ªäººï¼Œè€Œç”¨ â€œè¯·ä»¥â€ çš„è¯æ„Ÿè§‰ä¼šä»Žè¯¥é¢†åŸŸä¸­æ¯ä¸ªäººä¸åŒè§†è§’æ¥æ€è€ƒï¼Œå¯èƒ½ä¼šæ›´å…¨é¢ä¸€ç‚¹ï¼Ÿ

- çœæµï¼Œå¤šç”¨è§’è‰²æ‰®æ¼”æ³•ã€‚ æ­¤äº‹åœ¨è„‘ç­‹æ€¥è½¬å¼¯ä¸­äº¦æœ‰è®°å½•ã€‚ éžæ€è€ƒæ¨¡åž‹ä½ ç›´æŽ¥æé—® è„‘ç­‹æ€¥è½¬å¼¯ç±»åž‹çš„é¢˜ç›® å¤§å¤šæ¨¡åž‹å¾ˆå¯èƒ½è¢«æ¬ºéª—ã€‚ ä½†ä½ å¼ºè°ƒè¿™æ˜¯è„‘ç­‹æ€¥è½¬å¼¯ï¼Œ è®©ä»–åŽ»æ‰®æ¼”ç±»ä¼¼é¢†åŸŸçš„é«˜æ‰‹ï¼Œ å³ä½¿æ˜¯éžæ€è€ƒæ¨¡åž‹ä¹Ÿèƒ½ç ´è§£é™·é˜±

- â€œä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ swift ç¨‹åºå‘˜â€  â†’  â€œè¯·ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„ swift ç¨‹åºå‘˜ï¼Œxxxxâ€ è¿™ä¹ˆç†è§£å¯¹å—

- è¿™äº›æŠ€å·§ç›´æŽ¥è°ƒç”¨ api æ•ˆæžœæœ€æ˜Žæ˜¾å§ï¼Ÿç½‘é¡µç‰ˆçš„å¤§æ¨¡åž‹å’Œå…¶ä»–å·¥å…·é‡Œè°ƒ APIï¼ŒåŸºæœ¬éƒ½é¢„è®¾äº†è§’è‰²ã€‚å³ä½¿ä¸ç”¨ç¬¬ä¸€äºŒäººç§°ï¼Œæ•ˆæžœæå‡ä¹Ÿä¸å¤§å§ï¼Ÿ

- ## [8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/)
  - https://github.com/lemonade-sdk/lemonade
  - Lemonade v9.0.6 came out today, making it really easy to run many models at the same time... and this is the best demo I could think of. Hope it makes someone laugh today!
  - This is a Ryzen AI MAX 395+ (aka Strix Halo). The models are between 3 and 8B parameters (size on disk is visible at the start of the video).
  - Lemonade is a free and open local LLM server made by AMD to make sure we have something optimized for AMD PCs. Today we released a new version that lets many LLMs run at the same time.
  - I made this quick HTML/CSS/JS app to demo the capability. It loads 8 LLMs, has them share a chat history, and then keeps prompting them until they vote yes or no on the user's question.
  - In the backend, there are 8 llama-server processes running on the Strix Halo's GPU. The web app talks to lemonade server at http://localhost:8000, and then lemonade routes the request to the right llama-server process based on the model ID in the request.
  - I thought it was funny the LLMs couldn't come to a final decision on this question. Just like people!

- A tie? That's no good. There should have been 9 LLMs, like the Supreme Court.

- Are they debating each other? Seems like they donâ€™t spend much time disagreeing. I want to see one where they are forced into consensus or it doesnâ€™t end (or maybe time it out and score it then)
  - Thereâ€™s 5 rounds of debate. First round they are supposed to give a hot take. Rounds 2 and 3 theyâ€™re supposed to react to each other (shared chat history). Rounds 4 and 5 theyâ€™re supposed to vote.

- ## ["We're in an LLM bubble, not an AI bubble" - Here's what's actually getting downloaded on HuggingFace and how you can start to really use AI. : r/LlamaFarm _202512](https://www.reddit.com/r/LlamaFarm/comments/1pb2wr2/were_in_an_llm_bubble_not_an_ai_bubble_heres/)
  - Encoder-only models (BERT family) account for 45% of HuggingFace downloads, nearly 5x more than decoder-only LLMs at 9.5%. 
  - Every one of these model families exists because someone realized the "one model to rule them all" approach was failing for their use case
  - This is "Mixture of Experts" at the application level. Many small, specialized models working together instead of one massive model trying to do everything.

- ## [What broke when you tried to take local LLMs to production? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p91p4k/what_broke_when_you_tried_to_take_local_llms_to/)
- Ollama broke a lot of the time because we devâ€™d on a Mac but pushed production to nvidia. Switching to vllm has largely solved this and pushed it to more of an interface rather than model problem.

- Hosting several Models on one GPU is always really annoying with vLLM. You basically have to fiddle with the memory utilisation until all models fit. There is no clean way I found to calculate memory usage. You just have two tweak, boot, look at the logs and repeat. And even if you got all models to fit, vLLM sometimes crashes with cuda out of memory exceptions when loading the models since there seems to be a peak in memory consumption on boot up.

- Why doesn't anyone mention tabbyAPI/exllamav3? It's much more memory-efficient than AWQ, also it loads quicker, and the quantization is more optimized. Also K/V cache can be quantized between 2 and 8 bits seperately (=more room for context)

- ## [Why do you use open-source LLMs ? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p16kxx/why_do_you_use_opensource_llms/)
- Freedom. I can do whatever I want without having my data stolen.

- The benefit of open source LLMs for me is that you can access the internal layers to do things like attention injection, intermediate layer feature extraction, attention map extraction, token-wise early stoppage, adaptive layer skips, conversion to latent attention, conversion to mamba or gated delta net hybrid etc
  - This is it. Intervening mathematically in something that can talk like a human is an otherworldly experience. I do it almost every day and it makes me feel like an alchemist tinkering with a homunculus. Thereâ€™s nothing else like it. Most other intellectual activities feel shallow by comparison.

- I just think it's very cool to have a summary of the vast wealth of human text and knowledge in a 30GB file on my computer.

- I want to own my capabilities. I donâ€™t want my coding skills to be tied to some external service that I have zero control over.

- ## [What do you use local LLMs for? What is your use case? : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1ms4gmz/what_do_you_use_local_llms_for_what_is_your_use/)
- They are best used for small context tasks like classification/sentiment, asking it summarize, pull out keywords, grammar, etc.
  - Not for long conversations or complex tasks, but system prompt, user request, and then get a single response.
  - For example, I've been experimenting with a local LLM checking chat messages for rule-breaking in a Discord server, flag the suspicious once, and then I checked the flagged messages and take action myself.

- I used it to generate a synthetic dataset for training my first neural network, with successful results.

- Structured data extraction from private documents (millions of documents)
  - We have produced an annotated dataset of a few thousand records. We used this to fine tune a model and evaluate performance. We are getting >0.98 F1 score which is a margin of error we are willing to tolerate given the scale and time saved... It's for a very specific type of short documents. Our extraction pipeline has multiple extraction and validation steps.

- Data extraction and classification.

- ## [What are you using your local models for ? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1oz6k5j/what_are_you_using_your_local_models_for/)
- Confidential research. In SillyTavern. Just yet with a quantized Skyfall-31b from Drummer, apparently some blend of Mistral Small or whatever. It's fun.

- Qwen3-VL-30B-A3B-Thinking is heating my home by processing video. I've posted about it before but llm-ffmpeg-edit.bash handles the logic & llm-python-vision-multi-images.py handles sending the images/frames to the LLM backend.
  - I was using Mistral 3.2 (24B dense) before Qwen3-VL got support. The speed increase from 24B -> 30B-A3B has been incredible, while maintaining accuracy.

- I'm bound by the legal terms of my employment to not discuss the technologies we use there, but am free to talk about my personal use-cases.
- STEM research assistant -- I give it my technical notes (usually physics and/or math) and a question, and get back helpful replies. My go-to is Phi-4-25B, and when it's not smart enough I escalate to Tulu3-70B, sometimes Qwen3-235B pipelined with Tulu3-70B.
- Creative writing -- Cthulhu-24B, Big-Tiger-Gemma-27B-v3, or Valkyrie-49B-v2. Mostly sci-fi (space opera or Murderbot fanfic).
- Evol-Instruct and synthetic dataset generation or augmentation -- again, mostly Phi-4-25B or Tulu3-70B, though recently I have been using Valkyrie-49B-v2 to bulk up a RAG database of technical troubleshooting advice/solutions. To my surprise Valkyrie is a lot better at this than Tulu3-70B, even though they are derived from similar models (Tulu3 from Llama-3.1, Valkyrie from Llama-3.3-Nemotron-Super-49B-v1.5 which in turn is based on Llama-3.3).
- Persuasion research -- studying the capacity for LLM inference to change people's minds. Big-Tiger-Gemma-27B-v3 is excellent at this.
- Wikipedia-backed RAG for general question-and-answer. I use Big-Tiger-Gemma-27B-v3 for this as well.
- Describing images so I can index them in a locally hosted search engine. Qwen2.5-VL-72B is still the best vision model I've yet used, but I haven't had a chance yet to compare it against Qwen3-VL-32B. I am hoping Qwen3 is better, despite having fewer than half as many parameters.
- I also run an IRC bot for a technical support channel, which is mostly GOFAI-driven but I've been working on a plugin for it to be RAG/LLM-driven too. That, too, uses Big-Tiger-Gemma-27B-v3.
- Recently I've been trying to use Phi-4 (14B) as a synthetic dataset rewriter, to salvage low-quality inferred data I would normally prune from the dataset. I read a paper suggesting even very small models (4B) are effective at this. So far my results have been mixed. I've been meaning to try Tiger-Gemma-12B-v3 as well; possibly Phi-4 just isn't the right model for this.
- GLM-4.5-Air for slow inference of entire programming projects (which I don't do much, since I don't want my coding skills to atrophy) or to find bugs in my own code.
- Qwen3-Coder-REAP-25B-A3B for fast FIM code inference. The model doesn't have to be smart to figure out what my "for"-statement is going to look like, but it does need to be fast enough that it can suggest a completion before I've finished typing the "for"-statement myself. I use the REAPed version of this model so that it fits in my GPU's VRAM (at Q4_K_M); the original 30B-A3B didn't quite fit.
- I'm also tentatively using Phi-4 as a judge, comparing two replies to the same prompt and telling me which is better. It's early days yet, for this project, and it might not be the right model for this. We will see.
- Sometimes I use Big-Tiger-Gemma-27B-v3 or Phi-4 (14B) for language translation (mostly Spanish to English, but sometimes German or Russian to English). Overall Big Tiger is better at this, though Phi-4 does surprisingly well, and is better than Big Tiger at taking situational context into account with its replies. It's also a lot faster than Big Tiger, which is sometimes important for translation tasks.

- ## [Are any of you using local llms for "real" work? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1otnj2k/are_any_of_you_using_local_llms_for_real_work/)
- The most "real" work I've done is that Qwen3-VL-30B-A3B-Thinking is currently going through videos 10-seconds at a time. 
  - Based on the bot's True/False boolean output a wrapping program keeps track of what segments `<thing I'm looking for>` is within. At the end, we're done using Qwen3-VL and the wrapping program uses the segment information to use FFMPEG to make a clipped version where `<thing I'm looking for>` should always be present.
  - A general example is you could have the bot look for every explosion in an action film. Maybe it considers muzzle flash from a gun to be an explosion, not technically wrong as far as I know. So you could prompt that it should only be explosions not from gunfire and try that.

- I use an LLM, a scraper to fetch security vulnerabilities (CVEs), and an internal API that lists my running services, and the LLM generates a daily report for me about whether any of the software I'm running might have been mentioned.

- Using qwen3 vl to parse vids to extract car plate numbers snd other vehicle markings

- I used a local LLM and a vector database to help me with my corporate taxes. I first fed all past 3 or 4 years of expenses along with their categorizations (meals, fuel, office supplies etc.) into a Postgres PG Vector database and embedded the data using the Nomic model. I then classified several hundred transactions for the past tax year using this vector set of data to categorized the transactions.

- I use nvidia/Llama3-ChatQA-1.5-8B to index 2M similar insurance docs using ollama. I load the index into Meilisearch and sell access to it. I did this after a trip to micro center and a little over $3k.

- I use them as zeroshot NER/Classification models. They're pretty decent at it out of the box. Training isn't too complicated, but for small repetitive tasks, they're often good enough.

- ## [Why we shifted to Spec-Driven Development (and how we did it) : r/ClaudeCode _202511](https://www.reddit.com/r/ClaudeCode/comments/1op8b6i/why_we_shifted_to_specdriven_development_and_how/)
  - Over the last few months we came up with our own Spec-Driven Development (SDD) flow that we feel has some benefits over other approaches out there. 
  - Specifically, using a structured execution workflow and including the results of the agent work. 
- In short: you design your docs/specs first, then use them as input into implementation. And then you capture what happens during the implementation (research, agent discussion, review etc.) as output specs for future reference. 
- The cycle is:
  - Input specs: product brief, technical brief, user stories, task requirements.
  - Workflow: research â†’ plan â†’ code â†’ review â†’ revisions.
  - Output specs: research logs, coding plan, code notes, review results, findings.
- By making the docs (both input and output) first-class artifacts, you force understanding, and traceability. The goal isnâ€™t to create a mountain of docs. 
  - The goal is to create just enough structure so your decisions are traceable and the agent has context for the next iteration of a given feature area.
- First, worth mentioning this approach really only applies to a decent sized feature. Bug fixes, small tweaks or clean up items are better served just by giving a brief explanation and letting the agent do its thing.
- How we implemented it (step-by-step)
  - Define your prd.md**:** goals for the feature, user journey, basic requirements.
  - Define your tech_brief.md: high-level architecture, constraints, tech-stack, definitions.
  - For each feature/user story, write a requirements.md file: what the story is, acceptance criteria, dependencies.
  - For each task under the story, write an instructions.md: detailed task instructions
  - To start implementation, create a custom set of commands that do the following for each task
  - Commit these spec files alongside code so future folks (agents, humans) have full context.
  - Use folder conventions: e.g., project/story/task/requirements.md, â€¦/instructions.md etc. So itâ€™s intuitive.
- Bonus: If you want a tool that automates this kind of workflow opposed to doing it yourself (input specs creation, task management, output specs), Iâ€™m working on one called Devplan that might be interesting for you.

- Bmad does exactly this: https://github.com/bmad-code-org/BMAD-METHOD

- have you tried any of BMAD, GitHub/Spec-kit or Privacy-AI/spec-kitty for a community fork with extensive git worktree support

- ## [Which model do you wish could run locally but still canâ€™t? : r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1omoodc/which_model_do_you_wish_could_run_locally_but/)
- It's not really about particular models at this point. I'm way more interested in the local infrastructure around it. The main thing I'm still missing is simple knowledge-base integration for something like Open-WebUI. I would really like to just point the front-end at my local Kiwix-server and a 1TB-eBook collection, let it index to its heart's content for a few weeks and then have any model be able to reference and integrate all those information.

- Qwen3 Omni, how come no one is talking about a 30B model that can do video and speech?

- Something which I can install with just a dmg or exe file please
  - I tried a couple but I am not tech savvy and had to install a lot of other stuff to my laptop

- For me, itâ€™s not that itâ€™s a single model I canâ€™t runâ€¦ itâ€™s the fact that I canâ€™t run multiple models.

- ## [What's the missing piece in the LLaMA ecosystem right now? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o5dh3v/whats_the_missing_piece_in_the_llama_ecosystem/)
  - For me, it's the data prep and annotation tools. The models are getting powerful, but cleaning and structuring quality training data for fine-tuning is still a major, manual bottleneck.

- Training-Data is the biggest issue for local ecosystem right now i think. There is so many datasets, but who knows about their real quality.
  - For me personally, finetuning an LLM is like 500x harder than a diffusion model, simply due to the lack of tooling. Unsloth is nice and all, but i dont want to run fucking Jupyter Notebooks, i want something akin to kohya_ss with as many of the relevant hyperparameters exposed.
- Hardware accessibility is only secondary. If you have a small Model, e.g. the Qwen3 0.6B full finetune should be possible on local hardware. If that proves to be effective, renting a GPU machine somewhere for a few bucks shouldnt be the issue.

- llms are very bad at image recognition, give it a civ or other strategy game screenshot and it gets nearly everything wrong.

- Benchmarks. There has been little to no progress in the past two years regarding how LLMs are evaluated. Itâ€™s still mostly huge catalogues of questions with predetermined answers. Thatâ€™s a very poor system for testing intelligence.
# discuss
- ## 

- ## 

- ## 

- ## 

- ## 

- ## çŽ°åœ¨æ‰€æœ‰çš„å¤§è¯­è¨€æ¨¡åž‹ï¼Œæ— è®ºå®ƒå·ç§°ä¸Šä¸‹æ–‡çª—å£å¯ä»¥å¤šå°‘ï¼Œè¾“å…¥æ˜¯çœŸçš„å¯ä»¥å¾ˆé•¿ï¼Œä½†æ˜¯è¾“å‡ºä¸èƒ½å¤ªé•¿ï¼Œè¾“å‡ºé•¿äº†å°±å¹»è§‰ä¸¥é‡ï¼Œç›¸å¯¹å¥½ä¸€äº›çš„æ˜¯ Geminiï¼Œ
- https://x.com/dotey/status/1995667479377707123
  - æ‰€ä»¥ä½¿ç”¨æ—¶ï¼Œä½ å¯ä»¥è¾“å…¥å¾ˆå¤šèµ„æ–™ç»™å®ƒå‚è€ƒï¼Œä½†æ˜¯æ¯æ¬¡ä¸è¦è¾“å‡ºå¤ªå¤šï¼Œæ¯”å¦‚ä¸€æ¬¡æœ€å¤šè¾“å‡ºå‡ åƒå­—ï¼Œå¤šäº†å°±è¦åˆ†é¡µã€‚
- å¯ä»¥é‡‡ç”¨åˆ†å‡ æ­¥ï¼Œç„¶åŽæä¾›ä¸€ä¸ªchecklistç¡®ä¿ä»–è‡ªå·±å®¡æ ¸
  - å¯¹ï¼Œtodo listæœ‰åŠ©äºŽæ‹‰å›žæ³¨æ„åŠ›å¥½ä¸€äº›ï¼Œä½†ä¸€æ ·è¾“å‡ºå†…å®¹ä¸èƒ½å¤ªé•¿

- è¯´ä¸‹æˆ‘çš„ç†è§£ï¼šæ¯ä¸ªtokenè¾“å‡ºéƒ½æ˜¯ä¸€ä¸ªæ¦‚çŽ‡ï¼Œä¸¾ä¸ªä¾‹å­ï¼šæ¯”å¦‚è¯´â€œæˆ‘â€è¿™ä¸ªå­—åŽé¢è·Ÿç€â€œä»¬â€è¿˜æ˜¯â€œçš„â€ï¼Œå¯¹llmæ¥è¯´å°±æ˜¯å–æ¦‚çŽ‡é«˜çš„ã€‚ä½†æ˜¯æ¦‚çŽ‡æœ‰2ä¸ªé—®é¢˜ï¼š
  1. å°±ç®—æ¯æ¬¡éƒ½å–æœ€é«˜çš„ï¼Œ0.9*0.9*0.9â€¦ï¼Œæ•´ä½“æ¥çœ‹ï¼Œç¬¦åˆé¢„æœŸçš„æ¦‚çŽ‡è¶Šæ¥è¶Šä½Žï¼Œæ‰€ä»¥è¾“å‡ºå¤ªå¤šä¹‹åŽæ²¡æ³•ä¿è¯ä¸èµ°å
  2. ä¸­é—´æŸä¸€ä¸ªä½ç½®ï¼Œä¸¤ä¸ªtokençš„æ¦‚çŽ‡ç›¸è¿‘ï¼Œllmé€‰æ‹©ä¸å°å¿ƒèµ°åäº†ä¹‹åŽï¼ŒåŽé¢è¾“å‡ºçš„tokenä¼šä¸€ç›´åœ¨è¿™ä¸ªèµ°åçš„tokenä¸Šç»§ç»­è¾“å‡º
  - è¡¥å……ä¸€ç‚¹ï¼ŒçŽ°åœ¨LLMæœ‰æ¸©åº¦è®¾ç½®ï¼Œä¸æ˜¯çº¯è´ªå¿ƒå–æœ€å¤§æ¦‚çŽ‡çš„ã€‚è€Œä¸”ç®—åŠ›æœ‰é™ï¼Œæ— æ³•åšåˆ°å…¨å±€æœ€ä¼˜ã€‚åªèƒ½è¯´ç¯‡å¹…è¶ŠçŸ­ï¼Œè¶Šå‡†ç¡®ã€‚ çŽ°åœ¨å¤§å®¶åšçš„CoTï¼Œä»¥åŠåˆ©ç”¨Agentic workflowä¸æ–­é‡å†™ï¼Œå…¶å®žéƒ½æ˜¯åœ¨é€¼è¿‘å…¨å±€æœ€ä¼˜ã€‚

- ä¸ªäººæ„Ÿè§‰ï¼šGemini å·ç§°æœ‰ 100 ä¸‡çš„ä¸Šä¸‹æ–‡ï¼Œå…¶å®žä¸€èˆ¬è·‘åˆ° 20-30 ä¸‡å­—å·¦å³å°±ä¸è¡Œäº†

- è¾“å…¥å’Œè¾“å‡ºå¯¹æ¨¡åž‹æ¥è¯´éƒ½æ˜¯ä¸€æ ·çš„ï¼Œåªè¦ä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œæ¨¡åž‹æˆ–å¤šæˆ–å°‘éƒ½ä¼šå‡ºçŽ°æŒ‡ä»¤ä¸è·Ÿéšã€‚

- ## â³ [A Tribute to MetaAI and Stability AI - 2 Giants Who Brought us so Much Joy... And, 2025 is the Year they Die... So Sad! : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1p9ah2v/a_tribute_to_metaai_and_stability_ai_2_giants_who/)
- People think the trillion dollar spend is for the r&d and scale. Research is a fraction of the opex, and the scale is not about breaking through new model features but to suck in as much actual human usage data as possible to train exclusively better models.

- ## [China just passed the US in open model downloads for the first time : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p7alka/china_just_passed_the_us_in_open_model_downloads/)
- This analysis includes all models on HuggingFace, not just LLMs. The most downloaded models (by far) are smaller stuff like embedding models, classifiers, VAD, etc., and small base models like BERT and GPT-2. The most popular properly large instruction-tuned LLM is Qwen2.5-VL-3B-Instruct in 24th place.
  - I suspect a lot of downloads are coming from poorly configured CI/CD pipelines and stuff, rather than individual users

- I'm surprised this hasn't happened earlier, the only good open weight models from the US in the past like 6 months has been Gpt-oss.

- How tf Germany is third place?
  - They have Black Forest Labs
  - stable diffusion guys (now bfl aka flux) are from germany
- Sentence Transformers (SBERT) is originally German. Gets an ungodly number of downloads, and because the models are small people just grab it directly from HF instead of running a local mirror.
  - (Also LAION (CLAP, some popular CLIP-ViT models) and Stable Diffusion/Black Forest Labs (FLUX), though I don't think those get nearly as many downloads.)

- Chinese users predominantly go to modelscope

- ## ðŸ’° [How are Chinese AI models claiming such low training costs? Did some research : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p6cf2p/how_are_chinese_ai_models_claiming_such_low/)
  - deepseek claims $6M training cost. Everyones losing their minds cause ChatGPT-4 cost $40-80M and Gemini Ultra hit $190M.
  - glm-4.6: $8-12M estimated, 357B parameters (thats model size)
  - Kimi K2-0905: $25-35M estimated, 1T parameters total (MoE architecture, only ~32B active at once)
  - MiniMax: $15-20M estimated, Mid-range model, mid-range cost
  - ðŸ§® Training cost = GPU hours Ã— GPU price + electricity + data costs.
  - çœŸå®žçš„æˆæœ¬æ•°å­—éƒ½åº”è¯¥æŽ¥è¿‘ä¸Šé¢çš„å…¬å¼
  - deepseeks $6M feels like marketing. You cant rent enough H100s for months and only spend $6M unless youre getting massive subsidies or cutting major corners.
  - glms $8-12M is more realistic. Still cheap compared to Western models but not suspiciously fake-cheap.
  - Kimi at $25-35M shows you CAN build competitive models for less than $100M+ but probably not for $6M.

- One difference could be the accounting methodology. I can for sure guarantee that not every training attempt is successful, and companies spend a fortune of gpu-hours on practice runs training smaller models; and then there might be a rollback or two to earlier checkpoints in the big run. Then imagine one company counting the entire cost, while the other accounts only for the end run, and boom - you got drastically different reported figures while effectively the same amount of spent money.
- In the paper for Deepseek they are actually never claiming 6 million - there saying at an assumed price per GPU hour (canâ€™t remember from the top of my head) the final run would be around 6 million.
  - OpenAI also estimated GPT-OSS final training cost like this too iirc. They just didn't for other models.

- They were very transparent about this, and have stated multiple times that it was just the final training run in that estimate and explicitly did not include prior incremental runs.

- The costs listed are likely just the literal hardware cost for the final training run for the model. Every other aspect of the model training process is ignored.

- metas new ai team are getting 9 figure salaries lol makes it hard to profit when youâ€™re paying people such insane salaries.

- Because they're leveraging Western frontier models. Let's be clear, the Chinese labs aren't doing any hard training. All they're really doing is distilling the hard work done by Western labs.
  - I remember it being reported that Western frontier models trained on copyrighted data (even pirated material).

- [Are Chinese AI models really that cheap to train? Did some research. : r/LLMDevs](https://www.reddit.com/r/LLMDevs/comments/1p77x5k/are_chinese_ai_models_really_that_cheap_to_train/)
  - Itâ€™s a combo of all those things, especially lying, plus they used a ton of distillation techniques - i.e. they trained a ton on the output of ChatGPT and other established models. 

- ## åšäº§å“çº§æœç´¢ä»¥åŽå‘çŽ° 3.7 æ•ˆæžœç‰¹å¥½ä½†æ˜¯ç‰¹åˆ«è´µï¼Œæ‰€ä»¥å¿…é¡»å¾—åš content cache é™ä½Žæˆæœ¬ã€‚
- https://x.com/arvin17x/status/1896922111505285484
  - ä½†ä¸ºäº†åš context cache ï¼Œå‘çŽ°å¿…é¡»å¾—è®°å½• token usageï¼Œä¸ç„¶æ„ŸçŸ¥ä¸åˆ° cache çš„æ•ˆæžœã€‚

- å¯ä»¥åˆ†äº«ä¸‹ Context Cache çš„åŽŸç†ä¹ˆï¼Œæˆ‘è¿˜ä»¥ä¸ºè¿™ç§ä¸œè¥¿åªèƒ½åœ¨æ¨¡åž‹ä¾›åº”å•†ä¾§åšã€‚
  - å°±æ˜¯åœ¨æ¨¡åž‹ä¾›åº”å•†ä¾§åšã€‚åªæ˜¯ OpenAI å’Œ DeepSeek åšçš„æ˜¯é™é»˜æ–¹æ¡ˆï¼Œè€Œ Anthropic å’Œ Google æ˜¯éœ€è¦å¼€å‘è€…æ‰‹åŠ¨å¼€çš„
- åº”è¯¥æ˜¯ prompt caching å§
  - Anthropicå®¶è‡ªå·±çš„è¯´æ³•æ˜¯ prompt Caching. ä½†æˆ‘æ„Ÿè§‰è¡Œä¸šé‡Œæ„Ÿè§‰è¿˜æ˜¯ä¹ æƒ¯å« context caching çš„ã€‚
  - ä¸è¿‡ OpenAI å®˜æ–¹å’Œ Anthropic å®˜æ–¹éƒ½ç§°ä¹‹ä¸º prompt caching

- å¾ˆå¥‡æ€ªï¼Œè¿™çŽ©æ„ä¸ºå•¥ä¸é»˜è®¤å¼€å¯ã€‚
  - æŸäº›åœºæ™¯ä¸‹çš„ç¡®ä¸ä¸€å®šé€‚åˆï¼Œå› ä¸ºå†™å…¥ cache çš„æˆæœ¬æ˜¯åŽŸä»·çš„ 1.25å€

- ## It is common to generate train and validation sets using random splitting.
- https://x.com/_avichawla/status/1898622288737767785
  - However, in many situations, it can be fatal for model building.
  - Consider building a model that generates captions for images.
  - Group shuffle split solves this.

- ## ç”±äºŽDeepSeek-R1 çˆ†ç«ï¼Œæ‰€ä»¥ä¸ºå¤§å®¶å¸¦æ¥ä»€ä¹ˆæ˜¯LLMè’¸é¦æŠ€æœ¯çš„ç¬”è®°ã€‚
- https://x.com/karminski3/status/1882233538042597423
  - å‡ ä¸ªåŠ©è®°è¯ï¼šæ•™å¸ˆæ¨¡åž‹ï¼Œå­¦ç”Ÿæ¨¡åž‹ï¼Œè½¯ç›®æ ‡ï¼Œç¡¬ç›®æ ‡ã€‚

- https://x.com/ShanghaoJin/status/1882679738789216456
  - ä½ å¬è¯´è¿‡ä»€ä¹ˆå«â€œè’¸é¦â€ä¹ˆï¼Ÿè¯´ä¸ªå¤§ç™½è¯ï¼šå°±æ˜¯æ‹¿äººå®¶ç®—å‡ºæ¥çš„æ¨¡åž‹å‚æ•°ï¼Œè·³è¿‡æ‰€æœ‰æ•°æ®æ¸…æ´—ã€è®­ç»ƒï¼Œåšæœ€åŽä¸€ç¨‹ã€‚å…¶å®žæ²¡æœ‰ä»»ä½•åˆ›æ–°
  - å¥½åƒäººå®¶è¯æ˜Žäº†Ï€=3.14ï¼Œä»–æ‹¿ç»“æžœåŽ»ç®—äº†åœ†é¢ç§¯ã€‚è®©ä»–å†è‡ªå·±åŽ»è¯æ˜Žç®—ä¸€ä¸ªeï¼Œä»–åˆæŠ“çžŽäº†
- åƒä¸‡ä¸è¦ç”¨â€œå¤§ç™½è¯â€æ¥è§£é‡Šè‡ªå·±éƒ½æ²¡å®Œå…¨ç†è§£çš„æ¦‚å¿µï¼Œ åªèƒ½è®©å¤–è¡Œæ‹æ‰‹ï¼Œæ‡‚çš„äººåªä¼šç¬‘è¯ä½ ã€‚ ä½ å®Œå…¨ä¸çŸ¥é“è’¸é¦æ˜¯åœ¨å¹²ä»€ä¹ˆã€‚å¦‚æžœä½ çŸ¥é“çš„è¯ï¼Œé‚£å°±æ˜¯å®Œå…¨ä¸çŸ¥é“Deepseek åœ¨åšä»€ä¹ˆã€‚
- â€œè’¸é¦â€è¯´æ³•ä¸æ­£ç¡®ã€‚1. è’¸é¦æ•ˆæžœä¸€èˆ¬ä¸ä¼šè¶…è¿‡åŽŸæ¨¡åž‹ 2. deepseekçš„ reasoningè¡Œä¸ºå’Œå¸‚é¢ä¸Šå…¶ä»–æ¨¡åž‹ä¸ä¸€è‡´(æœ‰è¶…è¶Šäººç±»æ ‡æ³¨çš„å¥‡å¦™è¡Œä¸º) 3. å¼€æ™ºå¯¹å†è®­ç»ƒæ¨¡åž‹æœ‰ç¦æ­¢å¹¶ä¼šç›‘æŽ§ APIæ»¥ç”¨

- ## æˆ‘æ—¥å¸¸ç”¨ Cursor å†™ä»£ç çš„åœºæ™¯ä¹‹ä¸€ï¼šâ€œè¯·å‚è€ƒä»£ç  @ XXX1 @ XXXn åš YYY äº‹ã€‚â€
- https://x.com/dotey/status/1869436413600731146
  - ç®€å•æ¥è¯´å°±æ˜¯è®© AI ç…§è‘«èŠ¦ç”»ç“¢ï¼Œé‡è¦çš„æ˜¯ç»™å‡ºå……è¶³çš„ä¸Šä¸‹æ–‡ï¼Œè®© AI å¯ä»¥å­¦ä¹ å’Œæ¨¡ä»¿ã€‚å‰©ä¸‹çš„å°±æ˜¯ Review + Acceptï¼Œå¾ˆç®€å•é«˜æ•ˆã€‚
  - ç‰¹åˆ«è¦æ³¨æ„çš„æ˜¯ç¬¬ä¸€ä¸ªâ€œè‘«èŠ¦â€è¦æ‰“ç£¨å¥½ï¼Œè¿™æ ·åŽç»­çš„â€œç“¢â€æ‰ä¸ä¼šç”»æ­ªã€‚

- æ›´ç®€å•çš„åšæ³•æœ‰æ—¶å€™å¯ä»¥ç›´æŽ¥ @ git æŸæ¬¡æäº¤

- æˆ‘çŽ°åœ¨æ˜¯æ–°é¡¹ç›®é‡Œåˆ›å»ºä¸€ä¸ªtxtæ–‡ä»¶ï¼Œé‡Œé¢å†™ä¸Šæƒ³æ³•å’Œgptå¯¹æˆ‘æƒ³æ³•çš„å»ºè®®ï¼Œç„¶åŽè®©cursor å‚è€ƒè¿™ä¸ªæ–‡ä»¶æ¥å¼€å‘ï¼Œé‡Œé¢æˆ‘ä¹Ÿæœ‰æ—¶ä¼šå†™ä¸Šæ­¥éª¤ï¼Œé¦–å…ˆå®žçŽ°ä»€ä¹ˆï¼Œç„¶åŽå®žçŽ°ä»€ä¹ˆï¼Œåšä¸€æ®µäº†ï¼Œè®©cursoræ ¹æ®è¿™ä¸ªæ–‡ä»¶æ£€æŸ¥ä¸€ä¸‹é¡¹ç›®å®Œæˆåº¦ï¼Œåˆ—å‡ºæ¥å“ªäº›æ²¡åšï¼Œè¿™æ ·åå¤è¿­ä»£å‘å‰

- LM å……åˆ†è¯æ˜Žäº†äººç±»çš„æœ¬è´¨å°±æ˜¯å¤è¯»æœºã€‚ å“ªé‡Œæœ‰ä»€ä¹ˆæŒ‡ä»¤éµå¾ªï¼ŒæŽ¨ç†ï¼Œå¤§å®¶éƒ½æ˜¯ä»Žä¸åŒçš„ç»´åº¦ç”¨ä¸åŒçš„æ–¹å¼åœ¨å¤è¯»ä¸€äº›ä¸œè¥¿è€Œå·²

- ## å¦‚æžœæƒ³è¦è®© LLM ç¨³å®šç”Ÿæˆ JSON å¯¹è±¡ï¼Œæœ€ç®€å•çš„æ–¹å¼å°±æ˜¯ä½¿ç”¨ zod å®šä¹‰ schema å¹¶é…åˆ @vercel ai sdkçš„ generateObjectä½¿ç”¨ï¼Œæ¯”å¦‚è¿™é‡Œæˆ‘æƒ³è¦ä»Žç½‘é¡µæ–‡æœ¬å†…å®¹æå–ç»“æž„åŒ–çš„ä¿¡æ¯ã€‚
- https://x.com/FeigelC35583/status/1819558128297648412
  - è¿™ç§æ–¹å¼å’Œå½“åˆ langchain åœ¨ prompt é‡Œå†™ä¸€å¤§å †json å®šä¹‰æœ‰æœ¬è´¨åŒºåˆ«ï¼Œåœ¨äºŽä½¿ç”¨äº† function call çš„èƒ½åŠ›
  - ä»Žè¯·æ±‚ä¸­å¯ä»¥çœ‹åˆ°ï¼Œæœ¬è´¨ä¸Šæ˜¯åœ¨è°ƒç”¨æ¨¡åž‹çš„æ—¶å€™ï¼Œæž„å»ºäº†ä¸€ä¸ªåä¸º json çš„ å‡½æ•°, æè¿°æ˜¯ respond with a json object, å…¶ä¸­å‚æ•°æ˜¯è‡ªå·±å®šä¹‰çš„ schemaï¼Œç„¶åŽåœ¨ tool_choice ä¸­é™åˆ¶å¿…é¡»è¦ä½¿ç”¨è¿™ä¸ª json å‡½æ•°ï¼Œé‚£ä¹ˆæ¨¡åž‹å°±ä¼šè¿”å›žè°ƒç”¨json å‡½æ•°çš„å‚æ•°ï¼Œå³ä½ å®šä¹‰çš„ schema
  - ç¤ºä¾‹ä»£ç æ¥è‡ªäºŽhttps://github.com/DiscovAI/DiscovAI-crawl æˆ‘æ­£åœ¨ building çš„ä¸€ä¸ªé¢å‘ RAG åº”ç”¨çš„çˆ¬è™«å¹³å°
- åº”è¯¥åªæœ‰GPTç³»åˆ—èƒ½ç”¨å§
  - æ”¯æŒfunction callå°±å¯ä»¥ï¼Œdeepseekåº”è¯¥ä¹Ÿå¯ä»¥çš„
- åœ¨è¿™åŸºç¡€ä¸Šã€‚æˆ‘ä¼šè€ƒè™‘ä½¿ç”¨jsonrepairè¿™ä¸ªåŒ…ï¼Œæ‰‹åŠ¨ä¿®å¤ä¸‹ï¼Œå¢žåŠ å®¹é”™
- å¦‚æžœå¤§æ¨¡åž‹æ²¡æœ‰æ²¡æœ‰è¿”å›žå¯¹åº”è¦æ±‚çš„å­—æ®µæ•°æ®ï¼Œæˆ–è€…è¿”å›žé”™äº†ç±»åž‹ï¼Œå®ƒä¼šæ€Žä¹ˆæ ·ï¼Œä¼šè‡ªå·±è¡¥å……ç©ºçš„ï¼Œæˆ–è€…è‡ªåŠ¨è½¬æ¢ç±»åž‹å—ï¼Ÿ
  - ä¸ä¼šè¡¥å……ï¼Œä¼šthrow errorï¼Œä¹Ÿå¯ä»¥ç”¨ä¸Šé¢æŽ¨å‹æŽ¨èçš„jsonrepairæ‰‹åŠ¨fix

- èƒ½æ”¯æŒå¼€æºæ¨¡åž‹å—
  - å–å†³äºŽæ¨¡åž‹æ”¯ä¸æ”¯æŒfunction callï¼Œæ”¯æŒçš„è¯å°±å¯ä»¥ï¼Œæ•ˆæžœçš„è¯è¦çœ‹æ¨¡åž‹çš„èƒ½åŠ›
- ç”¨ function call æ„Ÿè§‰æ¨¡åž‹çš„èƒ½åŠ›é™äº†ä¸€ä¸ªç»´åº¦ï¼Œä¸å¦‚ç›´æŽ¥ç»™æ–‡æœ¬ï¼Œæˆ‘è¿˜æ˜¯æ›´å–œæ¬¢ç”¨xmlè‡ªå·±æå–ã€‚

- æˆ‘æ˜¯ç”¨ä¼ªä»£ç âž•ç±»åž‹å£°æ˜Ž, ä¹Ÿæ˜¯ä¸€æ ·çš„ç¨³å®šè¾“å‡º json
- langchainæ¡†æž¶ä¸­æœ‰Pydantic json è§£æžå™¨å¯ä»¥ç›´æŽ¥ç”¨ï¼Œæœ¬è´¨ä¹Ÿæ˜¯ç”Ÿæˆschemaï¼Œå†é…åˆé‡è¯•è§£æžå™¨ä¹Ÿå¯ä»¥ç¨³å®šç”Ÿæˆjsonæ ¼å¼

- ## ðŸ’¡ LLMs are literally the most unreliable technology of all time (followed by **ing bluetooth)
- https://x.com/Steve8708/status/1819448686424084892
  - After an absurd amount of trial and error, we've internally created a set of rules for make LLMs considerably more reliable
  - our secrets: restrict the llm to only what rag provides

- what's your stance on AI for no-code? Do people prefer drag-and-drop vs prompting?
  - i think the winning move is combining both

- Bluetooth is hell and causes frustration daily.

- ## ðŸŒ° Firefox will use Transformers.js to power on-device features
- https://x.com/osanseviero/status/1797291569348751848
  - In their PDF Editor to generate alt text for images
  - Improve translations
  - Fully offline, open-source and with <200M models
- [Experimenting with local alt text generation in Firefox Nightly - Mozilla Hacks - the Web developer blog _202405](https://hacks.mozilla.org/2024/05/experimenting-with-local-alt-text-generation-in-firefox-nightly/)
  - https://huggingface.co/Mozilla

    - æä¾›äº†æ•°æ®é›†å’Œæ¨¡åž‹

- Offline and open-source is a big win for privacy-focused tools

- ## [langchainåˆ°åº•è¯¥æ€Žä¹ˆä½¿ç”¨ï¼Œå¤§å®¶åœ¨é¡¹ç›®ä¸­å®žè·µæœ‰æˆåŠŸçš„æ¡ˆä¾‹å—? - çŸ¥ä¹Ž](https://www.zhihu.com/question/609483833)
- LangChainä¹‹æ‰€ä»¥å¤§ç«ï¼Œæ˜¯å› ä¸ºå®ƒæä¾›äº†ä¸€ç³»åˆ—æ–¹ä¾¿çš„å·¥å…·ã€ç»„ä»¶å’ŒæŽ¥å£ï¼Œå¤§å¤§é™ä½Žäº† AI åº”ç”¨å¼€å‘çš„é—¨æ§›ï¼Œä¹Ÿæžå¤§ç®€åŒ–äº†å¤§æ¨¡åž‹åº”ç”¨ç¨‹åºçš„å¼€å‘è¿‡ç¨‹ã€‚
  - LangChainæ¡†æž¶èƒŒåŽçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†è‡ªç„¶è¯­è¨€å¤„ç†åºåˆ—åˆ†è§£ä¸ºå„ä¸ªéƒ¨åˆ†ï¼Œå…è®¸å¼€å‘äººå‘˜æ ¹æ®è‡ªå·±çš„éœ€æ±‚é«˜æ•ˆåœ°å®šåˆ¶å·¥ä½œæµç¨‹ã€‚
- Langchainæœ‰6å¤§æ ¸å¿ƒæ¨¡å—ï¼š
  - Modelsï¼šæ¨¡åž‹ï¼Œæ˜¯å„ç§ç±»åž‹çš„æ¨¡åž‹å’Œæ¨¡åž‹é›†æˆã€‚
  - Promptsï¼šæç¤ºï¼ŒåŒ…æ‹¬æç¤ºç®¡ç†ã€æç¤ºä¼˜åŒ–å’Œæç¤ºåºåˆ—åŒ–ã€‚
  - Memoryï¼šè®°å¿†ï¼Œç”¨æ¥ä¿å­˜å’Œæ¨¡åž‹äº¤äº’æ—¶çš„ä¸Šä¸‹æ–‡çŠ¶æ€ã€‚
  - Indexesï¼šç´¢å¼•ï¼Œç”¨æ¥ç»“æž„åŒ–æ–‡æ¡£ï¼Œä»¥ä¾¿å’Œæ¨¡åž‹äº¤äº’ã€‚åŒ…æ‹¬æ–‡æ¡£åŠ è½½ç¨‹åºã€å‘é‡å­˜å‚¨å™¨ã€æ–‡æœ¬åˆ†å‰²å™¨å’Œæ£€ç´¢å™¨ç­‰ã€‚
  - Agentsï¼šä»£ç†ï¼Œå†³å®šæ¨¡åž‹é‡‡å–å“ªäº›è¡ŒåŠ¨ï¼Œæ‰§è¡Œå¹¶ä¸”è§‚å¯Ÿæµç¨‹ï¼Œç›´åˆ°å®Œæˆä¸ºæ­¢ã€‚
  - Chainsï¼šé“¾ï¼Œä¸€ç³»åˆ—å¯¹å„ç§ç»„ä»¶çš„è°ƒç”¨ã€‚
- LangChain é€šå¸¸è¢«ç”¨ä½œã€Œç²˜åˆå‰‚ã€ï¼Œå°†æž„å»º LLM åº”ç”¨æ‰€éœ€çš„å„ä¸ªæ¨¡å—è¿žæŽ¥åœ¨ä¸€èµ·ã€‚ä½¿ç”¨Langchainä¸­ä¸åŒç»„ä»¶çš„ç‰¹æ€§å’Œèƒ½åŠ›ï¼Œå¯ä»¥æž„å»ºä¸åŒåœºæ™¯ä¸‹çš„åº”ç”¨ï¼Œå¦‚èŠå¤©æœºå™¨äººã€åŸºäºŽæ–‡æ¡£çš„é—®ç­”ã€çŸ¥è¯†ç®¡ç†ã€ä¸ªäººåŠ©ç†ã€Agentæ™ºèƒ½ä½“ç­‰ç­‰ã€‚

- ä½ çš„è¿™ä¸ªè®¤è¯†å­˜åœ¨ä¸€äº›åå·®ï¼Œé¦–å…ˆï¼Œä¾èµ–API key æ˜¯ä¸ºäº†ä½ ä½¿ç”¨å¤§æ¨¡åž‹åŽ‚å•†çš„æœåŠ¡å’Œé‰´æƒï¼Œè¿™æ²¡æœ‰ä»€ä¹ˆæ‹‰è·¨çš„ã€‚å¾ˆå¤šç¬¬ä¸‰æ–¹çš„æœåŠ¡éƒ½éœ€è¦é‰´æƒéªŒè¯ï¼Œè¿™æ˜¯æ¯”è¾ƒä¸»æµçš„æ–¹å¼ã€‚
- å¯ä»¥ä¼ä¸šè‡ªå·±éƒ¨ç½²å¤§æ¨¡åž‹ï¼Œè¿™ç§æˆæœ¬æ˜¯å¾ˆé«˜çš„ã€‚ä»Žæˆ‘ä»¬è‡ªå·±çš„å®žéªŒæ•ˆæžœæ¥çœ‹ï¼Œ13B ä»¥ä¸‹çš„å¤§æ¨¡åž‹åŸºæœ¬å°±æ˜¯çŽ©å…·ï¼Œä¼˜åŒ–åŠå¤©è´¹æ—¶è´¹åŠ›ï¼Œè€Œ 34B æˆ–è€…æ›´å¤§çš„æ¨¡åž‹ï¼Œå…¬å¸éƒ¨ç½²æˆæœ¬åˆå¾ˆé«˜ã€‚
- langchain ä¸­çš„ç‰¹è‰²æ˜¯å®ƒçš„ langchain expression language (LCELï¼‰ï¼Œæ˜¯ä¸€ç§ç±»ä¼¼ linux ç®¡é“å½¢å¼çš„è°ƒç”¨æ–¹å¼ï¼Œå¯ä»¥å¾ˆç®€å•çš„å®žçŽ°å®ƒçš„ chain ç›¸å…³çš„åŠŸèƒ½ã€‚è¿™ä¸ªï¼Œåœ¨æˆ‘å®žé™…ä½¿ç”¨çš„æ—¶å€™ï¼Œæ²¡æœ‰æƒ³è±¡çš„é‚£ä¹ˆå¥½ç”¨ï¼Œå¯ä»¥æ ¹æ®å®žé™…æƒ…å†µåŽ»å­¦ä¹ ã€‚
- æœ€åŽï¼Œlangchain ä¸­è¿˜æœ‰ä¸€ä¸ªå«åš langgraph çš„ç»„ä»¶ï¼Œèƒ½å¤Ÿå’Œ pytorch ä¸€æ ·ç”¨æ­ç§¯æœ¨çš„æ–¹å¼åŽ»æž„é€ ä¸€ä¸ªæœ‰å‘æ— çŽ¯å›¾ã€å¾ªçŽ¯çš„é“¾ï¼Œæ¯” LCEL æ›´é«˜çº§ã€‚

- 
- 

- ## LLMæžåç¼–è¯‘ï¼Œ.not careå’ŒJvavç”¨æˆ·å†ä¹Ÿä¸ç”¨æŠ˜è…¾ä»€ä¹ˆæ··æ·†äº†ï¼Œéƒ½æ²¡ç”¨äº†
- https://twitter.com/geniusvczh/status/1774053196039962758
  - æ–‡ç« é‡Œåç¼–è¯‘çš„æ˜¯x86, x86éƒ½å¯ä»¥ï¼ŒILéš¾åº¦åªä¼šæ›´ä½Ž
- å¤§æ¦‚çœ‹äº†ä¸€ä¸‹ï¼Œå°±æ˜¯æŠŠç¼–è¯‘å‡ºçš„æ±‡ç¼–è·Ÿæºä»£ç åšäº†ä¸€ä¸ªç®€å•çš„seq2seqçš„fine tuneï¼Œè®­ç»ƒé›†è¿žæ··æ·†éƒ½æ²¡æœ‰ï¼Œç¦»è®©æ‰€æœ‰æ··æ·†éƒ½æ²¡ç”¨é‚£æ›´æ˜¯è¿˜å·®å¾—è¿œã€‚
- 17å¹´googleé‚£ç¯‡transformerçš„è®ºæ–‡å°±é è¿™æ ·å®Œæˆäº†è‡ªç„¶è¯­è¨€çš„ç¿»è¯‘ï¼Œè¿™äº›éƒ½æ˜¯è¿Ÿæ—©çš„äº‹ï¼Œåç¼–è¯‘å’Œåæ··æ·†çš„è®­ç»ƒæ•°æ®éƒ½æ˜¯å¯ä»¥æ‰¹é‡ç”Ÿæˆçš„ï¼Œåšèµ·æ¥ç®€å•å¤šäº†
  - æˆ‘è§‰å¾—LLMå¯¹äºŽåç¼–è¯‘å’Œåæ··æ·†ï¼Œå¯èƒ½æ›´å¤§çš„ä½œç”¨åœ¨äºŽç”Ÿæˆäººç±»å‹å¥½çš„å˜é‡/ç¨‹åºç»“æž„ã€‚æ¯•ç«Ÿåç¼–è¯‘å’Œåæ··æ·†æ˜¯çŒ«é¼ æ¸¸æˆï¼Œæ€»å¯ä»¥æƒ³å‡ºæ–°ç‚¹å­ï¼Œäººç±»çš„å¹²é¢„è¿˜æ˜¯å¿…ä¸å¯å°‘çš„ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼ŒåŸºäºŽè§„åˆ™/ç¨‹åºåˆ†æžçš„ä¼ ç»Ÿæ–¹æ³•å¯èƒ½æ›´å¥½ï¼Œç„¶åŽå†ç”¨LLMçŒœå˜é‡å
- ä¸ºäº†æ‹‰èµ„é‡‘è€Œå·²ï¼Œé’±ç”³è¯·åˆ°äº†è®ºæ–‡å°±æ²¡å•¥ç”¨äº†â€¦â€¦å¤„ç†å±Žå±±ç•™ç»™å·¨å¤´çš„ç¨‹åºå‘˜å°±è¡Œäº†ï¼Œè¿˜è½®ä¸åˆ°å­¦æœ¯åœˆæ¥æŒ‡ç‚¹æ±Ÿå±±
- è¿™ç§å±€é™äºŽå‡½æ•°çš„åæ··æ·†å•¥ç”¨éƒ½æ²¡æœ‰ï¼Œå¯¹ä»˜ç‚¹ä¸‰è„šçŒ«åŠŸå¤«çš„æ··æ·†è¿˜å·®ä¸å¤š

- ## ðŸª§ ç ”ç©¶äº†ä¸€ä¸‹æœ¬åœ°å¤§æ¨¡åž‹çš„åœºæ™¯ï¼š
- https://twitter.com/changmingY/status/1773336179296887162
  1. ä¸èƒ½è”ç½‘çš„å›½å†…ç”¨æˆ·
  2. ä¸€èˆ¬ç”¨æˆ·æœºå™¨é…ç½®è¾¾ä¸åˆ°ï¼Œæ•ˆçŽ‡å¤ªå·®
  3. æœ¬åœ°çŸ¥è¯†åº“ç®—æ˜¯ä¸€ä¸ªåˆšæ€§éœ€æ±‚
  4. åž‚ç›´é¢†åŸŸæ¨¡åž‹è¶Šæ¥è¶Šå¤š, ä¸€ä¸ªhubé›†ä¸­ä½¿ç”¨
  5. å°è€Œç¾Žçš„æ¨¡åž‹ä¼šè¶Šæ¥è¶Šå¤šï¼Œå®Œæˆä¸€ä¸ªç‰¹å®šåŠŸèƒ½

- ## ollama çš„ç¼–è¯‘çŽ©çš„å¤ªèŠ±äº†ï¼Œå…ˆæ˜¯å§ llama.cpp åœ¨ä¸åŒ cpu å’Œ gpu çš„åŠ¨æ€é“¾æŽ¥åº“éƒ½ç¼–è¯‘äº†å‡ºæ¥é¿å…ç”¨æˆ·åœ¨è¿è¡Œæ—¶å†åŽ»ç¼–è¯‘ï¼Œ
- https://twitter.com/liumengxinfly/status/1767073319956971891
  - ç„¶åŽç”¨ go çš„ embed ç‰¹æ€§ç›´æŽ¥æŠŠè¿™äº›åŠ¨æ€åº“å…¨éƒ½æ‰“åŒ…åˆ° go çš„äºŒè¿›åˆ¶é‡Œï¼Œç„¶åŽåœ¨ç”¨ cgo å’Œ dlfcn åŠ è½½å’Œè°ƒç”¨ llama.cppï¼Œå®žçŽ°äº†ä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶å…ç¼–è¯‘ï¼Œå…å®‰è£…çš„è§£å†³æ‰€æœ‰é—®é¢˜

- https://twitter.com/holegots/status/1767427148506431665
  - ä¸è¿‡è¿™ä¸ªæœ¬è´¨ä¹Ÿæ˜¯ llama.cpp å¥—å£³å§ , åº•å±‚è¿˜æ˜¯ cpp, golang å¹¶ä¸å‚ä¸Žå®žé™…çš„æŽ¨ç†.

- ## æœ€æ–°ç‰ˆçš„ OpenAI Translator å·²ç»æ— ç¼æ”¯æŒæœ¬åœ°å¤§æ¨¡åž‹äº†ï¼ˆOllamaï¼‰ï¼Œæ— éœ€è”ç½‘ï¼Œå¿«é€Ÿä¾¿æ·ï¼Œå®‰å…¨ç¨³å®šï¼å†ä¹Ÿä¸æ€• OpenAI è´¦å·è¢«å°äº†ï¼ç¿»è¯‘æ•ˆæžœå¯¹æ¯”å¤§å®¶å¯ä»¥çœ‹ä¸€ä¸‹æˆªå›¾ï¼Œå¤§å®¶å¿«æ¥ä¸‹è½½ä½“éªŒä¸€ä¸‹å§ï¼ _202402
- https://twitter.com/yetone/status/1761607398819840511
- çŽ°åœ¨å¥½åƒè¿˜ä¸æ”¯æŒè‡ªå®šä¹‰æ¨¡åž‹ï¼Ÿåªæœ‰æœ‰é™çš„å‡ ä¸ªæ¨¡åž‹å¯ä¾›é€‰æ‹©ï¼Œæœ€å¥½æ˜¯æœ‰ä¸€ä¸ªæ–‡æœ¬æ¡†å¯ä»¥è‡ªå®šä¹‰è¾“å…¥
- è¿™æ˜¯Mistralå¤šå¤§çš„æ¨¡åž‹ï¼Œ7Bçš„å—ï¼Ÿ
  - æ˜¯çš„

- ä¸çŸ¥é“è¿™äº›7b 13bçš„å°æ¨¡åž‹å“ªä¸ªç¿»è¯‘è´¨é‡æ›´é«˜

- ## é˜¿é‡Œäº‘ç«Ÿç„¶æ”¯æŒè¿™ä¹ˆå¤šæ¨¡åž‹äº†
- https://twitter.com/yihong0618/status/1746745371441967540
- http://ai.azureä¹ŸåŒ…å«äº†å¥½å¤šæ¨¡åž‹ï¼Œæ˜¨å¤©æƒŠåˆ°äº†

- ## è¶Šæ¥è¶Šè§‰å¾— RAG è¿™ä¸œè¥¿æœ‰æ„æ€ã€‚
- https://twitter.com/wwwgoubuli/status/1737471851654160548
  - åŠå¹´å‰æŽ¥è§¦åˆ°è¿™ä¸ªè¯çš„æ—¶å€™å¼€å§‹æˆ‘è¿˜æœ‰äº›ä¸å±‘ï¼Œæœç´¢å†…å®¹æ’å…¥åˆ°æç¤ºè¯ç®—ä»€ä¹ˆå˜›ï¼Œå°å­¦äºŒå¹´çº§éƒ½èƒ½æ˜Žç™½ã€‚å°¤å…¶æ˜¯çœ‹åˆ°éšä¾¿ä¸¢å‘é‡åº“éƒ½èƒ½è·‘å‡ºä¸ªä¸ƒä¸ƒå…«å…«ï¼Œè¶Šå‘è§‰å¾—è¿™ä¸ªç®€å•ã€‚
  - ä½†çŽ°åœ¨çœŸçš„æžäº†åŠå¹´ï¼Œæˆ‘è¶Šå‘çš„è§‰å¾—è¿™æ‰æ˜¯ä¸‹ä¸€ä¸ªå¤§å¤šæ•°äººå¯ä»¥å‚ä¸Žçš„é£Žå£ã€‚å®ƒæœ‰é—¨æ§›ã€‚
- æŠ€å·§å¾ˆå¤š æ‰€ä»¥å¥½çŽ© ä½†é£Žé™©æ˜¯å¤§éƒ¨ä»½æŠ€å·§éƒ½è¢«æ¨¡åž‹æä¾›å•†çŽ©è¿‡ï¼Œ80%éœ€æ±‚éƒ½å¯èƒ½è¢«ä»–ä»¬ç›´æŽ¥è¦†ç›–
  - RAGä¸å°±æ˜¯query transformation/rewrite/expanding, hybrid search, reranking, etcå—ï¼Ÿå½“ç„¶è¿˜æœ‰äº›å…¶ä»–æŠ€å·§å•¥IAGä¹‹ç±»çš„ã€‚æ•°æ®ingestionä¹Ÿæœ‰äº›æŠ€å·§ï¼Œä¸è¿‡æˆ‘çœ‹ä¸»è¦è¿˜æ˜¯åœ¨queryä¸Šã€‚ è¿™äº›å¤§éƒ¨åˆ†OAI, Baichuan, æœˆä¹‹æš—é¢å†…éƒ¨éƒ½æŽ¢ç´¢è¿‡äº†å§
- RAGä¸€çœ‹å°±æ˜¯ä¸€ä¸ªæœ‰é—®é¢˜çš„åŒºåŸŸï¼Œå¤§æ¨¡åž‹éšæ—¶ä¸‹ä¸€æ¬¡å‡çº§å¯èƒ½å°±ä¼šæ”¹å˜æ•´ä¸ªæ¡†æž¶ï¼Œ3.5è¿˜èƒ¡è¯´å…«é“ï¼Œ4å·²ç»å¾ˆå¤šéƒ½æ˜¯æœ‰æ ¹æœ‰æ®çš„äº†
- æžåˆ°æœ€åŽï¼Œè¿˜æ˜¯æ¸…æ´—æ•°æ®ï¼ŒRAGåªç”¨ç®€å•ç­–ç•¥è§£å†³å¤§å¤šæ•°é—®é¢˜ï¼Œå¯è§‚æµ‹ã€‚å‰ææ˜¯æ‰€æœ‰å¤æ‚ç­–ç•¥éƒ½è¦è¯•è¿‡æ‰çŸ¥é“ã€‚

- ## LangChainå¼€æºäº†AnythingLLMï¼šå¯ä»¥ä¸Žä»»ä½•å†…å®¹èŠå¤©çš„ç§äºº ChatGPTï¼Œåº”è¯¥å°±æ˜¯ä»–ä»¬è‡ªå·±æ–‡æ¡£ç³»ç»Ÿç”¨çš„é‚£ä¸€å¥—ã€‚
- https://twitter.com/op7418/status/1733893368974073873
  - An efficient, customizable, and open-source enterprise-ready document chatbot solution.
  - https://github.com/Mintplex-Labs/anything-llm /MIT/js/python

- æœ‰æ²¡æœ‰è¯¦ç»†è¯´æ˜Žï¼Ÿæœ€å¤§å¯ä»¥æ”¯æ’‘å¤šå¤§çš„æ–‡æ¡£ï¼Ÿ
  - åº”è¯¥æ˜¯ä¸é™å¤§å°çš„ï¼Œæ‹†å¼€å°±å¥½äº†
- è¯´æ²¡è¯´ç¡¬ä»¶éœ€æ±‚ï¼Ÿ

- ## å¤§æ¨¡åž‹çš„è¿™äº› benchmark åº”è¯¥æ˜¯å…¨å®‡å®™æœ€æ²¡ç”¨çš„ benchmark äº†å§ï¼Ÿ
- https://twitter.com/yihong0618/status/1721401347533324688
- ä¹Ÿä¸æ˜¯å…¨æ²¡ç”¨ï¼Œä¹Ÿæœ‰ä¸€äº›æœ‰ç”¨çš„, å°¤å…¶ç»†åˆ†ä»»åŠ¡ä¸Šçš„ï¼Œè¿˜æ˜¯æŒºæœ‰ç”¨çš„ã€‚å½“å‰ç›¸æ¯”å…¶ä»–benchmarkï¼Œå¯æ“ä½œç©ºé—´ç¡®å®žå¤§
- å…¬å¼€çš„åªèƒ½å…¨çœ‹è‡ªè§‰

- ## ä¸­æ–‡å¼€æºæ¨¡åž‹è™½å¤šï¼Œæ•°æ®é›†å´å¾ˆå°‘å¼€æºã€‚
- https://twitter.com/9hills/status/1718828132046942218
  - ç›®å‰è‹±æ–‡ 7B è§„æ¨¡çš„ SOTA æ¨¡åž‹æ˜¯ zephyr-7b-betaã€‚å®ƒæ”¾å¼ƒäº†è´¨é‡å‚å·®ä¸é½çš„å¼€æºæ•°æ®é›†ï¼Œä½¿ç”¨ChatGPTå’ŒGPT-4 å…¨æ–°æ ‡æ³¨äº† UltraChat å’Œ UltraFeedback æ•°æ®é›†ï¼ˆå·²å¼€æºï¼‰ã€‚æ˜¯ llama-index é¡¹ç›®å®žæµ‹å‡ºæ¥å”¯ä¸€èƒ½å¤Ÿæ”¯æŒ Agent çš„å°å‚æ•°æ¨¡åž‹ã€‚
- ä¸­æ–‡æ•°æ®é›†éƒ½æ˜¯æ‹¿æ¥å–é’±çš„
