---
title: lib-ai-app-community-model
tags: [ai, community]
created: 2023-10-30T07:33:56.233Z
modified: 2023-10-30T07:34:03.602Z
---

# lib-ai-app-community-model

# guide

- tips
  - å¤§æ¨¡å‹ç›¸å…³çš„äº§å“ç ”å‘ï¼ŒåŸç†çš„å¯è§£é‡Šæ€§å¾ˆå·®ï¼Œæ•ˆæœçš„å¯è§£é‡Šæ€§ä¹Ÿå·®

- model-features
  - ğŸ¤¼: speed, quality, size
  - reasoning/thinking
  - tool-use/function-call
  - vision
  - embedding
  - moe
  - æ³¨æ„æœ‰äº›ç¤¾åŒºé‡åŒ–çš„æ¨¡å‹å¯èƒ½é—æ¼æ ‡æ³¨äº†éƒ¨åˆ†features, å¯åœ¨æœ¬åœ°æµ‹è¯•æ¥ç¡®å®šæ˜¯å¦æ”¯æŒ

- ç§»åŠ¨ç«¯å¤§æ¨¡å‹
  - å‚è€ƒgoogle-gemma-1b

- [å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼šä»ç†è®ºåˆ°å®è·µ](https://intro-llm.github.io/)
  - å¤æ—¦å¤§å­¦å¼ å¥‡æ•™æˆå›¢é˜Ÿå†™äº†ä¸€æœ¬åœ¨çº¿å…è´¹çš„ç”µå­ä¹¦ï¼Œå¤§æ¦‚æœ‰ 300 é¡µç¯‡å¹…ï¼Œå°†å¤§æ¨¡å‹ä»ç†è®ºåˆ°å®æˆ˜çš„æ¯ä¸ªé˜¶æ®µéƒ½æè¿°çš„è¾ƒä¸ºæ¸…æ¥š
# discuss-stars
- ## 

- ## ğŸ“Œ LLM å‡ºæ¥ä¹‹åï¼Œåœ¨åº”ç”¨å±‚çš„æŠ˜è…¾ä»æœªåœæ­‡ã€‚ä» Prompt è°ƒä¼˜åˆ° Workflow é…ç½®ï¼Œå†åˆ° Agent æ„å»ºï¼Œæœ€ç»ˆç›®çš„éƒ½æ˜¯ä¸€æ ·çš„ï¼šè®© LLM æ›´å¥½åœ°ä¸ºäººç±»å¹²æ´»ï¼ŒæŠŠæœºå™¨çš„æ€§èƒ½å‹æ¦¨åˆ°æè‡´ã€‚
- https://x.com/Barret_China/status/1973188130091180466
- å¯¹ LLM çš„å‹æ¦¨ï¼Œå¯ä»¥åˆ†ä¸ºä¸¤ä¸ªç»´åº¦ã€‚
- ä¸€æ˜¯å¸®åŠ©å®ƒæ‰¾åˆ°æœ€ä¼˜ç®—æ³•ï¼Œè®©æ¨ç†å°‘èµ°å¼¯è·¯ã€‚
  - ä¸ºæ­¤æˆ‘ä»¬å‡ ä¹æŠŠèƒ½æƒ³åˆ°çš„è·¯å­éƒ½èµ°äº†ä¸€éï¼Œè®© LLM å­¦ä¼šåæ€ï¼ˆreflectionã€self-consistencyã€self-criticsï¼‰ï¼Œå­¦ä¼šæ¨ç†å’Œè§„åˆ’ï¼ˆreasoningã€planningã€chain-of-thoughtã€tree-of-thoughtï¼‰ï¼›å­¦ä¼šè®°å¿†ï¼ˆshort-term memoryã€long-term memoryï¼‰ï¼Œä¸è‡³äºå¯¹è¯ä¸€é•¿å°±å¤±å¿†ï¼›å­¦ä¼šæ‰¾çŸ¥è¯†ï¼ˆRAGã€knowledge graphï¼‰ï¼Œåœ¨å¤–éƒ¨ä¸–ç•Œé‡Œè¡¥å……äº‹å®ï¼›å­¦ä¼šæ„å»ºä¸Šä¸‹æ–‡ï¼ˆcontext buildingï¼‰ï¼Œåœ¨æœ‰é™ token é‡Œå¡ä¸‹æ›´å¤šæœ‰æ•ˆä¿¡æ¯ï¼›å­¦ä¼šç”¨å·¥å…·ï¼ˆtool-useï¼Œfunction callingï¼ŒMCPï¼‰ï¼ŒæŠŠäº‹æƒ…äº¤ç»™å¤–éƒ¨ç¨‹åºå»è·‘ï¼Œè€Œä¸æ˜¯å…‰é è‡ªå·±ç”Ÿæˆï¼›ç­‰ç­‰ã€‚
  - è¿™äº›ä¸œè¥¿ï¼Œè¯´åˆ°åº•éƒ½æ˜¯æŠ€å·§å’Œæœºåˆ¶ï¼Œæœ¬è´¨ç›®çš„æ˜¯è®© LLM æ›´å¿«ç†è§£äººç±»è¦å¹²å•¥ï¼Œå›´ç»•ç›®æ ‡ï¼ˆgoal-orientedï¼‰å°½å¯èƒ½æ‰¾åˆ°ä¸€æ¡ä»£ä»·æœ€å°çš„è·¯ï¼Œè·‘åˆ°æœ€ä¼˜è§£ä¸Šå»ã€‚
- ç¬¬äºŒä¸ªç»´åº¦ï¼Œæ˜¯å¯¹æ—¶é—´çš„å‹æ¦¨ï¼Œè®© LLM å¯ä»¥åšåˆ° 7Ã—24 å°æ—¶ä¸åœæ­‡ã€‚
  - å½“æˆ‘ä»¬å¯¹ LLM æœ‰äº†æ›´æ·±å…¥çš„ç†è§£ä¹‹åï¼Œå¾ˆå®¹æ˜“æƒ³åˆ°æŠŠå®ƒæ‰“é€ æˆå±äºè‡ªå·±æˆ–ç»„ç»‡çš„â€œæ•°å­—å‘˜å·¥â€ï¼Œå®ƒä¸çŸ¥ç–²æƒ«ã€ä¸ä¼šæŠ±æ€¨ï¼Œå¯ä»¥æŒç»­è¿è½¬ã€ä¸æ–­å­¦ä¹ ã€‚
- å¤§éƒ¨åˆ†äººä»Šå¤©ç”¨ AI çš„æ–¹å¼ï¼Œè¿˜åœç•™åœ¨æŸ¥èµ„æ–™ã€æ€»ç»“å†…å®¹ã€å†™å‘¨æŠ¥æœˆæŠ¥è¿™äº›å•ç‚¹åœºæ™¯ä¸Šï¼Œå¦‚æœè¦çœŸæ­£æ„å»ºä¸€åâ€œä¸åœæ­‡çš„ AI æ•°å­—å‘˜å·¥â€ï¼Œå…‰é è¿™äº›è¿˜ä¸å¤Ÿã€‚æˆ‘ä»¬éœ€è¦å…ˆè§„åˆ’å‡ºå±äºè‡ªå·±çš„ AI æ•°å­—å·¥å‚ â€”â€”æƒ³æ¸…æ¥šè¦é€ å‡ºæ¥çš„â€œäº§å“â€æ˜¯ä»€ä¹ˆï¼Œæ˜¯æ²‰æ·€çŸ¥è¯†çš„ç³»ç»Ÿï¼Œæ˜¯è‡ªåŠ¨åŒ–çš„ä¸šåŠ¡æµç¨‹ï¼Œè¿˜æ˜¯ä¸€ä¸ªå¯ä»¥é•¿æœŸè¿­ä»£çš„æœåŠ¡ã€‚
  - åœ¨è¿™åº§å·¥å‚é‡Œï¼ŒAI æ˜¯ç”Ÿäº§çº¿ä¸Šçš„æ‰§è¡Œè€…ï¼Œå®ƒè´Ÿè´£å…·ä½“çš„åŠ å·¥ä¸äº§å‡ºï¼›è€Œäººç±»çš„è§’è‰²å‘ç”Ÿäº†è½¬å˜ï¼Œä»â€œäº²è‡ªå¹²æ´»çš„å·¥äººâ€å˜æˆâ€œç›‘å·¥ä¸ç®¡ç†è€…â€ã€‚ äººç±»ä¸å†äº²æ‰‹å®Œæˆæ¯ä¸€æ­¥ï¼Œè€Œæ˜¯è¦è®¾è®¡æµæ°´çº¿ï¼Œè®¾å®šè§„åˆ™ï¼Œåˆ¶å®šæŒ‡æ ‡ï¼Œç›‘æ§è´¨é‡ï¼Œå¹¶åœ¨éœ€è¦æ—¶è°ƒåº¦èµ„æºã€‚æ¢å¥è¯è¯´ï¼ŒAI çš„ä»·å€¼ä¸åœ¨äºæ›¿æˆ‘ä»¬â€œå¹²ä¸€ç‚¹æ´»â€ï¼Œè€Œåœ¨äºå¸®æŠŠæ•´æ¡æµæ°´çº¿è·‘èµ·æ¥ï¼Œè€Œäººç±»æ›´åƒæ˜¯â€œæ•°å­—å·¥å‚çš„ç®¡ç†è€…â€ã€‚
  - å½“è¿™ä¸¤ä¸ªç»´åº¦ç»“åˆèµ·æ¥æ—¶ï¼ŒçœŸæ­£çš„æ‹ç‚¹å°±å‡ºç°äº†ã€‚LLM ä¸å†åªæ˜¯ä¸€ä¸ªå†·å†°å†°çš„å·¥å…·ï¼Œè€Œæ˜¯é€æ¸å˜æˆäº†å¯ä»¥é•¿æœŸåä½œçš„ä¼™ä¼´ã€‚å®ƒæ—¢èƒ½æ‰¿æ‹…é‡å¤æ€§åŠ³åŠ¨ï¼Œä¹Ÿèƒ½åœ¨å¤æ‚é—®é¢˜ä¸Šæä¾›æ´è§ã€‚å®ƒä¸ä»…ä»…æ˜¯â€œå¸®ä½ åšäº‹â€ï¼Œæ›´æ˜¯â€œå’Œä½ ä¸€èµ·åšäº‹â€ã€‚
  - æœªæ¥çš„å·®è·ï¼Œä¸åœ¨äºè°èƒ½å†™å‡ºæ›´æ¼‚äº®çš„ Promptï¼Œè€Œåœ¨äºè°èƒ½æŠŠ LLM çœŸæ­£èå…¥åˆ°è‡ªå·±çš„æ—¶é—´å’Œç»„ç»‡é‡Œï¼Œå½¢æˆç¨³å®šçš„ç”Ÿäº§æ–¹å¼ã€‚
  - å› æ­¤ï¼Œä¼šä¸ä¼šç”¨ã€ç”¨åˆ°ä»€ä¹ˆæ·±åº¦ã€èƒ½å¦æŒç»­ä¼˜åŒ–ï¼Œè¿™äº›æ‰æ˜¯é•¿æœŸçš„ç«äº‰åŠ›æ¥æºã€‚è°èƒ½æŠŠ AI è¿è¡Œæˆâ€œå·¥å‚â€ï¼Œè®©è‡ªå·±ä»æ‰§è¡Œè€…è½¬ä¸ºç›‘å·¥å’Œç®¡ç†è€…ï¼Œè°å°±èƒ½åœ¨æœªæ¥çš„æ—¥å¸¸å·¥ä½œå’Œä¸šåŠ¡ä¸­ï¼Œè·å¾—çœŸæ­£å¯å¤ç”¨ã€å¯ç´¯ç§¯çš„ä¼˜åŠ¿ã€‚

- ä»£ç æŠ½è±¡çœŸå®ä¸–ç•Œçš„æ—¶ä»£å¿«è¦ç»“æŸäº†ï¼Œæ¬¢è¿æ¥åˆ°token æŠ½è±¡çœŸå®ä¸–ç•Œçš„æ—¶ä»£ã€‚

- å¦‚æœTOKENæŠ•å…¥äº§å‡ºæ˜¯æ­£å‘çš„ï¼Œæ¨ä¸å¾—ä»–ä¸ä¼‘æ¯ 

- é—®é¢˜æ¥äº†ï¼Œæˆ‘è¦åšä»€ä¹ˆï¼ŒAIè¦æ€ä¹ˆå¸®æˆ‘åš

- ## ğŸ“± [ç«¯ä¾§æ¨¡å‹ä¼šæ˜¯ AI æŠ€æœ¯æ¼”è¿›çš„ä¸‹ä¸€ä¸ª ã€Œå¿…äº‰ä¹‹åœ°ã€ å—ï¼Ÿå½“å‰è½åœ°é¢ä¸´å“ªäº›æ ¸å¿ƒç“¶é¢ˆï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/1914319403023032351)
- TO Cåœºæ™¯åº”ç”¨æœ€å¤§ç‰¹ç‚¹æ˜¯ç¡¬ä»¶å‚å·®ä¸é½ã€‚æ€§èƒ½å·®èƒ½å¤§åˆ°10å¤šå€ä»¥ä¸Šã€‚
  - æŠ€æœ¯æ ˆå¿…é¡»æ»¡è¶³æ‰€æœ‰ä¸»æµç¡¬ä»¶ç»“æ„å·®å¼‚å’Œæ€§èƒ½æ€§èƒ½ä¸‹ï¼Œä¿æŒç›¸å¯¹ä¸€è‡´çš„ä½¿ç”¨ä½“éªŒã€‚è¿™æ˜¯ä¸€ä»¶æç´¯çš„æ´»å„¿ã€‚
  - æœåŠ¡ç«¯ä¸‹ä¸ªå¼€æºæ¨¡å‹æ­ä¸ªWEBæœåŠ¡å†™ä¸ªHTMLå°±èƒ½å–æœåŠ¡äº†ã€‚ç«¯ä¾§æƒ³å†…åµŒAIæ¨¡å‹äº§å“åŒ–ï¼ŒåŒæ—¶è¿˜è¦è§£å†³å®æ—¶æ€§ï¼Œè¦è§£å†³çš„å·¥ç¨‹é—®é¢˜è¦å¤š10å€ä¸æ­¢ã€‚è¦è‡ªå·±ä¼˜åŒ–æ¨¡å‹ã€‚ç”šè‡³è¦è‡ªå·±æ­å»ºæ¨¡å‹ç»“æ„ï¼Œè‡ªå·±å‡†å¤‡æ•°æ®è®­ç»ƒã€‚
- ç›®å‰çœ‹ç«¯ä¾§å®Œå…¨è°ˆä¸ä¸ŠAIæ¼”è¿›å¿…äº‰ä¹‹åœ°ã€‚ä¸€æ¥ç¡¬ä»¶è¾ƒå¼±ä¸”å‚å·®ä¸é½ï¼Œå¯¼è‡´TO Cå·¥ç¨‹ä¿éšœä½“éªŒæåº¦å›°éš¾å’Œå¤æ‚ã€‚äºŒæ¥PCé«˜ç®—åŠ›ç¡¬ä»¶å®Œå…¨ä¸æ˜¯å¤§ä¼—æ¶ˆè´¹çº§çš„å”®ä»·å’Œå®šä½ï¼Œæœ€åï¼Œè¿˜æ²¡æœ‰ä»€ä¹ˆç«¯ä¾§AIè½åœ°äº§å“çš„åº”ç”¨èŒƒå¼ã€‚
  - ä»åº”ç”¨ä½“éªŒä¸Šï¼Œé‚£äº›å…è®¸å»¶è¿Ÿ500æ¯«ç§’ä»¥ä¸Šï¼Œå¯¹å¸¦å®½éœ€æ±‚ä¸å¤§çš„åº”ç”¨ï¼Œéƒ½å¯ä»¥æ”¾åˆ°äº‘ç«¯ã€‚
- çœŸéœ€è¦åšåˆ°ç«¯ä¾§çš„ï¼Œå…¶å®æ˜¯é‚£äº›éœ€è¦å³æ—¶å“åº”çš„åº”ç”¨åœºæ™¯ã€‚
  - ä¸ºæ•°ä¸å¤šçš„åº”ç”¨ç±»åˆ«ï¼Œæ¯”å¦‚å³æ—¶äº¤äº’å½±åƒç›¸å…³ï¼Œè¯­éŸ³ç›¸å…³çš„ï¼Œè¯¸å¦‚æ¸¸æˆï¼Œéœ€è¦å³æ—¶äº¤äº’çš„æ•°å­—äººï¼Œ3Dè™šæ‹ŸAIäººï¼Œè™šæ‹Ÿäººç›´æ’­ç­‰ï¼Œå¯èƒ½ä¼šå¯¹ç«¯ä¾§AIæœ‰éƒ¨åˆ†éœ€æ±‚
- è¿™ç±»åº”ç”¨æœ‰3ä¸ªç‰¹å¾ï¼š 
  - 1 å³æ—¶æ€§ç›´æ¥å½±å“ä½“éªŒï¼Œä¸èƒ½ç­‰ã€‚ 
  - 2 å¯¹ç®—åŠ›è¦æ±‚ç›¸å¯¹ä½ï¼Œè¿ç®—æœ¬åœ°ç®—åŠ›èƒ½å¤Ÿæ»¡è¶³ï¼Œå¹¶ä¸ä¸€å®šéè¦äº‘ç®—åŠ›æ”¯æŒã€‚ 
  - 3 å¯¹å¸¦å®½è¦æ±‚è¾ƒå¤§ï¼Œäº‘æœåŠ¡çš„æˆæœ¬è¿‡é«˜ä½¿å¾—å•†ä¸šæ¨¡å¼ä¸æˆç«‹ï¼Œæ‰€ä»¥éœ€è¦æ”¾å…¥æœ¬åœ°ã€‚
- MOBILEç¦»ç«¯ä¾§å¯èƒ½æ›´è¿œã€‚ ç”±äºç®—åŠ›æä¸ºä½ä¸‹ï¼Œåªæœ‰å¾ˆå°‘çš„æƒ…å†µæ‰éœ€è¦æ‰‹æœºä¸Šå³æ—¶å“åº”AIæ¨ç†ç»™å‡ºçš„ç»“æœã€‚å³é‚£äº›æŠŠæ‰‹æœºä½œä¸ºä¿¡å·ä¼ æ„Ÿå™¨ï¼Œå³æ—¶å¯¹ä¿¡å·è¿ç®—çš„åœºåˆï¼šæ¯”å¦‚è¡¨æƒ…æ•æ‰ã€æ‰‹åŠ¿ã€è‚¢ä½“åŠ¨ä½œæ•æ‰å’Œè¯†åˆ«ï¼Œè¯­éŸ³ã€è¿åŠ¨æ•°æ®å¤„ç†å’Œç”Ÿç‰©ä¿¡å·å¤„ç†ç­‰ã€‚ 
  - å¦‚æœæ‰‹æœºæ²¡æœ‰ä¸“ç”¨ç¥ç»/å¼ é‡èŠ¯ç‰‡ï¼ŒGPUè¿˜éœ€è¦æ‰¿æ‹…æ¸²æŸ“3Då›¾å½¢ï¼Œé‚£ä¹ˆèƒ½è·‘çš„AIæ¨¡å‹ä¼šæ›´åŠ å—é™ã€‚
  - ç›®å‰æµ‹è¯•ï¼ŒæŸæ‰‹æœºç”¨NPUè·‘æ¨¡å‹æ¯”ä¸ç”¨æœ€å¤šèƒ½å¿«10å€ã€‚ 
  - æ‰‹æœºèŠ¯ç‰‡å•†æ˜¯å¦æœ‰åŠ¨åŠ›å‘å±•ï¼Œè¦çœ‹æ‰‹æœºä½œä¸ºæ•°æ®ä¼ æ„Ÿå™¨å³æ—¶å¤„ç†æ•°æ®èƒ½å¸¦æ¥å¤šå¤§çš„åº”ç”¨å¸‚åœºã€‚

- æœ¬åœ°éƒ¨ç½²æ¨¡å‹çš„ä¼˜åŠ¿åœ¨äºä½å»¶è¿Ÿï¼Œè¿™åœ¨æˆ‘çœ‹æ¥å…¶å®ä¹Ÿæ˜¯ä¸ªç›¸å¯¹åä¼ªçš„ä¼˜åŠ¿ã€‚
  - äº‹å®ä¸Šï¼Œç°ä»£ç½‘ç»œç¯å¢ƒå®é™…ä¸Šä»¥åŠè¶³å¤Ÿå¿«é€Ÿå’Œç¨³å®šï¼Œæ— è®ºæ˜¯æµé‡è¿˜æ˜¯WiFiï¼Œè¾…ä»¥CDNè¾¹ç¼˜èŠ‚ç‚¹ä¼˜åŒ–åï¼Œç»å¤§å¤šæ•°ä¸»æµAIåº”ç”¨çš„äº‘ç«¯å“åº”éƒ½èƒ½ç¨³å®šåœ¨å‡ åæ¯«ç§’ä»¥å†…ã€‚
- ç«¯ä¾§è®¾å¤‡å—é™äºåŠŸè€—ã€çƒ­é‡ã€ç®—åŠ›ç­‰ç‰©ç†æ¡ä»¶ï¼Œå¾€å¾€åªèƒ½éƒ¨ç½²è½»é‡åŒ–æ¨¡å‹

- ç§»åŠ¨è®¾å¤‡ä¸Šé•¿æœŸè¿è¡Œä¸€ä¸ªæœ‰ç«äº‰åŠ›çš„å¤§æ¨¡å‹ä»ç„¶ä¸ç°å®ï¼Œè´Ÿè½½ã€èµ„æºå ç”¨ã€ç”µé‡æ¶ˆè€—ã€å‘çƒ­ç­‰ç­‰é—®é¢˜ï¼Œå¾ˆéš¾åšåˆ°å¥½çš„ä½“éªŒï¼Œæ›´ä¸è¦è¯´ç‰©è”ç½‘è®¾å¤‡ã€æ™ºèƒ½éŸ³ç®±ç­‰ä½åŠŸè€—äº§å“äº†ã€‚
  - å¾ˆå¤šå£°ç§°è‡ªå·±ç”¨äº†ç«¯ä¾§å¤§æ¨¡å‹çš„ï¼Œå®é™…ä¸Šä»ç„¶ç¦»ä¸å¼€äº‘ç«¯çš„ååŒé…åˆï¼Œæˆ–è€…è¯´ä¸»è¦é äº‘ç«¯ï¼Œå¤‡ç”¨æ–¹æ¡ˆå¯èƒ½æ˜¯ç«¯ä¾§ï¼Œä½†æŠŠç«¯ä¾§æ‹¿å‡ºæ¥å¤§è‚†å®£æ‰¬ï¼Œæ¥åšå¹¿å‘Šè€Œå·²ã€‚

- ğŸ› ç«¯ä¾§æ¨¡å‹çš„åŠ£åŠ¿
  - ç«¯ä¾§æ¨¡å‹ä»ç„¶å—é™äºè®¾å¤‡æœ¬èº«çš„å†…å­˜ã€åŠŸè€—å’Œç®—åŠ›
  - äº‘ç«¯æ¨¡å‹å¯ä»¥éšæ—¶å‡çº§ï¼Œçƒ­æ›´æ–°æœºåˆ¶ä¿è¯æ‰€æœ‰ç”¨æˆ·ç¬¬ä¸€æ—¶é—´äº«å—æ–°ä¸€ä»£æ¨¡å‹
  - ç«¯ä¾§æ¨¡å‹å¯¹äºä¸åŒè®¾å¤‡å’Œç¡¬ä»¶æ¶æ„éœ€è¦åˆ†åˆ«é€‚é…

- ç«¯ä¾§é€‚åˆçš„åœºæ™¯è¿˜æ˜¯éšç§ã€æä½å»¶è¿Ÿï¼Œæˆ–è€…å…¨å¤©å€™ã€‚

- å¦‚æœå®Œå…¨è·‘åœ¨ç«¯ä¾§ï¼Œé‚£å•†ä¸šæ¨¡å¼æ€ä¹ˆåšï¼Ÿç°åœ¨çš„ä¸»æµæ˜¯è®¢é˜…åˆ¶ï¼Œä½†å®Œå…¨æœ¬åœ°çš„è®¢é˜…åˆ¶ï¼Œå¸å¼•åŠ›æ¯”è¾ƒä½ï¼Œç ´è§£é£é™©æ¯”è¾ƒé«˜å§

- ## deepseek 3fs å…¶å®ä¸æ˜¯å¾ˆç†è§£è¿™æ ·çš„æ„ä¹‰æ˜¯å•¥â€¦ å·²ç»è„±ç¦» FS çš„é€šç”¨æ¥å£äº†ï¼Œä¸ºå•¥è¦ç¡¬æŒ‚ä¸€ä¸ª FUSE VFS å±‚â€¦ ç›´æ¥æ‘’å¼ƒ VFS èµ°ä¸ªç§æœ‰çš„ protocol ä¸å¹²å‡€å¤šäº†â€¦
- https://x.com/silsrc/status/1895390926098571505
- > Most applications use FUSE client, which has a low adoption barrier. Performance-critical applications are integrated with the native client.

- æˆ‘çš„ç¬¬ä¸€ååº”æ˜¯ç°æœ‰æ¡†æ¶åº”è¯¥æ²¡åŠæ³•å¤§æ‰¹é‡æ”¹ç”¨ç§æœ‰çš„ APIï¼Œæ¯”å¦‚åœ¨ Python æˆ‘å°±æ˜¯è¦ç”¨ pathlib. Path å¯¹æ–‡ä»¶åšç‚¹ç®€å•æ“ä½œï¼Œé‚£ç¡®å®åªèƒ½æ˜¯æŒ‚ FUSE ä¸Šå»äº†

- ## èŠä¸€èŠå›½å†…å¤§æ¨¡å‹çš„å®‰å…¨æœºåˆ¶ï¼š ä¸€èˆ¬æ˜¯ä¸¤å¥—ï¼Œåˆ†åˆ«ä½œç”¨äºtrain-timeå’Œtest-timeã€‚
- https://x.com/9hills/status/1840786446153921017
  - è®­ç»ƒçš„æ—¶å€™å¢åŠ å®‰å…¨å’Œä»·å€¼è§‚å¯¹é½çš„SFTå’Œåå¥½å¯¹é½æ•°æ®ã€‚æœ€ç»ˆæ•ˆæœç±»ä¼¼å¼€æºçš„Qwen2æ¨¡å‹ï¼Œæœ‰ç‚¹ç”¨ä½†æ˜¯å¾ˆå®¹æ˜“è¢«Jailbreakã€‚
  - æ¨ç†æ—¶å¢åŠ å®‰å…¨ç®—å­ï¼Œå…·ä½“æœ‰å‡ ç§
- æŠ€æœ¯éš¾ç‚¹æœ‰ä¸¤ä¸ªï¼š
  1. è®­ç»ƒåˆ†ç±»å™¨çš„å¤§é‡éå®‰å…¨æ•°æ®ï¼Œæ‰€ä»¥è¯´ä½ å…ˆæˆä¸ºåè´¼æ‰èƒ½è¯†åˆ«åè´¼ã€‚
  2. æ¨¡å‹è¦åšçš„è¶³å¤Ÿå°è¶³å¤Ÿå¿«ï¼Œæœ€å°åŒ–å½±å“æ¨¡å‹ttftå’Œtpsã€‚
  3. æµå¼å¾ˆéš¾ï¼ŒæŸæ¨¡å‹æœ€æ—©æ˜¯ä¸€å¥å¥è¾“å‡ºçš„ï¼Œåæ¥æ‰æ”¹æˆtokençº§æµå¼ã€‚
- è¯·æ•™ä¸€ä¸‹ï¼Œç°åœ¨å¸‚åœºä¸Šçš„å¤§æ¨¡å‹ï¼Œæ€ä¹ˆçŸ¥é“ä»–ä»¬çš„è®­ç»ƒæ•°æ®æ˜¯å¤šä¹…çš„å‘¢ï¼Ÿ
  - å¯ä»¥é—®ä¸€äº›ç‰¹å®šæ—¶é—´çš„æ–°é—»æ¥éªŒè¯ï¼Œä½†æ˜¯å…¶å®æ²¡å…³ç³»ã€‚æ¨¡å‹çš„ç²¾ç¡®çŸ¥è¯†ä¸é‡è¦ï¼Œä¹Ÿå……æ»¡å¹»è§‰ã€‚

- ## Attention is *Not* All You Needï¼Œè¿˜æ˜¯æœ‰äººåœ¨å°è¯• transformer ä»¥å¤–çš„æ¶æ„ï¼Œ
- https://x.com/liumengxinfly/status/1835251398692508114
  - æ¯•ç«Ÿ transfermor æ¨ç†å¤æ‚åº¦åœ¨æ•°å­¦ä¸Šæ˜¯æ— æ³•çº¿æ€§æ‰©å±•çš„ï¼Œæ—©æ™šä¼šèµ°åˆ°ç“¶é¢ˆ

- ## å›½äº§188ä¸ªå¤§æ¨¡å‹çš„excelæ–‡æ¡£ï¼š åŒ—äº¬69 ä¸Šæµ·22 æ­å·15 å¹¿ä¸œ26ä¸ª æ±Ÿè‹15ä¸ª
- https://twitter.com/FinanceYF5/status/1730912502312296935
  - [å›½äº§å¤§æ¨¡å‹188ä¸ªlist - Feishu Docs](https://zw73xyquvv.feishu.cn/wiki/WXLmwBbYuiTobkkJ6Ojc2cxqnj0?sheet=2XjJlJ&table=tblS2Jv7isKtSODz&view=vewfCdOf0U)

# discuss-llm-architecture
- ## 

- ## 

- ## 

- ## [Q: When will there be fast and competent SLMs for laptops? : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pcurp8/q_when_will_there_be_fast_and_competent_slms_for/)
  - Qwen3-30B-A3B and GPT-OSS-20B both uses Mixture-of-Experts instead of dense layers for their SLM
  - Kimi-Linear and Qwen3-Next-80B-A3B moved along to use "mixed attention" (majority of layers with linear attention) to speed things up AND have longer contexts
  - Not enough people getting into ternary attention like BitNet a4.8 / BitNet v2 or ternary quantization (PTQ)
  - Whatever layer routing is to reduce the amount of RAM needed, including Ouro-2.6B-Thinking these days and Mixture-of-Depths back in 2024
  - Are all of these different techniques conflicting with one another? If it is just a lack of funding for fine-tuning/modding an existing SLM into something fast (assuming QAFT and RL), how much would it cost to crowdfund a project like this?

- It depends on your standards, I believe that for the average Joe, something like Ling Mini 2.0 would already check those requirements (fast -> 1B active is doable for most modern laptops at 20+ tok/s) (Competent -> 16B total parameters makes it decent enough for 99% of the tasks an average person would likely use it for)

- For most people, Llama3 8B was the moment where their laptop could handle a ton of their work and queries locally.

- I use Ling mini to correctly format the ocr result of screenshots. Its the fastest and adheres well to long system prompt. All on cpu.

- We already have that. Llama 3.2 3B for writing. Gemma 3 for multimodal. Qwen 3 4B for stem. Granite 4 for rag.
  - The issue is that the active/ dense parameter count determines the limit of how intelligent the overall model is. The mixture of experts kinda determines how big the model's encyclopaedia is.
  - Its possible to argue that llama 3.2 3B, gemma 3 4B and Qwen 3 30b are around the same level in writing. But Qwen 3 30b is clearly the one with more knowledge.
  - There is also the requirement for AI to have ethics and emotions, which is frankly way too complex to fit into an SLM without lobotomising it.
  - Laptop ram is just too limited in bandwidth and capacity

- I am tempted to suggest RAG-ing an SLM into being better than just being slowed down by a 32B dense model (8B or 14B on higher-performance laptops), and since dense models are "more intelligent" compared to MoE... Maybe BitNet is a more or flexible layer activation is a workaround then?

- Bitnet and ternary are out as they need specific hardware

- ## [I'm surprised how simple Qwen3 VL's architecture is. : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/)
- Most machine learning architecture isnâ€™t really that complicated when you look at it in code. Plus, in software development simplicity = better.

- training pipeline probably much more complicated

- To be honest... The entire domain of LLMs and even VLMs are fairly simple... Working in self driving for over 5 years exposed to bespoke perception and multi task models, it shocked me how simple LLMs are, especially training it from the model side.
  - The literal loss function for LLMs during pretraining and finetuning is just cross entropy... Compare that to something more complicated like YOLO, it's actually insane in terms of difference of complexity.
  - Really the solution now.... Stack some transformers, use a LM head, chunk input for VLMs into patches... Pretty damn simple I have to say

- The nicest part of Qwen3-VL is that most of the â€œmagicâ€ comes from small, well-chosen inductive biases rather than a baroque stack. Itâ€™s basically ViT â†’ lightweight bridge â†’ plain decoder LLM, with two tasteful upgrades: interleaved 3D positional encoding (i-MRoPE) and DeepStack feature fusion.

- In my experience it is not very good at producing accurate coordinates of items it sees

- ## [Finally DeepSeek supports interleave thinking : r/LocalLLaMA _202512](https://www.reddit.com/r/LocalLLaMA/comments/1pbal3o/finally_deepseek_supports_interleave_thinking/)
  - If a thinking model supports multi-step tool calls and can incorporate thinking from historical steps during these calls, then this model supports interleaved thinking.
  - So far, among open-source models, only GPT-OSS, Kimi K2 Thinking, and MiniMax M2 support it, and I believe this feature is crucial for agents.
  - Interleave thinking lets an AI agent reason, act, and observe in tight loops, so it can adapt step-by-step to new information instead of blindly following a fixed plan.

- Why is special support needed? Each request to an LLM is whole conversation, and you can eliminate previous thinking blocks at each request. What am I missing here?

- ## [Experimenting with Multiple LLMs at once? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p4pre3/experimenting_with_multiple_llms_at_once/)
- I do mostly coding, so different families of models get wildly different python training data. Having each do the same coding task and then have another model pick the best components of the script for a new third script works really well.
  - Open web UI also has channels, which is a discord style chat room. You can tell the models you have to collaborate with each other on a project and they will take turns with sections of code.

- I had them do a few rounds of back and forth checking each other's work. It produced noticably better results than either model could on their own - but the time it took made it pointless. It was faster to just use a larger/slower model, allow a reasoning model to go nuts with thinking tokens, or just iterate myself. 
  - For tasks that aren't time sensitive there's some value there. 

- I like the idea of this and have experimented. I donâ€™t have a great way to have them collaborate in real time, but using two to check each others work just seems smart to me.

- Andrej Karpathy just posted about a vibe coded project he did called LLM Council that seems pretty cool: https://github.com/karpathy/llm-council

- Here's mine: GitHub.com/irthomasthomas/llm-consortium It's cli based but you can also save a multi-model consortium and use it like a regular model. Then you can use llm-model-gateway to serve that on a openai proxy and use it like a normal model in your tools.

- ## [Instead of either one huge model or one multi-purpose small model, why not have multiple different "small" models all trained for each specific individual use case?   r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1ovsb2x/instead_of_either_one_huge_model_or_one/)
- This is already how MoE (Mixture of Experts) models work. There are plenty of them on huggingface, though I think the separation of the 'experts' often isn't quite so clean. You also have to consider some other things -- your 20B param 'python only' expert model is almost unusable if it doesn't also understand variable names and comments and instructions in natural language, there does have to be some baseline knowledge to make a coding model actually useful, not *just* code in the training data.
- That's kinda what a MOE model is/does. Each of the "experts" are specialists in different trained things and it picks the active ones to use based on the prompt from the user.

- some people mentioned MoE, but MoE works differently. MoE makes decisions for each token separately, not for whole answers. So training many small, specialized models is not the same thing as using MoE.

- Your idea is exactly what Microsoft is attempting with its Phi series of models. They intend to build an ecosystem of models trained on very specific tasks, and then have an agentic system that uses a simple model to process the users request, decide the best model to use, and use that model to answer the question.
  - I believe Google is doing something similar with Gemini, and there are rumors that GPT 5 functions the same way.

- Perfect routing is not a solved problem. In general you have to get part of the way through solving a problem before figuring out exactly what is necessary to solve it.

- ## Every LangGraph user I know is making the same mistake! They all use the popular supervisor pattern to build conversational agents.
- https://x.com/akshay_pachaar/status/1983874390149484881
  - The pattern defines a supervisor agent that analyzes incoming queries and routes them to specialized sub-agents. Each sub-agent handles a specific domain (returns, billing, technical support) with its own system prompt.
  - This works beautifully when there's a clear separation of concerns.
  - The problem is that it always selects just one route.
  - For instance, if a customer asks: "I need to return this laptop. Also, what's your warranty on replacements?"
  - The supervisor routes this to the Returns Agent, which knows returns perfectly but has no idea about warranties.
  - This gets worse as conversations progress because real users don't think categorically. They mix topics, jump between contexts, and still expect the agent to keep up.
  - This isn't a bug you can fix since this is fundamentally how router patterns work.
  - Now, let's see how we can solve this problem.
  - Instead of routing between Agents, first, define some Guidelines.
  - Each guideline has two parts: - Condition: When it gets activated? - Action: What should the agent do?
  - Based on the user's query, relevant guidelines are dynamically loaded into the Agent's context.
  - This approach is actually implemented in Parlant - a recently trending open-source framework (15k+ stars).
  - Instead of routing between specialized agents, Parlant uses dynamic guideline matching. At each turn, it evaluates ALL your guidelines and loads only the relevant ones, maintaining coherent flow across different topics.
  - Another key advantage is that dynamic guidelines keep the system prompt clean, ensuring the agent receives only the right instructions at the right time.

- Hmm wonâ€™t a fully connected sub agent flow with planning enabled work here where one agent can route to any sub agent if it canâ€™t handle the task. So if realize the user needs more than one thing to be done the agent plans the steps and assigns to the required agent , task is executed result return until the final agent and one agent gives a combined answer

- It's written in python, nothing is stopping a string split and route two agents and join intelligently at the end.

- This seems like another form of "skills" that claude just released. Where the different skills are used to solve the flows and problems independently.  Its just making sure that all the parts are handled.
  - Yes the ideas are certainly relatable

- You can have the supervisor select a set of relevant agents instead of just one though
  - I get your point. However, real users don't think categorically. They mix topics, jump between contexts, and still expect the agent to keep up at every turn of the conversation. You need something that can dynamically load context at every turn. That's where @ParlantIO shines!
- I agree that this per turn dynamic context loading for the supervisor is effective however if there are agents added as tools to one of these guidelines then effectively its like dynamically selecting a subset of composite agents and then ReAct on this context. I actually did something very similar in one of my agents about data analysis where the per turn strategy would be selected dynamic (model, prompt and tools). I think as i said subset selection is just one of the possible single route out of all possible subset routes.

- ## [I Built Pocket Flow, an LLM Framework in just 100 Lines â€” Here is Why _202503](https://medium.com/@zh2408/i-built-an-llm-framework-in-just-100-lines-83ff1968014b)
- After a year of struggling with bloated frameworks, I decided to strip away anything unnecessary. The result is Pocket Flow, a minimalist LLM framework in just 100 lines of code.

- After a year of building LLM applications from scratch, I had a revelation: beneath all the complexity, LLM systems are fundamentally just simple directed graphs.
  -  By stripping away the unnecessary layers, I created Pocket Flow â€” a framework with zero bloat, zero dependencies, and zero vendor lock-in, all in just 100 lines of code.
- We also support batch processing, asynchronous execution, and parallel processing for both nodes and flows.

- Unlike other frameworks, Pocket Flow deliberately avoids bundling vendor-specific APIs. 
  - No Vendor Lock-in: Youâ€™re free to use any model you want, including local models like OpenLLaMA, without changing your core architecture.

- [I Built an LLM Framework in 179 Linesâ€”Why Are the Others So Bloated?  : r/LangChain _202502](https://www.reddit.com/r/LangChain/comments/1iwrhuu/i_built_an_llm_framework_in_179_lineswhy_are_the/)
- I had a look at the code. You've built a simple state machine. 
  - State machine are almost aways the worst time of choice when it comes to composition. 
  - In other words organising everything in flows of Nodes while still writing code is worse then for example carefully providing the data structures yourself and deal with the concurrency and dataflow based on the application specific requirements.
- Thanks for the feedback and example. I opted for a state machine for its simplicity! I'm open to finding a way to balance simple state transitions with tailored data structures though

- how is this different to the approach LangGraph took?
  - We think LangGraphâ€™s approach can feel rigid and tends to enforce a strictly linear, single-threaded execution model! We also want to do more than just manage state transitions!!
  - For example, we can handle asynchronous capabilitiesâ€”like node cloning to avoid race conditions and dedicated AsyncParallelBatchNodes and AsyncParallelBatchFlowsâ€”to enable parallel execution. This means you can run I/O-bound tasks concurrently (such as multiple LLM calls) without being restricted to a single-threaded, linear flow.

- ## ğŸ¤” [I built an AI orchestration platform that breaks your promot and runs GPT-5, Claude Opus 4.1, Gemini 2.5 Pro, and 17+ other models together - with an Auto-Router that picks the best approach : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o76n9d/i_built_an_ai_orchestration_platform_that_breaks/)
  - I've been frustrated with choosing between AI models - GPT-5 is great at reasoning, Claude excels at creative writing, Gemini handles data well, Perplexity is best for research - so I built LLM Hub to orchestrate them all intelligently.
  - [LLM HUB - AI Pipeline Orchestration](https://llm-hub.tech/)
  - The Core Problem: Each AI has strengths and weaknesses. Using just one means compromising on quality.
  - ğŸ’¡ The Solution: LLM Hub coordinates 20+ models across 4 execution modes:
- 4 EXECUTION MODES:
  - Single Mode - One model, one response (traditional chat)
  - Sequential Mode - Chain models where each builds on the previous (research â†’ analysis â†’ writing)
  - Parallel Mode - Multiple models tackle the same task, synthesized by a judge model
  - ğŸŒŸ Specialist Mode (the game-changer) - Breaks complex tasks into up to 4 specialized segments, routes each to the expert model, runs them in parallel, then synthesizes everything

- ## Code2Video - é€šè¿‡ä»£ç ç”Ÿæˆæ•™è‚²è§†é¢‘çš„ AI Agent æ¡†æ¶ï¼Œæ¥è‡ªæ–°åŠ å¡å›½ç«‹å¤§å­¦ Show Lab çš„æœ€æ–°ç ”ç©¶ï¼Œè®ºæ–‡å’Œå¼€æºé¡¹ç›®éƒ½å‘å¸ƒäº†ã€‚
- https://x.com/shao__meng/status/1974280130420961548
  - é€šè¿‡ AI Agent çš„æ–¹å¼ç”Ÿæˆç±»ä¼¼ 3Blue1Brown çš„è§†é¢‘ï¼Œå¾ˆæœ‰è¶£ï¼Œå’±ä»¬ä¸€èµ·çœ‹çœ‹ã€‚
  - æ–¹æ³•æ ¸å¿ƒï¼šä¸‰æ™ºèƒ½ä½“åä½œ, è®¾è®¡æ¡†æ¶åˆ†è§£ä»»åŠ¡ä¸ºä¸‰ä¸ªåä½œæ™ºèƒ½ä½“ï¼Œå½¢æˆæ¨¡å—åŒ–ç®¡é“ï¼š
  - Â· Plannerï¼ˆè§„åˆ’è€…ï¼‰ï¼šä»å­¦ä¹ ä¸»é¢˜ï¼ˆå¦‚â€œçº¿æ€§å˜æ¢ä¸çŸ©é˜µâ€ï¼‰ç”Ÿæˆå¤§çº²å’Œæ•…äº‹æ¿ï¼Œç¡®ä¿æ—¶é—´è¿è´¯æ€§ï¼ˆå¦‚æ¦‚å¿µå¼•å…¥ã€æ‰©å±•å’Œå›é¡¾ï¼‰ã€‚å®ƒé›†æˆå¤–éƒ¨æ•°æ®åº“ï¼Œæ£€ç´¢å‚è€ƒå›¾åƒå’Œè§†è§‰èµ„äº§ï¼ˆå¦‚å›¾æ ‡ï¼‰ï¼Œå¹¶è€ƒè™‘å—ä¼—æ°´å¹³ï¼ˆå¦‚é«˜ä¸­ç”Ÿ vs. å¤§å­¦ç”Ÿï¼‰ï¼Œä»¥æå‡äº‹å®å‡†ç¡®æ€§å’Œè§†è§‰ä¸€è‡´æ€§ã€‚
  - Â· Coderï¼ˆç¼–ç è€…ï¼‰ï¼šå°†æ•…äº‹æ¿è½¬æ¢ä¸ºå¯æ‰§è¡Œ Manim åŠ¨ç”»ä»£ç ã€‚é‡‡ç”¨å¹¶è¡Œåˆæˆç­–ç•¥åŠ é€Ÿç”Ÿæˆï¼Œå¹¶å¼•å…¥ ScopeRefineï¼ˆèŒƒå›´å¼•å¯¼ä¿®å¤ï¼‰æœºåˆ¶ï¼šä»è¡Œçº§ï¼ˆå±€éƒ¨ä¿®å¤é”™è¯¯è¡Œï¼‰é€æ­¥æ‰©å±•åˆ°å—çº§å’Œå…¨å±€é‡ç”Ÿï¼Œç¡®ä¿ä»£ç é«˜æ•ˆä¸”æ— è¯­æ³•é”™è¯¯ï¼ŒåŒæ—¶ä¿æŒè·¨èŠ‚ä¸€è‡´æ€§ã€‚
  - Â· Criticï¼ˆæ‰¹è¯„è€…ï¼‰ï¼šä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œé”šç‚¹è§†è§‰æç¤ºï¼ˆå¦‚å ç”¨è¡¨å’Œä½ç½®ç½‘æ ¼ï¼‰è¿­ä»£ç²¾ç‚¼æ¸²æŸ“è§†é¢‘ã€‚é’ˆå¯¹ç©ºé—´é—®é¢˜ï¼ˆå¦‚é‡å æˆ–ç©ºæ—·åŒºåŸŸï¼‰ï¼Œå®ƒæä¾›å…·ä½“è§£å†³æ–¹æ¡ˆï¼ˆå¦‚è°ƒæ•´å¯¹è±¡ä½ç½®æˆ–ç¼©æ”¾ï¼‰ï¼Œæå‡å¸ƒå±€æ¸…æ™°åº¦å’Œæ•™è‚²å¸å¼•åŠ›ã€‚
  - æ•´ä¸ªæµç¨‹ä»ç”¨æˆ·æŸ¥è¯¢è¾“å…¥ï¼Œåˆ°è¾“å‡ºæ•™è‚²è§†é¢‘ï¼Œé€šå¸¸éœ€æ•°åˆ†é’Ÿæ¸²æŸ“ï¼Œè¿œä¼˜äºç«¯åˆ°ç«¯åƒç´ ç”Ÿæˆã€‚

- ## [å¦‚ä½•çœ‹å¾…è§‚ç‚¹ï¼šAI çš„å…³é”®ç‚¹ä¸æ˜¯promptï¼Œè€Œæ˜¯Context Engineeringï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/1923364519964545063)
- Contextè¿œä¸æ­¢æ˜¯ä¸€å¥promptï¼Œå®ƒåŒ…æ‹¬ï¼š
  - æŒ‡ä»¤/ç³»ç»Ÿæç¤ºï¼šå®šä¹‰æ¨¡å‹è¡Œä¸ºçš„åˆå§‹æŒ‡ä»¤
  - ç”¨æˆ·æç¤ºï¼šæ¥è‡ªç”¨æˆ·çš„å³æ—¶ä»»åŠ¡æˆ–é—®é¢˜
  - çŠ¶æ€/å†å²ï¼šå½“å‰å¯¹è¯çš„çŸ­æœŸè®°å¿†
  - é•¿æœŸè®°å¿†ï¼šè·¨å¤šæ¬¡å¯¹è¯æ”¶é›†çš„æŒä¹…
  - çŸ¥è¯†åº“æ£€ç´¢ä¿¡æ¯(RAG)ï¼šæ¥è‡ªæ–‡æ¡£ã€æ•°æ®åº“æˆ–APIçš„å¤–éƒ¨çŸ¥è¯†
  - å¯ç”¨å·¥å…·ï¼šæ¨¡å‹å¯ä»¥è°ƒç”¨çš„æ‰€æœ‰åŠŸèƒ½å®šä¹‰
  - ç»“æ„åŒ–è¾“å‡ºï¼šå¯¹æ¨¡å‹å“åº”æ ¼å¼çš„å®šä¹‰
- è¿™å…¶å®å°±æ˜¯åœ¨æ„å»ºä¸€ä¸ªå®Œæ•´çš„ã€ŒAIå·¥ä½œç¯å¢ƒã€ï¼Œè€Œä¸æ˜¯ç®€å•åœ°ç»™AIä¸‹æŒ‡ä»¤ã€‚

- å³ä½¿ä½ å†™ä¸å¥½promptï¼Œå¸‚é¢ä¸Šä¹Ÿæœ‰å¾ˆå¤šAIå·¥å…·å¯ä»¥å¸®ä½ åšprompt enhancement
  - æœŸå¾…AIèƒ½ã€Œæœ‰è®°å¿†ï¼Œæ‡‚ç”¨æˆ·ã€ã€‚å› æ­¤ï¼ŒContext Engineeringï¼ˆä¸Šä¸‹æ–‡å·¥ç¨‹ï¼‰å°±å¼€å§‹è¢«å…³æ³¨åˆ°ã€‚
  - context engineering, æ˜¯æŒ‡é€šè¿‡ç³»ç»Ÿæ€§æ„å»ºã€ç®¡ç†å’Œä¼˜åŒ– AI æ¨¡å‹çš„è¾“å…¥ä¸Šä¸‹æ–‡ï¼Œä»¥æå‡æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„ç†è§£ä¸è¾“å‡ºèƒ½åŠ›ã€‚
  - ç¬¬ä¸€æ˜¯å†å²äº¤äº’æ•°æ®ã€‚
  - ç¬¬äºŒï¼Œæ˜¯contextçš„æ€»ç»“ã€‚å¯¹è¯å¯ä»¥ä¸€ç›´å»¶ç»­ï¼Œä½†æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£æœ‰é™ï¼Œæ€ä¹ˆè®°ä½ä¹‹å‰çš„å¯¹è¯ï¼Ÿå½“ç„¶æ˜¯å‹ç¼©ï¼Œä¾‹å¦‚æŠŠæœ€è¿‘5æ¡å¯¹è¯çš„å†…å®¹åŸå°ä¿ç•™ï¼Œå†ä¹‹å‰çš„å†…å®¹æ€»ç»“ä¸€ä¸‹ã€‚
  - ä½†æ›´é‡è¦çš„ï¼Œæ˜¯é¢†åŸŸçŸ¥è¯†ä¸èƒŒæ™¯ä¿¡æ¯ã€‚
- AIçš„èƒ½åŠ›æ¥æºäºä¸¤ä¸ªæ–¹é¢ in-weights memoryï¼ˆè®­ç»ƒé›†é‡Œå­¦åˆ°çš„ï¼‰ å’Œ in-context memoryï¼ˆå‚è€ƒèµ„æ–™é‡Œå­¦åˆ°çš„ï¼‰ã€‚ in-weights memoryåŸºæœ¬ä¸Šæ˜¯å¾ˆéš¾ä¿®æ”¹çš„, ä½†in-context memory æ›´å®¹æ˜“ä¿®æ”¹å’Œæ›´æ–°ï¼Œä½ åªè¦æä¾›æ–°çš„èµ„æ–™ï¼Œæ­£ç¡®çš„èµ„æ–™ï¼Œæ¨¡å‹å°±èƒ½æœ‰ã€Œæ–°çŸ¥ã€

- å¤æ‚çš„AIåº”ç”¨ï¼Œä¸æ˜¯é chatè¾“å…¥æç¤ºè¯è¿™ä¹ˆç®€å•ï¼Œè€Œæ˜¯éœ€è¦è‡ªåŠ¨åŒ–å¤šæ¬¡å’Œå¤§æ¨¡å‹çš„äº¤äº’ï¼Œè¿™ä¸ªè‡ªåŠ¨çš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦èƒ½å¤Ÿè‡ªåŠ¨äº§ç”Ÿç»™å¤§æ¨¡å‹çš„æç¤ºè¯ã€‚

- Agent = LLM + Prompt + å·¥å…·è°ƒç”¨ï¼Œä½†ä»å·¥ç¨‹è§†è§’æ¥çœ‹ï¼Œè¿™äº›æœ¬è´¨éƒ½æ˜¯Context Engineeringï¼Œä¹Ÿå°±æ˜¯ä¸Šä¸‹æ–‡å·¥ç¨‹ã€‚
  - Context Engineeringå°±æ˜¯åœ¨æ¯æ¬¡è°ƒç”¨é‡Œï¼ŒæŠŠæ¨¡å‹å®Œæˆä»»åŠ¡æ‰€å¿…éœ€çš„ä¿¡æ¯æŒ‰å¯¹çš„æ ¼å¼ã€åœ¨å¯¹çš„æ—¶æœºå‡†ç¡®æ‰“åŒ…è¿›å»ã€‚

- ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼ŒAIç»˜å›¾ç”¨æˆ·æ—©å°±å¤©å¤©åœ¨ç”¨äº†
  - å½“ä½ ä½¿ç”¨ Inpaintï¼ˆå±€éƒ¨é‡ç»˜ï¼‰ åŠŸèƒ½æ—¶ï¼ŒAIä¸æ˜¯å‡­ç©ºæƒ³è±¡ï¼Œè€Œæ˜¯åŸºäºä½ ç•™ä¸‹çš„ç”»å¸ƒã€è¾¹ç¼˜çº¿æ¡ã€å…‰çº¿æ–¹å‘ã€å·²æœ‰ç”»é£æ¥è¡¥å…¨åŒºåŸŸï¼Œè¿™å°±æ˜¯â€œå›¾åƒä¸Šä¸‹æ–‡â€
  - å½“ä½ è¿›è¡Œ Outpaintï¼ˆå›¾åƒæ‰©å±•ï¼‰ æ—¶ï¼ŒAIå¿…é¡»å‚è€ƒåŸå›¾çš„æ„å›¾é€»è¾‘ã€è‰²å½©æ¸å˜ã€é£æ ¼çº¹ç†ã€äººç‰©é€è§†æ¥ç”Ÿæˆè‡ªç„¶è¡”æ¥çš„éƒ¨åˆ†ã€‚è¿™ç§å¯¹ä¸Šä¸‹æ–‡çš„â€œç†è§£â€å’Œâ€œå»ºæ¨¡â€ï¼Œæ¯”æç¤ºè¯æœ¬èº«æ›´é‡è¦
  - å½“ä½ ç”¨ Photoshopå†…ç½®çš„Fireflyè¿›è¡Œå±€éƒ¨ä¿®æ”¹åˆ›æˆå¼å¡«å……æ—¶ï¼ŒçœŸæ­£èµ·å†³å®šä½œç”¨çš„ä¸æ˜¯ä½ è¯´äº†â€œç»™æˆ‘åŠ ä¸€ä¸ªç¯å¡”â€ï¼Œè€Œæ˜¯AIæ˜¯å¦å‡†ç¡®è¯»å–äº†å½“å‰ç”»é¢æ˜¯å¤œæ™šã€æ˜¯æ°´è¾¹ã€æœ‰å…‰æºæŠ•å°„ã€æœ‰é€è§†æ¶ˆå¤±ç‚¹ã€‚
  - ä¸Šé¢è¿™äº›åŠŸèƒ½ï¼Œå“ªæ€•ä½ æ ¹æœ¬ä¸å†™æç¤ºè¯ï¼Œå¥½çš„å·¥å…·ä¹Ÿèƒ½ç»™ä½ è„‘è¡¥å¥½ï¼Œæ¯”å¦‚æˆ‘ç°åœ¨æ ¹æœ¬æ”¾ä¸å¼€è®¢é˜…çš„Photoshop AIã€‚

- 
- 
- 
- 
- 

- ## DeepSeekæœ€å¤§çš„åˆ›æ–°ï¼Œæ˜¯ä¸éœ€è¦å¤§é‡çš„äººå·¥æ ‡æ³¨ï¼Œè€Œæ˜¯ç›´æ¥ä»å…¶ä»–å¤§æ¨¡å‹è’¸é¦æˆ–è€…ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼ˆGRPOï¼‰ã€CoTï¼ˆè‡ªæˆ‘åæ€ï¼‰æ¥ç»™å¤§æ¨¡å‹åé¦ˆï¼Œ
- https://x.com/seclink/status/1888011462008005030
  - å°±ç›¸å½“äºå®Œå…¨ä½¿ç”¨RLï¼ˆæˆ–è€…å¦ä¸€ä¸ªåŸºç¡€å¤§æ¨¡å‹ï¼‰æ¥æ›¿ä»£äººå·¥æ ‡æ³¨äº†ã€‚
  - è¿™å®é™…ä¸Šæ˜¯æŠ¢äº†Scale AI è¿™ç§å…¬å¸çš„è›‹ç³•ï¼ŒDeepSeekç‰›Xä¹‹å¤„åœ¨äºï¼Œå¾ˆå¤šè€å¤–ä¸€å¼€å§‹ä¸ä¿¡ï¼Œç„¶åç…§ç€è®ºæ–‡é‡Œçš„æ–¹æ³•å¿«é€Ÿï¼ˆå±€ä¿ƒåœ°ï¼‰å¤ç°ï¼Œå´å‘ç°ç«Ÿç„¶ä¹Ÿèƒ½å¤ç°æˆåŠŸã€‚

- cotå’Œè’¸é¦ä¹‹å‰ï¼Œå°±é€šè¿‡grpoè¿›è¡Œrlè·å¾—äº†ç›¸å½“ä¸é”™çš„æ¨ç†èƒ½åŠ›ã€‚ã€‚ä½ è¿™æ•´ç†çš„ä¸æ¸…æ™°ã€‚

- è¿™æ²¡æœ‰ä»»ä½•åˆ›æ–°ï¼ŒGPT1å°±ç”¨äº†åŒæ ·çš„æ–¹æ³•ï¼Œè€Œä¸”ç°åœ¨ä¸ä½¿ç”¨æ˜¯æœ‰ç†ç”±çš„ã€‚å› ä¸ºdeepseekæ¨¡å‹æœ¬èº«ä¸è¡Œï¼Œä¸€ä¸ªqueryéœ€è¦RL chainæ‰èƒ½è¾¾åˆ°å¯æ¥å—çš„ç­”æ¡ˆï¼Œè‡´ä½¿inference æ•ˆç‡ä½åˆ°å¯æ€•ï¼Œè™½ç„¶trainingä¾¿å®œï¼Œä½†æ˜¯operation costè¦å¤šå¥½å‡ å€ï¼Œå¾—ä¸å¿å¤±

- ä½ è¿™ä¸ªè¯´çš„å®Œå…¨ä¸å¯¹ï¼ŒCoTæ˜¯GPTå‘æ˜çš„ï¼Œè’¸é¦ä¹Ÿæ—©å°±æœ‰äº†ï¼ŒRLä¹Ÿæ—©å°±æœ‰äº†ï¼Œdeepseekæ˜¯å‘æ˜äº†RLé‡Œçš„GRPO

- ## I read up on DeepSeekâ€™s learning algo, GRPO. GRPO: group relative policy optimization
- https://x.com/virattt/status/1885102056546910672
- How GRPO works:
  1 â€¢ model generates a group of answers
  2 â€¢ compute score for each answer
  3 â€¢ compute avg score for entire group
  4 â€¢ compare each answer score to avg score
  5 â€¢ reinforce model to favor higher scores
  - This process is repeated, allowing the model to learn and improve over time.
- Other methods like PPO, use a value function model to do reinforcement learning.
  - GRPO does not, which reduces memory and computational overhead when training.

- GRPO was proposed for the first time in Feb 2024 in DeepSeekMath paper. It is not new.
  - Back then they used Neural Networks for Rewards (PRM/ORM). The magic happened when DeepSeek  replaced PRM/ORM with the exact reward (Verified Reward).

- ## ğŸ”¡ OpenAI's Deep Research is just a search+read+reasoning in a while-loop, right? here is my replicate of it in nodejs, using gemini-flash and jina reader
- https://x.com/hxiao/status/1886250705415229627
- If it includes evaluating js driven websites, including images Then yes 
  - To really replicate that, you'd probably wanna just use puppeteer, save the whole page as an image, extract the info from that, and then crunch that data

# discuss-multi-agents ğŸ˜ï¸
- ## 

- ## 

- ## 

- ## ğŸ†š ä»Šå¤©å¾ˆæœ‰è¶£ï¼Œä¸¤å®¶çŸ¥åçš„å…¬å¸å„å‡ºäº†ä¸€ç¯‡æ–‡ç« ï¼Œäº‰è®ºè¦ä¸è¦ä½¿ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚
- https://x.com/oran_ge/status/1933754019010539923
  - Claude çš„å®˜æ–¹ Anthropic ï¼šå¦‚ä½•æ„å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
  - Devin çš„å®˜æ–¹ Cognition ï¼šä¸è¦æ„å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
- è¿™æ ¸å¿ƒçš„äº‰è®®ç‚¹åœ¨äºï¼šContext ä¸Šä¸‹æ–‡åˆ°åº•åº”è¯¥å…±äº«è¿˜æ˜¯åˆ†å¼€ï¼Ÿ
  - Claude è¿™è¾¹çš„è§‚ç‚¹æ˜¯ï¼Œæœç´¢ä¿¡æ¯çš„æœ¬è´¨æ˜¯å‹ç¼©ï¼Œå•ä¸ªæ™ºèƒ½ä½“çš„ä¸Šä¸‹æ–‡æœ‰é™ï¼Œé¢å¯¹æ— é™çš„ä¿¡æ¯ï¼Œå‹ç¼©æ¯”å¤ªå¤§å°±ä¼šå¤±çœŸã€‚è¿™æ˜¯é›†ä½“æ™ºæ…§ï¼Œä¸€èµ·åä½œè·å¾—çš„èƒœåˆ©ã€‚
  - Devin è¿™è¾¹çš„è§‚ç‚¹æ˜¯ï¼Œå¤šä¸ªæ™ºèƒ½ä½“çš„ä¸Šä¸‹æ–‡ä¸ä¸€è‡´ï¼Œä¼šå¯¼è‡´ä¿¡æ¯å‰²è£‚ã€è¯¯è§£ã€ä»–ä»¬æ±‡æŠ¥ç»™è€æ¿çš„ä¿¡æ¯ç»å¸¸å……æ»¡äº†çŸ›ç›¾ã€‚
  - è¿™è®©æˆ‘æƒ³åˆ°ï¼Œè½¯ä»¶å·¥ç¨‹ä»æ¥ä¸æ˜¯è¿½æ±‚å®Œç¾ï¼Œè€Œæ˜¯æŒç»­è¿­ä»£ã€‚

- æ²¡å•¥çŸ›ç›¾çš„ã€‚çœ‹éœ€æ±‚ã€‚open-ended çš„é—®é¢˜æ¯”è¾ƒé€‚åˆ multi-agentï¼›ç›®æ ‡å¾ˆå…·ä½“çš„è¯ï¼Œå°±æ¯”è¾ƒé€‚åˆå•ä¸ª agent

- æ„Ÿè§‰å’Œç°å®ä¸­çš„ä¸¤å®¶ä¸åŒç­–ç•¥è¿è¥çš„å…¬å¸å·®ä¸å¤šï¼Œæ²¡æœ‰ç»å¯¹çš„å¯¹å’Œé”™ï¼Œéƒ½èƒ½èµ°å‡ºæ¥çš„å¯èƒ½æ€§ä¹Ÿè¶…å¤§ã€‚

- ## @KuraAIAgents é€šè¿‡åˆ›æ–°çš„äº”é‡ Agent æ¶æ„ï¼ˆè§„åˆ’ã€æ‰§è¡Œã€è¯„ä¼°ï¼‰å®ç°äº† 87% çš„æµè§ˆå™¨è‡ªåŠ¨åŒ–å‡†ç¡®ç‡ï¼Œè¶…è¶Š Claude è®¡ç®—æœºæ“ä½œ 28 ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶æ”¯æŒä½æˆæœ¬æ¨¡å‹æ›¿æ¢æ–¹æ¡ˆ
- https://x.com/shao__meng/status/1857586562588094918
  - åŒ…å«5ä¸ªä¸“é—¨çš„ Agentï¼Œå…¶ä¸­3ä¸ªæ ¸å¿ƒ Agent å½¢æˆä¸€ä¸ªå¾ªç¯ç³»ç»Ÿ
  - åœ¨ WebVoyager åŸºå‡†æµ‹è¯•ä¸­å–å¾— 87% çš„æˆç»©
  - æ¯” Claude çš„è®¡ç®—æœºæ“ä½œé«˜å‡º 28%
- äº”ä¸ªæ ¸å¿ƒ Agentï¼š
a) åˆå§‹è§„åˆ’è€…(Initial Planner)
- è´Ÿè´£åˆ¶å®šé«˜å±‚æ¬¡è®¡åˆ’
- ä½¿ç”¨ OpenAI o1 æ¨¡å‹è¿›è¡Œæ¨ç†
b) å¾ªç¯è§„åˆ’è€…(Agent Loop Planner)
- è¯„ä¼°ä»»åŠ¡æ˜¯å¦å®Œæˆæˆ–ä¸å¯èƒ½å®Œæˆ
- ä¸ºæ‰§è¡Œè€…æä¾›ä¸‹ä¸€æ­¥æŒ‡ä»¤
- æ ¹æ®éœ€è¦ä¿®æ”¹è®¡åˆ’
c) æ‰§è¡Œè€…(Executor)å…·å¤‡ä¸‰é¡¹æ ¸å¿ƒæŠ€èƒ½ï¼š
- ç½‘å€å¯¼èˆªå’Œè¿”å›
- è¯»å–å½“å‰é¡µé¢æ•°æ®
- æ‰§è¡Œå±å¹•æ“ä½œ(ç‚¹å‡»ã€æ»šåŠ¨ã€è¾“å…¥)
d) å¾ªç¯è¯„è®ºè€…(Agent Loop Critic)
- è¯„ä¼°æ‰§è¡Œè€…çš„è¡¨ç°
- ç‰¹åˆ«åœ¨å¤æ‚ç•Œé¢æ“ä½œä¸­èµ·å…³é”®ä½œç”¨
e) æœ€ç»ˆè¯„è®ºè€…(Final Critic)
- è¯„ä¼°æ•´ä¸ªä»»åŠ¡è½¨è¿¹
- å¿…è¦æ—¶æä¾›åé¦ˆå¹¶å¯åŠ¨æ–°çš„å¾ªç¯

- ## OpenAI å‘å¸ƒå¤š Agent ç¼–æ’æ¡†æ¶èƒŒåçš„æ€è€ƒä»¥åŠå®è·µè¿‡ç¨‹
- https://x.com/tuturetom/status/1845634978530693494
  - æ ¸å¿ƒæ˜¯ OpenAI çš„å·¥ç¨‹å¸ˆåœ¨æ€è€ƒ  Agent çš„ã€Œè·¯ç”±ã€ + ã€Œç§»äº¤ã€ç­‰èƒ½åŠ›æ—¶æ‹“å±•å‡ºæ¥çš„ä¸€ä¸ªç¤ºä¾‹ï¼Œè¿›è€Œå‘ç°è¿™ä¸ªç¤ºä¾‹åŸè¯­å¾ˆæ™®é€‚ï¼Œæ‰€ä»¥å¼€å‘äº† Swarm æ¡†æ¶
  - æ‰€æœ‰ä¼Ÿå¤§çš„æ€è€ƒéƒ½æºäºå‘¨æœ«ä¸šä½™å·¥ä½œ

- ## OpenAI æ‚„æ‚„å¼€æºäº†æ„å»ºå¤šä»£ç†æ™ºèƒ½ä½“ååŒæ¡†æ¶ï¼šSwarm
- https://x.com/aigclink/status/1844936446416912628
  - ç”¨äºæ„å»ºã€ç¼–æ’å’Œéƒ¨ç½²å¤šä»£ç†

# discuss-ai-format/interop
- ## 

- ## 

- ## 

- ## [Use YAML over JSON when dumping into prompts for ~2x token saving : r/ChatGPTCoding _202509](https://www.reddit.com/r/ChatGPTCoding/comments/1nl7xux/use_yaml_over_json_when_dumping_into_prompts_for/)
  - [YAML vs. JSON: Which Is More Efficient for Language Models? _202307](https://medium.com/better-programming/yaml-vs-json-which-is-more-efficient-for-language-models-5bc11dd0f6df)
  - It's been pointed out in the comments (with sass) that minifying your JSON is another, perhaps even better, alternative than transforming to YAML. So now there's two options for saving tokens.

- Does the guy who wrote the article know that you don't need to use whitepaces in JSON and you can minify it to consume less space than YAML? Generally speaking, JSON is more space-efficient and compact than YAML.

- Just remove the spaces and condence the JSON into a single line. LLMs don't care about spaces, it's a visual thing for us.

- Thought LLM's don't count white space as context... or if they did, it would be incredibly minimal
  - They kind of have to, if only to correctly write Python
  - ASCII art too

- Another point is accuracy... some like XML more as well - and there is BAML. If i just wanna save money I could get a cheaper model too.
- xml is also what Claude officially recommends for better accuracy.

- I use YAML and JSON, because i use the CMS Drupal since 2006 - so this fits quite well in my workflow

- TOML is actually more verbose when it comes to complex data structures.
  - Which makes sense since it was designed to be a JSON/YAML mappable language for better human readability.
# discuss-local-llm-usecases
- ## 

- ## 

- ## 

- ## 

- ## [The curious case of Qwen3-4B (or; are <8b models *actually* good?) : r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1p76wtf/the_curious_case_of_qwen34b_or_are_8b_models/)
  - how good are the smaller models at answering some of the sort of questions I might ask of them, chatting, instruction following etc?
  - I ran each model's output against the "council of AI elders", then got GPT 5.1 (my paid account craps out today, so as you can see I am putting it to good use) to run a tally and provide final meta-commentary.
  - The results, per GPT 5.1: GPT-OSS 20B and Qwen 3-4B emerged as the strongest overall performers. Mid-tier models like DeepThink 7B and Qwen 2.5 7B produced competent technical content but struggled severely with the style transform, while Phi-Mini 4B showed the weakest combination of accuracy, coherence, and instruction adherence.
  - The results align closely with real-world use cases: larger or better-trained models excel at technical clarity and instruction-following, whereas smaller models require caution for detail-sensitive or persona-driven tasks, underscoring that the most reliable workflow continues to be â€œstrong model for substance, optional model for vibe.â€

- Iâ€™m using qwen3-4b 2507 instruct for simple tasks such as meeting summarization, keeping track of to-doâ€™s and the likes. So far, it has performed quite well

- ## [My Journey to finding a Use Case for Local LLMs : r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1p1xy0q/my_journey_to_finding_a_use_case_for_local_llms/)
  - I have a lot of phone pictures of recipes, and a lot of inherited cookbooks. The thought of gathering the ones I really liked into one place was daunting. The recipes would get buried in mountains of photos of cats (yes, it happens), planes, landscapes etc. Google photos is pretty good at identifying recipe images, but not the greatest.
  - I landed on qwen3-vl:8b. It was able to take the image (with very strict prompting) and output the exact text from the image. I did have to verify and do some editing here and there. I was happy
  - I then turned to GPT-OSS:20b since it is newer, and asked it to convert the recipe text to json-ld recipe schema compatible format.
  - Now I can take a pic of any recipe I want, run it through the qwen-vl:8b model for OCR, verify the text, then have GPT-OSS:20b spit out json-ld recipe schema text that can be imported into the mealie database. (And verify the json-ld text again, of course).
  - I'm not using any system prompts at this time. Here is the prompt

- You could now take this even further and query it with a prompt on your cell phone.

- My use case is that I use local vlm to the content of the download file, then rename following the format I define and use bash script to route it to the its dedicated folder. Now every time a file is downloaded, it is renamed and route to its location automatically.
# discuss-local-llm-xp/tips
- ## 

- ## 

- ## 

- ## 

- ## 

- ## ["We're in an LLM bubble, not an AI bubble" - Here's what's actually getting downloaded on HuggingFace and how you can start to really use AI. : r/LlamaFarm _202512](https://www.reddit.com/r/LlamaFarm/comments/1pb2wr2/were_in_an_llm_bubble_not_an_ai_bubble_heres/)
  - Encoder-only models (BERT family) account for 45% of HuggingFace downloads, nearly 5x more than decoder-only LLMs at 9.5%. 
  - Every one of these model families exists because someone realized the "one model to rule them all" approach was failing for their use case
  - This is "Mixture of Experts" at the application level. Many small, specialized models working together instead of one massive model trying to do everything.

- ## [What broke when you tried to take local LLMs to production? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p91p4k/what_broke_when_you_tried_to_take_local_llms_to/)
- Ollama broke a lot of the time because we devâ€™d on a Mac but pushed production to nvidia. Switching to vllm has largely solved this and pushed it to more of an interface rather than model problem.

- Hosting several Models on one GPU is always really annoying with vLLM. You basically have to fiddle with the memory utilisation until all models fit. There is no clean way I found to calculate memory usage. You just have two tweak, boot, look at the logs and repeat. And even if you got all models to fit, vLLM sometimes crashes with cuda out of memory exceptions when loading the models since there seems to be a peak in memory consumption on boot up.

- Why doesn't anyone mention tabbyAPI/exllamav3? It's much more memory-efficient than AWQ, also it loads quicker, and the quantization is more optimized. Also K/V cache can be quantized between 2 and 8 bits seperately (=more room for context)

- ## [Why do you use open-source LLMs ? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p16kxx/why_do_you_use_opensource_llms/)
- Freedom. I can do whatever I want without having my data stolen.

- The benefit of open source LLMs for me is that you can access the internal layers to do things like attention injection, intermediate layer feature extraction, attention map extraction, token-wise early stoppage, adaptive layer skips, conversion to latent attention, conversion to mamba or gated delta net hybrid etc
  - This is it. Intervening mathematically in something that can talk like a human is an otherworldly experience. I do it almost every day and it makes me feel like an alchemist tinkering with a homunculus. Thereâ€™s nothing else like it. Most other intellectual activities feel shallow by comparison.

- I just think it's very cool to have a summary of the vast wealth of human text and knowledge in a 30GB file on my computer.

- I want to own my capabilities. I donâ€™t want my coding skills to be tied to some external service that I have zero control over.

- ## [What do you use local LLMs for? What is your use case? : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1ms4gmz/what_do_you_use_local_llms_for_what_is_your_use/)
- They are best used for small context tasks like classification/sentiment, asking it summarize, pull out keywords, grammar, etc.
  - Not for long conversations or complex tasks, but system prompt, user request, and then get a single response.
  - For example, I've been experimenting with a local LLM checking chat messages for rule-breaking in a Discord server, flag the suspicious once, and then I checked the flagged messages and take action myself.

- I used it to generate a synthetic dataset for training my first neural network, with successful results.

- Structured data extraction from private documents (millions of documents)
  - We have produced an annotated dataset of a few thousand records. We used this to fine tune a model and evaluate performance. We are getting >0.98 F1 score which is a margin of error we are willing to tolerate given the scale and time saved... It's for a very specific type of short documents. Our extraction pipeline has multiple extraction and validation steps.

- Data extraction and classification.

- ## [What are you using your local models for ? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1oz6k5j/what_are_you_using_your_local_models_for/)
- Confidential research. In SillyTavern. Just yet with a quantized Skyfall-31b from Drummer, apparently some blend of Mistral Small or whatever. It's fun.

- Qwen3-VL-30B-A3B-Thinking is heating my home by processing video. I've posted about it before but llm-ffmpeg-edit.bash handles the logic & llm-python-vision-multi-images.py handles sending the images/frames to the LLM backend.
  - I was using Mistral 3.2 (24B dense) before Qwen3-VL got support. The speed increase from 24B -> 30B-A3B has been incredible, while maintaining accuracy.

- I'm bound by the legal terms of my employment to not discuss the technologies we use there, but am free to talk about my personal use-cases.
- STEM research assistant -- I give it my technical notes (usually physics and/or math) and a question, and get back helpful replies. My go-to is Phi-4-25B, and when it's not smart enough I escalate to Tulu3-70B, sometimes Qwen3-235B pipelined with Tulu3-70B.
- Creative writing -- Cthulhu-24B, Big-Tiger-Gemma-27B-v3, or Valkyrie-49B-v2. Mostly sci-fi (space opera or Murderbot fanfic).
- Evol-Instruct and synthetic dataset generation or augmentation -- again, mostly Phi-4-25B or Tulu3-70B, though recently I have been using Valkyrie-49B-v2 to bulk up a RAG database of technical troubleshooting advice/solutions. To my surprise Valkyrie is a lot better at this than Tulu3-70B, even though they are derived from similar models (Tulu3 from Llama-3.1, Valkyrie from Llama-3.3-Nemotron-Super-49B-v1.5 which in turn is based on Llama-3.3).
- Persuasion research -- studying the capacity for LLM inference to change people's minds. Big-Tiger-Gemma-27B-v3 is excellent at this.
- Wikipedia-backed RAG for general question-and-answer. I use Big-Tiger-Gemma-27B-v3 for this as well.
- Describing images so I can index them in a locally hosted search engine. Qwen2.5-VL-72B is still the best vision model I've yet used, but I haven't had a chance yet to compare it against Qwen3-VL-32B. I am hoping Qwen3 is better, despite having fewer than half as many parameters.
- I also run an IRC bot for a technical support channel, which is mostly GOFAI-driven but I've been working on a plugin for it to be RAG/LLM-driven too. That, too, uses Big-Tiger-Gemma-27B-v3.
- Recently I've been trying to use Phi-4 (14B) as a synthetic dataset rewriter, to salvage low-quality inferred data I would normally prune from the dataset. I read a paper suggesting even very small models (4B) are effective at this. So far my results have been mixed. I've been meaning to try Tiger-Gemma-12B-v3 as well; possibly Phi-4 just isn't the right model for this.
- GLM-4.5-Air for slow inference of entire programming projects (which I don't do much, since I don't want my coding skills to atrophy) or to find bugs in my own code.
- Qwen3-Coder-REAP-25B-A3B for fast FIM code inference. The model doesn't have to be smart to figure out what my "for"-statement is going to look like, but it does need to be fast enough that it can suggest a completion before I've finished typing the "for"-statement myself. I use the REAPed version of this model so that it fits in my GPU's VRAM (at Q4_K_M); the original 30B-A3B didn't quite fit.
- I'm also tentatively using Phi-4 as a judge, comparing two replies to the same prompt and telling me which is better. It's early days yet, for this project, and it might not be the right model for this. We will see.
- Sometimes I use Big-Tiger-Gemma-27B-v3 or Phi-4 (14B) for language translation (mostly Spanish to English, but sometimes German or Russian to English). Overall Big Tiger is better at this, though Phi-4 does surprisingly well, and is better than Big Tiger at taking situational context into account with its replies. It's also a lot faster than Big Tiger, which is sometimes important for translation tasks.

- ## [Are any of you using local llms for "real" work? : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1otnj2k/are_any_of_you_using_local_llms_for_real_work/)
- The most "real" work I've done is that Qwen3-VL-30B-A3B-Thinking is currently going through videos 10-seconds at a time. 
  - Based on the bot's True/False boolean output a wrapping program keeps track of what segments `<thing I'm looking for>` is within. At the end, we're done using Qwen3-VL and the wrapping program uses the segment information to use FFMPEG to make a clipped version where `<thing I'm looking for>` should always be present.
  - A general example is you could have the bot look for every explosion in an action film. Maybe it considers muzzle flash from a gun to be an explosion, not technically wrong as far as I know. So you could prompt that it should only be explosions not from gunfire and try that.

- I use an LLM, a scraper to fetch security vulnerabilities (CVEs), and an internal API that lists my running services, and the LLM generates a daily report for me about whether any of the software I'm running might have been mentioned.

- Using qwen3 vl to parse vids to extract car plate numbers snd other vehicle markings

- I used a local LLM and a vector database to help me with my corporate taxes. I first fed all past 3 or 4 years of expenses along with their categorizations (meals, fuel, office supplies etc.) into a Postgres PG Vector database and embedded the data using the Nomic model. I then classified several hundred transactions for the past tax year using this vector set of data to categorized the transactions.

- I use nvidia/Llama3-ChatQA-1.5-8B to index 2M similar insurance docs using ollama. I load the index into Meilisearch and sell access to it. I did this after a trip to micro center and a little over $3k.

- I use them as zeroshot NER/Classification models. They're pretty decent at it out of the box. Training isn't too complicated, but for small repetitive tasks, they're often good enough.

- ## [Why we shifted to Spec-Driven Development (and how we did it) : r/ClaudeCode _202511](https://www.reddit.com/r/ClaudeCode/comments/1op8b6i/why_we_shifted_to_specdriven_development_and_how/)
  - Over the last few months we came up with our own Spec-Driven Development (SDD) flow that we feel has some benefits over other approaches out there. 
  - Specifically, using a structured execution workflow and including the results of the agent work. 
- In short: you design your docs/specs first, then use them as input into implementation. And then you capture what happens during the implementation (research, agent discussion, review etc.) as output specs for future reference. 
- The cycle is:
  - Input specs: product brief, technical brief, user stories, task requirements.
  - Workflow: research â†’ plan â†’ code â†’ review â†’ revisions.
  - Output specs: research logs, coding plan, code notes, review results, findings.
- By making the docs (both input and output) first-class artifacts, you force understanding, and traceability. The goal isnâ€™t to create a mountain of docs. 
  - The goal is to create just enough structure so your decisions are traceable and the agent has context for the next iteration of a given feature area.
- First, worth mentioning this approach really only applies to a decent sized feature. Bug fixes, small tweaks or clean up items are better served just by giving a brief explanation and letting the agent do its thing.
- How we implemented it (step-by-step)
  - Define your prd.md**:** goals for the feature, user journey, basic requirements.
  - Define your tech_brief.md: high-level architecture, constraints, tech-stack, definitions.
  - For each feature/user story, write a requirements.md file: what the story is, acceptance criteria, dependencies.
  - For each task under the story, write an instructions.md: detailed task instructions
  - To start implementation, create a custom set of commands that do the following for each task
  - Commit these spec files alongside code so future folks (agents, humans) have full context.
  - Use folder conventions: e.g., project/story/task/requirements.md, â€¦/instructions.md etc. So itâ€™s intuitive.
- Bonus: If you want a tool that automates this kind of workflow opposed to doing it yourself (input specs creation, task management, output specs), Iâ€™m working on one called Devplan that might be interesting for you.

- Bmad does exactly this: https://github.com/bmad-code-org/BMAD-METHOD

- have you tried any of BMAD, GitHub/Spec-kit or Privacy-AI/spec-kitty for a community fork with extensive git worktree support

- ## [Which model do you wish could run locally but still canâ€™t? : r/LocalLLM _202511](https://www.reddit.com/r/LocalLLM/comments/1omoodc/which_model_do_you_wish_could_run_locally_but/)
- It's not really about particular models at this point. I'm way more interested in the local infrastructure around it. The main thing I'm still missing is simple knowledge-base integration for something like Open-WebUI. I would really like to just point the front-end at my local Kiwix-server and a 1TB-eBook collection, let it index to its heart's content for a few weeks and then have any model be able to reference and integrate all those information.

- Qwen3 Omni, how come no one is talking about a 30B model that can do video and speech?

- Something which I can install with just a dmg or exe file please
  - I tried a couple but I am not tech savvy and had to install a lot of other stuff to my laptop

- For me, itâ€™s not that itâ€™s a single model I canâ€™t runâ€¦ itâ€™s the fact that I canâ€™t run multiple models.

- ## [What's the missing piece in the LLaMA ecosystem right now? : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o5dh3v/whats_the_missing_piece_in_the_llama_ecosystem/)
  - For me, it's the data prep and annotation tools. The models are getting powerful, but cleaning and structuring quality training data for fine-tuning is still a major, manual bottleneck.

- Training-Data is the biggest issue for local ecosystem right now i think. There is so many datasets, but who knows about their real quality.
  - For me personally, finetuning an LLM is like 500x harder than a diffusion model, simply due to the lack of tooling. Unsloth is nice and all, but i dont want to run fucking Jupyter Notebooks, i want something akin to kohya_ss with as many of the relevant hyperparameters exposed.
- Hardware accessibility is only secondary. If you have a small Model, e.g. the Qwen3 0.6B full finetune should be possible on local hardware. If that proves to be effective, renting a GPU machine somewhere for a few bucks shouldnt be the issue.

- llms are very bad at image recognition, give it a civ or other strategy game screenshot and it gets nearly everything wrong.

- Benchmarks. There has been little to no progress in the past two years regarding how LLMs are evaluated. Itâ€™s still mostly huge catalogues of questions with predetermined answers. Thatâ€™s a very poor system for testing intelligence.
# discuss
- ## 

- ## 

- ## 

- ## 

- ## 

- ## ç°åœ¨æ‰€æœ‰çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œæ— è®ºå®ƒå·ç§°ä¸Šä¸‹æ–‡çª—å£å¯ä»¥å¤šå°‘ï¼Œè¾“å…¥æ˜¯çœŸçš„å¯ä»¥å¾ˆé•¿ï¼Œä½†æ˜¯è¾“å‡ºä¸èƒ½å¤ªé•¿ï¼Œè¾“å‡ºé•¿äº†å°±å¹»è§‰ä¸¥é‡ï¼Œç›¸å¯¹å¥½ä¸€äº›çš„æ˜¯ Geminiï¼Œ
- https://x.com/dotey/status/1995667479377707123
  - æ‰€ä»¥ä½¿ç”¨æ—¶ï¼Œä½ å¯ä»¥è¾“å…¥å¾ˆå¤šèµ„æ–™ç»™å®ƒå‚è€ƒï¼Œä½†æ˜¯æ¯æ¬¡ä¸è¦è¾“å‡ºå¤ªå¤šï¼Œæ¯”å¦‚ä¸€æ¬¡æœ€å¤šè¾“å‡ºå‡ åƒå­—ï¼Œå¤šäº†å°±è¦åˆ†é¡µã€‚
- å¯ä»¥é‡‡ç”¨åˆ†å‡ æ­¥ï¼Œç„¶åæä¾›ä¸€ä¸ªchecklistç¡®ä¿ä»–è‡ªå·±å®¡æ ¸
  - å¯¹ï¼Œtodo listæœ‰åŠ©äºæ‹‰å›æ³¨æ„åŠ›å¥½ä¸€äº›ï¼Œä½†ä¸€æ ·è¾“å‡ºå†…å®¹ä¸èƒ½å¤ªé•¿

- è¯´ä¸‹æˆ‘çš„ç†è§£ï¼šæ¯ä¸ªtokenè¾“å‡ºéƒ½æ˜¯ä¸€ä¸ªæ¦‚ç‡ï¼Œä¸¾ä¸ªä¾‹å­ï¼šæ¯”å¦‚è¯´â€œæˆ‘â€è¿™ä¸ªå­—åé¢è·Ÿç€â€œä»¬â€è¿˜æ˜¯â€œçš„â€ï¼Œå¯¹llmæ¥è¯´å°±æ˜¯å–æ¦‚ç‡é«˜çš„ã€‚ä½†æ˜¯æ¦‚ç‡æœ‰2ä¸ªé—®é¢˜ï¼š
  1. å°±ç®—æ¯æ¬¡éƒ½å–æœ€é«˜çš„ï¼Œ0.9*0.9*0.9â€¦ï¼Œæ•´ä½“æ¥çœ‹ï¼Œç¬¦åˆé¢„æœŸçš„æ¦‚ç‡è¶Šæ¥è¶Šä½ï¼Œæ‰€ä»¥è¾“å‡ºå¤ªå¤šä¹‹åæ²¡æ³•ä¿è¯ä¸èµ°å
  2. ä¸­é—´æŸä¸€ä¸ªä½ç½®ï¼Œä¸¤ä¸ªtokençš„æ¦‚ç‡ç›¸è¿‘ï¼Œllmé€‰æ‹©ä¸å°å¿ƒèµ°åäº†ä¹‹åï¼Œåé¢è¾“å‡ºçš„tokenä¼šä¸€ç›´åœ¨è¿™ä¸ªèµ°åçš„tokenä¸Šç»§ç»­è¾“å‡º
  - è¡¥å……ä¸€ç‚¹ï¼Œç°åœ¨LLMæœ‰æ¸©åº¦è®¾ç½®ï¼Œä¸æ˜¯çº¯è´ªå¿ƒå–æœ€å¤§æ¦‚ç‡çš„ã€‚è€Œä¸”ç®—åŠ›æœ‰é™ï¼Œæ— æ³•åšåˆ°å…¨å±€æœ€ä¼˜ã€‚åªèƒ½è¯´ç¯‡å¹…è¶ŠçŸ­ï¼Œè¶Šå‡†ç¡®ã€‚ ç°åœ¨å¤§å®¶åšçš„CoTï¼Œä»¥åŠåˆ©ç”¨Agentic workflowä¸æ–­é‡å†™ï¼Œå…¶å®éƒ½æ˜¯åœ¨é€¼è¿‘å…¨å±€æœ€ä¼˜ã€‚

- ä¸ªäººæ„Ÿè§‰ï¼šGemini å·ç§°æœ‰ 100 ä¸‡çš„ä¸Šä¸‹æ–‡ï¼Œå…¶å®ä¸€èˆ¬è·‘åˆ° 20-30 ä¸‡å­—å·¦å³å°±ä¸è¡Œäº†

- è¾“å…¥å’Œè¾“å‡ºå¯¹æ¨¡å‹æ¥è¯´éƒ½æ˜¯ä¸€æ ·çš„ï¼Œåªè¦ä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œæ¨¡å‹æˆ–å¤šæˆ–å°‘éƒ½ä¼šå‡ºç°æŒ‡ä»¤ä¸è·Ÿéšã€‚

- ## â³ [A Tribute to MetaAI and Stability AI - 2 Giants Who Brought us so Much Joy... And, 2025 is the Year they Die... So Sad! : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1p9ah2v/a_tribute_to_metaai_and_stability_ai_2_giants_who/)
- People think the trillion dollar spend is for the r&d and scale. Research is a fraction of the opex, and the scale is not about breaking through new model features but to suck in as much actual human usage data as possible to train exclusively better models.

- ## [China just passed the US in open model downloads for the first time : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p7alka/china_just_passed_the_us_in_open_model_downloads/)
- This analysis includes all models on HuggingFace, not just LLMs. The most downloaded models (by far) are smaller stuff like embedding models, classifiers, VAD, etc., and small base models like BERT and GPT-2. The most popular properly large instruction-tuned LLM is Qwen2.5-VL-3B-Instruct in 24th place.
  - I suspect a lot of downloads are coming from poorly configured CI/CD pipelines and stuff, rather than individual users

- I'm surprised this hasn't happened earlier, the only good open weight models from the US in the past like 6 months has been Gpt-oss.

- How tf Germany is third place?
  - They have Black Forest Labs
  - stable diffusion guys (now bfl aka flux) are from germany
- Sentence Transformers (SBERT) is originally German. Gets an ungodly number of downloads, and because the models are small people just grab it directly from HF instead of running a local mirror.
  - (Also LAION (CLAP, some popular CLIP-ViT models) and Stable Diffusion/Black Forest Labs (FLUX), though I don't think those get nearly as many downloads.)

- Chinese users predominantly go to modelscope

- ## ğŸ’° [How are Chinese AI models claiming such low training costs? Did some research : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1p6cf2p/how_are_chinese_ai_models_claiming_such_low/)
  - deepseek claims $6M training cost. Everyones losing their minds cause ChatGPT-4 cost $40-80M and Gemini Ultra hit $190M.
  - glm-4.6: $8-12M estimated, 357B parameters (thats model size)
  - Kimi K2-0905: $25-35M estimated, 1T parameters total (MoE architecture, only ~32B active at once)
  - MiniMax: $15-20M estimated, Mid-range model, mid-range cost
  - ğŸ§® Training cost = GPU hours Ã— GPU price + electricity + data costs.
  - çœŸå®çš„æˆæœ¬æ•°å­—éƒ½åº”è¯¥æ¥è¿‘ä¸Šé¢çš„å…¬å¼
  - deepseeks $6M feels like marketing. You cant rent enough H100s for months and only spend $6M unless youre getting massive subsidies or cutting major corners.
  - glms $8-12M is more realistic. Still cheap compared to Western models but not suspiciously fake-cheap.
  - Kimi at $25-35M shows you CAN build competitive models for less than $100M+ but probably not for $6M.

- One difference could be the accounting methodology. I can for sure guarantee that not every training attempt is successful, and companies spend a fortune of gpu-hours on practice runs training smaller models; and then there might be a rollback or two to earlier checkpoints in the big run. Then imagine one company counting the entire cost, while the other accounts only for the end run, and boom - you got drastically different reported figures while effectively the same amount of spent money.
- In the paper for Deepseek they are actually never claiming 6 million - there saying at an assumed price per GPU hour (canâ€™t remember from the top of my head) the final run would be around 6 million.
  - OpenAI also estimated GPT-OSS final training cost like this too iirc. They just didn't for other models.

- They were very transparent about this, and have stated multiple times that it was just the final training run in that estimate and explicitly did not include prior incremental runs.

- The costs listed are likely just the literal hardware cost for the final training run for the model. Every other aspect of the model training process is ignored.

- metas new ai team are getting 9 figure salaries lol makes it hard to profit when youâ€™re paying people such insane salaries.

- Because they're leveraging Western frontier models. Let's be clear, the Chinese labs aren't doing any hard training. All they're really doing is distilling the hard work done by Western labs.
  - I remember it being reported that Western frontier models trained on copyrighted data (even pirated material).

- [Are Chinese AI models really that cheap to train? Did some research. : r/LLMDevs](https://www.reddit.com/r/LLMDevs/comments/1p77x5k/are_chinese_ai_models_really_that_cheap_to_train/)
  - Itâ€™s a combo of all those things, especially lying, plus they used a ton of distillation techniques - i.e. they trained a ton on the output of ChatGPT and other established models. 

- ## åšäº§å“çº§æœç´¢ä»¥åå‘ç° 3.7 æ•ˆæœç‰¹å¥½ä½†æ˜¯ç‰¹åˆ«è´µï¼Œæ‰€ä»¥å¿…é¡»å¾—åš content cache é™ä½æˆæœ¬ã€‚
- https://x.com/arvin17x/status/1896922111505285484
  - ä½†ä¸ºäº†åš context cache ï¼Œå‘ç°å¿…é¡»å¾—è®°å½• token usageï¼Œä¸ç„¶æ„ŸçŸ¥ä¸åˆ° cache çš„æ•ˆæœã€‚

- å¯ä»¥åˆ†äº«ä¸‹ Context Cache çš„åŸç†ä¹ˆï¼Œæˆ‘è¿˜ä»¥ä¸ºè¿™ç§ä¸œè¥¿åªèƒ½åœ¨æ¨¡å‹ä¾›åº”å•†ä¾§åšã€‚
  - å°±æ˜¯åœ¨æ¨¡å‹ä¾›åº”å•†ä¾§åšã€‚åªæ˜¯ OpenAI å’Œ DeepSeek åšçš„æ˜¯é™é»˜æ–¹æ¡ˆï¼Œè€Œ Anthropic å’Œ Google æ˜¯éœ€è¦å¼€å‘è€…æ‰‹åŠ¨å¼€çš„
- åº”è¯¥æ˜¯ prompt caching å§
  - Anthropicå®¶è‡ªå·±çš„è¯´æ³•æ˜¯ prompt Caching. ä½†æˆ‘æ„Ÿè§‰è¡Œä¸šé‡Œæ„Ÿè§‰è¿˜æ˜¯ä¹ æƒ¯å« context caching çš„ã€‚
  - ä¸è¿‡ OpenAI å®˜æ–¹å’Œ Anthropic å®˜æ–¹éƒ½ç§°ä¹‹ä¸º prompt caching

- å¾ˆå¥‡æ€ªï¼Œè¿™ç©æ„ä¸ºå•¥ä¸é»˜è®¤å¼€å¯ã€‚
  - æŸäº›åœºæ™¯ä¸‹çš„ç¡®ä¸ä¸€å®šé€‚åˆï¼Œå› ä¸ºå†™å…¥ cache çš„æˆæœ¬æ˜¯åŸä»·çš„ 1.25å€

- ## It is common to generate train and validation sets using random splitting.
- https://x.com/_avichawla/status/1898622288737767785
  - However, in many situations, it can be fatal for model building.
  - Consider building a model that generates captions for images.
  - Group shuffle split solves this.

- ## ç”±äºDeepSeek-R1 çˆ†ç«ï¼Œæ‰€ä»¥ä¸ºå¤§å®¶å¸¦æ¥ä»€ä¹ˆæ˜¯LLMè’¸é¦æŠ€æœ¯çš„ç¬”è®°ã€‚
- https://x.com/karminski3/status/1882233538042597423
  - å‡ ä¸ªåŠ©è®°è¯ï¼šæ•™å¸ˆæ¨¡å‹ï¼Œå­¦ç”Ÿæ¨¡å‹ï¼Œè½¯ç›®æ ‡ï¼Œç¡¬ç›®æ ‡ã€‚

- https://x.com/ShanghaoJin/status/1882679738789216456
  - ä½ å¬è¯´è¿‡ä»€ä¹ˆå«â€œè’¸é¦â€ä¹ˆï¼Ÿè¯´ä¸ªå¤§ç™½è¯ï¼šå°±æ˜¯æ‹¿äººå®¶ç®—å‡ºæ¥çš„æ¨¡å‹å‚æ•°ï¼Œè·³è¿‡æ‰€æœ‰æ•°æ®æ¸…æ´—ã€è®­ç»ƒï¼Œåšæœ€åä¸€ç¨‹ã€‚å…¶å®æ²¡æœ‰ä»»ä½•åˆ›æ–°
  - å¥½åƒäººå®¶è¯æ˜äº†Ï€=3.14ï¼Œä»–æ‹¿ç»“æœå»ç®—äº†åœ†é¢ç§¯ã€‚è®©ä»–å†è‡ªå·±å»è¯æ˜ç®—ä¸€ä¸ªeï¼Œä»–åˆæŠ“çäº†
- åƒä¸‡ä¸è¦ç”¨â€œå¤§ç™½è¯â€æ¥è§£é‡Šè‡ªå·±éƒ½æ²¡å®Œå…¨ç†è§£çš„æ¦‚å¿µï¼Œ åªèƒ½è®©å¤–è¡Œæ‹æ‰‹ï¼Œæ‡‚çš„äººåªä¼šç¬‘è¯ä½ ã€‚ ä½ å®Œå…¨ä¸çŸ¥é“è’¸é¦æ˜¯åœ¨å¹²ä»€ä¹ˆã€‚å¦‚æœä½ çŸ¥é“çš„è¯ï¼Œé‚£å°±æ˜¯å®Œå…¨ä¸çŸ¥é“Deepseek åœ¨åšä»€ä¹ˆã€‚
- â€œè’¸é¦â€è¯´æ³•ä¸æ­£ç¡®ã€‚1. è’¸é¦æ•ˆæœä¸€èˆ¬ä¸ä¼šè¶…è¿‡åŸæ¨¡å‹ 2. deepseekçš„ reasoningè¡Œä¸ºå’Œå¸‚é¢ä¸Šå…¶ä»–æ¨¡å‹ä¸ä¸€è‡´(æœ‰è¶…è¶Šäººç±»æ ‡æ³¨çš„å¥‡å¦™è¡Œä¸º) 3. å¼€æ™ºå¯¹å†è®­ç»ƒæ¨¡å‹æœ‰ç¦æ­¢å¹¶ä¼šç›‘æ§ APIæ»¥ç”¨

- ## æˆ‘æ—¥å¸¸ç”¨ Cursor å†™ä»£ç çš„åœºæ™¯ä¹‹ä¸€ï¼šâ€œè¯·å‚è€ƒä»£ç  @ XXX1 @ XXXn åš YYY äº‹ã€‚â€
- https://x.com/dotey/status/1869436413600731146
  - ç®€å•æ¥è¯´å°±æ˜¯è®© AI ç…§è‘«èŠ¦ç”»ç“¢ï¼Œé‡è¦çš„æ˜¯ç»™å‡ºå……è¶³çš„ä¸Šä¸‹æ–‡ï¼Œè®© AI å¯ä»¥å­¦ä¹ å’Œæ¨¡ä»¿ã€‚å‰©ä¸‹çš„å°±æ˜¯ Review + Acceptï¼Œå¾ˆç®€å•é«˜æ•ˆã€‚
  - ç‰¹åˆ«è¦æ³¨æ„çš„æ˜¯ç¬¬ä¸€ä¸ªâ€œè‘«èŠ¦â€è¦æ‰“ç£¨å¥½ï¼Œè¿™æ ·åç»­çš„â€œç“¢â€æ‰ä¸ä¼šç”»æ­ªã€‚

- æ›´ç®€å•çš„åšæ³•æœ‰æ—¶å€™å¯ä»¥ç›´æ¥ @ git æŸæ¬¡æäº¤

- æˆ‘ç°åœ¨æ˜¯æ–°é¡¹ç›®é‡Œåˆ›å»ºä¸€ä¸ªtxtæ–‡ä»¶ï¼Œé‡Œé¢å†™ä¸Šæƒ³æ³•å’Œgptå¯¹æˆ‘æƒ³æ³•çš„å»ºè®®ï¼Œç„¶åè®©cursor å‚è€ƒè¿™ä¸ªæ–‡ä»¶æ¥å¼€å‘ï¼Œé‡Œé¢æˆ‘ä¹Ÿæœ‰æ—¶ä¼šå†™ä¸Šæ­¥éª¤ï¼Œé¦–å…ˆå®ç°ä»€ä¹ˆï¼Œç„¶åå®ç°ä»€ä¹ˆï¼Œåšä¸€æ®µäº†ï¼Œè®©cursoræ ¹æ®è¿™ä¸ªæ–‡ä»¶æ£€æŸ¥ä¸€ä¸‹é¡¹ç›®å®Œæˆåº¦ï¼Œåˆ—å‡ºæ¥å“ªäº›æ²¡åšï¼Œè¿™æ ·åå¤è¿­ä»£å‘å‰

- LM å……åˆ†è¯æ˜äº†äººç±»çš„æœ¬è´¨å°±æ˜¯å¤è¯»æœºã€‚ å“ªé‡Œæœ‰ä»€ä¹ˆæŒ‡ä»¤éµå¾ªï¼Œæ¨ç†ï¼Œå¤§å®¶éƒ½æ˜¯ä»ä¸åŒçš„ç»´åº¦ç”¨ä¸åŒçš„æ–¹å¼åœ¨å¤è¯»ä¸€äº›ä¸œè¥¿è€Œå·²

- ## å¦‚æœæƒ³è¦è®© LLM ç¨³å®šç”Ÿæˆ JSON å¯¹è±¡ï¼Œæœ€ç®€å•çš„æ–¹å¼å°±æ˜¯ä½¿ç”¨ zod å®šä¹‰ schema å¹¶é…åˆ @vercel ai sdkçš„ generateObjectä½¿ç”¨ï¼Œæ¯”å¦‚è¿™é‡Œæˆ‘æƒ³è¦ä»ç½‘é¡µæ–‡æœ¬å†…å®¹æå–ç»“æ„åŒ–çš„ä¿¡æ¯ã€‚
- https://x.com/FeigelC35583/status/1819558128297648412
  - è¿™ç§æ–¹å¼å’Œå½“åˆ langchain åœ¨ prompt é‡Œå†™ä¸€å¤§å †json å®šä¹‰æœ‰æœ¬è´¨åŒºåˆ«ï¼Œåœ¨äºä½¿ç”¨äº† function call çš„èƒ½åŠ›
  - ä»è¯·æ±‚ä¸­å¯ä»¥çœ‹åˆ°ï¼Œæœ¬è´¨ä¸Šæ˜¯åœ¨è°ƒç”¨æ¨¡å‹çš„æ—¶å€™ï¼Œæ„å»ºäº†ä¸€ä¸ªåä¸º json çš„ å‡½æ•°, æè¿°æ˜¯ respond with a json object, å…¶ä¸­å‚æ•°æ˜¯è‡ªå·±å®šä¹‰çš„ schemaï¼Œç„¶ååœ¨ tool_choice ä¸­é™åˆ¶å¿…é¡»è¦ä½¿ç”¨è¿™ä¸ª json å‡½æ•°ï¼Œé‚£ä¹ˆæ¨¡å‹å°±ä¼šè¿”å›è°ƒç”¨json å‡½æ•°çš„å‚æ•°ï¼Œå³ä½ å®šä¹‰çš„ schema
  - ç¤ºä¾‹ä»£ç æ¥è‡ªäºhttps://github.com/DiscovAI/DiscovAI-crawl æˆ‘æ­£åœ¨ building çš„ä¸€ä¸ªé¢å‘ RAG åº”ç”¨çš„çˆ¬è™«å¹³å°
- åº”è¯¥åªæœ‰GPTç³»åˆ—èƒ½ç”¨å§
  - æ”¯æŒfunction callå°±å¯ä»¥ï¼Œdeepseekåº”è¯¥ä¹Ÿå¯ä»¥çš„
- åœ¨è¿™åŸºç¡€ä¸Šã€‚æˆ‘ä¼šè€ƒè™‘ä½¿ç”¨jsonrepairè¿™ä¸ªåŒ…ï¼Œæ‰‹åŠ¨ä¿®å¤ä¸‹ï¼Œå¢åŠ å®¹é”™
- å¦‚æœå¤§æ¨¡å‹æ²¡æœ‰æ²¡æœ‰è¿”å›å¯¹åº”è¦æ±‚çš„å­—æ®µæ•°æ®ï¼Œæˆ–è€…è¿”å›é”™äº†ç±»å‹ï¼Œå®ƒä¼šæ€ä¹ˆæ ·ï¼Œä¼šè‡ªå·±è¡¥å……ç©ºçš„ï¼Œæˆ–è€…è‡ªåŠ¨è½¬æ¢ç±»å‹å—ï¼Ÿ
  - ä¸ä¼šè¡¥å……ï¼Œä¼šthrow errorï¼Œä¹Ÿå¯ä»¥ç”¨ä¸Šé¢æ¨å‹æ¨èçš„jsonrepairæ‰‹åŠ¨fix

- èƒ½æ”¯æŒå¼€æºæ¨¡å‹å—
  - å–å†³äºæ¨¡å‹æ”¯ä¸æ”¯æŒfunction callï¼Œæ”¯æŒçš„è¯å°±å¯ä»¥ï¼Œæ•ˆæœçš„è¯è¦çœ‹æ¨¡å‹çš„èƒ½åŠ›
- ç”¨ function call æ„Ÿè§‰æ¨¡å‹çš„èƒ½åŠ›é™äº†ä¸€ä¸ªç»´åº¦ï¼Œä¸å¦‚ç›´æ¥ç»™æ–‡æœ¬ï¼Œæˆ‘è¿˜æ˜¯æ›´å–œæ¬¢ç”¨xmlè‡ªå·±æå–ã€‚

- æˆ‘æ˜¯ç”¨ä¼ªä»£ç â•ç±»å‹å£°æ˜, ä¹Ÿæ˜¯ä¸€æ ·çš„ç¨³å®šè¾“å‡º json
- langchainæ¡†æ¶ä¸­æœ‰Pydantic json è§£æå™¨å¯ä»¥ç›´æ¥ç”¨ï¼Œæœ¬è´¨ä¹Ÿæ˜¯ç”Ÿæˆschemaï¼Œå†é…åˆé‡è¯•è§£æå™¨ä¹Ÿå¯ä»¥ç¨³å®šç”Ÿæˆjsonæ ¼å¼

- ## ğŸ’¡ LLMs are literally the most unreliable technology of all time (followed by **ing bluetooth)
- https://x.com/Steve8708/status/1819448686424084892
  - After an absurd amount of trial and error, we've internally created a set of rules for make LLMs considerably more reliable
  - our secrets: restrict the llm to only what rag provides

- what's your stance on AI for no-code? Do people prefer drag-and-drop vs prompting?
  - i think the winning move is combining both

- Bluetooth is hell and causes frustration daily.

- ## ğŸŒ° Firefox will use Transformers.js to power on-device features
- https://x.com/osanseviero/status/1797291569348751848
  - In their PDF Editor to generate alt text for images
  - Improve translations
  - Fully offline, open-source and with <200M models
- [Experimenting with local alt text generation in Firefox Nightly - Mozilla Hacks - the Web developer blog _202405](https://hacks.mozilla.org/2024/05/experimenting-with-local-alt-text-generation-in-firefox-nightly/)
  - https://huggingface.co/Mozilla

    - æä¾›äº†æ•°æ®é›†å’Œæ¨¡å‹

- Offline and open-source is a big win for privacy-focused tools

- ## [langchainåˆ°åº•è¯¥æ€ä¹ˆä½¿ç”¨ï¼Œå¤§å®¶åœ¨é¡¹ç›®ä¸­å®è·µæœ‰æˆåŠŸçš„æ¡ˆä¾‹å—? - çŸ¥ä¹](https://www.zhihu.com/question/609483833)
- LangChainä¹‹æ‰€ä»¥å¤§ç«ï¼Œæ˜¯å› ä¸ºå®ƒæä¾›äº†ä¸€ç³»åˆ—æ–¹ä¾¿çš„å·¥å…·ã€ç»„ä»¶å’Œæ¥å£ï¼Œå¤§å¤§é™ä½äº† AI åº”ç”¨å¼€å‘çš„é—¨æ§›ï¼Œä¹Ÿæå¤§ç®€åŒ–äº†å¤§æ¨¡å‹åº”ç”¨ç¨‹åºçš„å¼€å‘è¿‡ç¨‹ã€‚
  - LangChainæ¡†æ¶èƒŒåçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†è‡ªç„¶è¯­è¨€å¤„ç†åºåˆ—åˆ†è§£ä¸ºå„ä¸ªéƒ¨åˆ†ï¼Œå…è®¸å¼€å‘äººå‘˜æ ¹æ®è‡ªå·±çš„éœ€æ±‚é«˜æ•ˆåœ°å®šåˆ¶å·¥ä½œæµç¨‹ã€‚
- Langchainæœ‰6å¤§æ ¸å¿ƒæ¨¡å—ï¼š
  - Modelsï¼šæ¨¡å‹ï¼Œæ˜¯å„ç§ç±»å‹çš„æ¨¡å‹å’Œæ¨¡å‹é›†æˆã€‚
  - Promptsï¼šæç¤ºï¼ŒåŒ…æ‹¬æç¤ºç®¡ç†ã€æç¤ºä¼˜åŒ–å’Œæç¤ºåºåˆ—åŒ–ã€‚
  - Memoryï¼šè®°å¿†ï¼Œç”¨æ¥ä¿å­˜å’Œæ¨¡å‹äº¤äº’æ—¶çš„ä¸Šä¸‹æ–‡çŠ¶æ€ã€‚
  - Indexesï¼šç´¢å¼•ï¼Œç”¨æ¥ç»“æ„åŒ–æ–‡æ¡£ï¼Œä»¥ä¾¿å’Œæ¨¡å‹äº¤äº’ã€‚åŒ…æ‹¬æ–‡æ¡£åŠ è½½ç¨‹åºã€å‘é‡å­˜å‚¨å™¨ã€æ–‡æœ¬åˆ†å‰²å™¨å’Œæ£€ç´¢å™¨ç­‰ã€‚
  - Agentsï¼šä»£ç†ï¼Œå†³å®šæ¨¡å‹é‡‡å–å“ªäº›è¡ŒåŠ¨ï¼Œæ‰§è¡Œå¹¶ä¸”è§‚å¯Ÿæµç¨‹ï¼Œç›´åˆ°å®Œæˆä¸ºæ­¢ã€‚
  - Chainsï¼šé“¾ï¼Œä¸€ç³»åˆ—å¯¹å„ç§ç»„ä»¶çš„è°ƒç”¨ã€‚
- LangChain é€šå¸¸è¢«ç”¨ä½œã€Œç²˜åˆå‰‚ã€ï¼Œå°†æ„å»º LLM åº”ç”¨æ‰€éœ€çš„å„ä¸ªæ¨¡å—è¿æ¥åœ¨ä¸€èµ·ã€‚ä½¿ç”¨Langchainä¸­ä¸åŒç»„ä»¶çš„ç‰¹æ€§å’Œèƒ½åŠ›ï¼Œå¯ä»¥æ„å»ºä¸åŒåœºæ™¯ä¸‹çš„åº”ç”¨ï¼Œå¦‚èŠå¤©æœºå™¨äººã€åŸºäºæ–‡æ¡£çš„é—®ç­”ã€çŸ¥è¯†ç®¡ç†ã€ä¸ªäººåŠ©ç†ã€Agentæ™ºèƒ½ä½“ç­‰ç­‰ã€‚

- ä½ çš„è¿™ä¸ªè®¤è¯†å­˜åœ¨ä¸€äº›åå·®ï¼Œé¦–å…ˆï¼Œä¾èµ–API key æ˜¯ä¸ºäº†ä½ ä½¿ç”¨å¤§æ¨¡å‹å‚å•†çš„æœåŠ¡å’Œé‰´æƒï¼Œè¿™æ²¡æœ‰ä»€ä¹ˆæ‹‰è·¨çš„ã€‚å¾ˆå¤šç¬¬ä¸‰æ–¹çš„æœåŠ¡éƒ½éœ€è¦é‰´æƒéªŒè¯ï¼Œè¿™æ˜¯æ¯”è¾ƒä¸»æµçš„æ–¹å¼ã€‚
- å¯ä»¥ä¼ä¸šè‡ªå·±éƒ¨ç½²å¤§æ¨¡å‹ï¼Œè¿™ç§æˆæœ¬æ˜¯å¾ˆé«˜çš„ã€‚ä»æˆ‘ä»¬è‡ªå·±çš„å®éªŒæ•ˆæœæ¥çœ‹ï¼Œ13B ä»¥ä¸‹çš„å¤§æ¨¡å‹åŸºæœ¬å°±æ˜¯ç©å…·ï¼Œä¼˜åŒ–åŠå¤©è´¹æ—¶è´¹åŠ›ï¼Œè€Œ 34B æˆ–è€…æ›´å¤§çš„æ¨¡å‹ï¼Œå…¬å¸éƒ¨ç½²æˆæœ¬åˆå¾ˆé«˜ã€‚
- langchain ä¸­çš„ç‰¹è‰²æ˜¯å®ƒçš„ langchain expression language (LCELï¼‰ï¼Œæ˜¯ä¸€ç§ç±»ä¼¼ linux ç®¡é“å½¢å¼çš„è°ƒç”¨æ–¹å¼ï¼Œå¯ä»¥å¾ˆç®€å•çš„å®ç°å®ƒçš„ chain ç›¸å…³çš„åŠŸèƒ½ã€‚è¿™ä¸ªï¼Œåœ¨æˆ‘å®é™…ä½¿ç”¨çš„æ—¶å€™ï¼Œæ²¡æœ‰æƒ³è±¡çš„é‚£ä¹ˆå¥½ç”¨ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µå»å­¦ä¹ ã€‚
- æœ€åï¼Œlangchain ä¸­è¿˜æœ‰ä¸€ä¸ªå«åš langgraph çš„ç»„ä»¶ï¼Œèƒ½å¤Ÿå’Œ pytorch ä¸€æ ·ç”¨æ­ç§¯æœ¨çš„æ–¹å¼å»æ„é€ ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ã€å¾ªç¯çš„é“¾ï¼Œæ¯” LCEL æ›´é«˜çº§ã€‚

- 
- 

- ## LLMæåç¼–è¯‘ï¼Œ.not careå’ŒJvavç”¨æˆ·å†ä¹Ÿä¸ç”¨æŠ˜è…¾ä»€ä¹ˆæ··æ·†äº†ï¼Œéƒ½æ²¡ç”¨äº†
- https://twitter.com/geniusvczh/status/1774053196039962758
  - æ–‡ç« é‡Œåç¼–è¯‘çš„æ˜¯x86, x86éƒ½å¯ä»¥ï¼ŒILéš¾åº¦åªä¼šæ›´ä½
- å¤§æ¦‚çœ‹äº†ä¸€ä¸‹ï¼Œå°±æ˜¯æŠŠç¼–è¯‘å‡ºçš„æ±‡ç¼–è·Ÿæºä»£ç åšäº†ä¸€ä¸ªç®€å•çš„seq2seqçš„fine tuneï¼Œè®­ç»ƒé›†è¿æ··æ·†éƒ½æ²¡æœ‰ï¼Œç¦»è®©æ‰€æœ‰æ··æ·†éƒ½æ²¡ç”¨é‚£æ›´æ˜¯è¿˜å·®å¾—è¿œã€‚
- 17å¹´googleé‚£ç¯‡transformerçš„è®ºæ–‡å°±é è¿™æ ·å®Œæˆäº†è‡ªç„¶è¯­è¨€çš„ç¿»è¯‘ï¼Œè¿™äº›éƒ½æ˜¯è¿Ÿæ—©çš„äº‹ï¼Œåç¼–è¯‘å’Œåæ··æ·†çš„è®­ç»ƒæ•°æ®éƒ½æ˜¯å¯ä»¥æ‰¹é‡ç”Ÿæˆçš„ï¼Œåšèµ·æ¥ç®€å•å¤šäº†
  - æˆ‘è§‰å¾—LLMå¯¹äºåç¼–è¯‘å’Œåæ··æ·†ï¼Œå¯èƒ½æ›´å¤§çš„ä½œç”¨åœ¨äºç”Ÿæˆäººç±»å‹å¥½çš„å˜é‡/ç¨‹åºç»“æ„ã€‚æ¯•ç«Ÿåç¼–è¯‘å’Œåæ··æ·†æ˜¯çŒ«é¼ æ¸¸æˆï¼Œæ€»å¯ä»¥æƒ³å‡ºæ–°ç‚¹å­ï¼Œäººç±»çš„å¹²é¢„è¿˜æ˜¯å¿…ä¸å¯å°‘çš„ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼ŒåŸºäºè§„åˆ™/ç¨‹åºåˆ†æçš„ä¼ ç»Ÿæ–¹æ³•å¯èƒ½æ›´å¥½ï¼Œç„¶åå†ç”¨LLMçŒœå˜é‡å
- ä¸ºäº†æ‹‰èµ„é‡‘è€Œå·²ï¼Œé’±ç”³è¯·åˆ°äº†è®ºæ–‡å°±æ²¡å•¥ç”¨äº†â€¦â€¦å¤„ç†å±å±±ç•™ç»™å·¨å¤´çš„ç¨‹åºå‘˜å°±è¡Œäº†ï¼Œè¿˜è½®ä¸åˆ°å­¦æœ¯åœˆæ¥æŒ‡ç‚¹æ±Ÿå±±
- è¿™ç§å±€é™äºå‡½æ•°çš„åæ··æ·†å•¥ç”¨éƒ½æ²¡æœ‰ï¼Œå¯¹ä»˜ç‚¹ä¸‰è„šçŒ«åŠŸå¤«çš„æ··æ·†è¿˜å·®ä¸å¤š

- ## ğŸª§ ç ”ç©¶äº†ä¸€ä¸‹æœ¬åœ°å¤§æ¨¡å‹çš„åœºæ™¯ï¼š
- https://twitter.com/changmingY/status/1773336179296887162
  1. ä¸èƒ½è”ç½‘çš„å›½å†…ç”¨æˆ·
  2. ä¸€èˆ¬ç”¨æˆ·æœºå™¨é…ç½®è¾¾ä¸åˆ°ï¼Œæ•ˆç‡å¤ªå·®
  3. æœ¬åœ°çŸ¥è¯†åº“ç®—æ˜¯ä¸€ä¸ªåˆšæ€§éœ€æ±‚
  4. å‚ç›´é¢†åŸŸæ¨¡å‹è¶Šæ¥è¶Šå¤š, ä¸€ä¸ªhubé›†ä¸­ä½¿ç”¨
  5. å°è€Œç¾çš„æ¨¡å‹ä¼šè¶Šæ¥è¶Šå¤šï¼Œå®Œæˆä¸€ä¸ªç‰¹å®šåŠŸèƒ½

- ## ollama çš„ç¼–è¯‘ç©çš„å¤ªèŠ±äº†ï¼Œå…ˆæ˜¯å§ llama.cpp åœ¨ä¸åŒ cpu å’Œ gpu çš„åŠ¨æ€é“¾æ¥åº“éƒ½ç¼–è¯‘äº†å‡ºæ¥é¿å…ç”¨æˆ·åœ¨è¿è¡Œæ—¶å†å»ç¼–è¯‘ï¼Œ
- https://twitter.com/liumengxinfly/status/1767073319956971891
  - ç„¶åç”¨ go çš„ embed ç‰¹æ€§ç›´æ¥æŠŠè¿™äº›åŠ¨æ€åº“å…¨éƒ½æ‰“åŒ…åˆ° go çš„äºŒè¿›åˆ¶é‡Œï¼Œç„¶ååœ¨ç”¨ cgo å’Œ dlfcn åŠ è½½å’Œè°ƒç”¨ llama.cppï¼Œå®ç°äº†ä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶å…ç¼–è¯‘ï¼Œå…å®‰è£…çš„è§£å†³æ‰€æœ‰é—®é¢˜

- https://twitter.com/holegots/status/1767427148506431665
  - ä¸è¿‡è¿™ä¸ªæœ¬è´¨ä¹Ÿæ˜¯ llama.cpp å¥—å£³å§ , åº•å±‚è¿˜æ˜¯ cpp, golang å¹¶ä¸å‚ä¸å®é™…çš„æ¨ç†.

- ## æœ€æ–°ç‰ˆçš„ OpenAI Translator å·²ç»æ— ç¼æ”¯æŒæœ¬åœ°å¤§æ¨¡å‹äº†ï¼ˆOllamaï¼‰ï¼Œæ— éœ€è”ç½‘ï¼Œå¿«é€Ÿä¾¿æ·ï¼Œå®‰å…¨ç¨³å®šï¼å†ä¹Ÿä¸æ€• OpenAI è´¦å·è¢«å°äº†ï¼ç¿»è¯‘æ•ˆæœå¯¹æ¯”å¤§å®¶å¯ä»¥çœ‹ä¸€ä¸‹æˆªå›¾ï¼Œå¤§å®¶å¿«æ¥ä¸‹è½½ä½“éªŒä¸€ä¸‹å§ï¼ _202402
- https://twitter.com/yetone/status/1761607398819840511
- ç°åœ¨å¥½åƒè¿˜ä¸æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹ï¼Ÿåªæœ‰æœ‰é™çš„å‡ ä¸ªæ¨¡å‹å¯ä¾›é€‰æ‹©ï¼Œæœ€å¥½æ˜¯æœ‰ä¸€ä¸ªæ–‡æœ¬æ¡†å¯ä»¥è‡ªå®šä¹‰è¾“å…¥
- è¿™æ˜¯Mistralå¤šå¤§çš„æ¨¡å‹ï¼Œ7Bçš„å—ï¼Ÿ
  - æ˜¯çš„

- ä¸çŸ¥é“è¿™äº›7b 13bçš„å°æ¨¡å‹å“ªä¸ªç¿»è¯‘è´¨é‡æ›´é«˜

- ## é˜¿é‡Œäº‘ç«Ÿç„¶æ”¯æŒè¿™ä¹ˆå¤šæ¨¡å‹äº†
- https://twitter.com/yihong0618/status/1746745371441967540
- http://ai.azureä¹ŸåŒ…å«äº†å¥½å¤šæ¨¡å‹ï¼Œæ˜¨å¤©æƒŠåˆ°äº†

- ## è¶Šæ¥è¶Šè§‰å¾— RAG è¿™ä¸œè¥¿æœ‰æ„æ€ã€‚
- https://twitter.com/wwwgoubuli/status/1737471851654160548
  - åŠå¹´å‰æ¥è§¦åˆ°è¿™ä¸ªè¯çš„æ—¶å€™å¼€å§‹æˆ‘è¿˜æœ‰äº›ä¸å±‘ï¼Œæœç´¢å†…å®¹æ’å…¥åˆ°æç¤ºè¯ç®—ä»€ä¹ˆå˜›ï¼Œå°å­¦äºŒå¹´çº§éƒ½èƒ½æ˜ç™½ã€‚å°¤å…¶æ˜¯çœ‹åˆ°éšä¾¿ä¸¢å‘é‡åº“éƒ½èƒ½è·‘å‡ºä¸ªä¸ƒä¸ƒå…«å…«ï¼Œè¶Šå‘è§‰å¾—è¿™ä¸ªç®€å•ã€‚
  - ä½†ç°åœ¨çœŸçš„æäº†åŠå¹´ï¼Œæˆ‘è¶Šå‘çš„è§‰å¾—è¿™æ‰æ˜¯ä¸‹ä¸€ä¸ªå¤§å¤šæ•°äººå¯ä»¥å‚ä¸çš„é£å£ã€‚å®ƒæœ‰é—¨æ§›ã€‚
- æŠ€å·§å¾ˆå¤š æ‰€ä»¥å¥½ç© ä½†é£é™©æ˜¯å¤§éƒ¨ä»½æŠ€å·§éƒ½è¢«æ¨¡å‹æä¾›å•†ç©è¿‡ï¼Œ80%éœ€æ±‚éƒ½å¯èƒ½è¢«ä»–ä»¬ç›´æ¥è¦†ç›–
  - RAGä¸å°±æ˜¯query transformation/rewrite/expanding, hybrid search, reranking, etcå—ï¼Ÿå½“ç„¶è¿˜æœ‰äº›å…¶ä»–æŠ€å·§å•¥IAGä¹‹ç±»çš„ã€‚æ•°æ®ingestionä¹Ÿæœ‰äº›æŠ€å·§ï¼Œä¸è¿‡æˆ‘çœ‹ä¸»è¦è¿˜æ˜¯åœ¨queryä¸Šã€‚ è¿™äº›å¤§éƒ¨åˆ†OAI, Baichuan, æœˆä¹‹æš—é¢å†…éƒ¨éƒ½æ¢ç´¢è¿‡äº†å§
- RAGä¸€çœ‹å°±æ˜¯ä¸€ä¸ªæœ‰é—®é¢˜çš„åŒºåŸŸï¼Œå¤§æ¨¡å‹éšæ—¶ä¸‹ä¸€æ¬¡å‡çº§å¯èƒ½å°±ä¼šæ”¹å˜æ•´ä¸ªæ¡†æ¶ï¼Œ3.5è¿˜èƒ¡è¯´å…«é“ï¼Œ4å·²ç»å¾ˆå¤šéƒ½æ˜¯æœ‰æ ¹æœ‰æ®çš„äº†
- æåˆ°æœ€åï¼Œè¿˜æ˜¯æ¸…æ´—æ•°æ®ï¼ŒRAGåªç”¨ç®€å•ç­–ç•¥è§£å†³å¤§å¤šæ•°é—®é¢˜ï¼Œå¯è§‚æµ‹ã€‚å‰ææ˜¯æ‰€æœ‰å¤æ‚ç­–ç•¥éƒ½è¦è¯•è¿‡æ‰çŸ¥é“ã€‚

- ## LangChainå¼€æºäº†AnythingLLMï¼šå¯ä»¥ä¸ä»»ä½•å†…å®¹èŠå¤©çš„ç§äºº ChatGPTï¼Œåº”è¯¥å°±æ˜¯ä»–ä»¬è‡ªå·±æ–‡æ¡£ç³»ç»Ÿç”¨çš„é‚£ä¸€å¥—ã€‚
- https://twitter.com/op7418/status/1733893368974073873
  - An efficient, customizable, and open-source enterprise-ready document chatbot solution.
  - https://github.com/Mintplex-Labs/anything-llm /MIT/js/python

- æœ‰æ²¡æœ‰è¯¦ç»†è¯´æ˜ï¼Ÿæœ€å¤§å¯ä»¥æ”¯æ’‘å¤šå¤§çš„æ–‡æ¡£ï¼Ÿ
  - åº”è¯¥æ˜¯ä¸é™å¤§å°çš„ï¼Œæ‹†å¼€å°±å¥½äº†
- è¯´æ²¡è¯´ç¡¬ä»¶éœ€æ±‚ï¼Ÿ

- ## å¤§æ¨¡å‹çš„è¿™äº› benchmark åº”è¯¥æ˜¯å…¨å®‡å®™æœ€æ²¡ç”¨çš„ benchmark äº†å§ï¼Ÿ
- https://twitter.com/yihong0618/status/1721401347533324688
- ä¹Ÿä¸æ˜¯å…¨æ²¡ç”¨ï¼Œä¹Ÿæœ‰ä¸€äº›æœ‰ç”¨çš„, å°¤å…¶ç»†åˆ†ä»»åŠ¡ä¸Šçš„ï¼Œè¿˜æ˜¯æŒºæœ‰ç”¨çš„ã€‚å½“å‰ç›¸æ¯”å…¶ä»–benchmarkï¼Œå¯æ“ä½œç©ºé—´ç¡®å®å¤§
- å…¬å¼€çš„åªèƒ½å…¨çœ‹è‡ªè§‰

- ## ä¸­æ–‡å¼€æºæ¨¡å‹è™½å¤šï¼Œæ•°æ®é›†å´å¾ˆå°‘å¼€æºã€‚
- https://twitter.com/9hills/status/1718828132046942218
  - ç›®å‰è‹±æ–‡ 7B è§„æ¨¡çš„ SOTA æ¨¡å‹æ˜¯ zephyr-7b-betaã€‚å®ƒæ”¾å¼ƒäº†è´¨é‡å‚å·®ä¸é½çš„å¼€æºæ•°æ®é›†ï¼Œä½¿ç”¨ChatGPTå’ŒGPT-4 å…¨æ–°æ ‡æ³¨äº† UltraChat å’Œ UltraFeedback æ•°æ®é›†ï¼ˆå·²å¼€æºï¼‰ã€‚æ˜¯ llama-index é¡¹ç›®å®æµ‹å‡ºæ¥å”¯ä¸€èƒ½å¤Ÿæ”¯æŒ Agent çš„å°å‚æ•°æ¨¡å‹ã€‚
- ä¸­æ–‡æ•°æ®é›†éƒ½æ˜¯æ‹¿æ¥å–é’±çš„
