---
title: lib-ai-app-community-model
tags: [ai, community]
created: 2023-10-30T07:33:56.233Z
modified: 2023-10-30T07:34:03.602Z
---

# lib-ai-app-community-model

# guide

- tips
  - Â§ßÊ®°ÂûãÁõ∏ÂÖ≥ÁöÑ‰∫ßÂìÅÁ†îÂèëÔºåÂéüÁêÜÁöÑÂèØËß£ÈáäÊÄßÂæàÂ∑ÆÔºåÊïàÊûúÁöÑÂèØËß£ÈáäÊÄß‰πüÂ∑Æ

- model-features
  - reasoning/thinking
  - tool-use/function-call
  - vision
  - embedding
  - Ê≥®ÊÑèÊúâ‰∫õÁ§æÂå∫ÈáèÂåñÁöÑÊ®°ÂûãÂèØËÉΩÈÅóÊºèÊ†áÊ≥®‰∫ÜÈÉ®ÂàÜfeatures, ÂèØÂú®Êú¨Âú∞ÊµãËØïÊù•Á°ÆÂÆöÊòØÂê¶ÊîØÊåÅ

- ÁßªÂä®Á´ØÂ§ßÊ®°Âûã
  - ÂèÇËÄÉgoogle-gemma-1b

- [Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÔºö‰ªéÁêÜËÆ∫Âà∞ÂÆûË∑µ](https://intro-llm.github.io/)
  - Â§çÊó¶Â§ßÂ≠¶Âº†Â•áÊïôÊéàÂõ¢ÈòüÂÜô‰∫Ü‰∏ÄÊú¨Âú®Á∫øÂÖçË¥πÁöÑÁîµÂ≠ê‰π¶ÔºåÂ§ßÊ¶ÇÊúâ 300 È°µÁØáÂπÖÔºåÂ∞ÜÂ§ßÊ®°Âûã‰ªéÁêÜËÆ∫Âà∞ÂÆûÊàòÁöÑÊØè‰∏™Èò∂ÊÆµÈÉΩÊèèËø∞ÁöÑËæÉ‰∏∫Ê∏ÖÊ•ö
# discuss-stars
- ## 

- ## 

- ## ü§î [Á´Ø‰æßÊ®°Âûã‰ºöÊòØ AI ÊäÄÊúØÊºîËøõÁöÑ‰∏ã‰∏Ä‰∏™ „ÄåÂøÖ‰∫â‰πãÂú∞„Äç ÂêóÔºüÂΩìÂâçËêΩÂú∞Èù¢‰∏¥Âì™‰∫õÊ†∏ÂøÉÁì∂È¢àÔºü - Áü•‰πé](https://www.zhihu.com/question/1914319403023032351)
- TO CÂú∫ÊôØÂ∫îÁî®ÊúÄÂ§ßÁâπÁÇπÊòØÁ°¨‰ª∂ÂèÇÂ∑Æ‰∏çÈΩê„ÄÇÊÄßËÉΩÂ∑ÆËÉΩÂ§ßÂà∞10Â§öÂÄç‰ª•‰∏ä„ÄÇ
  - ÊäÄÊúØÊ†àÂøÖÈ°ªÊª°Ë∂≥ÊâÄÊúâ‰∏ªÊµÅÁ°¨‰ª∂ÁªìÊûÑÂ∑ÆÂºÇÂíåÊÄßËÉΩÊÄßËÉΩ‰∏ãÔºå‰øùÊåÅÁõ∏ÂØπ‰∏ÄËá¥ÁöÑ‰ΩøÁî®‰ΩìÈ™å„ÄÇËøôÊòØ‰∏Ä‰ª∂ÊûÅÁ¥ØÁöÑÊ¥ªÂÑø„ÄÇ
  - ÊúçÂä°Á´Ø‰∏ã‰∏™ÂºÄÊ∫êÊ®°ÂûãÊê≠‰∏™WEBÊúçÂä°ÂÜô‰∏™HTMLÂ∞±ËÉΩÂçñÊúçÂä°‰∫Ü„ÄÇÁ´Ø‰æßÊÉ≥ÂÜÖÂµåAIÊ®°Âûã‰∫ßÂìÅÂåñÔºåÂêåÊó∂ËøòË¶ÅËß£ÂÜ≥ÂÆûÊó∂ÊÄßÔºåË¶ÅËß£ÂÜ≥ÁöÑÂ∑•Á®ãÈóÆÈ¢òË¶ÅÂ§ö10ÂÄç‰∏çÊ≠¢„ÄÇË¶ÅËá™Â∑±‰ºòÂåñÊ®°Âûã„ÄÇÁîöËá≥Ë¶ÅËá™Â∑±Êê≠Âª∫Ê®°ÂûãÁªìÊûÑÔºåËá™Â∑±ÂáÜÂ§áÊï∞ÊçÆËÆ≠ÁªÉ„ÄÇ
- ÁõÆÂâçÁúãÁ´Ø‰æßÂÆåÂÖ®Ë∞à‰∏ç‰∏äAIÊºîËøõÂøÖ‰∫â‰πãÂú∞„ÄÇ‰∏ÄÊù•Á°¨‰ª∂ËæÉÂº±‰∏îÂèÇÂ∑Æ‰∏çÈΩêÔºåÂØºËá¥TO CÂ∑•Á®ã‰øùÈöú‰ΩìÈ™åÊûÅÂ∫¶Âõ∞ÈöæÂíåÂ§çÊùÇ„ÄÇ‰∫åÊù•PCÈ´òÁÆóÂäõÁ°¨‰ª∂ÂÆåÂÖ®‰∏çÊòØÂ§ß‰ºóÊ∂àË¥πÁ∫ßÁöÑÂîÆ‰ª∑ÂíåÂÆö‰ΩçÔºåÊúÄÂêéÔºåËøòÊ≤°Êúâ‰ªÄ‰πàÁ´Ø‰æßAIËêΩÂú∞‰∫ßÂìÅÁöÑÂ∫îÁî®ËåÉÂºè„ÄÇ
  - ‰ªéÂ∫îÁî®‰ΩìÈ™å‰∏äÔºåÈÇ£‰∫õÂÖÅËÆ∏Âª∂Ëøü500ÊØ´Áßí‰ª•‰∏äÔºåÂØπÂ∏¶ÂÆΩÈúÄÊ±Ç‰∏çÂ§ßÁöÑÂ∫îÁî®ÔºåÈÉΩÂèØ‰ª•ÊîæÂà∞‰∫ëÁ´Ø„ÄÇ
- ÁúüÈúÄË¶ÅÂÅöÂà∞Á´Ø‰æßÁöÑÔºåÂÖ∂ÂÆûÊòØÈÇ£‰∫õÈúÄË¶ÅÂç≥Êó∂ÂìçÂ∫îÁöÑÂ∫îÁî®Âú∫ÊôØ„ÄÇ
  - ‰∏∫Êï∞‰∏çÂ§öÁöÑÂ∫îÁî®Á±ªÂà´ÔºåÊØîÂ¶ÇÂç≥Êó∂‰∫§‰∫íÂΩ±ÂÉèÁõ∏ÂÖ≥ÔºåËØ≠Èü≥Áõ∏ÂÖ≥ÁöÑÔºåËØ∏Â¶ÇÊ∏∏ÊàèÔºåÈúÄË¶ÅÂç≥Êó∂‰∫§‰∫íÁöÑÊï∞Â≠ó‰∫∫Ôºå3DËôöÊãüAI‰∫∫ÔºåËôöÊãü‰∫∫Áõ¥Êí≠Á≠âÔºåÂèØËÉΩ‰ºöÂØπÁ´Ø‰æßAIÊúâÈÉ®ÂàÜÈúÄÊ±Ç
- ËøôÁ±ªÂ∫îÁî®Êúâ3‰∏™ÁâπÂæÅÔºö 
  - 1 Âç≥Êó∂ÊÄßÁõ¥Êé•ÂΩ±Âìç‰ΩìÈ™åÔºå‰∏çËÉΩÁ≠â„ÄÇ 
  - 2 ÂØπÁÆóÂäõË¶ÅÊ±ÇÁõ∏ÂØπ‰ΩéÔºåËøêÁÆóÊú¨Âú∞ÁÆóÂäõËÉΩÂ§üÊª°Ë∂≥ÔºåÂπ∂‰∏ç‰∏ÄÂÆöÈùûË¶Å‰∫ëÁÆóÂäõÊîØÊåÅ„ÄÇ 
  - 3 ÂØπÂ∏¶ÂÆΩË¶ÅÊ±ÇËæÉÂ§ßÔºå‰∫ëÊúçÂä°ÁöÑÊàêÊú¨ËøáÈ´ò‰ΩøÂæóÂïÜ‰∏öÊ®°Âºè‰∏çÊàêÁ´ãÔºåÊâÄ‰ª•ÈúÄË¶ÅÊîæÂÖ•Êú¨Âú∞„ÄÇ
- MOBILEÁ¶ªÁ´Ø‰æßÂèØËÉΩÊõ¥Ëøú„ÄÇ Áî±‰∫éÁÆóÂäõÊûÅ‰∏∫‰Ωé‰∏ãÔºåÂè™ÊúâÂæàÂ∞ëÁöÑÊÉÖÂÜµÊâçÈúÄË¶ÅÊâãÊú∫‰∏äÂç≥Êó∂ÂìçÂ∫îAIÊé®ÁêÜÁªôÂá∫ÁöÑÁªìÊûú„ÄÇÂç≥ÈÇ£‰∫õÊääÊâãÊú∫‰Ωú‰∏∫‰ø°Âè∑‰º†ÊÑüÂô®ÔºåÂç≥Êó∂ÂØπ‰ø°Âè∑ËøêÁÆóÁöÑÂú∫ÂêàÔºöÊØîÂ¶ÇË°®ÊÉÖÊçïÊçâ„ÄÅÊâãÂäø„ÄÅËÇ¢‰ΩìÂä®‰ΩúÊçïÊçâÂíåËØÜÂà´ÔºåËØ≠Èü≥„ÄÅËøêÂä®Êï∞ÊçÆÂ§ÑÁêÜÂíåÁîüÁâ©‰ø°Âè∑Â§ÑÁêÜÁ≠â„ÄÇ 
  - Â¶ÇÊûúÊâãÊú∫Ê≤°Êúâ‰∏ìÁî®Á•ûÁªè/Âº†ÈáèËäØÁâáÔºåGPUËøòÈúÄË¶ÅÊâøÊãÖÊ∏≤Êüì3DÂõæÂΩ¢ÔºåÈÇ£‰πàËÉΩË∑ëÁöÑAIÊ®°Âûã‰ºöÊõ¥Âä†ÂèóÈôê„ÄÇ
  - ÁõÆÂâçÊµãËØïÔºåÊüêÊâãÊú∫Áî®NPUË∑ëÊ®°ÂûãÊØî‰∏çÁî®ÊúÄÂ§öËÉΩÂø´10ÂÄç„ÄÇ 
  - ÊâãÊú∫ËäØÁâáÂïÜÊòØÂê¶ÊúâÂä®ÂäõÂèëÂ±ïÔºåË¶ÅÁúãÊâãÊú∫‰Ωú‰∏∫Êï∞ÊçÆ‰º†ÊÑüÂô®Âç≥Êó∂Â§ÑÁêÜÊï∞ÊçÆËÉΩÂ∏¶Êù•Â§öÂ§ßÁöÑÂ∫îÁî®Â∏ÇÂú∫„ÄÇ

- Êú¨Âú∞ÈÉ®ÁΩ≤Ê®°ÂûãÁöÑ‰ºòÂäøÂú®‰∫é‰ΩéÂª∂ËøüÔºåËøôÂú®ÊàëÁúãÊù•ÂÖ∂ÂÆû‰πüÊòØ‰∏™Áõ∏ÂØπÂÅè‰º™ÁöÑ‰ºòÂäø„ÄÇ
  - ‰∫ãÂÆû‰∏äÔºåÁé∞‰ª£ÁΩëÁªúÁéØÂ¢ÉÂÆûÈôÖ‰∏ä‰ª•ÂèäË∂≥Â§üÂø´ÈÄüÂíåÁ®≥ÂÆöÔºåÊó†ËÆ∫ÊòØÊµÅÈáèËøòÊòØWiFiÔºåËæÖ‰ª•CDNËæπÁºòËäÇÁÇπ‰ºòÂåñÂêéÔºåÁªùÂ§ßÂ§öÊï∞‰∏ªÊµÅAIÂ∫îÁî®ÁöÑ‰∫ëÁ´ØÂìçÂ∫îÈÉΩËÉΩÁ®≥ÂÆöÂú®Âá†ÂçÅÊØ´Áßí‰ª•ÂÜÖ„ÄÇ
- Á´Ø‰æßËÆæÂ§áÂèóÈôê‰∫éÂäüËÄó„ÄÅÁÉ≠Èáè„ÄÅÁÆóÂäõÁ≠âÁâ©ÁêÜÊù°‰ª∂ÔºåÂæÄÂæÄÂè™ËÉΩÈÉ®ÁΩ≤ËΩªÈáèÂåñÊ®°Âûã

- ÁßªÂä®ËÆæÂ§á‰∏äÈïøÊúüËøêË°å‰∏Ä‰∏™ÊúâÁ´û‰∫âÂäõÁöÑÂ§ßÊ®°Âûã‰ªçÁÑ∂‰∏çÁé∞ÂÆûÔºåË¥üËΩΩ„ÄÅËµÑÊ∫êÂç†Áî®„ÄÅÁîµÈáèÊ∂àËÄó„ÄÅÂèëÁÉ≠Á≠âÁ≠âÈóÆÈ¢òÔºåÂæàÈöæÂÅöÂà∞Â•ΩÁöÑ‰ΩìÈ™åÔºåÊõ¥‰∏çË¶ÅËØ¥Áâ©ËÅîÁΩëËÆæÂ§á„ÄÅÊô∫ËÉΩÈü≥ÁÆ±Á≠â‰ΩéÂäüËÄó‰∫ßÂìÅ‰∫Ü„ÄÇ
  - ÂæàÂ§öÂ£∞Áß∞Ëá™Â∑±Áî®‰∫ÜÁ´Ø‰æßÂ§ßÊ®°ÂûãÁöÑÔºåÂÆûÈôÖ‰∏ä‰ªçÁÑ∂Á¶ª‰∏çÂºÄ‰∫ëÁ´ØÁöÑÂçèÂêåÈÖçÂêàÔºåÊàñËÄÖËØ¥‰∏ªË¶ÅÈù†‰∫ëÁ´ØÔºåÂ§áÁî®ÊñπÊ°àÂèØËÉΩÊòØÁ´Ø‰æßÔºå‰ΩÜÊääÁ´Ø‰æßÊãøÂá∫Êù•Â§ßËÇÜÂÆ£Êâ¨ÔºåÊù•ÂÅöÂπøÂëäËÄåÂ∑≤„ÄÇ

- üêõ Á´Ø‰æßÊ®°ÂûãÁöÑÂä£Âäø
  - Á´Ø‰æßÊ®°Âûã‰ªçÁÑ∂ÂèóÈôê‰∫éËÆæÂ§áÊú¨Ë∫´ÁöÑÂÜÖÂ≠ò„ÄÅÂäüËÄóÂíåÁÆóÂäõ
  - ‰∫ëÁ´ØÊ®°ÂûãÂèØ‰ª•ÈöèÊó∂ÂçáÁ∫ßÔºåÁÉ≠Êõ¥Êñ∞Êú∫Âà∂‰øùËØÅÊâÄÊúâÁî®Êà∑Á¨¨‰∏ÄÊó∂Èó¥‰∫´ÂèóÊñ∞‰∏Ä‰ª£Ê®°Âûã
  - Á´Ø‰æßÊ®°ÂûãÂØπ‰∫é‰∏çÂêåËÆæÂ§áÂíåÁ°¨‰ª∂Êû∂ÊûÑÈúÄË¶ÅÂàÜÂà´ÈÄÇÈÖç

- Á´Ø‰æßÈÄÇÂêàÁöÑÂú∫ÊôØËøòÊòØÈöêÁßÅ„ÄÅÊûÅ‰ΩéÂª∂ËøüÔºåÊàñËÄÖÂÖ®Â§©ÂÄô„ÄÇ

- Â¶ÇÊûúÂÆåÂÖ®Ë∑ëÂú®Á´Ø‰æßÔºåÈÇ£ÂïÜ‰∏öÊ®°ÂºèÊÄé‰πàÂÅöÔºüÁé∞Âú®ÁöÑ‰∏ªÊµÅÊòØËÆ¢ÈòÖÂà∂Ôºå‰ΩÜÂÆåÂÖ®Êú¨Âú∞ÁöÑËÆ¢ÈòÖÂà∂ÔºåÂê∏ÂºïÂäõÊØîËæÉ‰ΩéÔºåÁ†¥Ëß£È£éÈô©ÊØîËæÉÈ´òÂêß

- ## deepseek 3fs ÂÖ∂ÂÆû‰∏çÊòØÂæàÁêÜËß£ËøôÊ†∑ÁöÑÊÑè‰πâÊòØÂï•‚Ä¶ Â∑≤ÁªèËÑ±Á¶ª FS ÁöÑÈÄöÁî®Êé•Âè£‰∫ÜÔºå‰∏∫Âï•Ë¶ÅÁ°¨ÊåÇ‰∏Ä‰∏™ FUSE VFS Â±Ç‚Ä¶ Áõ¥Êé•ÊëíÂºÉ VFS Ëµ∞‰∏™ÁßÅÊúâÁöÑ protocol ‰∏çÂπ≤ÂáÄÂ§ö‰∫Ü‚Ä¶
- https://x.com/silsrc/status/1895390926098571505
- > Most applications use FUSE client, which has a low adoption barrier. Performance-critical applications are integrated with the native client.

- ÊàëÁöÑÁ¨¨‰∏ÄÂèçÂ∫îÊòØÁé∞ÊúâÊ°ÜÊû∂Â∫îËØ•Ê≤°ÂäûÊ≥ïÂ§ßÊâπÈáèÊîπÁî®ÁßÅÊúâÁöÑ APIÔºåÊØîÂ¶ÇÂú® Python ÊàëÂ∞±ÊòØË¶ÅÁî® pathlib. Path ÂØπÊñá‰ª∂ÂÅöÁÇπÁÆÄÂçïÊìç‰ΩúÔºåÈÇ£Á°ÆÂÆûÂè™ËÉΩÊòØÊåÇ FUSE ‰∏äÂéª‰∫Ü

- ## ËÅä‰∏ÄËÅäÂõΩÂÜÖÂ§ßÊ®°ÂûãÁöÑÂÆâÂÖ®Êú∫Âà∂Ôºö ‰∏ÄËà¨ÊòØ‰∏§Â•óÔºåÂàÜÂà´‰ΩúÁî®‰∫étrain-timeÂíåtest-time„ÄÇ
- https://x.com/9hills/status/1840786446153921017
  - ËÆ≠ÁªÉÁöÑÊó∂ÂÄôÂ¢ûÂä†ÂÆâÂÖ®Âíå‰ª∑ÂÄºËßÇÂØπÈΩêÁöÑSFTÂíåÂÅèÂ•ΩÂØπÈΩêÊï∞ÊçÆ„ÄÇÊúÄÁªàÊïàÊûúÁ±ª‰ººÂºÄÊ∫êÁöÑQwen2Ê®°ÂûãÔºåÊúâÁÇπÁî®‰ΩÜÊòØÂæàÂÆπÊòìË¢´Jailbreak„ÄÇ
  - Êé®ÁêÜÊó∂Â¢ûÂä†ÂÆâÂÖ®ÁÆóÂ≠êÔºåÂÖ∑‰ΩìÊúâÂá†Áßç
- ÊäÄÊúØÈöæÁÇπÊúâ‰∏§‰∏™Ôºö
  1. ËÆ≠ÁªÉÂàÜÁ±ªÂô®ÁöÑÂ§ßÈáèÈùûÂÆâÂÖ®Êï∞ÊçÆÔºåÊâÄ‰ª•ËØ¥‰Ω†ÂÖàÊàê‰∏∫ÂèçË¥ºÊâçËÉΩËØÜÂà´ÂèçË¥º„ÄÇ
  2. Ê®°ÂûãË¶ÅÂÅöÁöÑË∂≥Â§üÂ∞èË∂≥Â§üÂø´ÔºåÊúÄÂ∞èÂåñÂΩ±ÂìçÊ®°ÂûãttftÂíåtps„ÄÇ
  3. ÊµÅÂºèÂæàÈöæÔºåÊüêÊ®°ÂûãÊúÄÊó©ÊòØ‰∏ÄÂè•Âè•ËæìÂá∫ÁöÑÔºåÂêéÊù•ÊâçÊîπÊàêtokenÁ∫ßÊµÅÂºè„ÄÇ
- ËØ∑Êïô‰∏Ä‰∏ãÔºåÁé∞Âú®Â∏ÇÂú∫‰∏äÁöÑÂ§ßÊ®°ÂûãÔºåÊÄé‰πàÁü•ÈÅì‰ªñ‰ª¨ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÊòØÂ§ö‰πÖÁöÑÂë¢Ôºü
  - ÂèØ‰ª•ÈóÆ‰∏Ä‰∫õÁâπÂÆöÊó∂Èó¥ÁöÑÊñ∞ÈóªÊù•È™åËØÅÔºå‰ΩÜÊòØÂÖ∂ÂÆûÊ≤°ÂÖ≥Á≥ª„ÄÇÊ®°ÂûãÁöÑÁ≤æÁ°ÆÁü•ËØÜ‰∏çÈáçË¶ÅÔºå‰πüÂÖÖÊª°ÂπªËßâ„ÄÇ

- ## Attention is *Not* All You NeedÔºåËøòÊòØÊúâ‰∫∫Âú®Â∞ùËØï transformer ‰ª•Â§ñÁöÑÊû∂ÊûÑÔºå
- https://x.com/liumengxinfly/status/1835251398692508114
  - ÊØïÁ´ü transfermor Êé®ÁêÜÂ§çÊùÇÂ∫¶Âú®Êï∞Â≠¶‰∏äÊòØÊó†Ê≥ïÁ∫øÊÄßÊâ©Â±ïÁöÑÔºåÊó©Êôö‰ºöËµ∞Âà∞Áì∂È¢à

- ## ÂõΩ‰∫ß188‰∏™Â§ßÊ®°ÂûãÁöÑexcelÊñáÊ°£Ôºö Âåó‰∫¨69 ‰∏äÊµ∑22 Êù≠Â∑û15 Âπø‰∏ú26‰∏™ Ê±üËãè15‰∏™
- https://twitter.com/FinanceYF5/status/1730912502312296935
  - [ÂõΩ‰∫ßÂ§ßÊ®°Âûã188‰∏™list - Feishu Docs](https://zw73xyquvv.feishu.cn/wiki/WXLmwBbYuiTobkkJ6Ojc2cxqnj0?sheet=2XjJlJ&table=tblS2Jv7isKtSODz&view=vewfCdOf0U)

# discuss-vllm
- ## 

- ## 

- ## 

- ## 

- ## [Â§ßÊ®°ÂûãÊé®ÁêÜÊ°ÜÊû∂ÔºåSGLangÂíåvLLMÊúâÂì™‰∫õÂå∫Âà´Ôºü - Áü•‰πé](https://www.zhihu.com/question/666943660)
- PagedAttention vs RadixAttentionÔºöÂÜÖÂ≠òÁÆ°ÁêÜÁöÑ‰∏§ÁßçÂì≤Â≠¶
- ÂÖàËØ¥vLLMÁöÑPagedAttention„ÄÇ
  - ËøòËÆ∞ÂæóÊìç‰ΩúÁ≥ªÁªüËØæ‰∏äÁöÑËôöÊãüÂÜÖÂ≠òÂêóÔºüvLLMÂ∞±ÊòØÊääËøôÂ•óÁé©Ê≥ïÊê¨Âà∞‰∫ÜGPU‰∏ä„ÄÇÊàëÁ¨¨‰∏ÄÊ¨°ÁúãÂà∞Ëøô‰∏™ËÆæËÆ°Êó∂ÔºåÁõ¥Êé•ÊÉä‰∫Ü‚Äî‚ÄîÂ±ÖÁÑ∂ËÉΩÊääÂÜÖÂ≠òÂà©Áî®Áéá‰ªé20%Âπ≤Âà∞96%
- SGLangÁöÑRadixAttention„ÄÇ
  - Áî®ÁöÑÊòØRadixÊ†ëÔºàÂ∞±ÊòØÈÇ£‰∏™ÂéãÁº©ÂâçÁºÄÊ†ëÔºâ„ÄÇ
  - ÂÉèÊòØÁªôKV cacheÂª∫‰∫Ü‰∏™"ÊóèË∞±"
  - Âú®Â§öËΩÆÂØπËØùÂú∫ÊôØÔºåSGLangÁ°ÆÂÆûÁàΩ„ÄÇ
  - ÈÉ®ÁΩ≤Ëµ∑Êù•ÊòØÁúüÁöÑÈ∫ªÁÉ¶

- ÁºñÁ®ã‰ΩìÈ™åÔºö
  - vLLMÁöÑAPIËÆæËÆ°ÔºåÊÄé‰πàËØ¥Âë¢ÔºåÂ∞±ÊòØÈÇ£Áßç"ÂÇªÁìúÂºè"ÁöÑÔºàË§í‰πâÔºâ„ÄÇ‰∏âË°å‰ª£Á†ÅË∑ëËµ∑Êù•
  - SGLangÁöÑÁºñÁ®ãÊ®°Âûã, Ëøô‰∏™DSLÔºàÈ¢ÜÂüüÁâπÂÆöËØ≠Ë®ÄÔºâÂ≠¶‰π†Êõ≤Á∫øÊúâÁÇπÈô°

- ÂèØÊòØÔºåÈÄöÂ∏∏ÈÉ®ÁΩ≤ÈÉΩÊòØÂè™‰Ωú‰∏∫ÂêéÁ´ØÊèê‰æõ APIÔºåËÄå‰∏çÊòØÁ≠î‰∏ªËøôÊ†∑Âú® Python ÈáåÁõ¥Êé•Âä†ËΩΩÊ®°Âûã„ÄÇÊâÄ‰ª•Á≠î‰∏ªÁöÑÂÅöÊ≥ïÊØîËæÉÂ∞ëËßÅÂï¶„ÄÇ

- sglang‰∏çÁ®≥ÂÆöÂÖ∑‰ΩìÊåáÁöÑÊòØÔºü
  - ÂêÑÁßçÂÜÖÂ≠òÊ≥ÑÊºèÔºåoomÔºåcrashÔºå‰ª£Á†ÅËßÑÊ®°‰∏çÂ§ßÔºåÂ∞±ÂºÄÂßãÂ±éÂ±±Â†ÜÂ±éÔºåÁîü‰∫ßÂà´Áî®Ôºå‰πüÂà´ËøΩÊõ¥Ôºåmerge masterÂü∫Êú¨ÁöÑÂõûÂΩíÊµãËØïÈÉΩ‰ºöÁõ¥Êé•Ë∑≥ËøáÁöÑ„ÄÇÂÅöÂºÄÊ∫êÁöÑÔºåÂ∞±Ëøô

- vllm ‰ªÖÊîØÊåÅ NVIDIA GPU„ÄÅÈÉ®ÁΩ≤Â§çÊùÇ„ÄÅÊòæÂ≠òÈúÄÊ±ÇÂ§ß
# discuss-vision-llm
- ## 

- ## 

- ## [moondream 0.5B - the world's smallest vision language model : r/LocalLLaMA _202412](https://www.reddit.com/r/LocalLLaMA/comments/1h7ivts/moondream_05b_the_worlds_smallest_vision_language/)
  - https://github.com/vikhyat/moondream
  - Moondream 0.5B offers a significantly lower download size and memory usage than moondream 2B.
  - It is intended to be used as a distillation target‚Äîstart building with moondream 2B, and distill your use-cases onto the 0.5B model before deployment.
  - This model was built using structured pruning on 2B with quantization-aware training. This means we can easily distill from 2B to recover accuracy on the specific target tasks an application needs, and run with int8 quantization without any loss of accuracy.
  - Today we are releasing int8 and int4 weights for moondream 0.5B, as well as fast CPU inference support in the Python client library. 16-bit weights and distillation support will be coming soon, so stay tuned!

- doesnt look like this new model has been added to ollama as yet (and no GGUFs available)

- Awesome. Florence is nice and small too, but could only really handle a finite list of specific prompts. It seems this small models retains the ability to ask free-form questions, which would make it extremely useful for mobile devices.
  - Florence 2 base is smaller. You can also fine tune it to work with any specific prompt you like if you have consistent prompts.

- ## LGMÔºöÁîüÊàêÈ´òË¥®Èáè3DÊ®°ÂûãÔºåÊîØÊåÅÊñáÂ≠óÁîüÊàêÊ®°Âûã„ÄÅÂõæÁâáÁîüÊàêÊ®°ÂûãÔºåÂàÜËæ®Áéá512*512Ôºå5ÁßíÂÜÖÂç≥ÂèØÁîüÊàê„ÄÇ
- https://x.com/Gorden_Sun/status/1784230776311284205
  - https://github.com/3DTopia/LGM

- ## ÂÖçË¥πÁé©ÁöÑÊú¨Âú∞Â§ßÊ®°ÂûãÔºåÊú¨Âú∞Êê≠Âª∫ÁéØÂ¢ÉÊé®ËçêÁî®OllamaÂíåChatbotAI
- https://x.com/vista8/status/1862696894172209476
- Vision Âª∫ËÆÆÊç¢ÊàêÁî® Llama 3.2 Vision 11bÔºåÊØîllavaË¶ÅÂ•ΩÂæàÂ§öÔºå‰∏îÊîØÊåÅÂ§öËØ≠Ë®ÄÔºàÂåÖÊã¨‰∏≠ÊñáÔºâÁöÑÊñáÂ≠óËØÜÂà´

- ## 
- 
- 
- 
- 

# discuss-local-llm-tips/tricks
- ## 

- ## 

- ## 

- ## [188GB VRAM on Mac Studio M2 Ultra - EASY : r/LocalLLaMA _202401](https://www.reddit.com/r/LocalLLaMA/comments/192uirj/188gb_vram_on_mac_studio_m2_ultra_easy/)
- I think "time for first token" is slow because people don't use --mlock option, which preloads model and force it to stay in RAM and this is not default. It should not be a problem if use it.
  - This is true and will keep the model in along with additional memory for context which, depending in what you are using may not be allocated until it is required. MLX uses lazy allocation, only grabbing memory when it is needed. So, mlock is something you would always want set so the model doesn‚Äôt get swapped or paged out.

- ## [intelÁöÑcpuËøûÂ§ßÊ®°ÂûãÈÉΩÊ≤°Ê≥ïË∑ë, ÊÄé‰πàËøòÂ§©Â§©Âú®Êé®aipc? - Áü•‰πé](https://www.zhihu.com/question/668042879/answers/updated)
- ÂØπ‰∫éÁ´Ø‰æßAIÔºåÊàë‰∏™‰∫∫ÁöÑÊÉ≥Ê≥ïÔºåÊúÄÂ§ßÁöÑ‰ª∑ÂÄºÂ∫îËØ•ÊòØÊãâÈ´ò‰∏ä‰∏ãÊñáÁ™óÂè£ÔºåÂú®Êú¨Âú∞ÂÅö‰∏™‰∫∫Áü•ËØÜÂ∫ìÔºå‰ª•ÂèäÊú¨Âú∞ÊâπÈáèÊé®ÁêÜÔºåÊØîÂ¶ÇÂÅöÁßëÁ†îÁöÑÔºåÊáíÂæóËØªËÆ∫ÊñáÔºåËÆ©AIÊâπÈáèÊÄªÁªìÂÜô‰∏™ÁªºËø∞„ÄÇËøô‰∏§ÁßçÂÅöÊ≥ïÂ¶ÇÊûúË∞ÉÁî®Á∫ø‰∏äÁöÑAPIÔºåÂÖ∂ÂÆûÊå∫Ë¥µÁöÑ„ÄÇÈòÖËØª‰∏ÄÁØáËÆ∫ÊñáÂ∞ëÂàôÂá†ÂçÉtokensÔºåÂ§öÂàô‰∏§‰∏â‰∏átokens„ÄÇÊú¨Âú∞‰ΩøÁî®32768ÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÁöÑQwen3 8BÔºå‰πüËÉΩÂÆåÊàêÂæó‰∏çÈîô
  - ÂÅöÈïøÁ™óÂè£Êú¨Ë∫´Â∞±ÈúÄË¶ÅËµÑÊ∫êÔºåtransformerÁöÑkqvËÆ°ÁÆóÊòØ‰∏™o(n^2)ÁöÑÂ§çÊùÇÂ∫¶

- Á´Ø‰æß AI Áé∞Âú®ÁöÑÂ∫îÁî®Âú∫ÊôØÂÆûÂú®ÊòØÂ§™Â∞è„ÄÇÂ∞§ÂÖ∂ÊòØËÆæÂ§áÊó†Êó∂Êó†ÂàªÂú®Á∫øÁöÑÊÉÖÂÜµ‰∏ãÔºåÊàë‰∏∫‰ªÄ‰πàÊîæÁùÄÂÖçË¥πÁöÑÂº∫Â§ßÁöÑÂú®Á∫ø AI ‰∏çÁî®ÔºåËΩ¨ËÄåÂéªÁî®‰∏çËÉΩËÅîÁΩëÊêúÁ¥¢„ÄÅÊõ¥Âº±ÔºåÊõ¥Âç°„ÄÅÊõ¥ËÄóÁîµÁöÑÁ´Ø‰æßÁöÑ AI Âë¢Ôºü
  - ÊàëÊú¨Âú∞ÈÉ®ÁΩ≤‰∫Üchatgml ÁöÑ 6B ÂíåÁßãÂè∂ÁöÑ [SD Êï¥ÂêàÂåÖ] Ôºå‰ΩÜÊòØÁî®Âà∞ÁöÑÁúüÁöÑ‰∏çÂ§öÔºåÂæóÂà∞ÁöÑÁªìÊûú‰πüÊØî‰∏ç‰∫ÜÂú®Á∫øÁöÑ

- ## üÜö [‰∏∫‰ªÄ‰πàÈÉΩÂú®Áî®ollamaËÄålm studioÂç¥Êõ¥Â∞ë‰∫∫‰ΩøÁî®? - Áü•‰πé](https://www.zhihu.com/question/654357364)
- ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºå‰∏çÂêå‰∫éÂÖ®ÈÉ®‰ª£Á†ÅÂú®githubÂºÄÊ∫ê„ÄÅÁîöËá≥ÂèØ‰ª•Ëá™Â∑±Âä®ÊâãÁºñËØëÁöÑollamaÔºålm studioËá≥‰ªä‰ªçÊòØÈó≠Ê∫êÂïÜ‰∏öËΩØ‰ª∂Ôºà‰ªÖ‰∏ÄÈÉ®ÂàÜÈùûÊ†∏ÂøÉ‰ª£Á†ÅÈô§Â§ñÔºâ

- Ollama ‰∫é 5 Êúà‰ªΩÊé®Âá∫ÁöÑÂÖ®Êñ∞Â§öÊ®°ÊÄÅÂºïÊìé„ÄÇÂü∫‰∫éÂ§öÊ®°ÊÄÅÂºïÊìéÔºåOllama ÂèØ‰ª•ÊîØÊåÅËøêË°åËÉΩÂêåÊó∂Â§ÑÁêÜÂõæÂÉè„ÄÅÊñáÊú¨ÁöÑÊ®°Âûã„ÄÇÊñ∞Êû∂ÊûÑ‰∏ç‰ªÖËß£ÂÜ≥ÂΩìÂâçÂ§öÊ®°ÊÄÅÊåëÊàòÔºå‰πü‰∏∫ÈõÜÊàêÊõ¥Â§çÊùÇÁöÑËÉΩÂäõÔºàËØ≠Èü≥„ÄÅÁîüÊàêÁ≠âÔºâÂíå‰ºòÂåñÊÄßËÉΩÔºàÊõ¥Èïø‰∏ä‰∏ãÊñá„ÄÅÊõ¥È´òÂπ∂ÂèëÔºâÊâì‰∏ãÂü∫Á°Ä„ÄÇ

- ÊàëÁöÑ‰Ωì‰ºöÔºö
  - ollamaÁî®Ëµ∑Êù•Âíådocker‰∏ÄÊ†∑ÁöÑÊÑüËßâÔºåpullÊ®°ÂûãÔºårunÊ®°ÂûãÔºålsÁúãÊ®°ÂûãÔºåpsÁúãËøêË°å„ÄÇÈùûÂ∏∏È°∫Êâã‰∏ùÊªëÔºåÂÖ•ÊâãÊó†Èó®Êßõ„ÄÇ
  - llamaÁöÑ‰∏≠ÊñáÔºåÂæÆË∞ÉÂêÑÁßçchatÔºåcodeÔºåÂ§üÁî®„ÄÇËÄå‰∏îÈÉΩÊòØÈáèÂåñÂ•ΩÁöÑÔºåÈöèÊãâÈöèÁî®Ôºå4090Â∞±Ë∑ëÁöÑËµ∑Êù•„ÄÇÂ∞§ÂÖ∂ÊòØÂú®ÂõΩÂÜÖÊãâÊ®°ÂûãÈÄüÂ∫¶ÊûÅÂø´ÔºåÊàëÁöÑÁéØÂ¢ÉÊúÄÈ´òÂèØËææ15m/s
  - ollamaÊòØllama.cppÂÆûÁé∞Ê®°ÂûãÊé®ÁêÜÔºåÊ®°ÂûãÂ∞èÔºåÈÄüÂ∫¶Âø´„ÄÇ
  - ollamaÊèê‰æõ11434Á´ØÂè£ÁöÑwebÊúçÂä°ÔºåÈáçË¶ÅÁöÑÊòØËøòÂÖºÂÆπopenaiÁöÑÁ´ØÁÇπÊé•Âè£ÔºåÂèØ‰ª•ÂíåÂêÑÁßçÂâçÁ´ØÈÖçÂêàÔºåÊØîÂ¶ÇollamaËá™Â∑±open webuiÔºåÂõΩ‰∫ßÁöÑchatboxÔºåËøûÂêéÁ´ØÂ∏¶ÁïåÈù¢Ôºå‰∏ÄÂ•óÊêûÂÆö
  - ollamaÊòØÁ≥ªÁªüÊúçÂä°ÂΩ¢ÂºèÔºà‰πüËÉΩÂÆπÂô®ËøêË°åÔºâÔºåÂâçÂêéÁ´ØÂàÜÁ¶ªÔºà ‰∏•Ê†ºÊù•ËØ¥Ê≤°ÊúâÂâçÁ´ØÔºåÂè™ÊúâÂëΩ‰ª§Ë°åÂÖ•Âè£ÔºâÔºåËÄ¶ÂêàÂ∞èÔºåÊê≠ÈÖçÁÅµÊ¥ª„ÄÇ
- ollamaÁöÑËø≠‰ª£ÂæàÂø´ÔºåÁé∞Âú®Â§öÊ®°ÂûãÂπ∂ÂèëÁöÑÈóÆÈ¢òÂ∑≤ÁªèËß£ÂÜ≥‰∫Ü

- ollamaÁé∞Âú®ÊîØÊåÅÂàÜÂ∏ÉÂºèÊé®ÁêÜÂêó
  - Áé∞Âú®‰∏çÊîØÊåÅÔºå‰ΩÜÊîØÊåÅÂçïÊú∫Â§ögpuËøêË°å

- ÊãâÊ®°ÂûãÈ∫ªÁÉ¶Â∞±Âõ∞Êâ∞ÂæàÂ§ö‰∫∫‰∫ÜÔºåollamaÁÖßÁùÄÊïôÁ®ãÂÅöÔºåÂü∫Êú¨Ê≤°ÊúâÈóÆÈ¢ò

- ollamaÈÖçÂêàopen webui‰∏çÈîôÁöÑ

- ollamaÁúãÁùÄÈÄüÂ∫¶Âø´ÔºåÂÖ∂ÂÆûÂÆÉÊúÄÂ§ßÈóÆÈ¢òÊòØ‰øÆÊîπ‰∏™‰∏ä‰∏ãÊñáÈÉΩÂæàÈ∫ªÁÉ¶
  - ÂèØ‰ª•Áî®chatbox‰πãÁ±ªÁöÑËÅäÂ§©Ê°Ü

- lm studio ‰πüÊèê‰æõopenai-likeÁöÑapiÂêéÁ´ØÔºåÊîØÊåÅÂâçÂêéÁ´ØÂàÜÁ¶ªÔºå‰πüÊîØÊåÅllama.cppÔºàÂú®macËøòÊîØÊåÅmlxÊ°ÜÊû∂Ôºâ. Ê®°ÂûãÂü∫Êú¨‰∏ä‰πüÈÉΩÊîØÊåÅÔºåËÄå‰∏îÈÉΩÊòØ‰ªéhf‰∏ä‰∏ÄÈîÆÂºè‰∏ãËΩΩÈÉ®ÁΩ≤Ôºà‰∏çËøáÁΩëÁªúÁ°ÆÂÆûÊòØÈóÆÈ¢òÔºâ
  - ÊÄªÁöÑËØ¥Ôºålinux‰∏äÈÉ®ÁΩ≤ËøòÊòØÂêàÈÄÇÔºå‰∏çÁî®ollamaÔºåËøòÊúâvllmÔºålmdeloyÔºå‰ª•ÂèäsglangÔºåÈÉΩÊòØÂºÄÊ∫êÁöÑÔºåÂæàÈÄÇÂêàÂºÄÂèë‰∫∫ÂëòÊàñËÄÖÊòØÁªÑÁªáÂÜÖÈÉ®ÈÉ®ÁΩ≤„ÄÇwindows‰∏äÈÉΩÂ∑ÆÁÇπÔºålmstudioÁïåÈù¢ËÄ¶ÂêàÔºåÈó≠Ê∫êÔºåÊãâÊ®°ÂûãËµ∞ÈïúÂÉèÁÇπÔºå‰∏çËÉΩÂàÜÂ∏ÉÂºèÊé®ÁêÜÔºå‰∏™‰∫∫Ëá™Â∑±ÂÆûÈ™åÊÄßË∑ëË∑ëÂ•Ω‰∏Ä‰∫õ„ÄÇ

- LMÂõæÂΩ¢ÂåñÁïåÈù¢‰πüÂèØ‰ª•ÂæÆË∞ÉÂ§ßÈáèÁöÑÊ®°ÂûãËøêË°åÂèÇÊï∞ÔºåollamaËøôÊñπÈù¢Â∞±ÂØπÂ∞èÁôΩ‰∏çÂ§üÂèãÂ•Ω‰∫ÜÔºåÂè™ËÉΩ‰ΩøÁî®ÂëΩ‰ª§Ë°åÂèÇÊï∞
  - LMÂÅöserverÊúçÂä°Âô®Êó∂ÔºåÁïåÈù¢ÊúâÂä®ÊÄÅÂèçÈ¶àÁöÑÊâßË°å‰ø°ÊÅØÂèØ‰ª•‰æõÁî®Êà∑ËßÇÂØüÂà∞ÔºåË∞ÉËØïÈóÆÈ¢òÊñπ‰æø„ÄÇ
  - LMÊúÄÊñ∞ÁöÑÁâàÊú¨Âú®chatÁïåÈù¢‰∏ãÔºåËøòÊîØÊåÅ‰∫Üollama‰∏çÊîØÊåÅÁöÑLaTexÊï∞Â≠¶ÂÖ¨ÂºèË°®ËææÔºåÂíåÊú¨Âú∞RAGÊñáÊ°£‰ø°ÊÅØÊ£ÄÁ¥¢ÔºåËøô‰∫õÈÉΩÊòØollamaËøò‰∏çÊèê‰æõÁöÑÁâπÊÄß„ÄÇ

- ollama Âíå LM studio ÂùáÂèØ‰ª•ÊîØÊåÅ‰∏çÂêåÂ§ßÊ®°ÂûãÁöÑÊú¨Âú∞ÈÉ®ÁΩ≤ÔºåÂπ∂‰∏î‰ºö‰ºòÂÖà‰ΩøÁî® GPU ËøõË°åÊé®ÁêÜ„ÄÇÂ¶ÇÊûúÊ≤°ÊúâÂèëÁé∞ GPUÔºåÂ∞±‰ºö‰ΩøÁî® CPU Êé®ÁêÜÔºåÂõ†Ê≠§‰πü‰ºöÂç†Áî®‰∏ÄÈÉ®ÂàÜÂÜÖÂ≠ò„ÄÇ‰ªéÂÆûÈôÖ‰ΩøÁî®Êù•ÁúãÔºåÁ¨îËÆ∞Êú¨ÂÜÖÂ≠òÂ∫îËØ•Ëá≥Â∞ë‰∏∫ 8GB ÊâçËÉΩÊ≠£Â∏∏ËøêË°å„ÄÇ

- ÂÜôÁ®ãÂ∫èÂíåollama‰∫§‰∫íÁúüÁöÑÊòØÈ°∫ÊªëÔºåÂá†Âè•‰ª£Á†ÅÊ®°ÂûãÂø´ÈÄüÂàáÊç¢„ÄÇ

- ollamaË∑üvllm„ÄÅsglang„ÄÅtrtllmÁõ∏ÊØîÊòØ‰∏™Áé©ÂÖ∑
  - lm studioËøûÁé©ÂÖ∑ÈÉΩ‰∏çÂ¶Ç

- ÊÑüÂèó‰∏äÂéüÂõ†ÊòØdsÊúÄÁÅ´ÁöÑÊó∂ÂÄôÔºåÈì∫Â§©ÁõñÂú∞ÁöÑËá™Â™í‰ΩìÊïôÁ®ãÈÉΩÊòØollamaÔºåÊèêÂà∞lmÁöÑÂ∞ëÁöÑÂ§ö„ÄÇËøô‰∏™‰∏úË•ø‰∏ÄÊó¶ÂΩ¢ÊàêÂÆ£‰º†ÊïàÂ∫î‰∫ÜÔºåÈô§‰∫Ü‰∏ìÈó®ÊêûËøô‰∏™ÁöÑÔºåÂ∞±‰∏ç‰ºöÊúâ‰∫∫Ê∑±ÂÖ•Á†îÁ©∂‰∫Ü„ÄÇ

- lm studioËÉΩÂÉèollamaÈÇ£Ê†∑ÂêåÊó∂‰ΩøÁî®chatÂíåembeddingÂêóÔºülm studioÊØèÊ¨°ÈÉΩË¶ÅÈ¢ÑÂÖàÂä†ËΩΩ„ÄÇ

- ## [Â¶Ç‰ΩïÁúãÂæÖËãπÊûúÂèëÂ∏ÉÁöÑ MLX Êú∫Âô®Â≠¶‰π†Ê°ÜÊû∂Ôºü - Áü•‰πé _202312](https://www.zhihu.com/question/633585779)
- MLXÊòØ‰∏Ä‰∏™Á±ª‰ººNumPyÊï∞ÁªÑÁöÑÊ°ÜÊû∂ÔºåÁõÆÁöÑÊòØÂèØ‰ª•Âú®ËãπÊûúÁöÑËäØÁâá‰∏äÊõ¥Âä†È´òÊïàÂú∞ËøêË°åÂêÑÁßçÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÔºåÂΩìÁÑ∂ÊúÄ‰∏ªË¶ÅÁöÑÁõÆÁöÑÊòØÂ§ßÊ®°Âûã„ÄÇ
  - MLXÁöÑËÆæËÆ°ÂèóÂà∞PyTorch„ÄÅJaxÂíåArrayFileÁöÑÂêØÂèëÔºåÁõÆÁöÑÊòØËÆæËÆ°‰∏Ä‰∏™ÂØπÁî®Êà∑ÊûÅÂÖ∂ÂèãÂ•ΩÔºå‰ΩÜÂêåÊó∂Âú®ËÆ≠ÁªÉÂíåÈÉ®ÁΩ≤‰∏ä‰πüÈùûÂ∏∏È´òÊïàÁöÑÊ°ÜÊû∂„ÄÇ
  - ÊâÄ‰ª•ÔºåÂÆÉÁöÑÊé•Âè£‰Ω†‰ºöÈùûÂ∏∏ÁÜüÊÇâÔºåÂõ†‰∏∫ÂÆÉÁöÑPythonÊé•Âè£‰∏éNumPyÂæàÁõ∏‰ººÔºåËÄåÂÆÉÁöÑÁ•ûÁªèÁΩëÁªúÊ®°ÂûãÁöÑÊé•Âè£ÂíåPyTorchÈùûÂ∏∏Á±ª‰ºº„ÄÇ

- 2023Âπ¥‰∫ÜÔºåÁúüÁöÑËøòÈúÄË¶Å‰∏∫‰∫ÜËá™ÂÆ∂ËäØÁâá‰ªéÈõ∂ÂºÄÂßãÊûÑÂª∫‰∏Ä‰∏™Ê∑±Â∫¶Â≠¶‰π†ËÆ≠ÁªÉÊ°ÜÊû∂Âêó
  - ÊúâÁöÑÔºåÊÑüËßâÊòØ‰∏ìÈó®ÈíàÂØπuma‰ºòÂåñÁöÑ
- ÊØïËÆæÊòØÊ∑±Â∫¶Â≠¶‰π†ÂêéÈó®ÊîªÂáªÁõ∏ÂÖ≥ ËÆæÂ§áÊòØmacmini m4 ‰ªépytorchÊ°ÜÊû∂ÂàáÊç¢Âà∞mlxÊ°ÜÊû∂ ÂêåÊ†∑ÁöÑÂÆûÈ™åÂêåÊ†∑ÁöÑËÆ≠ÁªÉÈáè mlxÈÄüÂ∫¶Âø´‰∫ÜÂ•ΩÂá†ÂÄç ÂÆûÊâìÂÆûÁöÑÊèêÂçá
  - ‰ΩÜÊòØÈúÄË¶ÅÈáçÊûÑËÆ≠ÁªÉ‰ª£Á†ÅÔºåÊàë‰∏ÄÂ•ópytorch‰ª£Á†ÅÔºåÂè™ÈúÄÂÅöÂ∞ëÈáèÈÄÇÈÖçÔºåÂèØ‰ª•Ëá™Âä®Âú®ÊòáËÖæ„ÄÅnVidia„ÄÅmps‰πãÈó¥ÂàáÊç¢„ÄÇËÄå‰∏îÔºåmlxÁ°ÆÂÆûÊØîmpsÊèêÂçáÂ•ΩÂá†ÂÄçÔºå‰ΩÜÊòØÊàëÂ¶ÇÊûúÁî®nvidiaÊàñËÄÖÊòáËÖæ910BÔºåmlxÊÄßËÉΩËøòÊúâÂ∑ÆË∑ù„ÄÇ
- Âú® MacBook Air m3 ‰∏äÔºåmlx ÁöÑÁü©Èòµ‰πòÊ≥ï„ÄÅSVDÂàÜËß£ËøêË°åÈÄüÂ∫¶ÊòéÊòæÂø´‰∫é PyTorch
  - Â¶ÇÊûú‰∏ç‰ΩøÁî® mlx Êù•ËøõË°åÊ∑±Â∫¶Â≠¶‰π†Ôºå‰ªÖÁî®‰∫éÂ∏∏ËßÑÁßëÁ†îËÆ°ÁÆóÊàñ‰ºòÂåñ‰ªªÂä°ÔºåÂÆûÈôÖ‰∏äËøòÊòØËõÆÈÄÇÂêàÁöÑ„ÄÇ
  - ÁÑ∂ËÄåÔºåmlx ÁõÆÂâçÁîöËá≥‰∏çÊîØÊåÅ float64ÔºåÂºÄÂèëÂõ¢ÈòüÂΩìÂâçËÄÉËôëÁöÑÊòØÂÆÉ‰ª¨ GPU ÂèçÊ≠£Áî®‰∏çÂà∞ÔºåËøôÂØπ‰∏Ä‰∫õÈ´òÁ≤æÂ∫¶ËÆ°ÁÆóÈúÄÊ±ÇÂèàÊòØ‰∏ÄÁßçÈôêÂà∂ÔºåÊõ¥‰∏çÁî®ËØ¥‰∏Ä‰∫õÊú™ÂÆûÁé∞ÁöÑÁÆóÊ≥ïÊàñËÄÖÊï∞ÊçÆÁ±ªÂûãËøòÈúÄË¶Å‰ªéÈõ∂ÂÜô‰∫Ü„ÄÇ

- Ë¶ÅÊòØkerasÂêéÁª≠ËÉΩÂ§üÊîØÊåÅmlxÂêéÁ´ØÂ∞±Â•Ω‰∫ÜÔºåËøôÊ†∑ËãπÊûúËÆæÂ§á‰∏äÂÜôÁöÑ‰ª£Á†Å‰∏çËá≥‰∫éÂà∞Âà´ÁöÑÂπ≥Âè∞‰∏äË∑ë‰∏ç‰∫ÜËøòË¶ÅÈáçÊûÑ‰∫Ü„ÄÇ

- [ËãπÊûúÂêëËã±‰ºüËææÁîüÊÄÅÂ¶•Âçè‰∫ÜÔºÅMLXÊ°ÜÊû∂‰∏ªÂä®ÈÄÇÈÖçCUDA - Áü•‰πé _202507](https://zhuanlan.zhihu.com/p/1929178251202376826)
  - ËãπÊûú‰∏ÄÁõ¥‰ª•Êù•ÈÉΩ‰ª•‚ÄúÂ∞ÅÈó≠‚ÄùËëóÁß∞Ôºå‰ΩÜÈöèÁùÄËã±‰ºüËææCUDAÁîüÊÄÅÂú®AIÂºÄÂèëÈ¢ÜÂüüÂç†ÊçÆÁªùÂØπ‰∏ªÂØºÂú∞‰ΩçÔºåËãπÊûúËøô‰∏ã‰πü‰∏çÂæó‰∏çËΩ¨ÂèòÂßøÊÄÅ‰∫Ü„ÄÇ
  - ËøáËÆ©MLXÊ°ÜÊû∂‰∏ªÂä®ÈÄÇÈÖçCUDAÔºå‰ªäÂêéËãπÊûúÂºÄÂèëËÄÖ‰πüËÉΩÂà©Áî®Ëã±‰ºüËææGPUËÆ≠ÁªÉÊ®°Âûã„ÄÇÂÖ∂Êú¨Ë¥®ÊòØÂ¢ûÂä†‰∫ÜÂØπCUDAÁöÑÂêéÁ´ØÊîØÊåÅÔºåÊñπ‰æøÂºÄÂèëËÄÖÂú®Windows/LinuxÁöÑËã±‰ºüËææÊòæÂç°‰∏äËÆ≠ÁªÉÊ®°ÂûãÔºåÁÑ∂ÂêéÂÜçÈÉ®ÁΩ≤ÂõûMac„ÄÅiPhoneÁ≠âËÆæÂ§á„ÄÇ
  - ÊòØÁªôWin-vidia/Lin-vidiaÂÅöMLXÊîØÊåÅÔºå‰∏çÊòØÁªôMacÁâàMLXÂÅöCUDAÈ©±Âä®

- Ëøô‰∏§Â§©Â∞ùËØïÂéªÂ≠¶‰π†‰∫Ü‰∏Ä‰∏ã mlx Âíå mlx ÁöÑÊ∫êÁ†ÅÔºåÁ≤óÁï•ÁöÑÁúãÊ≥ïÊúâ 2 ÁÇπÔºö
  - ÂÆèËßÇ‰∏äÔºåÊ°ÜÊû∂ÁöÑËÆæËÆ°ÂæàÊ£íÔºåÊï¥‰ΩìËÆæËÆ°‰∏äÂæàÂ•ΩÂú∞Âê∏Âèñ‰∫ÜÂâç‰∫∫ÁöÑÁªèÈ™åÔºåÂú®‰øùËØÅÊòìÁî®ÊÄßÁöÑÂâçÊèê‰∏ãÔºå‰øùÁïô‰∫ÜË∂≥Â§üÈ´òÁöÑ‰ºòÂåñ‰∏äÈôêÔºõ
  - ÂæÆËßÇ‰∏äÔºåÈ°πÁõÆÁöÑ C++ Ê∞¥Âπ≥ÊØîËæÉ‰∏ÄËà¨ÔºåÊØîËæÉÂÉèÊòØÂá†‰∏™Â∑•Á®ãÂ∏àÁöÑ side projectÔºåÂ¶ÇÊûúË¶ÅÂèòÊàêÈïøÈùíÊ†ëÔºåÂÜÖÈÉ®Â§ßÈáèÁöÑÂÆûÁé∞ÈÉΩÈúÄË¶ÅÈáçÊûÑ
- ÂÖ≥‰∫éÊû∂ÊûÑ„ÄÇÊàëËßâÂæó‰πãÂâçÂá†Âπ¥ÔºåËÆ≠ÁªÉÊ°ÜÊû∂‰∏äÁöÑÂÆûË∑µÊÄªÁªì‰∫ÜËøôÊ†∑ÁöÑ‰∏Ä‰∫õÁªèÈ™åÔºö
  - pytorch Âíå tensorflow ÁöÑ‰∫âÊñóÁªìÊûúÂëäËØâÊàë‰ª¨ÔºåËÆ©Â§ßÂÆ∂ÂÜô pythonÔºåËÄå‰∏çÊòØÂú® python ÈáåÂÜô DSLÔºåÊèêÂçáÊòìÁî®ÊÄßÊòØÂæàÈáçË¶ÅÁöÑÔºõ
  - torchscript„ÄÅlibtorch ÁöÑÂêÑÁßçÂùëÂëäËØâÊàë‰ª¨ÔºåÂÆåÂÖ®‰æùËµñÂú® python ‰∏äÔºåÂõæ‰ºòÂåñÂæàÊ£òÊâãÔºåÊâîÊâãÊú∫ËøôÊ†∑Ê≤°Êúâ python runtime ÁöÑÂú∞Êñπ‰πüÊòØÁúüÁöÑÂ§¥Áñº„ÄÇÊúÄÂ•Ω‰∏çË¶ÅÁ∫Ø eager Ê®°ÂºèÔºåËÄåÊòØËÉΩËÆ©Ê°ÜÊû∂ÊèêÂèñÂá∫Êù•‰∏Ä‰∫õÂ≠êÂõæ„ÄÇÊàñËÄÖÊòØÊúâ‰∏™Âõ∫ÂÆöÁÇπÁöÑ c apiÔºåÁªô‰∏çËÆ≠Ê®°ÂûãÁöÑÂ∑•Á®ãÂ∏à‰ª¨ÁïôÊù°Ê¥ªË∑ØÔºõ
  - transformer ÂëäËØâÊàë‰ª¨Ôºå‰πüËÆ∏‰ªéÂ≠¶ÊúØËßíÂ∫¶Êù•ËØ¥ÔºåÊéßÂà∂ÊµÅ‰ºòÂåñËµ∑Êù•ÂæàÊÄßÊÑüÔºå‰ΩÜÊòØÂÆûÈôÖÂ∫îÁî®‰∏≠ÔºåÂÖ®ÊòØ‰∏ÄÊù°Ë∑ØËµ∞Âà∞ÈªëÔºõ
- ‰πãÂâçÊàëËßâÂæó jax Â∞±ÊòØËÆ≠ÁªÉÊ°ÜÊû∂‰∏äÁöÑÈõÜÂ§ßÊàêËÄÖÔºåÂêÑÁßçËÆæËÆ°ÁúüÁöÑÂæàÊºÇ‰∫Æ„ÄÇÂèØÊòØÂÆÉÊãñ‰∫Ü‰∏™ÈáçÈáçÁöÑ XLA ÁöÑÂ∞æÂ∑¥ÔºåÂèØËÉΩÊòØ‰∏∫‰∫ÜË∑ëÂú® TPU ‰∏äÔºå‰πüÂèØËÉΩÊòØ jax ÁöÑÊàêÂëò‰∏≠ÊúâÂæàÂ§ö tf ÈáåÂÅöÁºñËØë‰ºòÂåñÁöÑ‰∫∫Âêß„ÄÇËøô‰ΩøÂæóÊó©ÊúüÁöÑ jax ÂØπÁî® nv ÊòæÂç°ÁöÑÁî®Êà∑Âæà‰∏çÂèãÂ•ΩÔºåÁ≠â pytorch/huggingface Êàê‰∫ÜÊ∞îÂÄôÔºå‰πüÂ∞±Êó†ÂäõÂõûÂ§©‰∫Ü„ÄÇ
- Áé∞Âú® mlx Âü∫Êú¨Âê∏Êî∂‰∫Ü jax ÁöÑÁªèÈ™åÔºå
  - ÈááÁî®‰∫Ü numpy api + lazy evaluationÔºõ
  - Êúâ‰∏Ä‰∏™ÊØîËæÉÂπ≤ÂáÄÁöÑ c apiÔºåÁîöËá≥Â∑≤ÁªèÊúâ‰∫Ü swift bindingÔºõ
  - ÂÜÖÈÉ®ÁïôÂá∫‰∫ÜÁÆÄÂçïÁöÑÂõæ‰ºòÂåñÊé•Âè£ mlx.core.compile Ôºå‰øùËØÅ‰∫Ü‰ºòÂåñÁöÑ‰∏äÈôê„ÄÇ

- Áúã‰∫Ü‰∏ÄÁúºÊ°ÜÊû∂Ê∫êÁ†ÅÔºåÂÆåÂÖ®ÊòØÈáçÊñ∞Êêû‰∫Ü‰∏ÄÂ•óÊ°ÜÊû∂ÔºåAPIÂ∞ΩÈáèÂØπÊ†áPyTorchÔºåÁõÆÂâç‰πüÂè™ÊîØÊåÅÂ∞ëÈáèAPI„ÄÇËøôÊ†∑Ê°ÜÊû∂ÈÄÇÈÖçÊàêÊú¨ÊÑüËßâÊúâÁÇπÂ§ßÔºåËøûÂâçÁ´ØPython APIÈÉΩË¶ÅËá™Â∑±ÊûÑÂª∫ÔºåÂ§çÂàªÂ§ßÈáè‰ª£Á†ÅÈÄªËæë‰∏î‰∏çËØ¥ÔºåPyTorchÁ§æÂå∫ÁöÑÂÖ®ÈáèfeatureÁöÑË∑üË∏™ÊºîËøõÂ∞±‰ºöÊòØ‰∏Ä‰∏™ÈöæÈ¢ò„ÄÇ
  - ÊÑüËßâAppleÂíåÊòáËÖæËµ∞Âêë‰∫Ü‰∏§‰∏™ÊñπÂêëÁöÑÊûÅÁ´ØÔºåÊòáËÖæÊòØÂÆåÂÖ®‰æùËµñÂéüÁîüÁöÑÊèí‰ª∂ÂåñÔºå‰ºöÊÉ≥Â∞Ω‰∏ÄÂàáÂäûÊ≥ïÂ§çÁî®ÂéüÁîüÈÄªËæëÔºåÂÆûÂú®‰∏çË°åÂ∞±Â§çÂàªÔºõAppleÂàôÊòØÂÆåÂÖ®ÊäõÂºÉÂéüÁîüÈÄªËæëÔºåÁ´ØÂà∞Á´ØÈáçÊñ∞Êêû‰∏ÄÂ•ó„ÄÇ

- Áúã‰∫Ü‰∏ã‰ª£Á†ÅÔºåËãπÊûúÊòØÈáçÊñ∞ÈÄ†‰∫Ü‰∏™ËΩÆÂ≠êÔºåÂâçÁ´ØÂπ≤ÁöÑ‰∫ãÂíåPyTorchÂíåJaxÊ≤°Âï•Âå∫Âà´ÔºåÊÑüËßâÈáçÁÇπËøòÊòØÂú®Apple SiliconÁöÑÂêéÁ´Ø‰∏ä„ÄÇÁõÆÂâçÁöÑÂÆûÁé∞ËøòÊòØÊå∫Â∞èÂ∑ßÁöÑÔºåÁî®Êù•Â≠¶‰π†ÁöÑËØùËøò‰∏çÈîôÔºå‰ΩÜË¶ÅÂú®feature‰∏äÂØπÈΩêÁé∞ÊúâÁöÑÊ°ÜÊû∂ËøòÊúâ‰∏çÂ∞ëÁöÑÊ¥ªË¶ÅÂπ≤„ÄÇ

- ‰∏™‰∫∫ÊÑüËßâËøôÁé©ÊÑèÊúâÁÇπÈ∏°ËÇã„ÄÇ
  - macÂÜô‰ª£Á†Å„ÄÅË∞ÉËØïÔºåÁÑ∂ÂêéÊúçÂä°Âô®ÂÆåÊàêÊï¥‰∏™ËÆ≠ÁªÉ„ÄÇËøôÂ∞±ÊúâÈóÆÈ¢ò‰∫ÜÔºåÊúçÂä°Âô®‰∏çÊîØÊåÅMLXÊ°ÜÊû∂„ÄÇ
  - macÂÜô‰ª£Á†Å„ÄÅË∞ÉËØïÔºåÁÑ∂ÂêémacÂÆåÊàêÊï¥‰∏™ËÆ≠ÁªÉ„ÄÇ Ê≠£Â∏∏ÂÖ¨Âè∏Ë∑ëÊ®°ÂûãÔºåÂ¶ÇÊûúÂ∞ëÈáèÊï∞ÊçÆ‰ΩøÁî®Âá†Âº†Ê∑òÊ±∞‰∏ãÊù•ÁöÑ1080Â∞±ÂèØ‰ª•Ôºå‰ª∑Ê†ºËøò‰æøÂÆú„ÄÇÊï∞ÊçÆÈáèÂ§ßÔºåË¶Å‰ΩøÁî®ÂàÜÂ∏ÉÂºèÂπ∂Ë°åËÆ≠ÁªÉÔºåÈÇ£Â∞±Ê≤°mac‰ªÄ‰πà‰∫ãÊÉÖ‰∫Ü„ÄÇ
  - ÊúÄËøëÁÅ´ÁÉ≠ÁöÑÂ§ßÊ®°ÂûãËÆ≠ÁªÉÔºåÂì™ÂÆ∂ÂÖ¨Âè∏‰ºöÁî®macÊê≠Âª∫ÈõÜÁæ§ËÆ≠ÁªÉÂ§ßÊ®°ÂûãÔºåÂπ∂‰∏îËÆ≠ÁªÉÂÆåÊàêÂêé‰ΩøÁî®macÂÅöÊé®ÁêÜ„ÄÇÁî®‰∏çÂà∞macÂ∞±Ê≤°MLX‰ªÄ‰πà‰∫ãÊÉÖ‰∫Ü„ÄÇ

- ÊàëËßâÂæóÂèØ‰ª•ÂèÇËÄÉÊó©ÊúüswiftÔºå‰∏ÄÂπ¥‰∏Ä‰∏™ÁâàÊú¨ÔºåËÆ©‰Ω†ÊÑüÂà∞Â¥©Ê∫ÉÔºåËôΩÁÑ∂ËøôÊ¨°ÂºÄÊ∫êÔºå‰ΩÜÊòØ‰∏ªÂäõÂ∫îËØ•ËøòÊòØËãπÊûúËá™Â∑±

- [ÈòøÈáåÂ∑¥Â∑¥ÂèëÂ∏É‰∫Ü‰∏éËãπÊûú MLX Êû∂ÊûÑÂÖºÂÆπÁöÑ Qwen3 ÂçáÁ∫ßÁâàÔºåÊñ∞ÁâàÊú¨ÈÉΩÊúâÂì™‰∫õÊñ∞ÁâπÊÄßÔºü - Áü•‰πé _202506](https://www.zhihu.com/question/1918255662552582106)
  - MLXÊòØËãπÊûú‰∏ì‰∏∫Apple SiliconËäØÁâáÔºàMÁ≥ªÂàóÔºâËÆæËÆ°ÁöÑÂºÄÊ∫êÊú∫Âô®Â≠¶‰π†Ê°ÜÊû∂ÔºåÂÆÉÁöÑÊ†∏ÂøÉ‰ºòÂäøÂú®‰∫éÊ∑±Â∫¶Êï¥ÂêàËãπÊûúÁ°¨‰ª∂ÁâπÊÄßÔºåÂ∞§ÂÖ∂‰ª•‚ÄúÁªü‰∏ÄÂÜÖÂ≠òÊ®°Âûã‚Äù‰∏∫Ê†∏ÂøÉÂàõÊñ∞
  - Áªü‰∏ÄÂÜÖÂ≠òÊû∂ÊûÑÔºöÊï∞ÊçÆÂ≠òÂÇ®Âú®CPU‰∏éGPUÂÖ±‰∫´ÁöÑÂÜÖÂ≠òÁ©∫Èó¥‰∏≠ÔºåË∑®ËÆæÂ§áËÆ°ÁÆóÊó∂Êó†ÈúÄÊòæÂºèÊã∑Ë¥ùÊï∞ÊçÆÔºåÁõ∏ÊØî‰º†ÁªüÊ°ÜÊû∂ÔºàÂ¶ÇPyTorchÔºâÂáèÂ∞ë90%‰ª•‰∏äÁöÑÂÜÖÂ≠ò‰º†ËæìÂºÄÈîÄ„ÄÇ

- ## [Â¶Ç‰ΩïÁúãÂæÖ Google ÊúÄÊñ∞ÂºÄÊ∫êÁöÑ Gemma-3 Á≥ªÂàóÂ§ßÊ®°ÂûãÔºü - Áü•‰πé _202503](https://www.zhihu.com/question/14777841836)

- Gemma-3‰∏ªÊâìÁöÑÂçñÁÇπÊòØÂçïÂç°ÂèØË∑ë+Â§öÊ®°ÊÄÅ„ÄÇ
- Gemma-3ÁöÑÊ†∏ÂøÉÁ´û‰∫âÂäõÂú®‰∫éÊïàÁéáÈù©ÂëΩ„ÄÇÂÖ∂27BÁâàÊú¨‰ªÖÈúÄÂçïÂº†H100ÊòæÂç°Âç≥ÂèØËøêË°åÔºåËÄåÁ´ûÂìÅËææÂà∞ÂêåÁ≠âÊÄßËÉΩÂæÄÂæÄÈúÄË¶Å10ÂÄçÁÆóÂäõ„ÄÇËøôÁßçÊïàÁéáÊèêÂçáÂπ∂ÈùûÁÆÄÂçïÁöÑÂèÇÊï∞ÂéãÁº©ÔºåËÄåÊòØÊû∂ÊûÑÂ±ÇÈù¢ÁöÑÂàõÊñ∞
  - Áü•ËØÜËí∏È¶èÊäÄÊúØÔºöÂü∫‰∫éGemini 2.0ÁöÑ"ÊïôÂ∏àÊ®°Âûã"ËøõË°åËí∏È¶èÔºåÂ∞ÜÂ§ßÊ®°ÂûãËÉΩÂäõËøÅÁßªÂà∞Â∞èÊ®°ÂûãÔºåÂÆûÁé∞"Áî®1/10ÂèÇÊï∞ËææÂà∞80%ÊÄßËÉΩ"ÁöÑÊïàÊûú„ÄÇ
- Â∞ΩÁÆ°Gemma-3Ë°®Áé∞‰∫ÆÁúºÔºå‰ªçÈúÄÊ≥®ÊÑè‰ª•‰∏ãÈóÆÈ¢òÔºö 
  - 1. ÈïøÂ∞æ‰ªªÂä°Áº∫Èô∑ÔºöÂú®Êï∞Â≠¶Êé®ÁêÜÂíå‰º¶ÁêÜÂà§Êñ≠Á≠âÁâπÂÆöÂú∫ÊôØÔºåÂÆûÊµãÁªìÊûúÊòæÁ§∫ÂÖ∂Ë°®Áé∞‰∏çÂ¶ÇQwen2.5-Max„ÄÇ 
  - 2. Â§öÊ®°ÊÄÅÁ≤æÂ∫¶Áì∂È¢àÔºöÂ§ÑÁêÜÂ§çÊùÇÂõæÂÉèÊó∂ÔºåSigLIPÁºñÁ†ÅÂô®ÁöÑÁªÜËäÇËØÜÂà´ËÉΩÂäõ‰ªçËêΩÂêé‰∫é‰∏ì‰∏öCVÊ®°Âûã„ÄÇ 
  - 3. ËÆ≠ÁªÉÊï∞ÊçÆ‰æùËµñÔºöËôΩÁÑ∂ÊîØÊåÅ140ÁßçËØ≠Ë®ÄÔºå‰ΩÜÂÆûÈôÖË°®Áé∞Â≠òÂú®"Ëã±ËØ≠ÊúÄ‰ºòÔºåÂ∞èËØ≠ÁßçÊ¨°‰πã"ÁöÑÈóÆÈ¢ò„ÄÇ 
- Êú™Êù•ÂèØËÉΩÁöÑÁ™ÅÁ†¥ÊñπÂêëÂåÖÊã¨Ôºö 
  - Âä®ÊÄÅÊû∂ÊûÑÔºöÊ†πÊçÆËæìÂÖ•ÂÜÖÂÆπËá™Âä®ÂàáÊç¢ËÆ°ÁÆóËµÑÊ∫êÂàÜÈÖçÔºå‰æãÂ¶ÇËßÜÈ¢ëÂ§ÑÁêÜÊó∂‰∏¥Êó∂Ë∞ÉÁî®Êõ¥Â§öGPUÊ†∏ÂøÉ„ÄÇ 
  - Ë∑®Ê®°ÊÄÅÁîüÊàêÔºö‰ªé"ÁêÜËß£Â§öÊ®°ÊÄÅ"ÂçáÁ∫ßÂà∞"ÁîüÊàêÂ§öÊ®°ÊÄÅ"Ôºå‰æãÂ¶ÇÊ†πÊçÆÊñáÊú¨ÊèèËø∞Áõ¥Êé•ÁîüÊàê3DÊ®°Âûã„ÄÇ 
  - ËæπÁºòÁ´Ø‰ºòÂåñÔºöËøõ‰∏ÄÊ≠•ÂéãÁº©Ê®°Âûã‰ΩìÁßØÔºåÂú®ÊâãÊú∫Á´ØÂÆûÁé∞ÂÆûÊó∂Â§öÊ®°ÊÄÅ‰∫§‰∫í„ÄÇ

- ÂºÄÊ∫êGemmaÁ≥ªÊ®°ÂûãÊ≠£ÂºèËø≠‰ª£Âà∞Á¨¨‰∏â‰ª£ÔºåÂéüÁîüÊîØÊåÅÂ§öÊ®°ÊÄÅÔºå128k‰∏ä‰∏ãÊñá„ÄÇ
  - Gemma 3‰∏ÄÂÖ±ÂºÄÊ∫ê‰∫ÜÂõõÁßçÂèÇÊï∞Ôºå1B„ÄÅ4B„ÄÅ12BÂíå27B„ÄÇÊúÄÊúÄÊúÄÂÖ≥ÈîÆÁöÑÊòØÔºå‰∏ÄÂùóGPU/TPUÂ∞±ËÉΩË∑ëÊ®°Âûã„ÄÇ
  - Âú®LMArenaÁ´ûÊäÄÂú∫‰∏≠ÔºåGemma 3Êãø‰∏ã‰∫Ü1339 ELOÈ´òÂàÜÔºå‰ªÖ‰ª•27BÂèÇÊï∞ÂáªË¥•‰∫Üo1-preview„ÄÅo3-mini high„ÄÅDeepSeek V3ÔºåÂ†™Áß∞‰ªÖÊ¨°‰∫éDeepSeek R1ÊúÄ‰ºòÂºÄÊ∫êÊ®°Âûã„ÄÇ
  - Gemma3Á≥ª1B„ÄÅ4B„ÄÅ12B„ÄÅ27BÂàÜÂà´Âü∫‰∫é2T„ÄÅ4T„ÄÅ12T„ÄÅ14T tokenÊï∞ÊçÆÂÆåÊàêËÆ≠ÁªÉ„ÄÇ
  - ‰∏éÈó≠Ê∫êGemini 1.5Âíå2.0Áõ∏ÊØîÔºåGemma 3-27BÂü∫Êú¨‰∏äÁï•ÈÄäËâ≤‰∫éFlashÁâàÊú¨„ÄÇ
- ÈááÁî®‰∏éGemini 2.0Ê®°ÂûãÁõ∏ÂêåÁöÑÁ†îÁ©∂ÂíåÊäÄÊúØÊâìÈÄ†„ÄÇ

- ÊúÄÂ§ßÁöÑ17GBÊòØÁ®≥Á®≥ÂèØ‰ª•Ë∑ëÂú®Êã•Êúâ24GBÊòæÂ≠òÁöÑNvidia90Á≥ªÊòæÂç°ÁöÑ„ÄÇ

- Gemma 3n ÊòØ‰∏ÄÊ¨æ‰∏ì‰∏∫ÁßªÂä®ËÆæÂ§áÈáèË∫´ÊâìÈÄ†ÁöÑÂ§öÊ®°ÊÄÅ AI Êû∂ÊûÑÔºåÂÆÉ‰∏ç‰ªÖÊîØÊåÅÂõæÂÉè„ÄÅÈü≥È¢ë„ÄÅËßÜÈ¢ëÂíåÊñáÊú¨ÁöÑËæìÂÖ•ËæìÂá∫ÔºåËøòÈÄöËøáÂàõÊñ∞ÁöÑ MatFormer Êû∂ÊûÑÂíå Per-Layer Embeddings ÊäÄÊúØÔºåÂÆûÁé∞‰∫ÜÈ´òÊÄßËÉΩ‰∏é‰ΩéÂÜÖÂ≠òÂç†Áî®ÁöÑÂÆåÁæéÂπ≥Ë°°„ÄÇ
  - E2B Âíå E4B ‰∏§ÁßçÊ®°ÂûãÁâàÊú¨ÔºåÂàÜÂà´‰ªÖÈúÄ 2GB Âíå 3GB ÂÜÖÂ≠òÂç≥ÂèØËøêË°åÔºåËÄå‰∏îËøòËÉΩÈÄöËøá Mix-n-Match ÊñπÊ≥ïÁÅµÊ¥ªË∞ÉÊï¥Ê®°ÂûãÂ§ßÂ∞èÔºåÊª°Ë∂≥‰∏çÂêåÂú∫ÊôØÁöÑÈúÄÊ±Ç„ÄÇ

- Gemma 3n Êèê‰æõ‰∫ÜÂ§öÁßçÈÉ®ÁΩ≤ÈÄâÈ°πÔºåÂåÖÊã¨ Google GenAI API„ÄÅVertex AI„ÄÅSGLang„ÄÅvLLM Âíå NVIDIA API Catalog Á≠â„ÄÇ
  - Ê≠§Â§ñÔºåGemma 3n Ëøò‰∏é Hugging Face„ÄÅGoogle AI Edge„ÄÅOllama„ÄÅMLX Á≠âÂ§öÁßçÂ∑•ÂÖ∑ÂíåÂπ≥Âè∞Êó†ÁºùÈõÜÊàêÔºåËÆ©ÂºÄÂèëËÄÖÂèØ‰ª•Âø´ÈÄü‰∏äÊâãÔºåËΩªÊùæÈõÜÊàêÂà∞Áé∞ÊúâÈ°πÁõÆ‰∏≠„ÄÇ

- ### [Difference between Gemma and Gemini - Marvik _202407](https://blog.marvik.ai/2024/07/03/difference-between-gemma-and-gemini/)
- While Gemma is open source and lightweight, Gemini is proprietary and heavyweight. 

- ## [Â¶ÇÊûúÊú¨Âú∞Ë¶ÅË£ÖÂ§ßÊ®°ÂûãÔºåÂª∫ËÆÆÂì™‰∏™ÂºÄÊ∫êÂ§ßÊ®°ÂûãÔºü - Áü•‰πé](https://www.zhihu.com/question/653255605)
- ollamaÂéüÂßãÁöÑÁâàÊú¨ÊòØÂπ∂‰∏çÂÖºÂÆπ OpenAI Ê†ºÂºèÁöÑËØ∑Ê±ÇÔºåÂõ†Ê≠§Âú®‰ΩøÁî®ÊØîÂ¶Ç langchain„ÄÅllamaindex Á≠âÊ°ÜÊû∂ÁöÑÊó∂ÂÄôÔºå‰ºöÈÅáÂà∞‰∏Ä‰∫õÈ∫ªÁÉ¶ÁöÑÂú∞ÊñπÔºåÈúÄË¶ÅÂÜçËΩ¨Âåñ‰∏ã„ÄÇ
  - Áé∞Âú®ÔºåÊõ¥Êñ∞Âà∞Êñ∞ÁâàÊú¨ÁöÑÊó∂ÂÄôÔºåÂ∞±Êèê‰æõ‰∫Ü OpenAI Ê†ºÂºèÁöÑÊé•Âè£Ë∞ÉÁî®
  - ÁõÆÂâçËøô‰∏™ OpenAI-compatible API ËøòÂ§Ñ‰∫éÊµãËØïÈò∂ÊÆµÔºåÂπ∂‰∏çÊîØÊåÅ function calling ÁöÑÂäüËÉΩ„ÄÇ

- ÊàëÊú¨Âú∞ollamaË∑ë‰∏ÄÂº†P40/24GÊòæÂ≠òÔºåÁé©‰∫Ü‰∏ÄÂπ¥Â§ö‰∫Ü„ÄÇÊàë‰∏ÄËà¨Ë∑ëÈáèÂåñËøáÁöÑ32BÁöÑÊ®°ÂûãÔºåÂÜçÂæÄ‰∏äÈÄüÂ∫¶Â§™ÊÖ¢ÊàñËÄÖÈáèÂåñÂ§™Áã†ÔºåÈÉΩÊ≤°Â§™Â§ßÂÆûÁî®‰ª∑ÂÄº„ÄÇ

- Áúã‰Ω†ÊòæÂ≠òÂ§ßÂ∞èÈÄÇÂêàÂ§öÂ∞ëBÁöÑÊ®°Âûã„ÄÇËØÑ‰º∞Êó∂ÂÄôË¶ÅËÄÉËôëÈáèÂåñint4„ÄÅint8ËøòÊòØfloat16„ÄÇÂèØ‰ª•Âà©Áî®huggingface‰∏äÁöÑËÆ°ÁÆóÂô®ËØÑ‰º∞ https://hf-accelerate-model-memory-usage.hf.space
  - Èô§‰∫ÜollamaÂÆòÊñπÂàóË°® ‰πüÂèØ‰ª•ÂéªÊâæhugging face‰∏äÈáèÂåñÂ•ΩÁöÑÊ®°Âûã„ÄÇ
- ollamaÊòØÂü∫‰∫éllama.cpp, ÊâÄ‰ª•‰πüÂèØ‰ª•Âü∫‰∫éllama.cpp Ëá™Â∑±ÊäòËÖæ‰∏ÄÈÅçÔºå‰ªéËΩ¨Êç¢Ê®°Âûã‰∏∫ggufÂà∞ÈáèÂåñ‰∏∫int4 ËøòÊòØint8ÔºåÂèØ‰ª•ÊåâÁÖßÊàë‰πãÂâçÂÜôÁöÑÊïôÁ®ãËá™Â∑±Ëµ∞‰∏ÄÈÅçÔºåÂèØÁé©ÊÄßÊõ¥Âº∫‰∏ÄÁÇπ„ÄÇÊàëÁîµËÑëmac m1 Ë∑ëÈÄö‰πâÂçÉÈóÆÁöÑ7bÁöÑint4ÈáèÂåñÊ®°ÂûãÔºåÂ§ßÁ∫¶ÊØèÁßí12‰∏™token„ÄÇ

- Èùû‰∏ì‰∏ö‰∫∫Â£´ÁöÑËØùÔºåollamaÂíålocalaiÔºåËøô‰∏§Êó†ËÑëÈÉ®ÁΩ≤ËøêË°å„ÄÇÂêåÊó∂Ëøô‰∏§‰∏çÂ±ÄÈôê‰Ω†ÊúâÊ≤°ÊúâGPUÔºåcpu‰πüÂèØ‰ª•Êêû„ÄÇ

- Â§™Â∞èÁöÑÊ®°ÂûãÔºåÊÉ≥1.5bÂ§ßÂ∞èÁöÑÔºåÊÄßËÉΩËÇØÂÆöÂ∑Æ‰∏Ä‰∫õÔºå‰∏çËøáÁªìÂêàragËøô‰∫õÔºåÂÆûÁé∞Êú¨Âú∞Áü•ËØÜÂ∫ì‰πüÂ∑Æ‰∏çÂ§ö‰∫Ü„ÄÇÂÅöÂÆûÈ™åÔºåÊ≠£Â∏∏ËøòÊòØÁî®7bÂ∑¶Âè≥ÁöÑ„ÄÇ

- Ëá™Â∑±Êú¨Âú∞Êê≠Âª∫ÁöÑÊï∞ÊçÆÈáèÂ§™Â∞ë‰∫ÜÔºåËÅîÊÉ≥ÂÖ≥ÈîÆËØçÂõ∞Èöæ

- ## [ÁõÆÂâçÊúâ‰ªÄ‰πàÂèØ‰ª•Êú¨Âú∞ÈÉ®ÁΩ≤ÁöÑÂ§ßÊ®°ÂûãÊé®Ëçê? - Áü•‰πé](https://www.zhihu.com/question/648879790)
- ÂºÄÊ∫êÂ§ßÊ®°ÂûãÊõ¥Êñ∞Ëø≠‰ª£Â§™Âø´Ôºå‰ªäÂπ¥ÂàöÊé®Âá∫ÁöÑÊ®°ÂûãÂèØËÉΩËøáÂá†‰∏™ÊúàÂ∞±ËøáÊó∂‰∫Ü„ÄÇ
- ‰∏ÄÊòØÂ¶Ç‰ΩïÊâæÂà∞ÊúÄÊñ∞ÁöÑÂ§ßÊ®°ÂûãÔºå
  - huggingfaceÂèØ‰ª•ÁêÜËß£‰∏∫ÂØπ‰∫éAIÂºÄÂèëËÄÖÁöÑGitHubÔºåÊèê‰æõ‰∫ÜÊ®°Âûã„ÄÅÊï∞ÊçÆÈõÜÔºàÊñáÊú¨|ÂõæÂÉè|Èü≥È¢ë|ËßÜÈ¢ëÔºâ„ÄÅÁ±ªÂ∫ìÔºàÊØîÂ¶Çtransformers|peft|accelerateÔºâ„ÄÅÊïôÁ®ãÁ≠â„ÄÇ
  - huggingfaceÊúâÊó∂Â≠òÂú®ÁΩëÁªú‰∏çÁ®≥ÂÆöÁöÑÈóÆÈ¢ò, ËøôÈáåÊé®ËçêÂõΩÂÜÖÊØîËæÉÂ•ΩÁöÑÂπ≥Âè∞modelscope
- ‰∫åÊòØÂ¶Ç‰ΩïÂà§Êñ≠Êú¨Âú∞Á°¨‰ª∂ËµÑÊ∫êÊòØÂê¶Êª°Ë∂≥Â§ßÊ®°ÂûãÁöÑÈúÄÊ±ÇÔºå
  - Êú¨Âú∞ÂèØ‰ª•ÈÉ®ÁΩ≤‰ªÄ‰πàÂ§ßÊ®°ÂûãÔºåÂèñÂÜ≥‰∫é‰Ω†ÁöÑÁ°¨‰ª∂ÈÖçÁΩÆÔºàÂ∞§ÂÖ∂ÂÖ≥Ê≥®‰Ω†GPUÁöÑÊòæÂ≠òÔºâ„ÄÇ
  - Âú®Ê≤°ÊúâËÄÉËôë‰ªª‰ΩïÊ®°ÂûãÈáèÂåñÊäÄÊúØÁöÑÂâçÊèê‰∏ãÔºö ÂÖ¨ÂºèÔºöÊ®°ÂûãÊòæÂ≠òÂç†Áî®ÔºàGBÔºâ = Â§ßÊ®°ÂûãÂèÇÊï∞ÔºàBÔºâ*2
  - Âú®4bitÈáèÂåñÁöÑÊÉÖÂÜµÔºåÊª°Ë∂≥Êú¨Âú∞Êú∫Âô®GPUÊòæÂ≠òÔºàGBÔºâ >= Â§ßÊ®°ÂûãÂèÇÊï∞ÔºàBÔºâ/2ÔºåÂèØ‰ª•Â∞ùËØïÊú¨Âú∞ÈÉ®ÁΩ≤„ÄÇ
- ‰∏âÊòØÂ¶Ç‰ΩïÂø´ÈÄüÈÉ®ÁΩ≤Â§ßÊ®°Âûã„ÄÇ
  - ollama pull llama3

- ÊàëËØ¥ÁöÑÊú¨Âú∞ÔºåÊåáÁöÑ‰∏çÊòØ‰∏ÄÂè∞‰∏™‰∫∫ÁîµËÑë‰∏äÔºåË∑ë‰∏Ä‰∏™7B„ÄÅ13BÂèÇÊï∞ÁöÑÂ§ßÊ®°Âûã„ÄÇËÄåÊòØÂú®‰ºÅ‰∏öÊú¨Âú∞ÁÆóÂäõÊúçÂä°Âô®‰∏äÔºåÁßÅÊúâÂåñÈÉ®ÁΩ≤ÁöÑ700‰∫øÂèÇÊï∞‰ª•‰∏äËßÑÊ®°ÁöÑÂ§ßÊ®°ÂûãÔºåËøôÁßçÂèÇÊï∞ËßÑÊ®°ÁöÑÂ§ßÊ®°ÂûãÔºåÊâçÊúâÊõ¥Â•ΩÁöÑÊåá‰ª§‰æù‰ªéÊÄßÔºåÁªìÂêàRAG„ÄÅAgentÁ≠âÊäÄÊúØÔºåËÉΩÊúâÊïàÁöÑÂÆåÊàê‰Ω†ÂàÜÈÖçÁªô‰ªñÁöÑ‰ªªÂä°„ÄÇ 

- 
- 
- 
- 
- 

# discuss-local-models
- ## 

- ## 

- ## 

- ## 

- ## 

- ## 

- ## [It's Mamba time: Comparing Nemotron Nano v2 vs Falcon-H1 vs Qwen (og) vs Qwen (2507) : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1my39ja/its_mamba_time_comparing_nemotron_nano_v2_vs/)
  - Conclusions
  - Nemotron Nano is the most powerful hybrid I've evaluated so far. It's major weakness is that it seems to have failed to generalize Arithmetic and it's selective attention (information-filtering ability) is noticeably weaker then SOTA transformers. Mid-tier for reasoning length.
  - While Hybrids are getting better, they don't yet beat pure Transformers when I evaluated Falcon-Mamba it got a big fat 0 - these new hybrid guys actually do work and are getting better with each iteration. I hope to see this conclusion flip in the future!
  - Qwen3-4B-Instruct-2507 is a little beast and can replace older 8B with similar if not better performance and lower token usage.

- ## üÜö [two models big difference in how it converses/answers. ie Qwen3 30B A3B vs Qwen3 32B : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mkgv1l/two_models_big_difference_in_how_it/)
  - I downloaded 2 8bit models (both use 32-33gb of ram)

- Qwen 3 30B A3B 2507 just got released and one of the biggest improvements was made to tone it seems, but Qwen 3 32B hasn't been updated and is still a part of the original Qwen 3 release. I've found Qwen 3 32B actually has better tone than the original Qwen 3 30B A3B does.
  - I really hope they come out with a Qwen 3 32b 2508 version. For my use case, the dense model wins every time, and an update with improvements on par with the other 2507 releases would be a huge win for my workflow.

- The 30B A3B model is MOE model. The A3B tells you that it's 3B active. So you get the speed of a model like it was 3B but the intelligence of a 30B model.
  - Qwen3 32B is dense, not moe, and so it's slower.

- The tone of the answers depends as much on the prompt than it does on the model. You can tell it in the system prompt how it should act and both of those should be able to handle that just fine. Just tell them to be more conversational or something like that.

- ## [DeepSeek‚Äôs MOE approach for lower model hope : r/LocalLLaMA _202508](https://www.reddit.com/r/LocalLLaMA/comments/1mk0fxu/deepseeks_moe_approach_for_lower_model_hope/)
  - Seeing recent Qwen3-30B-A3B, I am praying DeepSeek release something like that too

- I mean... They sort of did. Qwen3 30B was made borrowing techniques from Deepseek's public research and was distilled from the Qwen 235B MoE model which in turn was trained on outputs from R1.

- Deepseek‚Äôs distils are very strong. DeepSeek-R1-0528-Qwen3-8B is the one. It is distilled from the newer 0528 series and it literally beats Gemini at math.

- ## [DeepSeek‚Äôs new R1-0528-Qwen3-8B is the most intelligent 8B parameter model yet, but not by much: Alibaba‚Äôs own Qwen3 8B is just one point behind : r/LocalLLaMA _202506](https://www.reddit.com/r/LocalLLaMA/comments/1l41p1x/deepseeks_new_r10528qwen38b_is_the_most/)
- They are amazing for their size. Maybe, each model have a better perfomance in each benchmark and in final ranking it seems no diference. My experience:
  - Destill R1 8b is better in coding, math and reasoning on my tests
  - Qwen 8b is close in coding, but feels more natural in wrintting and multiligual tests (as a spanish native speaker I value this more ).
- R1 distill felt WAY better in writing to me, but I've only tested in english.

- That new R1-Q3-8b model has been a disaster for me. It overthinks, hallucinates, and doesn't seem to follow thinking parameters properly.

- What about qwen3-30b-a3b ?
  - It scores 55.6 on this aggregate of benchmarks, so about the same as Qwen3 14B and higher than DeepSeek V3. That also puts it 2 points below Claude 4 Opus... and 12 points below Gemini 2.5 Pro Preview (May).
# discuss-mac-llm
- ## 

- ## 

- ## ü§î [MLX you're using in release LM Studio seems to be outdated and broken for Q8 and larger models ¬∑ Issue ¬∑ lmstudio-ai/mlx-engine _202411](https://github.com/lmstudio-ai/mlx-engine/issues/34)
- The next version of our app should help remedy this, since we'll be shipping the latest version of MLX.

- The guidance is incomplete and solves nothing. The correct kernel tweaks to make these work are actually:
  - Without the `disable_wired_collector` setting, the problem remains.

```sh
sudo sysctl iogpu.disable_wired_collector=1
sudo sysctl iogpu.wired_limit_mb=n
```

- To make these persistent across reboots:
  - [Persistent sysctl Settings - Apple Community _202204](https://discussions.apple.com/thread/253840320?sortBy=rank)

- ## üí° [Run LLMs on macOS using llm-mlx and Apple‚Äôs MLX framework | Lobsters _202502](https://lobste.rs/s/1reyhf/run_llms_on_macos_using_llm_mlx_apple_s_mlx)
  - By default, MacOS allows 2/3rds of this RAM to be used by the GPU on machines with up to 36GB / RAM and up to 3/4s to be used on machines with >36GB RAM.

- I haven‚Äôt tried mlx the library yet, but when I was using LM Studio with their MLX backend, I had to run `sudo sysctl iogpu.wired_limit_mb=<something>` to increase the VRAM limit.
  - However, be aware that the system would panic when you try to load a big model or use a large context size when `<something>` is too big.

- By the way, I found that if you have models in` ~/.cache/huggingface/hub` already, due to having used mlx-lm directly, the llm mlx download-model mlx-community/‚Ä¶ command will skip the download and just add them to llm‚Äôs index.

- When comparing popular quant q4_K_M on Llama.cpp to MLX-4bit, in average, MLX processes tokens 1.14x faster and generates tokens 1.12x faster. This is what most people would be using.

- ## [Can't run Mixtral on GPU with M2 Pro 32 gb VRAM : r/LocalLLaMA _202312](https://www.reddit.com/r/LocalLLaMA/comments/18l9kal/cant_run_mixtral_on_gpu_with_m2_pro_32_gb_vram/)
  - I was able to run Mixtral Q5 on my macbook on CPU, but when I choose Apple Metal (GPU) in LM Studio I get this error:
  - GGML Metal Error: command buffer 4 failed with status 'Error'. Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory)
  - Worked for Q4_K_M when increasing the VRAM limit

- Gpu only has access to about 22gigs on a 32gig macbook, check the Llama.cpp or this community for a thread where setting a few command line arguments disables this limit.

- By default you can have up to 22GB of RAM allocated to the GPU but you can increase that amount up to 29GB with the system running fine. Try 27-28GB if you want to run a web browser on the side. In the terminal type the following (mb=0 to reset to default settings):
  - sudo sysctl iogpu.wired_limit_mb=29256
  - sudo sysctl iogpu.wired_limit_mb=0
  - You‚Äôd have to do it every boot as it‚Äôs not sticky .
- 32GB might be too tight for 32k ctx and would usually crash the machine. 12k ctx seems to be the limit for 32GB.

- Why no one talks about the new ml framework mlx from apple? Is it not applicable here?
  - It would hit the same limit that a q5 quant of dolphin-mixtral needs more GPU memory than can be allocated on a 32GB Mac, even after tweaking the kernel limit.

- ## üß© [First time using MLX and MacOS for inference: getting poor results : r/LocalLLaMA _202409](https://www.reddit.com/r/LocalLLaMA/comments/1fpy26p/first_time_using_mlx_and_macos_for_inference/)
- Set sysctls `sudo sysctl iogpu.wired_lwm_mb=400000 && sudo sysctl iogpu.wired_limit_mb=150000` , reboot
- iogpu.wired_limit_mb=150000 sets the maximum amount of wired memory the GPU is allowed to allocate to 150, 000 MB (~150 GB)
  - sets the maximum GPU memory allocation to 150GB
- iogpu.wired_lwm_mb=400000 sets the low-water mark (LWM) for the GPU‚Äôs wired memory to 400, 000 MB (~400 GB).
  - In memory management, a low water mark typically represents a threshold. When the amount of free memory drops below this mark, the system might take certain actions to free up more memory.

- iogpu.disable_wired_collector=1
  - tells macOS to disable the ‚Äúwired memory collector‚Äù for the GPU.
  - Normally, the collector periodically trims/reclaims GPU wired memory (VRAM allocations backed by unified RAM) when the system is under pressure.
  - Disabling it (=1) means the GPU can hold onto large wired allocations without the OS trying to recycle them. This is sometimes done when running ML/LLM workloads on Apple Silicon, because the collector can otherwise kill big GPU allocations and crash jobs.
  - Setting this to 1 disables this collector, potentially leaving GPU memory allocations "locked" and unadjusted even when the system needs memory.

- MLX is lazy so it won't load models until it needs to. So the time-to-first-token should be a lot lower on the second request.

- Run the command to expand the max VRAM allocation (needs to be done after each reboot):
  - sudo sysctl iogpu.wired_limit_mb=120000

- ## [Can the iogpu.wired_limit_mb setting on MacOS be persisted? üì£ YES! : r/LocalLLaMA _202405](https://www.reddit.com/r/LocalLLaMA/comments/1cncgqe/can_the_iogpuwired_limit_mb_setting_on_macos_be/)
- Can CPU intensive, RAM demanding tasks take RAM seemlessly when not running GPU inference?
  - Yes, this sets the higher limit. Clearly the problem is that under heavy load, GPU will take it all leaving just 8GB to the other apps and OS.

- ËÆæÁΩÆsysctlÂπ∂ÊâßË°å

```sh
sudo mv io.github.sysctl.plist /Library/LaunchDaemons/
sudo chown root:wheel /Library/LaunchDaemons/io.github.sysctl.plist
sudo launchctl load /Library/LaunchDaemons/io.yaoo.sysctl.plist
```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
 <key>Label</key>
 <string>io.github.sysctl</string>
 <key>ProgramArguments</key>
 <array>
 <string>/usr/sbin/sysctl</string>
 <string>-w</string>
 <string>iogpu.wired_limit_mb=27648</string>
 </array>
 <key>RunAtLoad</key>
 <true/>
</dict>
</plist>

```

- Rather than dealing with plist syntax, add `iogpu.wired_limit_mb=122800` on a line by itself to `/etc/sysctl.conf`. Create the file if it doesn't already exist. It should probably be owned by `root:wheel` and mode should be 644.

- ## üí° [M1/M2/M3: increase VRAM allocation with `sudo sysctl iogpu.wired\_limit\_mb=12345` (i.e. amount in mb to allocate) : r/LocalLLaMA _202311](https://www.reddit.com/r/LocalLLaMA/comments/186phti/m1m2m3_increase_vram_allocation_with_sudo_sysctl/)
  - One note on this ... All macos systems would be happiest to have at least 8gb available for OS stuff.
  - this is not a permanent change and it automatically resets on reboot.
  - PS: to get the current value use: sudo sysctl iogpu

- I looked at what wired memory (memory that can't be swapped) was without having an LLM loaded/running and then added a margin to that. I ended up allocating 26.5GB, up from 22.8GB default.
  - It worked, but it didn't work great because I still had a bunch of other stuff running on my Mac, so (not surprisingly) swapping slowed it down. For anything more than a proof of concept test I'd be shutting all the unnecessary stuff down.

- On my 32GB Mac, I allocate 30GB. [Macs with 32GB of memory can run 70B models with the GPU. : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/18674zd/macs_with_32gb_of_memory_can_run_70b_models_with/)
  - That's what I do and I have no swapping at all. I listed the two big things to turn off to save RAM. 
  - I don't use the GUI. Just simply logging in and doing nothing uses a fair amount of RAM. I run my Mac headless. I ssh in.
  - I stopped the `mds_stores` process from running

- consider llama.cpp instead of LM Studio to reduce RAM requirements a bit (still need to run terminal.app but this is pretty light)

- As per the latest developments in that discussion, "iogpu.wired_limit_mb" only works on Sonoma. So if you are on an older version of Mac OS, try "debug.iogpu.wired_limit" instead.

- How can I check if the changed worked? I did the intial code and said it was initially set to 0
  - One way I've verified is through the llama.cpp diagnostic output. It reports the available vram as well as the size of the model and how much vram it requires.
  - I've got 32gb total and I think the default availability is approx 22gb. So I can easily increase to 26gb and I see the difference immediately when I launch llama.cpp - the available vram will be reported as 26gb.

- ## üí° [Macs with 32GB of memory can run 70B models with the GPU. : r/LocalLLaMA _202311](https://www.reddit.com/r/LocalLLaMA/comments/18674zd/macs_with_32gb_of_memory_can_run_70b_models_with/)
  - I recently got a 32GB M1 Mac Studio. I was excited to see how big of a model it could run. It turns out that's 70B. It is a Q3_K_S model so the 2nd smallest for 70B in GGUF format
  - Mac shouldn't be able to dedicate that much RAM to the GPU. Apple limits it to 67%, which is about 21GB. This model is 28GB. So it shouldn't fit. But there's a solution to that thanks to these smart people here.
  - This new method is just setting the value of a variable. llama.cpp even tells you the value, how much RAM, it needs.
  - > sudo sysctl iogpu.wired_limit_mb=57344 ( I did that for my 64GB, you'd want to change the 57344 )

- ‚â•64GB allows 75% to be used by GPU. ‚â§32 its ~66%. Not sure about the 36GB machines.

- I also do these couple of things to save RAM.
  - I don't use the GUI. Just simply logging in and doing nothing uses a fair amount of RAM. I run my Mac headless. I ssh in.
  - I stopped the mds_stores process from running. I saw that it was using up between 500MB and 1GB of RAM. Its the processes that indexes the drives for faster search. 
  - With all that set, the highest I've seen in use memory is 31.02GB while running a 70B Q3_K_S model. So there's headroom. 

- You may try and run one of Q4 models without problems: because llama.cpp uses mmap to map files into memory, you can go above available RAM and because many models are sparse it will not use all mapped pages and even if it needs it, it will swap it out with other pages on demand... I was able to run falcon-180b-chat. Q6_K which uses about 141GB on a 128GB Windows PC with less than 1% SSD reads during inference... I could even run falcon-180b-chat. Q8 which uses about 182GB but in this case SSD was working heavily during inference and it was unbearably slow (0.01 tokens/second)...
  - Yes. I've done that before on my other machines. Llama.cpp in fact defaults to that. The hope for me was that since the models are sparse that the OS would cache the relevant parts of the models in RAM. So the first run through would be slow but subsequent runs would be fast since those pages are cached in RAM. 

- ## üß† [Instantly allocate more graphics memory on your Mac VRAM Pro : r/LocalLLaMA _202504](https://www.reddit.com/r/LocalLLaMA/comments/1k1tdpa/instantly_allocate_more_graphics_memory_on_your/)
  - I built a tiny macOS utility that does one very specific thing: It unlocks additional GPU memory on Apple Silicon Macs.
  - Why? Because macOS doesn‚Äôt give you any control over VRAM ‚Äî and hard caps it, leading to swap issues in certain use cases.
  - Do you need this app? No! You can do this with various commands in terminal. But wanted a nice and easy GUI way to do this.

- https://github.com/Sub-Soft/Siliv /NALic/202504/python
  - A simple macOS menu‚Äëbar utility to adjust Apple Silicon GPU VRAM allocation
  - Siliv provides a convenient way to view and set GPU VRAM allocation on Apple Silicon Macs via `sysctl` .
  - Persistence: `sysctl` changes may reset after reboot; 

- ü§î did you know that MLX ignores the VRAM limit global? do you know a way to make MLX use the extra VRAM allocated?
  - If you're on MacOS 15 or higher: Just run `sudo sysctl iogpu.wired_limit_mb=14336` on the terminal and replace 14336 (14GB) with your desired VRAM allocation in MB
- That's the global I use, and MLX ignores what's set for it. Using LM Studio, I can load and use q8 quants of 70B ggufs, but MLX models the same size, run out of memory.

- don't bother, this app just does this: `sudo sysctl iogpu.wired_limit_mb=24576`.
  - Yeah I already have a version of this aliased in my .zshrc file whenever I feel I need it (or to reset). 
  - Dont work , its a very old command for intel igpus ... 
# discuss-llama.cpp
- ## 

- ## 

- ## 

- ## üÜö‚ö°Ô∏è [Performance of llama.cpp on Apple Silicon M-series ¬∑ ggml-org/llama.cpp _202311](https://github.com/ggml-org/llama.cpp/discussions/4167)
  - Êèê‰æõ‰∫ÜÂêÑÁßçmac, airÁöÑÊ®°ÂûãÊµãËØïÊï∞ÊçÆ

# discuss-lmstudio-roadmap
- ## 

- ## 

- ## 

- ## [Incompatible type of `tool_choice` ¬∑ Issue ¬∑ lmstudio-ai/lmstudio-bug-tracker _202505](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/670)
- 
- 

- [[Bug]: Invalid value for 'tool\_choice' ¬∑ Issue #7039 ¬∑ All-Hands-AI/OpenHands](https://github.com/All-Hands-AI/OpenHands/issues/7039)
  - When I switch to use OpenAI (gpt-4o), it working fine.

- ## [Qwen3 Models not Recognized as Embedding Types for MLX Format ¬∑ Issue ¬∑ lmstudio-ai/lmstudio-bug-tracker _202507](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/808)
  - Qwen3 models not recognized as embedding models in MLX format (but same models are ok in GGUF)

- From what i can understand, while the chat models are using mlx as an inference engine, LMStudio would need to use a package like `mlx-embeddings` to properly handle MLX Embeddings, and integrate it in there LM Studio MLX Runtime.
  - This would explain why the `Override Domain Type` option is not available for MLX: Only the 'LLMs' models are supported for mlx.
- there is no support for mlx embeddings in LMStudio
- The only workaround for now using LMStudio is to use the GGUF versions and take the performance hit

- [[Models] Qwen3 Embedding shown as LLM instead of an embedding model ¬∑ Issue ¬∑ lmstudio-ai/lmstudio-bug-tracker](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/696)
# discuss-lmstudio
- ## 

- ## 

- ## 

- ## 

- ## 
# discuss-ollama-roadmap
- ## 

- ## 

- ## 

- ## 

- ## [MLX backend ¬∑ Issue ¬∑ ollama/ollama _202312](https://github.com/ollama/ollama/issues/1730)
- Add mlx-vlm backend also.

- Lack of MLX support is the only reason I don't use Ollama. In some cases MLX Q8 is 20% faster than GGUF, and memory usage is better handled.

- üì° [Draft MLX go backend for new engine by dhiltgen ¬∑ Pull Request ¬∑ ollama/ollama _202502](https://github.com/ollama/ollama/pull/9118)
# discuss-ollama

```sh
# ollamaÂéüÂßãÁöÑÁâàÊú¨ÊòØÂπ∂‰∏çÂÖºÂÆπ OpenAI Ê†ºÂºèÁöÑËØ∑Ê±Ç
curl http://localhost:11434/api/chat -d '{
  "model": "gemma3:4b",
  "messages": [
    { "role": "user", "content": "why is the sky blue?" }
  ]
}'
# Ê®°ÂûãÂìçÂ∫îËæìÂá∫Â¶Ç‰∏ã 
{"model":"gemma3:4b","created_at":"2025-07-31T16:54:00.804772Z","message":{"role":"assistant","content":"That"},"done":false}

```

- ## 

- ## 

- ## üèûÔ∏è [Can add Stable Diffusion 3.5 modelÔºü ¬∑ Issue ¬∑ ollama/ollama _202412](https://github.com/ollama/ollama/issues/8047)
  - Does Olama not support the Diffusion 3.5 model? Can we add an Diffusion 3.5 model?

- Short answer is no, you shouldn't be able to as Ollama is dedicated to Large LANGUAGE models.
  - Longer answer is that in general, Stable diffusion and Large Language Models require different architectures, and while some llava models (and more recently Llama models) do have the ability to reason about images, being able to identify an image / the contents of an image is inherently different from being able to generate one, and ollama isn't designed or equiped for that.
- For stable diffusion you should be looking into something like Automatic1111, forge, comfy, or swarm (to name a few of the more popular projects).

- ### [Image generation models ¬∑ Issue ¬∑ ollama/ollama](https://github.com/ollama/ollama/issues/786)
- Ollama currently uses llama.cpp to do a lot of the work of actually supporting a range of large language models. This choice allowed the team to focus on delivering value in other ways. Llama.cpp's reasons for not supporting text-to-image models are probably for similar reasons.

- There are plenty of Stable Diffusion UIs and all of them are drowning in issues and features because of this:
  - Automatic1111: most popular, most fully-integrated environment
  - ComfyUI: very modular, usually has the newest features first, allows flexible workflows
  - Fooocus: restricted and opinionated but focused on providing a simple UI which produces good output out of the box (original author is the inventor of ControlNets and has a deep understanding of the inner workings of Stable Diffusion). probably the closest to Dall-E 3 (Fooocus provides a GPT2 prompt rewrite engine in the background)
  - SD. Next: fork and overhaul of Automatic1111 because everything took sooo long to implement
  - (I think all of them provide API endpoints)

- I'd recommend Stable Diffusion 1.5 because it has the lowest hardware requirements and integration is simpler and will be faster. Later models have more specifics which require more attention to detail.

- ## [image generation : r/ollama _202410](https://www.reddit.com/r/ollama/comments/1g5lp9s/image_generation/)
  - is there any model that allows image generation?

- nope, but there are some vision models that capable of taking images as input

- You can add ComfyUI, Automatic1111 or LocalAI via API to generate images in OpenWebUI, though it‚Äôs somewhat finicky. It creates images from the LLM‚Äòs answer, not the Edit: prompt

- I am running Flux locally. It does a decent job in the dev model, there are 2 higher levels that aren‚Äôt for commercial use.

- [Image generator : r/ollama](https://www.reddit.com/r/ollama/comments/1iuzv2v/image_generator/)
  - No llm are able to generate image... However, you can let your llm reply with tool_call to fill a image promot to be sent to another ai model that generate images (chatgpt4o-dalle3 for exemple)

- [How can I generate images with ollama? : r/ollama](https://www.reddit.com/r/ollama/comments/18laoe6/how_can_i_generate_images_with_ollama/)
- Ollama doesn't yet support stable diffusion or creating images. It does support image to text though.

- If you are using Mac you can either use diffuser or diffustionbee.

- ## üéØ Ollama 0.10! Ollama's new app is here for macOS and Windows _20250731
- https://x.com/ollama/status/1950670503376761133
- [Ollama's new app](https://ollama.com/blog/new-app)
- Ollama‚Äôs macOS and Windows now include a way to download and chat with models.
  - Ollama‚Äôs new app supports file drag and drop, making it easier to reason with text or PDFs.
  - For processing large documents, Ollama‚Äôs context length can be increased in the settings. Note: this will require more memory.
- Building on Ollama‚Äôs new multimodal engine, images can be sent to models that support them, such as Google DeepMind‚Äôs Gemma 3 model.
- Code files can be processed by models for understanding.

- Is this ollama CLI with its own GUI?
  - Ollama‚Äôs new app includes the CLI. You can also use Ollama‚Äôs API or try Ollama‚Äôs Python/JavaScript libraries.

- It would be nice it it recommended models for my hardware? or is it already doing that?
  - Not yet, but we have hand selected the top models that will run on most users' hardware.

- What would be the use case over LMStudio on Mac?
  - quality, ease of use, model support directly with the major labs, hardware support
- üêõ I understand and appreciate ease of use and minimalism. It's really great when you'd want to chat with a model. However, as far as I understand there is still no MLX support. I'm strictly talking about MacOS version here.

- All that's missing now is MCP support.

- Can i choose where to save my models?
  - Yes, check out Ollama settings

- Feature request: Allow the user to set the Ollama endpoint. I run it on a beefy desktop computer but would like to use the app as a client.

- can you use it if the models are a other pc as a server
  - not yet. In the future, we'll add the support.

- Ollama natively support new AMD gpus?
  - We are working with @AMD to support the new GPUs

- We need our own completions API ‚Äî I have the feeling that tool calling doesn't work properly with the OpenAI SDK. The tool tags need improvement; for example, DeepSeek R1 shows tool support, but the 8B version doesn‚Äôt, even though it‚Äôs based on Qwen3, so it should.

- ÂÖ∂ÂÆûÊó©Â∞±ÊúâÁ¨¨‰∏âÊñπÁöÑ‰∫Ü...... ‰∏çËøáÊÑüËßâÂÆòÊñπÂêàÂπ∂ÂäüËÉΩ‰∫ÜÂêéÊõ¥Êñπ‰æø‰∫ÜÂ±û‰∫éÊòØ

- ## Ollama 0.2 is here! Concurrency is now enabled by default.  _20240709
- https://x.com/ollama/status/1810480544976626159
  - Parallel requests: Ollama can now serve multiple requests at the same time, using only a little bit of additional memory for each request.
  - Run multiple models: Ollama now supports loading different models at the same time

- ## So this is an LLM running locally? `ollama run llama3` .
- https://x.com/eatonphil/status/1797039865470570942
  - Asking it to help me with some Postgres C code. It's complete nonsense, but it's impressive complete nonsense
  - `ollama run codestral` looks about right though.
- How much RAM am I supposed to have for the 70B model?
  - B √ó Q / 8 ‚Üí RAM requirement for llm inference in GB
  - B: number of parameters
  - Q: quantization (16 = no quantization)
  - useful rule of thumb for RAM requirement for llm inference via hn user CobaltFire
- 3bit quantized should work with 32 GB RAM with other apps closed

- based on the download size that looks like the 7b model (smallest), which are pretty hopeless at anything beyond basic coding in my experience (codestral is a 22b iirc)
  - also if you like using models from the command line i highly rec @simonw 's `llm` cli! i believe it supports ollama

- ## üêõ [Error: pull model manifest ¬∑ ollama/ollama](https://github.com/ollama/ollama/issues/3434)
  - Error: pull model manifest: Get "https://registry.ollama.ai/v2/library/codellama/manifests/70b": read tcp 192.168.3.79:64976->172.67.182.229:443: read: operation timed out
- Error: max retries exceeded: Get "https://dd20bb891979d25aebc8bec07b2b3bbc.r2.cloudflarestorage.com/ollama/docker/registry/v2/blobs/sha256/14/1436d66b69757a245f02d000874c670507949d11ad5c188a623652052c6aa508/data?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=66040c77ac1b787c3af820529859349a%! F(MISSING)20240529%! F(MISSING)auto%! F(MISSING)s3%! F(MISSING)aws4_request&X-Amz-Date=20240529T155900Z&X-Amz-Expires=1200&X-Amz-SignedHeaders=host&X-Amz-Signature=cd4472bad19931e399f39a352a4a1b0902857996b7b784b8138f168d70532277": dial tcp 104.18.8.90:443: i/o timeout

- ## [‰∏∫‰ªÄ‰πàLlama2Â§ßÊ®°ÂûãÂèØ‰ª•Âú®‰∏™‰∫∫ÁîµËÑë‰∏äÈÉ®ÁΩ≤ Ôºü - Áü•‰πé](https://zhuanlan.zhihu.com/p/646939066)
- ÊàëÂú®MeatÁöÑÂÆòÁΩë‰∏äÁúãÂà∞ llama2 ÊòØÊûÑÂª∫Âú®PyTorch‰πã‰∏äÁöÑÔºåËÄåChatGPTÊòØÂü∫‰∫éTensorFlow ProbabilityÊ°ÜÊû∂ÁöÑÔºåÊú¨ÊñáÈáåÈù¢Â∞±ÁÆÄÁß∞TFP„ÄÇ

- ## [Meta AI ‰∏∫‰ªÄ‰πà‰ºöÂºÄÊ∫ê Llama2 Âë¢? - Áü•‰πé](https://www.zhihu.com/question/613072688/answers/updated)
- Âõ†‰∏∫ÊâÄË∞ìÁöÑLLMÂºÄÊ∫êÂè™ÊòØÂÖ¨Â∏ÉËÆ≠ÁªÉÂ•ΩÁöÑÁªìÊûÑÂíåÂèÇÊï∞ËÄåÂ∑≤ÔºåÁúüÊ≠£ÈáçË¶ÅÁöÑÊï∞ÊçÆÂíåËÆ≠ÁªÉ‰ª£Á†ÅÂπ∂Ê≤°ÊúâÂºÄÊ∫êÔºåÊõ¥Âà´ËØ¥Â§ßÈÉ®ÂàÜ‰∫∫ËøòÊ≤°ÊúâË∂≥Â§üÁöÑGPU„ÄÇ
  - Âç≥‰ΩøÂ¶ÇÊ≠§ÔºåÁõÆÂâçmistralËøôÊ†∑ÁöÑ‰πüÂè™ÂºÄÊ∫ê7b‰∏çÂºÄÊ∫êlargeÔºållamaÂêéÁª≠ËøòÂæóÁªßÁª≠ËßÇÂØü

- Llama2 ÂºÄÊ∫ê‰ΩÜ‰∏çÊòØÂèØ‰ª•Èöè‰æøÁî®ÁöÑÂïÜÁî®ËÆ∏ÂèØ„ÄÇ Áî®Êà∑Êï∞Âà∞‰∫Ü‰∏ÄÂÆöÁ®ãÂ∫¶Â∞±‰∏çÊòØÂÖçË¥πÁöÑ„ÄÇ
  - 7‰∫øÊúàÊ¥ª

- ## [Allow listening on all local interfaces _202310](https://github.com/ollama/ollama/issues/703)
- If you‚Äôre running Ollama directly from the command line, use the
`OLLAMA_HOST=0.0.0.0 ollama serve` command

- Edit the service file: Open `/etc/systemd/system/ollama.service` and add the following line inside the [Service] section:
 `Environment="OLLAMA_HOST=0.0.0.0"`

- sudo systemctl daemon-reload
- sudo systemctl restart ollama
# discuss-mobile-llm
- ## 

- ## 

- ## 

- ## [Meta released MobileLLM-R1 on Hugging Face : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1nf7zhq/meta_released_mobilellmr1_on_hugging_face/)
  - fair-noncommercial-research license
  - Its not just open weights its truly open source includes all the training data for full reproducabilility..

- it still gets beaten by qwen 0.6 so whats so special?
  - It's very close but it was trained on much less data
- The headline is less training compute. (Of course this is also the headline for Qwen3-Next, so that might perform similarly if scaled down; idk.)

- ## [Best local LLMs for mobile? : r/LocalLLaMA _202503](https://www.reddit.com/r/LocalLLaMA/comments/1j0v3b2/best_local_llms_for_mobile/)
  - I'm working on a project, trying out an in-browser inference engine (WebLLM) for the first time
  - Bonus points for the LLM being "uncensored"

- best models for mobile are mostly small, think 0.5B to 3B in complete size, but for usability I'll go for 3B, maybe 1.5B.
  - Phi-4 (Mini) [For Performance]: It's almost 4B (3.8B) in size but it's a great model for it's size and quite brand new.
  - Smollm2 [Recommended For Small Size]: Comes in sizes from 135M, 360M and 1.7B, made by HuggingFaceTB specially to be a small.

- Another model I just remembered (I was very tired when I wrote the response), Gemma 2 2B. I've heard it's pretty good for writing and creative responses!

- I'd recommend Qwen3 (4B/1.7B) at the moment, just to clarify.
- Qwen3 0.6B: It can reason and seems to be really good with funcion calling.
- ERNIE 4.5 (?) 0.3B: It's a really small model that maintains coherence, I personally like it and use it in places with low amount of RAM.

- Small models don't take Quantization well, they can get really dumbed down.
# discuss-ai-api/tools
- ## 

- ## 

- ## 

- ## 

- ## 

- ## openrouter ÊòØÁúüÊñπ‰æøÔºå‰∏Ä‰∏™ Key ÊâÄÊúâÊ®°ÂûãÈÉΩËÉΩÁî®„ÄÇ
- https://x.com/pengchujin/status/1894375539726803201
- Ê®°Âûã‰ª∑Ê†º‰∏ÄÊ†∑ÔºåÂÖÖÂÄº 5% ÁöÑÊâãÁª≠Ë¥π„ÄÇËøòÊúâ‰∏™Â•ΩÂ§ÑÊòØÊúâ‰∏ÄÈÉ®ÂàÜÂÖçË¥πÊ®°ÂûãÔºåÊØîÂ¶ÇAzÁöÑR1Âï•ÁöÑ
  - ‰ª∑Ê†º‰∏ÄÊ†∑ÁöÑÔºåÂÆòÊñπÈôç‰ª∑‰ªñ‰πüË∑üÁùÄÈôç
- QwenÁöÑkeyÔºåÁÅ´Â±±ÁöÑkey‰πüÊòØËøôÊ†∑ÁöÑ

- ## deepseek ÊúÄËøëÂºÄÊ∫êÁöÑÂâçÈù¢Âá†‰∏™È°πÁõÆÂèØËÉΩÊàë‰∏ç‰∫ÜËß£, ‰ªäÂ§©Ëøô‰∏™3fs, smallpond Ê≠£Â•ΩÊòØÊàë‰ª¨Ëøô‰∏™Ë°å‰∏öÁöÑ, ÊàëÁêÜËß£Âπ∂Ê≤°ÊúâÂÉèÊäÄÊúØÁÇ∏ÂºπÂêß, ÈÉΩÊòØÊØîËæÉÊàêÁÜüÁ®≥ÂÆöÁöÑÊäÄÊúØ‰∫Ü
- https://x.com/baotiao/status/1895395027129655691
- y1s1 ËøôÈáåÁöÑÈöæÁÇπÊòØÁ†çÈúÄÊ±ÇÔºå‰∏çÂÅö‰ªÄ‰πà

- ## DeepSeek ÂºÄÊ∫êÂë®ÁöÑ 5 Âè∑ÁÇ∏ÂºπÊù•Âï¶ÔºÅÂèàÊòØÈõÜÊùüÁÇ∏ÂºπÔºÅ3FS Âíå smallpondÔºÅ
- https://x.com/karminski3/status/1895280320989274560
  - Êàë‰∏çÊï¢Áõ∏‰ø°DeepSeekÁîöËá≥È¢†Ë¶Ü‰∫ÜÂ≠òÂÇ®Êû∂ÊûÑ...... Êàë‰∏äÊ¨°‰∏∫ÁΩëÁªúÊñá‰ª∂Á≥ªÁªüÈúáÊÉäËøòÊòØHDFSÂíåCEPH. ‰ΩÜËøô‰∫õÈÉΩÊòØÈù¢ÂêëÁ£ÅÁõòÁöÑÂàÜÂ∏ÉÂºèÊñá‰ª∂Á≥ªÁªü. Áé∞Âú®‰∏Ä‰∏™ÁúüÊ≠£ÊÑè‰πâ‰∏äÈù¢ÂêëÁé∞‰ª£SSDÂíåRDMAÁΩëÁªúÁöÑÊñá‰ª∂Á≥ªÁªüËØûÁîü‰∫ÜÔºÅ
  - È£ûÁÅ´ÊµÅÊòüÊñá‰ª∂Á≥ªÁªüÔºà3FSÔºâ- ‰∏ÄÁßçÂà©Áî®Áé∞‰ª£ SSD Âíå RDMA ÁΩëÁªúÂÖ®Â∏¶ÂÆΩÁöÑÂπ∂Ë°åÊñá‰ª∂Á≥ªÁªü
  - Ëøô‰∏™Êñá‰ª∂Á≥ªÁªüÂèØ‰ª•Âú® 180 ËäÇÁÇπÈõÜÁæ§‰∏≠ËææÂà∞6.6 TiB/s ÊÄªËØªÂèñÂêûÂêêÈáèÔºåÊØè‰∏™ÂÆ¢Êà∑Á´ØËäÇÁÇπ KVCache Êü•ÊâæÂ≥∞ÂÄºÂêûÂêêÈáè 40+ GiB„ÄÇ
  - Âè¶‰∏Ä‰∏™ smallpondÔºàÂ∞èÊ±†Â°òÔºâÊòØÂü∫‰∫é 3FS ÁöÑÊï∞ÊçÆÂ§ÑÁêÜÊ°ÜÊû∂ÔºÅ
  - Ëøô‰∏™Ê°ÜÊû∂Áî± DuckDB Êèê‰æõÁöÑÈ´òÊÄßËÉΩÊï∞ÊçÆÂ§ÑÁêÜÔºåÂèØÊâ©Â±ï‰ª•Â§ÑÁêÜ PB Á∫ßÊï∞ÊçÆÈõÜÔºÅ
  - ÊàëÁúã‰∫Ü‰∏ãÂ∫îËØ•ËøòÊòØKVÂ≠òÂÇ®ÁöÑÔºàÊØïÁ´üÈù¢ÂêëÊú∫Âô®Â≠¶‰π†ÔºâÔºåÂπ∂‰∏çÊòØÂùóÂ≠òÂÇ®„ÄÇÂõ†Ê≠§NAS‰Ω¨ËøòÊòØ‰∏çÂ§™ËÉΩÁî®Âæó‰∏äÁöÑ„ÄÇ  ‰∏ÄËá¥ÊÄßÂçèËÆÆÂü∫‰∫éCRAQÔºåÊØïÁ´üKVÂ≠òÂÇ®ÔºåÂü∫‰∫éÈìæÂºèÂ§çÂà∂ÁöÑÔºåÂÜôÊìç‰Ωú‰ªçÁÑ∂ÈúÄË¶ÅÈÄöËøáÊï¥‰∏™ÈìæÔºåÊâÄ‰ª•ÂÜôÂª∂ËøüÂ§ß„ÄÇ‰ΩÜ‰º∞ËÆ°ÂÖ∂ÂÆûÁªôËÆ≠ÁªÉÂΩíÊ°£Áî®ÔºåÂÜôÂª∂ËøüÂ§ßÊó†ÊâÄË∞ì„ÄÇÂºÇÊ≠•ÂΩíÊ°£ËÄåÂ∑≤„ÄÇ
- ÊòØÊñá‰ª∂Á≥ªÁªüÔºåÂÖÉÊï∞ÊçÆÊîæfoundationdbÈáåÔºåÊï∞ÊçÆÊîæxfsÈáå

- Âè¶Â§ñIntelÁöÑDAOSÂÖ∂ÂÆûÊàëÊÑüËßâÂÅöÁöÑÊõ¥ÊûÅËá¥ÔºåÁõ¥Êé•Áî®SPDKÊìç‰ΩúË£∏ÁõòÔºåËøòÂà©Áî®NVMe-oFÊäÄÊúØÂÖÖÂàÜÂèëÊå•RDMAÈõ∂Êã∑Ë¥ùÁöÑÁâπÊÄßÔºåÂè™ÂèØÊÉúÂÖÉÊï∞ÊçÆÁ≥ªÁªü‰æùËµñ‰∫éNVMÁ°¨‰ª∂ÔºåËÄåNVMÁ°¨‰ª∂Â∑≤ÁªèÂáâÂáâÔºåDAOS‰πüÂçñÁªôDell‰∫Ü

- cephÊúâobject storeÂëÄ‚Ä¶ Â§¥‰∏ÄÊ¨°ËßÅÂà∞ÁúüÁöÑÊúâ‰∫∫Áî®chain replicationÁöÑ‚Ä¶latency‰∏çË¶ÅÂï¶ÔºürdmaÁ°¨‰ª∂Ëøô‰πàË¥µÁõ¥Êé•Èù†Á°¨‰ª∂Á†∏ÂêóÔºü
- Êï∞ÊçÆ‰∏≠ÂøÉÔºåÂúüË±™Áé©Ê≥ïÔºåÈ¢†Ë¶Ü‰∏çËá≥‰∫éÂêß

- ## ÊàëÊâçÁü•ÈÅì Â∑•ÂÖ∑Ë∞ÉÁî®ÊòØ‰∏™ÂçïÁã¨ÁöÑ call ‰∏ÄÊó¶Ë∞ÉÁî®ÂÖ∂‰ªñ‰ªÄ‰πà‰∫ãÊÉÖÈÉΩÂπ≤‰∏ç‰∫Ü ÊÄé‰πàËøô‰∏™ llm Áé∞Âú®Ëøô‰πàËçâÂè∞Áè≠Â≠ê„ÄÇ„ÄÇ„ÄÇ
- https://x.com/Lakr233/status/1894950577416901018
  - ËøòÊúâ Agent ËøôÁßçÊ¶ÇÂøµÔºåÂè´ÂÖ®Ëá™Âä®ÁºñÊéíÊ£ÄÁ¥¢Â∑•ÂÖ∑‰ΩøÁî®‰∏çÂ•Ω‰πàÊêûÁöÑËøô‰πàÈ´òÂ§ß‰∏äÂà∞Â§¥Êù•ËøòÊòØ GPT 3.5 Â∞±ËÉΩÂπ≤ÁöÑÊ¥ª„ÄÇ„ÄÇ„ÄÇ

- tools Â∞±ÊòØ‰∏ÄËΩÆÊñ∞ÁöÑÂØπËØùÔºåÂëäËØâ AI ÂèØ‰ª•Ë∞ÉËøô‰∏™ÔºåÁÑ∂ÂêéÊääË∞ÉÁî®ÁªìÊûúÊãºÊé•Âà∞‰∏ä‰∏ÄËΩÆÂØπËØùÂêéÈù¢„ÄÇ
  - agent Â∞±ÊòØ pipeline/orchestration/scheduler„ÄÇ
  - embedding/RAG Â∞±ÊòØ search„ÄÇ
  - LLM framework Â∞±ÊòØ HTTP API calls„ÄÇ
- ‰Ω†ËøôÊ†∑ÔºåPPTËøòÊÄé‰πàÁºñÔºåÂõ¢ÈòüÈ¢ÑÁÆóÊÄé‰πàÊâπ

- ‰πüÊúâ‰∏Ä‰∫õÂºÇÊ≠•Ë∞ÉÁî® +event ÁöÑÁé©Ê≥ï, ‰ΩÜÊòØÊï¥‰∏™ Agent Á°ÆÂÆûÂ∞±ÊòØÂª∫Á´ãÂú®Êó†ÈôêÁöÑÊñáÊú¨ÁîüÊàê‰∏äÁöÑ
  - Agent = FunctionCall + LLM + LOOP

- Â∑•ÂÖ∑Ë∞ÉÁî®ÂÖ∂‰ªñ‰ªÄ‰πà‰∫ãÊÉÖÈÉΩÂπ≤‰∏ç‰∫ÜÊåáÁöÑÊòØÔºüÂèØ‰ª•Âπ∂ÂèëÂ§ÑÁêÜÂÖ∂‰ªñÈóÆÈ¢òÂëÄÔºü

- ## Êµ∑Â§ñÂ§ßÊ®°ÂûãÊé•ÂÖ•‰ΩøÁî®openrouter Â∑≤ÁªèÊòØÊúÄ‰Ω≥ÂÆûË∑µ‰∫ÜÂòõÔºü
- https://x.com/leeoxiang/status/1887714022327525797
  1„ÄÅÊîØÊåÅ‰∫ÜÂ∏ÇÈù¢‰∏äÂ§ßÈÉ®ÂàÜÊ®°ÂûãÔºõ
  2„ÄÅÂèØ‰ª•ËÆæÁΩÆÊ®°ÂûãÁöÑÊ∂àËÄóÈ¢ùÂ∫¶Ôºõ
  3„ÄÅËÉΩÊîØÊåÅÊ®°ÂûãÁöÑË¥üËΩΩÂùáË°°Ôºõ
  4„ÄÅÊîØÊåÅ‰∏ÄÂ•óapiÈÄÇÈÖçÂ§ßÂ§öÊï∞Ê®°ÂûãÔºõ
  5„ÄÅÊîØÊåÅÂ§ö‰∏™Ê®°ÂûãÁöÑÂ§±Ë¥•ÈáçËØïÔºõ

- Â§™Ë¥µ‰∫ÜÔºÅËÄå‰∏îÁâπÊÆäÊ®°ÂûãËøòÊòØË¶ÅÊ∑ªÂä†Ëá™Â∑±ÁöÑAPI Key

- ## We ( @jamesmurdza ) have been building Open Computer Use - 100% open source computer use agent.
- https://x.com/mlejva/status/1877054558481813799
  - The agent is using @e2b_dev 's Desktop Sandbox as virtual computer.
  - The agent is using 3 different LLMs: üî∏Llama 3.2 ( @AIatMeta ) üî∏Llama 3.3 üî∏OS-Atlas ( @Alibaba_Qwen )
  - It's slow and makes mistakes but this is a big milestone for OS AI community!

- ## ÊÉ≥Ë¶ÅÈÉ®ÁΩ≤Êú¨Âú∞Ê®°Âûã‰ΩÜÊòØ‰∏ç‰ºöËÆ°ÁÆó vRAM Âç†Áî® 
- https://x.com/tuturetom/status/1842492423848804686
  - https://huggingface.co/spaces/hf-accelerate/model-memory-usage

- ## ÁúãÊù•Â§ßÂÆ∂Áªà‰∫éËææÊàêÂÖ±ËØÜ‰∫ÜÔºölangchain ÊòØÁé©ÂÖ∑ÔºåÂ¶ÇÊûúÈùûË¶ÅÂú®Áîü‰∫ßÁéØÂ¢ÉÁî®ÂÆÉÔºåÈÇ£ÂÆÉÂ∞±‰ºöÂèòÊàêÂ∑•‰∏öÂûÉÂúæ„ÄÇ
- https://x.com/beihuo/status/1840058205768167699
  - ÁúãËøô‰ª£Á†ÅÊØîËæÉÔºålangchain Â∞±ÂÉè‰∏Ä‰∏™Ê≤°ÊúâÂ§™Â§öÂ∑•Á®ãÁªèÈ™åÔºå‰ΩÜÊòØÂèàÁúã‰∫ÜÂ§™Â§öËÆæËÆ°Ê®°ÂºèÊïôÁ®ãÁöÑ‰∫∫ÂÜôÂá∫Êù•ÁöÑ‰∏úË•ø„ÄÇ‰ΩøÁî®ÂÆÉÊù•ÂÆûÁé∞‰∏Ä‰∏™Áîü‰∫ßÁ≥ªÁªüÔºåÂ∞±ÊòØ‰∏Ä‰∏™ÁÅæÈöæ„ÄÇ
- ÂêåÊÑü longchainÊûÑÂª∫ÊÄùÁª¥Èìæ‰∏çÂ¶ÇÁõ¥Êé•ÊåâÁÖßÂ∑•‰ΩúÈÄªËæë‰∫∫Â∑•ÊûÑÂª∫ÊÄùÁª¥Èìæ„ÄÇÁúüÊ≠£ÁöÑËøûÊÄùÁª¥ÈìæÈÉΩ‰∏çÊ∏ÖÊ•öÁöÑÂàõÈÄ†ÂèëÊòéÔºåÁé∞Âú®Áî®AIÊù•ÂÅö‰∏∫Êó∂Â∞öÊó©„ÄÇ

- ## üíÑ ÁîüÊàêÂºèÁü•ËØÜ UI ÊúÄÊ†∏ÂøÉÁöÑÂü∫Á°ÄËÆæÊñΩÔºåÁõÆÂâçÂõ¥ÁªïÊ≠§Á±ªÂΩ¢ÊÄÅËÆæËÆ°ÁöÑ http://Me.bot ‰πüÊØîËæÉÂèóÊ¨¢Ëøé
- https://x.com/tuturetom/status/1835349759848333340

- ## Hugging Face ÂÆ£Â∏ÉÊäïÂÖ• 1000 ‰∏áÁæéÂÖÉÁî®‰∫éÂÖçË¥πÂÖ±‰∫´ GPUÔºåÊó®Âú®Â∏ÆÂä©Â∞èÂûãÂºÄÂèëËÄÖ„ÄÅÂ≠¶ÊúØÁïåÂíåÂàùÂàõÂÖ¨Âè∏ÂºÄÂèëÊñ∞ÁöÑ AI ÊäÄÊúØÔºåÊäóË°° AI ËøõÊ≠•ÁöÑÈõÜ‰∏≠Âåñ„ÄÇ
- https://x.com/glow1n/status/1791488036259434749
  - CEO Clem Delangue Ë°®Á§∫ÔºåËøô‰∏Ä‰∏æÊé™Â∞ÜÈÄöËøá ZeroGPU ËÆ°ÂàíÂÆûÁé∞Ôºå‰øÉËøõ AI ÊäÄÊúØÁöÑÂéª‰∏≠ÂøÉÂåñÂèëÂ±ï
  - ZeroGPU ‰ΩøÁî® Nvidia A100 GPU ËÆæÂ§áÔºåÊèê‰æõÈ´òÊïàÁöÑËÆ°ÁÆóËµÑÊ∫ê„ÄÇ
  - Hugging Face ÁöÑ Spaces Âπ≥Âè∞Â∑≤ÊúâË∂ÖËøá 30 ‰∏á‰∏™ AI ÊºîÁ§∫„ÄÇ

- ## Cloudflare ÁöÑ Workers AI ÊØèÂ§©ÂèØ‰ª•ÂÖçË¥π‰ΩøÁî® 10, 000 NeuronsÔºàÁõ∏ÂΩì‰∫éÁîüÊàê100-200‰∏™LLMÂìçÂ∫îÔºå500Ê¨°ÁøªËØëÔºå500ÁßíÁöÑËØ≠Èü≥ËΩ¨ÊñáÂ≠óÈü≥È¢ëÔºâ ÔºåË∞ÉÁî®ÊñπÂºèÂÖºÂÆπ OpenAI 
- https://x.com/scomper/status/1791804644332908646
- Â•ΩÂÉèÈÉΩÊòØÂ∞èÊ®°Âûã‰∏∫‰∏ªÂêß

# discuss-llm-architecture
- ## 

- ## 

- ## 

- ## 

- ## [Â¶Ç‰ΩïÁúãÂæÖËßÇÁÇπÔºöAI ÁöÑÂÖ≥ÈîÆÁÇπ‰∏çÊòØpromptÔºåËÄåÊòØContext EngineeringÔºü - Áü•‰πé](https://www.zhihu.com/question/1923364519964545063)
- ContextËøú‰∏çÊ≠¢ÊòØ‰∏ÄÂè•promptÔºåÂÆÉÂåÖÊã¨Ôºö
  - Êåá‰ª§/Á≥ªÁªüÊèêÁ§∫ÔºöÂÆö‰πâÊ®°ÂûãË°å‰∏∫ÁöÑÂàùÂßãÊåá‰ª§
  - Áî®Êà∑ÊèêÁ§∫ÔºöÊù•Ëá™Áî®Êà∑ÁöÑÂç≥Êó∂‰ªªÂä°ÊàñÈóÆÈ¢ò
  - Áä∂ÊÄÅ/ÂéÜÂè≤ÔºöÂΩìÂâçÂØπËØùÁöÑÁü≠ÊúüËÆ∞ÂøÜ
  - ÈïøÊúüËÆ∞ÂøÜÔºöË∑®Â§öÊ¨°ÂØπËØùÊî∂ÈõÜÁöÑÊåÅ‰πÖ
  - Áü•ËØÜÂ∫ìÊ£ÄÁ¥¢‰ø°ÊÅØ(RAG)ÔºöÊù•Ëá™ÊñáÊ°£„ÄÅÊï∞ÊçÆÂ∫ìÊàñAPIÁöÑÂ§ñÈÉ®Áü•ËØÜ
  - ÂèØÁî®Â∑•ÂÖ∑ÔºöÊ®°ÂûãÂèØ‰ª•Ë∞ÉÁî®ÁöÑÊâÄÊúâÂäüËÉΩÂÆö‰πâ
  - ÁªìÊûÑÂåñËæìÂá∫ÔºöÂØπÊ®°ÂûãÂìçÂ∫îÊ†ºÂºèÁöÑÂÆö‰πâ
- ËøôÂÖ∂ÂÆûÂ∞±ÊòØÂú®ÊûÑÂª∫‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ„ÄåAIÂ∑•‰ΩúÁéØÂ¢É„ÄçÔºåËÄå‰∏çÊòØÁÆÄÂçïÂú∞ÁªôAI‰∏ãÊåá‰ª§„ÄÇ

- Âç≥‰Ωø‰Ω†ÂÜô‰∏çÂ•ΩpromptÔºåÂ∏ÇÈù¢‰∏ä‰πüÊúâÂæàÂ§öAIÂ∑•ÂÖ∑ÂèØ‰ª•Â∏Æ‰Ω†ÂÅöprompt enhancement
  - ÊúüÂæÖAIËÉΩ„ÄåÊúâËÆ∞ÂøÜÔºåÊáÇÁî®Êà∑„Äç„ÄÇÂõ†Ê≠§ÔºåContext EngineeringÔºà‰∏ä‰∏ãÊñáÂ∑•Á®ãÔºâÂ∞±ÂºÄÂßãË¢´ÂÖ≥Ê≥®Âà∞„ÄÇ
  - context engineering, ÊòØÊåáÈÄöËøáÁ≥ªÁªüÊÄßÊûÑÂª∫„ÄÅÁÆ°ÁêÜÂíå‰ºòÂåñ AI Ê®°ÂûãÁöÑËæìÂÖ•‰∏ä‰∏ãÊñáÔºå‰ª•ÊèêÂçáÊ®°ÂûãÂú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑÁêÜËß£‰∏éËæìÂá∫ËÉΩÂäõ„ÄÇ
  - Á¨¨‰∏ÄÊòØÂéÜÂè≤‰∫§‰∫íÊï∞ÊçÆ„ÄÇ
  - Á¨¨‰∫åÔºåÊòØcontextÁöÑÊÄªÁªì„ÄÇÂØπËØùÂèØ‰ª•‰∏ÄÁõ¥Âª∂Áª≠Ôºå‰ΩÜÊ®°ÂûãÁöÑ‰∏ä‰∏ãÊñáÁ™óÂè£ÊúâÈôêÔºåÊÄé‰πàËÆ∞‰Ωè‰πãÂâçÁöÑÂØπËØùÔºüÂΩìÁÑ∂ÊòØÂéãÁº©Ôºå‰æãÂ¶ÇÊääÊúÄËøë5Êù°ÂØπËØùÁöÑÂÜÖÂÆπÂéüÂ∞Å‰øùÁïôÔºåÂÜç‰πãÂâçÁöÑÂÜÖÂÆπÊÄªÁªì‰∏Ä‰∏ã„ÄÇ
  - ‰ΩÜÊõ¥ÈáçË¶ÅÁöÑÔºåÊòØÈ¢ÜÂüüÁü•ËØÜ‰∏éËÉåÊôØ‰ø°ÊÅØ„ÄÇ
- AIÁöÑËÉΩÂäõÊù•Ê∫ê‰∫é‰∏§‰∏™ÊñπÈù¢ in-weights memoryÔºàËÆ≠ÁªÉÈõÜÈáåÂ≠¶Âà∞ÁöÑÔºâ Âíå in-context memoryÔºàÂèÇËÄÉËµÑÊñôÈáåÂ≠¶Âà∞ÁöÑÔºâ„ÄÇ in-weights memoryÂü∫Êú¨‰∏äÊòØÂæàÈöæ‰øÆÊîπÁöÑ, ‰ΩÜin-context memory Êõ¥ÂÆπÊòì‰øÆÊîπÂíåÊõ¥Êñ∞Ôºå‰Ω†Âè™Ë¶ÅÊèê‰æõÊñ∞ÁöÑËµÑÊñôÔºåÊ≠£Á°ÆÁöÑËµÑÊñôÔºåÊ®°ÂûãÂ∞±ËÉΩÊúâ„ÄåÊñ∞Áü•„Äç

- Â§çÊùÇÁöÑAIÂ∫îÁî®Ôºå‰∏çÊòØÈù†chatËæìÂÖ•ÊèêÁ§∫ËØçËøô‰πàÁÆÄÂçïÔºåËÄåÊòØÈúÄË¶ÅËá™Âä®ÂåñÂ§öÊ¨°ÂíåÂ§ßÊ®°ÂûãÁöÑ‰∫§‰∫íÔºåËøô‰∏™Ëá™Âä®ÁöÑËøáÁ®ã‰∏≠ÔºåÈúÄË¶ÅËÉΩÂ§üËá™Âä®‰∫ßÁîüÁªôÂ§ßÊ®°ÂûãÁöÑÊèêÁ§∫ËØç„ÄÇ

- Agent = LLM + Prompt + Â∑•ÂÖ∑Ë∞ÉÁî®Ôºå‰ΩÜ‰ªéÂ∑•Á®ãËßÜËßíÊù•ÁúãÔºåËøô‰∫õÊú¨Ë¥®ÈÉΩÊòØContext EngineeringÔºå‰πüÂ∞±ÊòØ‰∏ä‰∏ãÊñáÂ∑•Á®ã„ÄÇ
  - Context EngineeringÂ∞±ÊòØÂú®ÊØèÊ¨°Ë∞ÉÁî®ÈáåÔºåÊääÊ®°ÂûãÂÆåÊàê‰ªªÂä°ÊâÄÂøÖÈúÄÁöÑ‰ø°ÊÅØÊåâÂØπÁöÑÊ†ºÂºè„ÄÅÂú®ÂØπÁöÑÊó∂Êú∫ÂáÜÁ°ÆÊâìÂåÖËøõÂéª„ÄÇ

- ‰∏ä‰∏ãÊñáÂ∑•Á®ãÔºåAIÁªòÂõæÁî®Êà∑Êó©Â∞±Â§©Â§©Âú®Áî®‰∫Ü
  - ÂΩì‰Ω†‰ΩøÁî® InpaintÔºàÂ±ÄÈÉ®ÈáçÁªòÔºâ ÂäüËÉΩÊó∂ÔºåAI‰∏çÊòØÂá≠Á©∫ÊÉ≥Ë±°ÔºåËÄåÊòØÂü∫‰∫é‰Ω†Áïô‰∏ãÁöÑÁîªÂ∏É„ÄÅËæπÁºòÁ∫øÊù°„ÄÅÂÖâÁ∫øÊñπÂêë„ÄÅÂ∑≤ÊúâÁîªÈ£éÊù•Ë°•ÂÖ®Âå∫ÂüüÔºåËøôÂ∞±ÊòØ‚ÄúÂõæÂÉè‰∏ä‰∏ãÊñá‚Äù
  - ÂΩì‰Ω†ËøõË°å OutpaintÔºàÂõæÂÉèÊâ©Â±ïÔºâ Êó∂ÔºåAIÂøÖÈ°ªÂèÇËÄÉÂéüÂõæÁöÑÊûÑÂõæÈÄªËæë„ÄÅËâ≤ÂΩ©Ê∏êÂèò„ÄÅÈ£éÊ†ºÁ∫πÁêÜ„ÄÅ‰∫∫Áâ©ÈÄèËßÜÊù•ÁîüÊàêËá™ÁÑ∂Ë°îÊé•ÁöÑÈÉ®ÂàÜ„ÄÇËøôÁßçÂØπ‰∏ä‰∏ãÊñáÁöÑ‚ÄúÁêÜËß£‚ÄùÂíå‚ÄúÂª∫Ê®°‚ÄùÔºåÊØîÊèêÁ§∫ËØçÊú¨Ë∫´Êõ¥ÈáçË¶Å
  - ÂΩì‰Ω†Áî® PhotoshopÂÜÖÁΩÆÁöÑFireflyËøõË°åÂ±ÄÈÉ®‰øÆÊîπÂàõÊàêÂºèÂ°´ÂÖÖÊó∂ÔºåÁúüÊ≠£Ëµ∑ÂÜ≥ÂÆö‰ΩúÁî®ÁöÑ‰∏çÊòØ‰Ω†ËØ¥‰∫Ü‚ÄúÁªôÊàëÂä†‰∏Ä‰∏™ÁÅØÂ°î‚ÄùÔºåËÄåÊòØAIÊòØÂê¶ÂáÜÁ°ÆËØªÂèñ‰∫ÜÂΩìÂâçÁîªÈù¢ÊòØÂ§úÊôö„ÄÅÊòØÊ∞¥Ëæπ„ÄÅÊúâÂÖâÊ∫êÊäïÂ∞Ñ„ÄÅÊúâÈÄèËßÜÊ∂àÂ§±ÁÇπ„ÄÇ
  - ‰∏äÈù¢Ëøô‰∫õÂäüËÉΩÔºåÂì™ÊÄï‰Ω†Ê†πÊú¨‰∏çÂÜôÊèêÁ§∫ËØçÔºåÂ•ΩÁöÑÂ∑•ÂÖ∑‰πüËÉΩÁªô‰Ω†ËÑëË°•Â•ΩÔºåÊØîÂ¶ÇÊàëÁé∞Âú®Ê†πÊú¨Êîæ‰∏çÂºÄËÆ¢ÈòÖÁöÑPhotoshop AI„ÄÇ

- 
- 
- 
- 
- 

- ## DeepSeekÊúÄÂ§ßÁöÑÂàõÊñ∞ÔºåÊòØ‰∏çÈúÄË¶ÅÂ§ßÈáèÁöÑ‰∫∫Â∑•Ê†áÊ≥®ÔºåËÄåÊòØÁõ¥Êé•‰ªéÂÖ∂‰ªñÂ§ßÊ®°ÂûãËí∏È¶èÊàñËÄÖ‰ΩøÁî®Áæ§‰ΩìÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÁÆóÊ≥ïÔºàGRPOÔºâ„ÄÅCoTÔºàËá™ÊàëÂèçÊÄùÔºâÊù•ÁªôÂ§ßÊ®°ÂûãÂèçÈ¶àÔºå
- https://x.com/seclink/status/1888011462008005030
  - Â∞±Áõ∏ÂΩì‰∫éÂÆåÂÖ®‰ΩøÁî®RLÔºàÊàñËÄÖÂè¶‰∏Ä‰∏™Âü∫Á°ÄÂ§ßÊ®°ÂûãÔºâÊù•Êõø‰ª£‰∫∫Â∑•Ê†áÊ≥®‰∫Ü„ÄÇ
  - ËøôÂÆûÈôÖ‰∏äÊòØÊä¢‰∫ÜScale AI ËøôÁßçÂÖ¨Âè∏ÁöÑËõãÁ≥ïÔºåDeepSeekÁâõX‰πãÂ§ÑÂú®‰∫éÔºåÂæàÂ§öËÄÅÂ§ñ‰∏ÄÂºÄÂßã‰∏ç‰ø°ÔºåÁÑ∂ÂêéÁÖßÁùÄËÆ∫ÊñáÈáåÁöÑÊñπÊ≥ïÂø´ÈÄüÔºàÂ±Ä‰øÉÂú∞ÔºâÂ§çÁé∞ÔºåÂç¥ÂèëÁé∞Á´üÁÑ∂‰πüËÉΩÂ§çÁé∞ÊàêÂäü„ÄÇ

- cotÂíåËí∏È¶è‰πãÂâçÔºåÂ∞±ÈÄöËøágrpoËøõË°årlËé∑Âæó‰∫ÜÁõ∏ÂΩì‰∏çÈîôÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ„ÄÇ‰Ω†ËøôÊï¥ÁêÜÁöÑ‰∏çÊ∏ÖÊô∞„ÄÇ

- ËøôÊ≤°Êúâ‰ªª‰ΩïÂàõÊñ∞ÔºåGPT1Â∞±Áî®‰∫ÜÂêåÊ†∑ÁöÑÊñπÊ≥ïÔºåËÄå‰∏îÁé∞Âú®‰∏ç‰ΩøÁî®ÊòØÊúâÁêÜÁî±ÁöÑ„ÄÇÂõ†‰∏∫deepseekÊ®°ÂûãÊú¨Ë∫´‰∏çË°åÔºå‰∏Ä‰∏™queryÈúÄË¶ÅRL chainÊâçËÉΩËææÂà∞ÂèØÊé•ÂèóÁöÑÁ≠îÊ°àÔºåËá¥‰Ωøinference ÊïàÁéá‰ΩéÂà∞ÂèØÊÄïÔºåËôΩÁÑ∂training‰æøÂÆúÔºå‰ΩÜÊòØoperation costË¶ÅÂ§öÂ•ΩÂá†ÂÄçÔºåÂæó‰∏çÂÅøÂ§±

- ‰Ω†Ëøô‰∏™ËØ¥ÁöÑÂÆåÂÖ®‰∏çÂØπÔºåCoTÊòØGPTÂèëÊòéÁöÑÔºåËí∏È¶è‰πüÊó©Â∞±Êúâ‰∫ÜÔºåRL‰πüÊó©Â∞±Êúâ‰∫ÜÔºådeepseekÊòØÂèëÊòé‰∫ÜRLÈáåÁöÑGRPO

- ## I read up on DeepSeek‚Äôs learning algo, GRPO. GRPO: group relative policy optimization
- https://x.com/virattt/status/1885102056546910672
- How GRPO works:
  1 ‚Ä¢ model generates a group of answers
  2 ‚Ä¢ compute score for each answer
  3 ‚Ä¢ compute avg score for entire group
  4 ‚Ä¢ compare each answer score to avg score
  5 ‚Ä¢ reinforce model to favor higher scores
  - This process is repeated, allowing the model to learn and improve over time.
- Other methods like PPO, use a value function model to do reinforcement learning.
  - GRPO does not, which reduces memory and computational overhead when training.

- GRPO was proposed for the first time in Feb 2024 in DeepSeekMath paper. It is not new.
  - Back then they used Neural Networks for Rewards (PRM/ORM). The magic happened when DeepSeek  replaced PRM/ORM with the exact reward (Verified Reward).

- ## üî° OpenAI's Deep Research is just a search+read+reasoning in a while-loop, right? here is my replicate of it in nodejs, using gemini-flash and jina reader
- https://x.com/hxiao/status/1886250705415229627
- If it includes evaluating js driven websites, including images Then yes 
  - To really replicate that, you'd probably wanna just use puppeteer, save the whole page as an image, extract the info from that, and then crunch that data

# discuss-multi-agents üèòÔ∏è
- ## 

- ## 

- ## 

- ## üÜö ‰ªäÂ§©ÂæàÊúâË∂£Ôºå‰∏§ÂÆ∂Áü•ÂêçÁöÑÂÖ¨Âè∏ÂêÑÂá∫‰∫Ü‰∏ÄÁØáÊñáÁ´†Ôºå‰∫âËÆ∫Ë¶Å‰∏çË¶Å‰ΩøÁî®Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü„ÄÇ
- https://x.com/oran_ge/status/1933754019010539923
  - Claude ÁöÑÂÆòÊñπ Anthropic ÔºöÂ¶Ç‰ΩïÊûÑÂª∫Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü
  - Devin ÁöÑÂÆòÊñπ Cognition Ôºö‰∏çË¶ÅÊûÑÂª∫Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü
- ËøôÊ†∏ÂøÉÁöÑ‰∫âËÆÆÁÇπÂú®‰∫éÔºöContext ‰∏ä‰∏ãÊñáÂà∞Â∫ïÂ∫îËØ•ÂÖ±‰∫´ËøòÊòØÂàÜÂºÄÔºü
  - Claude ËøôËæπÁöÑËßÇÁÇπÊòØÔºåÊêúÁ¥¢‰ø°ÊÅØÁöÑÊú¨Ë¥®ÊòØÂéãÁº©ÔºåÂçï‰∏™Êô∫ËÉΩ‰ΩìÁöÑ‰∏ä‰∏ãÊñáÊúâÈôêÔºåÈù¢ÂØπÊó†ÈôêÁöÑ‰ø°ÊÅØÔºåÂéãÁº©ÊØîÂ§™Â§ßÂ∞±‰ºöÂ§±Áúü„ÄÇËøôÊòØÈõÜ‰ΩìÊô∫ÊÖßÔºå‰∏ÄËµ∑Âçè‰ΩúËé∑ÂæóÁöÑËÉúÂà©„ÄÇ
  - Devin ËøôËæπÁöÑËßÇÁÇπÊòØÔºåÂ§ö‰∏™Êô∫ËÉΩ‰ΩìÁöÑ‰∏ä‰∏ãÊñá‰∏ç‰∏ÄËá¥Ôºå‰ºöÂØºËá¥‰ø°ÊÅØÂâ≤Ë£Ç„ÄÅËØØËß£„ÄÅ‰ªñ‰ª¨Ê±áÊä•ÁªôËÄÅÊùøÁöÑ‰ø°ÊÅØÁªèÂ∏∏ÂÖÖÊª°‰∫ÜÁüõÁõæ„ÄÇ
  - ËøôËÆ©ÊàëÊÉ≥Âà∞ÔºåËΩØ‰ª∂Â∑•Á®ã‰ªéÊù•‰∏çÊòØËøΩÊ±ÇÂÆåÁæéÔºåËÄåÊòØÊåÅÁª≠Ëø≠‰ª£„ÄÇ

- Ê≤°Âï•ÁüõÁõæÁöÑ„ÄÇÁúãÈúÄÊ±Ç„ÄÇopen-ended ÁöÑÈóÆÈ¢òÊØîËæÉÈÄÇÂêà multi-agentÔºõÁõÆÊ†áÂæàÂÖ∑‰ΩìÁöÑËØùÔºåÂ∞±ÊØîËæÉÈÄÇÂêàÂçï‰∏™ agent

- ÊÑüËßâÂíåÁé∞ÂÆû‰∏≠ÁöÑ‰∏§ÂÆ∂‰∏çÂêåÁ≠ñÁï•ËøêËê•ÁöÑÂÖ¨Âè∏Â∑Æ‰∏çÂ§öÔºåÊ≤°ÊúâÁªùÂØπÁöÑÂØπÂíåÈîôÔºåÈÉΩËÉΩËµ∞Âá∫Êù•ÁöÑÂèØËÉΩÊÄß‰πüË∂ÖÂ§ß„ÄÇ

- ## @KuraAIAgents ÈÄöËøáÂàõÊñ∞ÁöÑ‰∫îÈáç Agent Êû∂ÊûÑÔºàËßÑÂàí„ÄÅÊâßË°å„ÄÅËØÑ‰º∞ÔºâÂÆûÁé∞‰∫Ü 87% ÁöÑÊµèËßàÂô®Ëá™Âä®ÂåñÂáÜÁ°ÆÁéáÔºåË∂ÖË∂ä Claude ËÆ°ÁÆóÊú∫Êìç‰Ωú 28 ‰∏™ÁôæÂàÜÁÇπÔºåÂêåÊó∂ÊîØÊåÅ‰ΩéÊàêÊú¨Ê®°ÂûãÊõøÊç¢ÊñπÊ°à
- https://x.com/shao__meng/status/1857586562588094918
  - ÂåÖÂê´5‰∏™‰∏ìÈó®ÁöÑ AgentÔºåÂÖ∂‰∏≠3‰∏™Ê†∏ÂøÉ Agent ÂΩ¢Êàê‰∏Ä‰∏™Âæ™ÁéØÁ≥ªÁªü
  - Âú® WebVoyager Âü∫ÂáÜÊµãËØï‰∏≠ÂèñÂæó 87% ÁöÑÊàêÁª©
  - ÊØî Claude ÁöÑËÆ°ÁÆóÊú∫Êìç‰ΩúÈ´òÂá∫ 28%
- ‰∫î‰∏™Ê†∏ÂøÉ AgentÔºö
a) ÂàùÂßãËßÑÂàíËÄÖ(Initial Planner)
- Ë¥üË¥£Âà∂ÂÆöÈ´òÂ±ÇÊ¨°ËÆ°Âàí
- ‰ΩøÁî® OpenAI o1 Ê®°ÂûãËøõË°åÊé®ÁêÜ
b) Âæ™ÁéØËßÑÂàíËÄÖ(Agent Loop Planner)
- ËØÑ‰º∞‰ªªÂä°ÊòØÂê¶ÂÆåÊàêÊàñ‰∏çÂèØËÉΩÂÆåÊàê
- ‰∏∫ÊâßË°åËÄÖÊèê‰æõ‰∏ã‰∏ÄÊ≠•Êåá‰ª§
- Ê†πÊçÆÈúÄË¶Å‰øÆÊîπËÆ°Âàí
c) ÊâßË°åËÄÖ(Executor)ÂÖ∑Â§á‰∏âÈ°πÊ†∏ÂøÉÊäÄËÉΩÔºö
- ÁΩëÂùÄÂØºËà™ÂíåËøîÂõû
- ËØªÂèñÂΩìÂâçÈ°µÈù¢Êï∞ÊçÆ
- ÊâßË°åÂ±èÂπïÊìç‰Ωú(ÁÇπÂáª„ÄÅÊªöÂä®„ÄÅËæìÂÖ•)
d) Âæ™ÁéØËØÑËÆ∫ËÄÖ(Agent Loop Critic)
- ËØÑ‰º∞ÊâßË°åËÄÖÁöÑË°®Áé∞
- ÁâπÂà´Âú®Â§çÊùÇÁïåÈù¢Êìç‰Ωú‰∏≠Ëµ∑ÂÖ≥ÈîÆ‰ΩúÁî®
e) ÊúÄÁªàËØÑËÆ∫ËÄÖ(Final Critic)
- ËØÑ‰º∞Êï¥‰∏™‰ªªÂä°ËΩ®Ëøπ
- ÂøÖË¶ÅÊó∂Êèê‰æõÂèçÈ¶àÂπ∂ÂêØÂä®Êñ∞ÁöÑÂæ™ÁéØ

- ## OpenAI ÂèëÂ∏ÉÂ§ö Agent ÁºñÊéíÊ°ÜÊû∂ËÉåÂêéÁöÑÊÄùËÄÉ‰ª•ÂèäÂÆûË∑µËøáÁ®ã
- https://x.com/tuturetom/status/1845634978530693494
  - Ê†∏ÂøÉÊòØ OpenAI ÁöÑÂ∑•Á®ãÂ∏àÂú®ÊÄùËÄÉ  Agent ÁöÑ„ÄåË∑ØÁî±„Äç + „ÄåÁßª‰∫§„ÄçÁ≠âËÉΩÂäõÊó∂ÊãìÂ±ïÂá∫Êù•ÁöÑ‰∏Ä‰∏™Á§∫‰æãÔºåËøõËÄåÂèëÁé∞Ëøô‰∏™Á§∫‰æãÂéüËØ≠ÂæàÊôÆÈÄÇÔºåÊâÄ‰ª•ÂºÄÂèë‰∫Ü Swarm Ê°ÜÊû∂
  - ÊâÄÊúâ‰ºüÂ§ßÁöÑÊÄùËÄÉÈÉΩÊ∫ê‰∫éÂë®Êú´‰∏ö‰ΩôÂ∑•‰Ωú

- ## OpenAI ÊÇÑÊÇÑÂºÄÊ∫ê‰∫ÜÊûÑÂª∫Â§ö‰ª£ÁêÜÊô∫ËÉΩ‰ΩìÂçèÂêåÊ°ÜÊû∂ÔºöSwarm
- https://x.com/aigclink/status/1844936446416912628
  - Áî®‰∫éÊûÑÂª∫„ÄÅÁºñÊéíÂíåÈÉ®ÁΩ≤Â§ö‰ª£ÁêÜ

# discuss-news-ai-model
- ## 

- ## 

- ## üß©üöÄ [Â≠óËäÇË∑≥Âä®ÂàöÂàöÂèëÂ∏É‰∫Ü‰ªñ‰ª¨ÁöÑÊñáÊú¨ Diffusion Ê®°ÂûãÔºÅ‚Äî‚Äî Seed Diffusion Preview _20250801](https://x.com/karminski3/status/1950995740408553826)
  - > ‚ÄúThe model currently supports coding tasks only; more general capabilities are coming soon.‚Äù
  - Áªô‰∏çÂ§™‰∫ÜËß£ÊñáÊú¨ Diffusion Ê®°ÂûãÁöÑÂêåÂ≠¶ÔºåÂ§ßÂÆ∂ÈÉΩÁü•ÈÅìÁé∞Âú® transformer Â§ßÊ®°ÂûãÊòØ‰∏Ä‰∏™Â≠ó‰∏Ä‰∏™Â≠óËπ¶Âá∫Êù•ÁöÑÔºåËÄåÊñáÊú¨Diffusion Ê®°ÂûãÂàôÊòØË∑üÂõæÂÉèDiffusion Ê®°ÂûãÂ∑Æ‰∏çÂ§öÔºåÊòØ‰∏Ä‰∏™ÂéªÂô™ËøáÁ®ãÔºåÊï¥ÊÆµËØùÈöèÊú∫Âá∫Áé∞ÊñáÊú¨ÊúÄÂêéÁªÑÊàêÊâÄÊúâËæìÂá∫„ÄÇ
  - Diffusion ÊñáÊú¨Ê®°ÂûãÁöÑ‰ºòÁÇπÊòØÂ∑®Âø´ÔºåÂ≠óËäÇËøô‰∏™Êúâ ÊØèÁßí 2146 ‰∏™ token ÁöÑÈÄüÂ∫¶ÔºàÂ∫îËØ•ÊòØÁé∞Âú®ÊúÄÂø´ÔºüÔºâ„ÄÇÊàëËÆ©ÂÆÉÁî® Rust ÂÜôÂÜíÊ≥°ÊéíÂ∫èÔºåÂá†‰πéÊòØÁßíÂá∫„ÄÇ
  - ÂΩìÁÑ∂ÁõÆÂâç Diffusion ÊñáÊú¨Ê®°ÂûãÊúÄÂ§ßÁöÑÈóÆÈ¢òËøòÊòØÊô∫ËÉΩÂ§™‰Ωé‰∫ÜÔºåÂæàÈöæÂπ≤Ê¥ª„ÄÇ
  - ÁõÆÂâçÈô§‰∫Ü Seed Diffusion Preview‰ª•Â§ñÔºåËøòÊúâÊúÄÁü•ÂêçÁöÑ Mercury Coder Âíå Google ÁöÑ Gemini Diffusion.

- ÁõÆÂâçÂ∑≤ÊúâÁöÑÊñáÊú¨ diffusion Ê®°ÂûãÁöÑÊô∫ËÉΩË°®Áé∞ÊòéÊòæÈÉΩ‰∏çÂèäÂ∏∏ËßÑ‰∏ªÊµÅÁöÑÊñáÊú¨ AI Â§ßÊ®°Âûã.
  - Áé∞Âú®ËøôÁ±ªÂïÜÁî®‰∫ÜÁöÑÊâ©Êï£Ê®°ÂûãÂè™ËææÂà∞GPT-3.5ÁöÑÊ∞¥Âπ≥ÔºåÈÉ®ÂàÜËææÂà∞GPT-4
- Âá†‰πéÊ≤°Êúâ‰∏ä‰∏ãÊñáÔºå‰πãÂâç‰ΩìÈ™åÁöÑgemini-diffusionÂá†‰πéÊó†Ê≥ïËÆ∞‰Ωè‰∏ä‰∏ãÊñáÂÜÖÂÆπ„ÄÇÂì™ÊÄïÊòØÂçïÊ¨°ËæìÂá∫ÂÜÖÂÆπËøáÂ§ö‰πü‰ºöÂºÄÂßã‰∏ÄÁõ¥Â§çËØª

- Diffusion + auto regressive ÊòØÊñπÂêë„ÄÇ

- ## [Â¶Ç‰ΩïËØÑ‰ª∑ DeepSeek ‰∫é 2025 Âπ¥ 8 Êúà 19 Êó•Êõ¥Êñ∞ÁöÑ V3.1 ÁâàÊú¨Ôºü - Áü•‰πé _202508](https://www.zhihu.com/question/1941218073152587548)
- ËôΩÁÑ∂ÊàëÁúãÂà∞‰∏çÂ∞ë‰∫∫Êä±ÊÄ®ÂàõÊÑèÂÜô‰ΩúËÉΩÂäõÂèòÂ∑Æ‰∫ÜÔºå‰ΩÜÂÆûÈôÖ‰∏äÂ¶ÇÊûú‰Ω†ÁúüÁöÑÊòØÊ∑±Â∫¶ai rpÁé©ÂÆ∂ÁöÑËØùÔºå‰Ω†‰ºöÂèëÁé∞Ôºåv3.1Áî±‰∫éÂÖ∂‰ºòÁßÄÁöÑÊåá‰ª§ÈÅµ‰ªéÔºåÊîπÂñÑ‰∫ÜÁöÑ‰∏ä‰∏ãÊñáÂíåÈÄªËæëËÉΩÂäõÔºå‰ºòÁßÄÁöÑ‰∏≠ÊñáËØ≠ÊñôÔºåÂÆûÈôÖ‰∏äÂ∞±ÊòØÁé∞Èò∂ÊÆµÂèØ‰ª•‰ΩøÁî®ÁöÑÊúÄ‰ºòÁßÄÁöÑrpÊ®°Âûã„ÄÇ2.5proÊÆãË°ÄÂ§ñÂä†ÁñØÁãÇÊà™Êñ≠ÔºåclaudeÁñØÁãÇÂà†ËØ≠ÊñôÔºågptÂíågrokÊõ¥ÊòØË∑ØËæπ‰∏ÄÊù°„ÄÇ
- ÊàëËøòÂú®ÊÉ≥rpÊ®°ÂûãÊòØ‰ªÄ‰πàÊäÄÊúØÁ±ªÂà´ÁöÑÊ®°Âûã, role playÔºü
  - ÊòØÁöÑÔºåds‰πüÊòØÂ∞ëÊï∞Êäärp‰∏ìÈó®ÊãâÂá∫Êù•Êèê‰∏ÄÂò¥ÁöÑÊ®°Âûã

- Âê¨ËØ¥‰πãÂâçÁªèÂ∏∏ÂèëÁô´
  - ‰Ω†ËØ¥ÁöÑÊòØÊò•ËäÇÁâàÊú¨ÁöÑr1ÔºåÈÇ£ÁöÑÁ°ÆÂ¶ÇÊ≠§
  - ËÄÅr1ÊòØËøôÊ†∑ÁöÑÔºåÂΩìÊó∂ÊúÄÂ•ΩÁöÑÂÖ∂ÂÆûÊòØclaude

- ‰ªéËøô‰∏™ËäÇÁÇπÊù•ÁúãÔºå[DeepSeek]  V3.1ÂØπÊ†áÂØπË±°Â∫îËØ•ÊòØQwen CoderËøôÁ±ªÊ®°ÂûãÔºå Coding+Agent ÔºåËøôÂùóËÇâÊúÄÂéöÔºåËøõÊ≠•‰πüÊòØÊúÄÊòéÊòæÁöÑÔºåÔºàÂèØËÉΩÂæàÂ§ö‰∫∫Ê≤°ÊÑüËßâÂà∞„ÄÇÔºâÊÑüËßâÊó†ËÆ∫ÊÑèÂõæÊçïÊçâÔºåËßÜËßâÂëàÁé∞ÔºåÂ∞èÂàõÊÑèÁöÑÂÆâÊéíÔºåËøêË°å‰πüÊõ¥È°∫ÁïÖÈÉΩÊúâ‰∫ÜÂæàÂ§ßËøõÊ≠•„ÄÇ

- ‰∫∫È∫ª‰∫Ü, Êõ¥Êñ∞ÂâçËÉΩ‰∏çËÉΩÂú®ÂÆòÁΩëÈ¢ÑÂëä‰∏Ä‰∏ã. ÊãøAPIÂÅöÂÆûÈ™åÁªìÊûúËØÑ‰º∞, Êò®Â§©ËøòÂ•ΩÂ•ΩÁöÑ‰ªäÂ§©‰∏ÄËµ∑Êù•ÂèëÁé∞ÁªìÊûúÁàÜÁÇ∏. Ë∞É‰∫ÜÂá†‰∏™Â∞èÊó∂‰ª£Á†ÅÂêéÂèëÁé∞Â±ÖÁÑ∂ÊòØ API ÂÅ∑ÂÅ∑Êõ¥Êñ∞Êàê 3.1 ‰∫Ü
  - deepseekÂÆòÁΩë‰∏çÊòØ‰∏ÄÁõ¥ÈÉΩÊòØËøôÊ†∑Á™ÅÁÑ∂Â∞±Êç¢ÁöÑÂêó ‰Ω†ÊòØÁ¨¨‰∏ÄÂ§©ËÆ§ËØÜÂÆÉÔºü

- V3.1 Âíå R1-0528 ÊúÄÂ§ßÁöÑÂÖ±ÂêåÁÇπÔºåÂ∞±ÊòØÂΩªÂ∫ïÊäõÂºÉ‰∫Ü DeepSeek Êó©ÊúüÈÇ£Áßç‚ÄúË∑≥Ë∑ÉÂºèÂπªËßâ‚ÄùÊñáÈ£éÔºåËΩ¨ËÄåÂ§ßÈáèÂÄüÈâ¥‰∫Ü Gemini ÁöÑËæìÂá∫ÁªìÊûÑ„ÄÅÊñáÈ£éÂíåËØ≠ÊñôÔºåÂÆûÁî®ÊÄßÁõ¥Êé•Ëµ∑È£û
  - ‰ΩÜ DeepSeek ËÇØÂÆö‰∏çÁîòÂøÉÂè™ÂÅö Gemini ÁöÑ‚ÄúÈ´ò‰ªø‚Äù„ÄÇÁõ∏ÊØî‰∫é R1-0528 ÂØπ Gemini ÊñáÈ£éÁöÑÁõ¥Êé•Â§çÂàªÔºåV3.1 Âàô‰ºöÂÖãÂà∂‰∏Ä‰∫õÔºåËûçÂÖ•‰∫Ü‰∏çÂ∞ë DeepSeek Ëá™Â∑±ÁöÑ‰∏≠ÊñáËØ≠Êñô„ÄÇËøôÁßç‰∏≠Ë•øÁªìÂêàÔºåËÆ© V3.1 Âú®ÊñáÈ£é‰∏ä‰øùÁïô‰∫Ü‰∏ÄÂÆöÁöÑÂ∑ÆÂºÇÊÑü„ÄÇ
  - Âè¶‰∏Ä‰∏™ÊòéÊòæÂèòÂåñÊòØÁØáÂπÖÊéßÂà∂„ÄÇÁõ∏ÊØî‰∫é R1-0528ÔºåV3.1 ÂØπÂêÑÊÆµËêΩÁöÑÂ≠óÊï∞ÈÉΩÂÅö‰∫ÜÊòéÊòæÁöÑÁ≤æÁÆÄ„ÄÇÊÉÖÁª™Ê∏≤Êüì‰∏ä‰πü‰∏çÂÜçÊòØ R1 ÈÇ£ÁßçÂÜóÈïø„ÄÅÁ≤òÁ®†ÁöÑË°®ËææÔºåÊõ¥Âä†Áõ¥Êé•ÂíåÊúâÊïàÔºåÁªô‰∫∫‰∏ÄÁßçÊ∏ÖÁàΩÁöÑÊÑüËßâ„ÄÇ

- ## üéØ Introducing DeepSeek-V3 _20241226
- https://x.com/deepseek_ai/status/1872242657348710721
  - 60 tokens/second (3x faster than V2!)
  - Fully open-source models & papers
  - 671B MoE parameters

- https://x.com/op7418/status/1872469838641406262
  - ‰ªñ‰ª¨Ëá™ÊµãÁöÑÊàêÁª©Êï¥‰ΩìË∑ü GPT-4o Âíå Claude 3.5 ÂØπÈΩê‰∫Ü
  - Êµ∑Â§ñÁ§æÂå∫ÊôÆÈÅçÊÉäÂèπ‰ªñ‰ª¨Áî® Llama 405B ÂçÅÂàÜ‰πã‰∏ÄÁöÑÁÆóÂäõÊàêÊú¨ËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™Êõ¥Â§ßÊõ¥Âº∫ÁöÑÊ®°Âûã
  - Llama 3 405B ‰ΩøÁî®‰∫Ü 30.8M GPU Â∞èÊó∂ÔºåËÄå DeepSeek-V3 ÁúãËµ∑Êù•ÊòØ‰∏Ä‰∏™Êõ¥Âº∫Â§ßÁöÑÊ®°ÂûãÔºå‰ªÖ‰ΩøÁî®‰∫Ü 2.8M GPU Â∞èÊó∂ÔºàËÆ°ÁÆóÈáèÂáèÂ∞ë‰∫ÜÁ∫¶ 11 ÂÄçÔºâ„ÄÇ
  - Âπ∂‰∏çÊÑèÂë≥ÁùÄÂâçÊ≤ø LLM ÈúÄË¶ÅË¶ÅÂ§ßÁöÑËÆ°ÁÆóÈõÜÁæ§ÔºåÂèçËÄåÊÑèÂë≥ÁùÄ‰Ω†ÂøÖÈ°ª‰∏çËÉΩÊµ™Ë¥π‰Ω†Êã•ÊúâÁöÑËµÑÊ∫ê

- https://x.com/amasad/status/1872320808028454976
  - Craziest thing is it took only $5.5m to train. US labs spend one ‚Äî maybe two ‚Äî order of magnitude more for frontier models.
# discuss
- ## 

- ## 

- ## 

- ## ÂÅö‰∫ßÂìÅÁ∫ßÊêúÁ¥¢‰ª•ÂêéÂèëÁé∞ 3.7 ÊïàÊûúÁâπÂ•Ω‰ΩÜÊòØÁâπÂà´Ë¥µÔºåÊâÄ‰ª•ÂøÖÈ°ªÂæóÂÅö content cache Èôç‰ΩéÊàêÊú¨„ÄÇ
- https://x.com/arvin17x/status/1896922111505285484
  - ‰ΩÜ‰∏∫‰∫ÜÂÅö context cache ÔºåÂèëÁé∞ÂøÖÈ°ªÂæóËÆ∞ÂΩï token usageÔºå‰∏çÁÑ∂ÊÑüÁü•‰∏çÂà∞ cache ÁöÑÊïàÊûú„ÄÇ

- ÂèØ‰ª•ÂàÜ‰∫´‰∏ã Context Cache ÁöÑÂéüÁêÜ‰πàÔºåÊàëËøò‰ª•‰∏∫ËøôÁßç‰∏úË•øÂè™ËÉΩÂú®Ê®°Âûã‰æõÂ∫îÂïÜ‰æßÂÅö„ÄÇ
  - Â∞±ÊòØÂú®Ê®°Âûã‰æõÂ∫îÂïÜ‰æßÂÅö„ÄÇÂè™ÊòØ OpenAI Âíå DeepSeek ÂÅöÁöÑÊòØÈùôÈªòÊñπÊ°àÔºåËÄå Anthropic Âíå Google ÊòØÈúÄË¶ÅÂºÄÂèëËÄÖÊâãÂä®ÂºÄÁöÑ
- Â∫îËØ•ÊòØ prompt caching Âêß
  - AnthropicÂÆ∂Ëá™Â∑±ÁöÑËØ¥Ê≥ïÊòØ prompt Caching. ‰ΩÜÊàëÊÑüËßâË°å‰∏öÈáåÊÑüËßâËøòÊòØ‰π†ÊÉØÂè´ context caching ÁöÑ„ÄÇ
  - ‰∏çËøá OpenAI ÂÆòÊñπÂíå Anthropic ÂÆòÊñπÈÉΩÁß∞‰πã‰∏∫ prompt caching

- ÂæàÂ•áÊÄ™ÔºåËøôÁé©ÊÑè‰∏∫Âï•‰∏çÈªòËÆ§ÂºÄÂêØ„ÄÇ
  - Êüê‰∫õÂú∫ÊôØ‰∏ãÁöÑÁ°Æ‰∏ç‰∏ÄÂÆöÈÄÇÂêàÔºåÂõ†‰∏∫ÂÜôÂÖ• cache ÁöÑÊàêÊú¨ÊòØÂéü‰ª∑ÁöÑ 1.25ÂÄç

- ## It is common to generate train and validation sets using random splitting.
- https://x.com/_avichawla/status/1898622288737767785
  - However, in many situations, it can be fatal for model building.
  - Consider building a model that generates captions for images.
  - Group shuffle split solves this.

- ## Áî±‰∫éDeepSeek-R1 ÁàÜÁÅ´ÔºåÊâÄ‰ª•‰∏∫Â§ßÂÆ∂Â∏¶Êù•‰ªÄ‰πàÊòØLLMËí∏È¶èÊäÄÊúØÁöÑÁ¨îËÆ∞„ÄÇ
- https://x.com/karminski3/status/1882233538042597423
  - Âá†‰∏™Âä©ËÆ∞ËØçÔºöÊïôÂ∏àÊ®°ÂûãÔºåÂ≠¶ÁîüÊ®°ÂûãÔºåËΩØÁõÆÊ†áÔºåÁ°¨ÁõÆÊ†á„ÄÇ

- https://x.com/ShanghaoJin/status/1882679738789216456
  - ‰Ω†Âê¨ËØ¥Ëøá‰ªÄ‰πàÂè´‚ÄúËí∏È¶è‚Äù‰πàÔºüËØ¥‰∏™Â§ßÁôΩËØùÔºöÂ∞±ÊòØÊãø‰∫∫ÂÆ∂ÁÆóÂá∫Êù•ÁöÑÊ®°ÂûãÂèÇÊï∞ÔºåË∑≥ËøáÊâÄÊúâÊï∞ÊçÆÊ∏ÖÊ¥ó„ÄÅËÆ≠ÁªÉÔºåÂÅöÊúÄÂêé‰∏ÄÁ®ã„ÄÇÂÖ∂ÂÆûÊ≤°Êúâ‰ªª‰ΩïÂàõÊñ∞
  - Â•ΩÂÉè‰∫∫ÂÆ∂ËØÅÊòé‰∫ÜœÄ=3.14Ôºå‰ªñÊãøÁªìÊûúÂéªÁÆó‰∫ÜÂúÜÈù¢ÁßØ„ÄÇËÆ©‰ªñÂÜçËá™Â∑±ÂéªËØÅÊòéÁÆó‰∏Ä‰∏™eÔºå‰ªñÂèàÊäìÁûé‰∫Ü
- ÂçÉ‰∏á‰∏çË¶ÅÁî®‚ÄúÂ§ßÁôΩËØù‚ÄùÊù•Ëß£ÈáäËá™Â∑±ÈÉΩÊ≤°ÂÆåÂÖ®ÁêÜËß£ÁöÑÊ¶ÇÂøµÔºå Âè™ËÉΩËÆ©Â§ñË°åÊãçÊâãÔºåÊáÇÁöÑ‰∫∫Âè™‰ºöÁ¨ëËØù‰Ω†„ÄÇ ‰Ω†ÂÆåÂÖ®‰∏çÁü•ÈÅìËí∏È¶èÊòØÂú®Âπ≤‰ªÄ‰πà„ÄÇÂ¶ÇÊûú‰Ω†Áü•ÈÅìÁöÑËØùÔºåÈÇ£Â∞±ÊòØÂÆåÂÖ®‰∏çÁü•ÈÅìDeepseek Âú®ÂÅö‰ªÄ‰πà„ÄÇ
- ‚ÄúËí∏È¶è‚ÄùËØ¥Ê≥ï‰∏çÊ≠£Á°Æ„ÄÇ1. Ëí∏È¶èÊïàÊûú‰∏ÄËà¨‰∏ç‰ºöË∂ÖËøáÂéüÊ®°Âûã 2. deepseekÁöÑ reasoningË°å‰∏∫ÂíåÂ∏ÇÈù¢‰∏äÂÖ∂‰ªñÊ®°Âûã‰∏ç‰∏ÄËá¥(ÊúâË∂ÖË∂ä‰∫∫Á±ªÊ†áÊ≥®ÁöÑÂ•áÂ¶ôË°å‰∏∫) 3. ÂºÄÊô∫ÂØπÂÜçËÆ≠ÁªÉÊ®°ÂûãÊúâÁ¶ÅÊ≠¢Âπ∂‰ºöÁõëÊéß APIÊª•Áî®

- ## ÊàëÊó•Â∏∏Áî® Cursor ÂÜô‰ª£Á†ÅÁöÑÂú∫ÊôØ‰πã‰∏ÄÔºö‚ÄúËØ∑ÂèÇËÄÉ‰ª£Á†Å @ XXX1 @ XXXn ÂÅö YYY ‰∫ã„ÄÇ‚Äù
- https://x.com/dotey/status/1869436413600731146
  - ÁÆÄÂçïÊù•ËØ¥Â∞±ÊòØËÆ© AI ÁÖßËë´Ëä¶ÁîªÁì¢ÔºåÈáçË¶ÅÁöÑÊòØÁªôÂá∫ÂÖÖË∂≥ÁöÑ‰∏ä‰∏ãÊñáÔºåËÆ© AI ÂèØ‰ª•Â≠¶‰π†ÂíåÊ®°‰ªø„ÄÇÂâ©‰∏ãÁöÑÂ∞±ÊòØ Review + AcceptÔºåÂæàÁÆÄÂçïÈ´òÊïà„ÄÇ
  - ÁâπÂà´Ë¶ÅÊ≥®ÊÑèÁöÑÊòØÁ¨¨‰∏Ä‰∏™‚ÄúËë´Ëä¶‚ÄùË¶ÅÊâìÁ£®Â•ΩÔºåËøôÊ†∑ÂêéÁª≠ÁöÑ‚ÄúÁì¢‚ÄùÊâç‰∏ç‰ºöÁîªÊ≠™„ÄÇ

- Êõ¥ÁÆÄÂçïÁöÑÂÅöÊ≥ïÊúâÊó∂ÂÄôÂèØ‰ª•Áõ¥Êé• @ git ÊüêÊ¨°Êèê‰∫§

- ÊàëÁé∞Âú®ÊòØÊñ∞È°πÁõÆÈáåÂàõÂª∫‰∏Ä‰∏™txtÊñá‰ª∂ÔºåÈáåÈù¢ÂÜô‰∏äÊÉ≥Ê≥ïÂíågptÂØπÊàëÊÉ≥Ê≥ïÁöÑÂª∫ËÆÆÔºåÁÑ∂ÂêéËÆ©cursor ÂèÇËÄÉËøô‰∏™Êñá‰ª∂Êù•ÂºÄÂèëÔºåÈáåÈù¢Êàë‰πüÊúâÊó∂‰ºöÂÜô‰∏äÊ≠•È™§ÔºåÈ¶ñÂÖàÂÆûÁé∞‰ªÄ‰πàÔºåÁÑ∂ÂêéÂÆûÁé∞‰ªÄ‰πàÔºåÂÅö‰∏ÄÊÆµ‰∫ÜÔºåËÆ©cursorÊ†πÊçÆËøô‰∏™Êñá‰ª∂Ê£ÄÊü•‰∏Ä‰∏ãÈ°πÁõÆÂÆåÊàêÂ∫¶ÔºåÂàóÂá∫Êù•Âì™‰∫õÊ≤°ÂÅöÔºåËøôÊ†∑ÂèçÂ§çËø≠‰ª£ÂêëÂâç

- LM ÂÖÖÂàÜËØÅÊòé‰∫Ü‰∫∫Á±ªÁöÑÊú¨Ë¥®Â∞±ÊòØÂ§çËØªÊú∫„ÄÇ Âì™ÈáåÊúâ‰ªÄ‰πàÊåá‰ª§ÈÅµÂæ™ÔºåÊé®ÁêÜÔºåÂ§ßÂÆ∂ÈÉΩÊòØ‰ªé‰∏çÂêåÁöÑÁª¥Â∫¶Áî®‰∏çÂêåÁöÑÊñπÂºèÂú®Â§çËØª‰∏Ä‰∫õ‰∏úË•øËÄåÂ∑≤

- ## Â¶ÇÊûúÊÉ≥Ë¶ÅËÆ© LLM Á®≥ÂÆöÁîüÊàê JSON ÂØπË±°ÔºåÊúÄÁÆÄÂçïÁöÑÊñπÂºèÂ∞±ÊòØ‰ΩøÁî® zod ÂÆö‰πâ schema Âπ∂ÈÖçÂêà @vercel ai sdkÁöÑ generateObject‰ΩøÁî®ÔºåÊØîÂ¶ÇËøôÈáåÊàëÊÉ≥Ë¶Å‰ªéÁΩëÈ°µÊñáÊú¨ÂÜÖÂÆπÊèêÂèñÁªìÊûÑÂåñÁöÑ‰ø°ÊÅØ„ÄÇ
- https://x.com/FeigelC35583/status/1819558128297648412
  - ËøôÁßçÊñπÂºèÂíåÂΩìÂàù langchain Âú® prompt ÈáåÂÜô‰∏ÄÂ§ßÂ†Üjson ÂÆö‰πâÊúâÊú¨Ë¥®Âå∫Âà´ÔºåÂú®‰∫é‰ΩøÁî®‰∫Ü function call ÁöÑËÉΩÂäõ
  - ‰ªéËØ∑Ê±Ç‰∏≠ÂèØ‰ª•ÁúãÂà∞ÔºåÊú¨Ë¥®‰∏äÊòØÂú®Ë∞ÉÁî®Ê®°ÂûãÁöÑÊó∂ÂÄôÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫ json ÁöÑ ÂáΩÊï∞, ÊèèËø∞ÊòØ respond with a json object, ÂÖ∂‰∏≠ÂèÇÊï∞ÊòØËá™Â∑±ÂÆö‰πâÁöÑ schemaÔºåÁÑ∂ÂêéÂú® tool_choice ‰∏≠ÈôêÂà∂ÂøÖÈ°ªË¶Å‰ΩøÁî®Ëøô‰∏™ json ÂáΩÊï∞ÔºåÈÇ£‰πàÊ®°ÂûãÂ∞±‰ºöËøîÂõûË∞ÉÁî®json ÂáΩÊï∞ÁöÑÂèÇÊï∞ÔºåÂç≥‰Ω†ÂÆö‰πâÁöÑ schema
  - Á§∫‰æã‰ª£Á†ÅÊù•Ëá™‰∫éhttps://github.com/DiscovAI/DiscovAI-crawl ÊàëÊ≠£Âú® building ÁöÑ‰∏Ä‰∏™Èù¢Âêë RAG Â∫îÁî®ÁöÑÁà¨Ëô´Âπ≥Âè∞
- Â∫îËØ•Âè™ÊúâGPTÁ≥ªÂàóËÉΩÁî®Âêß
  - ÊîØÊåÅfunction callÂ∞±ÂèØ‰ª•ÔºådeepseekÂ∫îËØ•‰πüÂèØ‰ª•ÁöÑ
- Âú®ËøôÂü∫Á°Ä‰∏ä„ÄÇÊàë‰ºöËÄÉËôë‰ΩøÁî®jsonrepairËøô‰∏™ÂåÖÔºåÊâãÂä®‰øÆÂ§ç‰∏ãÔºåÂ¢ûÂä†ÂÆπÈîô
- Â¶ÇÊûúÂ§ßÊ®°ÂûãÊ≤°ÊúâÊ≤°ÊúâËøîÂõûÂØπÂ∫îË¶ÅÊ±ÇÁöÑÂ≠óÊÆµÊï∞ÊçÆÔºåÊàñËÄÖËøîÂõûÈîô‰∫ÜÁ±ªÂûãÔºåÂÆÉ‰ºöÊÄé‰πàÊ†∑Ôºå‰ºöËá™Â∑±Ë°•ÂÖÖÁ©∫ÁöÑÔºåÊàñËÄÖËá™Âä®ËΩ¨Êç¢Á±ªÂûãÂêóÔºü
  - ‰∏ç‰ºöË°•ÂÖÖÔºå‰ºöthrow errorÔºå‰πüÂèØ‰ª•Áî®‰∏äÈù¢Êé®ÂèãÊé®ËçêÁöÑjsonrepairÊâãÂä®fix

- ËÉΩÊîØÊåÅÂºÄÊ∫êÊ®°ÂûãÂêó
  - ÂèñÂÜ≥‰∫éÊ®°ÂûãÊîØ‰∏çÊîØÊåÅfunction callÔºåÊîØÊåÅÁöÑËØùÂ∞±ÂèØ‰ª•ÔºåÊïàÊûúÁöÑËØùË¶ÅÁúãÊ®°ÂûãÁöÑËÉΩÂäõ
- Áî® function call ÊÑüËßâÊ®°ÂûãÁöÑËÉΩÂäõÈôç‰∫Ü‰∏Ä‰∏™Áª¥Â∫¶Ôºå‰∏çÂ¶ÇÁõ¥Êé•ÁªôÊñáÊú¨ÔºåÊàëËøòÊòØÊõ¥ÂñúÊ¨¢Áî®xmlËá™Â∑±ÊèêÂèñ„ÄÇ

- ÊàëÊòØÁî®‰º™‰ª£Á†Å‚ûïÁ±ªÂûãÂ£∞Êòé, ‰πüÊòØ‰∏ÄÊ†∑ÁöÑÁ®≥ÂÆöËæìÂá∫ json
- langchainÊ°ÜÊû∂‰∏≠ÊúâPydantic json Ëß£ÊûêÂô®ÂèØ‰ª•Áõ¥Êé•Áî®ÔºåÊú¨Ë¥®‰πüÊòØÁîüÊàêschemaÔºåÂÜçÈÖçÂêàÈáçËØïËß£ÊûêÂô®‰πüÂèØ‰ª•Á®≥ÂÆöÁîüÊàêjsonÊ†ºÂºè

- ## üí° LLMs are literally the most unreliable technology of all time (followed by **ing bluetooth)
- https://x.com/Steve8708/status/1819448686424084892
  - After an absurd amount of trial and error, we've internally created a set of rules for make LLMs considerably more reliable
  - our secrets: restrict the llm to only what rag provides

- what's your stance on AI for no-code? Do people prefer drag-and-drop vs prompting?
  - i think the winning move is combining both

- Bluetooth is hell and causes frustration daily.

- ## üå∞ Firefox will use Transformers.js to power on-device features
- https://x.com/osanseviero/status/1797291569348751848
  - In their PDF Editor to generate alt text for images
  - Improve translations
  - Fully offline, open-source and with <200M models
- [Experimenting with local alt text generation in Firefox Nightly - Mozilla Hacks - the Web developer blog _202405](https://hacks.mozilla.org/2024/05/experimenting-with-local-alt-text-generation-in-firefox-nightly/)
  - https://huggingface.co/Mozilla

    - Êèê‰æõ‰∫ÜÊï∞ÊçÆÈõÜÂíåÊ®°Âûã

- Offline and open-source is a big win for privacy-focused tools

- ## [langchainÂà∞Â∫ïËØ•ÊÄé‰πà‰ΩøÁî®ÔºåÂ§ßÂÆ∂Âú®È°πÁõÆ‰∏≠ÂÆûË∑µÊúâÊàêÂäüÁöÑÊ°à‰æãÂêó? - Áü•‰πé](https://www.zhihu.com/question/609483833)
- LangChain‰πãÊâÄ‰ª•Â§ßÁÅ´ÔºåÊòØÂõ†‰∏∫ÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÁ≥ªÂàóÊñπ‰æøÁöÑÂ∑•ÂÖ∑„ÄÅÁªÑ‰ª∂ÂíåÊé•Âè£ÔºåÂ§ßÂ§ßÈôç‰Ωé‰∫Ü AI Â∫îÁî®ÂºÄÂèëÁöÑÈó®ÊßõÔºå‰πüÊûÅÂ§ßÁÆÄÂåñ‰∫ÜÂ§ßÊ®°ÂûãÂ∫îÁî®Á®ãÂ∫èÁöÑÂºÄÂèëËøáÁ®ã„ÄÇ
  - LangChainÊ°ÜÊû∂ËÉåÂêéÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∫èÂàóÂàÜËß£‰∏∫ÂêÑ‰∏™ÈÉ®ÂàÜÔºåÂÖÅËÆ∏ÂºÄÂèë‰∫∫ÂëòÊ†πÊçÆËá™Â∑±ÁöÑÈúÄÊ±ÇÈ´òÊïàÂú∞ÂÆöÂà∂Â∑•‰ΩúÊµÅÁ®ã„ÄÇ
- LangchainÊúâ6Â§ßÊ†∏ÂøÉÊ®°ÂùóÔºö
  - ModelsÔºöÊ®°ÂûãÔºåÊòØÂêÑÁßçÁ±ªÂûãÁöÑÊ®°ÂûãÂíåÊ®°ÂûãÈõÜÊàê„ÄÇ
  - PromptsÔºöÊèêÁ§∫ÔºåÂåÖÊã¨ÊèêÁ§∫ÁÆ°ÁêÜ„ÄÅÊèêÁ§∫‰ºòÂåñÂíåÊèêÁ§∫Â∫èÂàóÂåñ„ÄÇ
  - MemoryÔºöËÆ∞ÂøÜÔºåÁî®Êù•‰øùÂ≠òÂíåÊ®°Âûã‰∫§‰∫íÊó∂ÁöÑ‰∏ä‰∏ãÊñáÁä∂ÊÄÅ„ÄÇ
  - IndexesÔºöÁ¥¢ÂºïÔºåÁî®Êù•ÁªìÊûÑÂåñÊñáÊ°£Ôºå‰ª•‰æøÂíåÊ®°Âûã‰∫§‰∫í„ÄÇÂåÖÊã¨ÊñáÊ°£Âä†ËΩΩÁ®ãÂ∫è„ÄÅÂêëÈáèÂ≠òÂÇ®Âô®„ÄÅÊñáÊú¨ÂàÜÂâ≤Âô®ÂíåÊ£ÄÁ¥¢Âô®Á≠â„ÄÇ
  - AgentsÔºö‰ª£ÁêÜÔºåÂÜ≥ÂÆöÊ®°ÂûãÈááÂèñÂì™‰∫õË°åÂä®ÔºåÊâßË°åÂπ∂‰∏îËßÇÂØüÊµÅÁ®ãÔºåÁõ¥Âà∞ÂÆåÊàê‰∏∫Ê≠¢„ÄÇ
  - ChainsÔºöÈìæÔºå‰∏ÄÁ≥ªÂàóÂØπÂêÑÁßçÁªÑ‰ª∂ÁöÑË∞ÉÁî®„ÄÇ
- LangChain ÈÄöÂ∏∏Ë¢´Áî®‰Ωú„ÄåÁ≤òÂêàÂâÇ„ÄçÔºåÂ∞ÜÊûÑÂª∫ LLM Â∫îÁî®ÊâÄÈúÄÁöÑÂêÑ‰∏™Ê®°ÂùóËøûÊé•Âú®‰∏ÄËµ∑„ÄÇ‰ΩøÁî®Langchain‰∏≠‰∏çÂêåÁªÑ‰ª∂ÁöÑÁâπÊÄßÂíåËÉΩÂäõÔºåÂèØ‰ª•ÊûÑÂª∫‰∏çÂêåÂú∫ÊôØ‰∏ãÁöÑÂ∫îÁî®ÔºåÂ¶ÇËÅäÂ§©Êú∫Âô®‰∫∫„ÄÅÂü∫‰∫éÊñáÊ°£ÁöÑÈóÆÁ≠î„ÄÅÁü•ËØÜÁÆ°ÁêÜ„ÄÅ‰∏™‰∫∫Âä©ÁêÜ„ÄÅAgentÊô∫ËÉΩ‰ΩìÁ≠âÁ≠â„ÄÇ

- ‰Ω†ÁöÑËøô‰∏™ËÆ§ËØÜÂ≠òÂú®‰∏Ä‰∫õÂÅèÂ∑ÆÔºåÈ¶ñÂÖàÔºå‰æùËµñAPI key ÊòØ‰∏∫‰∫Ü‰Ω†‰ΩøÁî®Â§ßÊ®°ÂûãÂéÇÂïÜÁöÑÊúçÂä°ÂíåÈâ¥ÊùÉÔºåËøôÊ≤°Êúâ‰ªÄ‰πàÊãâË∑®ÁöÑ„ÄÇÂæàÂ§öÁ¨¨‰∏âÊñπÁöÑÊúçÂä°ÈÉΩÈúÄË¶ÅÈâ¥ÊùÉÈ™åËØÅÔºåËøôÊòØÊØîËæÉ‰∏ªÊµÅÁöÑÊñπÂºè„ÄÇ
- ÂèØ‰ª•‰ºÅ‰∏öËá™Â∑±ÈÉ®ÁΩ≤Â§ßÊ®°ÂûãÔºåËøôÁßçÊàêÊú¨ÊòØÂæàÈ´òÁöÑ„ÄÇ‰ªéÊàë‰ª¨Ëá™Â∑±ÁöÑÂÆûÈ™åÊïàÊûúÊù•ÁúãÔºå13B ‰ª•‰∏ãÁöÑÂ§ßÊ®°ÂûãÂü∫Êú¨Â∞±ÊòØÁé©ÂÖ∑Ôºå‰ºòÂåñÂçäÂ§©Ë¥πÊó∂Ë¥πÂäõÔºåËÄå 34B ÊàñËÄÖÊõ¥Â§ßÁöÑÊ®°ÂûãÔºåÂÖ¨Âè∏ÈÉ®ÁΩ≤ÊàêÊú¨ÂèàÂæàÈ´ò„ÄÇ
- langchain ‰∏≠ÁöÑÁâπËâ≤ÊòØÂÆÉÁöÑ langchain expression language (LCELÔºâÔºåÊòØ‰∏ÄÁßçÁ±ª‰ºº linux ÁÆ°ÈÅìÂΩ¢ÂºèÁöÑË∞ÉÁî®ÊñπÂºèÔºåÂèØ‰ª•ÂæàÁÆÄÂçïÁöÑÂÆûÁé∞ÂÆÉÁöÑ chain Áõ∏ÂÖ≥ÁöÑÂäüËÉΩ„ÄÇËøô‰∏™ÔºåÂú®ÊàëÂÆûÈôÖ‰ΩøÁî®ÁöÑÊó∂ÂÄôÔºåÊ≤°ÊúâÊÉ≥Ë±°ÁöÑÈÇ£‰πàÂ•ΩÁî®ÔºåÂèØ‰ª•Ê†πÊçÆÂÆûÈôÖÊÉÖÂÜµÂéªÂ≠¶‰π†„ÄÇ
- ÊúÄÂêéÔºålangchain ‰∏≠ËøòÊúâ‰∏Ä‰∏™Âè´ÂÅö langgraph ÁöÑÁªÑ‰ª∂ÔºåËÉΩÂ§üÂíå pytorch ‰∏ÄÊ†∑Áî®Êê≠ÁßØÊú®ÁöÑÊñπÂºèÂéªÊûÑÈÄ†‰∏Ä‰∏™ÊúâÂêëÊó†ÁéØÂõæ„ÄÅÂæ™ÁéØÁöÑÈìæÔºåÊØî LCEL Êõ¥È´òÁ∫ß„ÄÇ

- 
- 

- ## LLMÊêûÂèçÁºñËØëÔºå.not careÂíåJvavÁî®Êà∑ÂÜç‰πü‰∏çÁî®ÊäòËÖæ‰ªÄ‰πàÊ∑∑Ê∑Ü‰∫ÜÔºåÈÉΩÊ≤°Áî®‰∫Ü
- https://twitter.com/geniusvczh/status/1774053196039962758
  - ÊñáÁ´†ÈáåÂèçÁºñËØëÁöÑÊòØx86, x86ÈÉΩÂèØ‰ª•ÔºåILÈöæÂ∫¶Âè™‰ºöÊõ¥‰Ωé
- Â§ßÊ¶ÇÁúã‰∫Ü‰∏Ä‰∏ãÔºåÂ∞±ÊòØÊääÁºñËØëÂá∫ÁöÑÊ±áÁºñË∑üÊ∫ê‰ª£Á†ÅÂÅö‰∫Ü‰∏Ä‰∏™ÁÆÄÂçïÁöÑseq2seqÁöÑfine tuneÔºåËÆ≠ÁªÉÈõÜËøûÊ∑∑Ê∑ÜÈÉΩÊ≤°ÊúâÔºåÁ¶ªËÆ©ÊâÄÊúâÊ∑∑Ê∑ÜÈÉΩÊ≤°Áî®ÈÇ£Êõ¥ÊòØËøòÂ∑ÆÂæóËøú„ÄÇ
- 17Âπ¥googleÈÇ£ÁØátransformerÁöÑËÆ∫ÊñáÂ∞±Èù†ËøôÊ†∑ÂÆåÊàê‰∫ÜËá™ÁÑ∂ËØ≠Ë®ÄÁöÑÁøªËØëÔºåËøô‰∫õÈÉΩÊòØËøüÊó©ÁöÑ‰∫ãÔºåÂèçÁºñËØëÂíåÂèçÊ∑∑Ê∑ÜÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈÉΩÊòØÂèØ‰ª•ÊâπÈáèÁîüÊàêÁöÑÔºåÂÅöËµ∑Êù•ÁÆÄÂçïÂ§ö‰∫Ü
  - ÊàëËßâÂæóLLMÂØπ‰∫éÂèçÁºñËØëÂíåÂèçÊ∑∑Ê∑ÜÔºåÂèØËÉΩÊõ¥Â§ßÁöÑ‰ΩúÁî®Âú®‰∫éÁîüÊàê‰∫∫Á±ªÂèãÂ•ΩÁöÑÂèòÈáè/Á®ãÂ∫èÁªìÊûÑ„ÄÇÊØïÁ´üÂèçÁºñËØëÂíåÂèçÊ∑∑Ê∑ÜÊòØÁå´Èº†Ê∏∏ÊàèÔºåÊÄªÂèØ‰ª•ÊÉ≥Âá∫Êñ∞ÁÇπÂ≠êÔºå‰∫∫Á±ªÁöÑÂπ≤È¢ÑËøòÊòØÂøÖ‰∏çÂèØÂ∞ëÁöÑÔºåËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÂü∫‰∫éËßÑÂàô/Á®ãÂ∫èÂàÜÊûêÁöÑ‰º†ÁªüÊñπÊ≥ïÂèØËÉΩÊõ¥Â•ΩÔºåÁÑ∂ÂêéÂÜçÁî®LLMÁåúÂèòÈáèÂêç
- ‰∏∫‰∫ÜÊãâËµÑÈáëËÄåÂ∑≤ÔºåÈí±Áî≥ËØ∑Âà∞‰∫ÜËÆ∫ÊñáÂ∞±Ê≤°Âï•Áî®‰∫Ü‚Ä¶‚Ä¶Â§ÑÁêÜÂ±éÂ±±ÁïôÁªôÂ∑®Â§¥ÁöÑÁ®ãÂ∫èÂëòÂ∞±Ë°å‰∫ÜÔºåËøòËΩÆ‰∏çÂà∞Â≠¶ÊúØÂúàÊù•ÊåáÁÇπÊ±üÂ±±
- ËøôÁßçÂ±ÄÈôê‰∫éÂáΩÊï∞ÁöÑÂèçÊ∑∑Ê∑ÜÂï•Áî®ÈÉΩÊ≤°ÊúâÔºåÂØπ‰ªòÁÇπ‰∏âËÑöÁå´ÂäüÂ§´ÁöÑÊ∑∑Ê∑ÜËøòÂ∑Æ‰∏çÂ§ö

- ## ü™ß Á†îÁ©∂‰∫Ü‰∏Ä‰∏ãÊú¨Âú∞Â§ßÊ®°ÂûãÁöÑÂú∫ÊôØÔºö
- https://twitter.com/changmingY/status/1773336179296887162
  1. ‰∏çËÉΩËÅîÁΩëÁöÑÂõΩÂÜÖÁî®Êà∑
  2. ‰∏ÄËà¨Áî®Êà∑Êú∫Âô®ÈÖçÁΩÆËææ‰∏çÂà∞ÔºåÊïàÁéáÂ§™Â∑Æ
  3. Êú¨Âú∞Áü•ËØÜÂ∫ìÁÆóÊòØ‰∏Ä‰∏™ÂàöÊÄßÈúÄÊ±Ç
  4. ÂûÇÁõ¥È¢ÜÂüüÊ®°ÂûãË∂äÊù•Ë∂äÂ§ö, ‰∏Ä‰∏™hubÈõÜ‰∏≠‰ΩøÁî®
  5. Â∞èËÄåÁæéÁöÑÊ®°Âûã‰ºöË∂äÊù•Ë∂äÂ§öÔºåÂÆåÊàê‰∏Ä‰∏™ÁâπÂÆöÂäüËÉΩ

- ## ollama ÁöÑÁºñËØëÁé©ÁöÑÂ§™Ëä±‰∫ÜÔºåÂÖàÊòØÂêß llama.cpp Âú®‰∏çÂêå cpu Âíå gpu ÁöÑÂä®ÊÄÅÈìæÊé•Â∫ìÈÉΩÁºñËØë‰∫ÜÂá∫Êù•ÈÅøÂÖçÁî®Êà∑Âú®ËøêË°åÊó∂ÂÜçÂéªÁºñËØëÔºå
- https://twitter.com/liumengxinfly/status/1767073319956971891
  - ÁÑ∂ÂêéÁî® go ÁöÑ embed ÁâπÊÄßÁõ¥Êé•ÊääËøô‰∫õÂä®ÊÄÅÂ∫ìÂÖ®ÈÉΩÊâìÂåÖÂà∞ go ÁöÑ‰∫åËøõÂà∂ÈáåÔºåÁÑ∂ÂêéÂú®Áî® cgo Âíå dlfcn Âä†ËΩΩÂíåË∞ÉÁî® llama.cppÔºåÂÆûÁé∞‰∫Ü‰∏Ä‰∏™‰∫åËøõÂà∂Êñá‰ª∂ÂÖçÁºñËØëÔºåÂÖçÂÆâË£ÖÁöÑËß£ÂÜ≥ÊâÄÊúâÈóÆÈ¢ò

- https://twitter.com/holegots/status/1767427148506431665
  - ‰∏çËøáËøô‰∏™Êú¨Ë¥®‰πüÊòØ llama.cpp Â•óÂ£≥Âêß , Â∫ïÂ±ÇËøòÊòØ cpp, golang Âπ∂‰∏çÂèÇ‰∏éÂÆûÈôÖÁöÑÊé®ÁêÜ.

- ## ÊúÄÊñ∞ÁâàÁöÑ OpenAI Translator Â∑≤ÁªèÊó†ÁºùÊîØÊåÅÊú¨Âú∞Â§ßÊ®°Âûã‰∫ÜÔºàOllamaÔºâÔºåÊó†ÈúÄËÅîÁΩëÔºåÂø´ÈÄü‰æøÊç∑ÔºåÂÆâÂÖ®Á®≥ÂÆöÔºÅÂÜç‰πü‰∏çÊÄï OpenAI Ë¥¶Âè∑Ë¢´Â∞Å‰∫ÜÔºÅÁøªËØëÊïàÊûúÂØπÊØîÂ§ßÂÆ∂ÂèØ‰ª•Áúã‰∏Ä‰∏ãÊà™ÂõæÔºåÂ§ßÂÆ∂Âø´Êù•‰∏ãËΩΩ‰ΩìÈ™å‰∏Ä‰∏ãÂêßÔºÅ _202402
- https://twitter.com/yetone/status/1761607398819840511
- Áé∞Âú®Â•ΩÂÉèËøò‰∏çÊîØÊåÅËá™ÂÆö‰πâÊ®°ÂûãÔºüÂè™ÊúâÊúâÈôêÁöÑÂá†‰∏™Ê®°ÂûãÂèØ‰æõÈÄâÊã©ÔºåÊúÄÂ•ΩÊòØÊúâ‰∏Ä‰∏™ÊñáÊú¨Ê°ÜÂèØ‰ª•Ëá™ÂÆö‰πâËæìÂÖ•
- ËøôÊòØMistralÂ§öÂ§ßÁöÑÊ®°ÂûãÔºå7BÁöÑÂêóÔºü
  - ÊòØÁöÑ

- ‰∏çÁü•ÈÅìËøô‰∫õ7b 13bÁöÑÂ∞èÊ®°ÂûãÂì™‰∏™ÁøªËØëË¥®ÈáèÊõ¥È´ò

- ## ÈòøÈáå‰∫ëÁ´üÁÑ∂ÊîØÊåÅËøô‰πàÂ§öÊ®°Âûã‰∫Ü
- https://twitter.com/yihong0618/status/1746745371441967540
- http://ai.azure‰πüÂåÖÂê´‰∫ÜÂ•ΩÂ§öÊ®°ÂûãÔºåÊò®Â§©ÊÉäÂà∞‰∫Ü

- ## Ë∂äÊù•Ë∂äËßâÂæó RAG Ëøô‰∏úË•øÊúâÊÑèÊÄù„ÄÇ
- https://twitter.com/wwwgoubuli/status/1737471851654160548
  - ÂçäÂπ¥ÂâçÊé•Ëß¶Âà∞Ëøô‰∏™ËØçÁöÑÊó∂ÂÄôÂºÄÂßãÊàëËøòÊúâ‰∫õ‰∏çÂ±ëÔºåÊêúÁ¥¢ÂÜÖÂÆπÊèíÂÖ•Âà∞ÊèêÁ§∫ËØçÁÆó‰ªÄ‰πàÂòõÔºåÂ∞èÂ≠¶‰∫åÂπ¥Á∫ßÈÉΩËÉΩÊòéÁôΩ„ÄÇÂ∞§ÂÖ∂ÊòØÁúãÂà∞Èöè‰æø‰∏¢ÂêëÈáèÂ∫ìÈÉΩËÉΩË∑ëÂá∫‰∏™‰∏É‰∏ÉÂÖ´ÂÖ´ÔºåË∂äÂèëËßâÂæóËøô‰∏™ÁÆÄÂçï„ÄÇ
  - ‰ΩÜÁé∞Âú®ÁúüÁöÑÊêû‰∫ÜÂçäÂπ¥ÔºåÊàëË∂äÂèëÁöÑËßâÂæóËøôÊâçÊòØ‰∏ã‰∏Ä‰∏™Â§ßÂ§öÊï∞‰∫∫ÂèØ‰ª•ÂèÇ‰∏éÁöÑÈ£éÂè£„ÄÇÂÆÉÊúâÈó®Êßõ„ÄÇ
- ÊäÄÂ∑ßÂæàÂ§ö ÊâÄ‰ª•Â•ΩÁé© ‰ΩÜÈ£éÈô©ÊòØÂ§ßÈÉ®‰ªΩÊäÄÂ∑ßÈÉΩË¢´Ê®°ÂûãÊèê‰æõÂïÜÁé©ËøáÔºå80%ÈúÄÊ±ÇÈÉΩÂèØËÉΩË¢´‰ªñ‰ª¨Áõ¥Êé•Ë¶ÜÁõñ
  - RAG‰∏çÂ∞±ÊòØquery transformation/rewrite/expanding, hybrid search, reranking, etcÂêóÔºüÂΩìÁÑ∂ËøòÊúâ‰∫õÂÖ∂‰ªñÊäÄÂ∑ßÂï•IAG‰πãÁ±ªÁöÑ„ÄÇÊï∞ÊçÆingestion‰πüÊúâ‰∫õÊäÄÂ∑ßÔºå‰∏çËøáÊàëÁúã‰∏ªË¶ÅËøòÊòØÂú®query‰∏ä„ÄÇ Ëøô‰∫õÂ§ßÈÉ®ÂàÜOAI, Baichuan, Êúà‰πãÊöóÈù¢ÂÜÖÈÉ®ÈÉΩÊé¢Á¥¢Ëøá‰∫ÜÂêß
- RAG‰∏ÄÁúãÂ∞±ÊòØ‰∏Ä‰∏™ÊúâÈóÆÈ¢òÁöÑÂå∫ÂüüÔºåÂ§ßÊ®°ÂûãÈöèÊó∂‰∏ã‰∏ÄÊ¨°ÂçáÁ∫ßÂèØËÉΩÂ∞±‰ºöÊîπÂèòÊï¥‰∏™Ê°ÜÊû∂Ôºå3.5ËøòËÉ°ËØ¥ÂÖ´ÈÅìÔºå4Â∑≤ÁªèÂæàÂ§öÈÉΩÊòØÊúâÊ†πÊúâÊçÆÁöÑ‰∫Ü
- ÊêûÂà∞ÊúÄÂêéÔºåËøòÊòØÊ∏ÖÊ¥óÊï∞ÊçÆÔºåRAGÂè™Áî®ÁÆÄÂçïÁ≠ñÁï•Ëß£ÂÜ≥Â§ßÂ§öÊï∞ÈóÆÈ¢òÔºåÂèØËßÇÊµã„ÄÇÂâçÊèêÊòØÊâÄÊúâÂ§çÊùÇÁ≠ñÁï•ÈÉΩË¶ÅËØïËøáÊâçÁü•ÈÅì„ÄÇ

- ## LangChainÂºÄÊ∫ê‰∫ÜAnythingLLMÔºöÂèØ‰ª•‰∏é‰ªª‰ΩïÂÜÖÂÆπËÅäÂ§©ÁöÑÁßÅ‰∫∫ ChatGPTÔºåÂ∫îËØ•Â∞±ÊòØ‰ªñ‰ª¨Ëá™Â∑±ÊñáÊ°£Á≥ªÁªüÁî®ÁöÑÈÇ£‰∏ÄÂ•ó„ÄÇ
- https://twitter.com/op7418/status/1733893368974073873
  - An efficient, customizable, and open-source enterprise-ready document chatbot solution.
  - https://github.com/Mintplex-Labs/anything-llm /MIT/js/python

- ÊúâÊ≤°ÊúâËØ¶ÁªÜËØ¥ÊòéÔºüÊúÄÂ§ßÂèØ‰ª•ÊîØÊíëÂ§öÂ§ßÁöÑÊñáÊ°£Ôºü
  - Â∫îËØ•ÊòØ‰∏çÈôêÂ§ßÂ∞èÁöÑÔºåÊãÜÂºÄÂ∞±Â•Ω‰∫Ü
- ËØ¥Ê≤°ËØ¥Á°¨‰ª∂ÈúÄÊ±ÇÔºü

- ## Â§ßÊ®°ÂûãÁöÑËøô‰∫õ benchmark Â∫îËØ•ÊòØÂÖ®ÂÆáÂÆôÊúÄÊ≤°Áî®ÁöÑ benchmark ‰∫ÜÂêßÔºü
- https://twitter.com/yihong0618/status/1721401347533324688
- ‰πü‰∏çÊòØÂÖ®Ê≤°Áî®Ôºå‰πüÊúâ‰∏Ä‰∫õÊúâÁî®ÁöÑ, Â∞§ÂÖ∂ÁªÜÂàÜ‰ªªÂä°‰∏äÁöÑÔºåËøòÊòØÊå∫ÊúâÁî®ÁöÑ„ÄÇÂΩìÂâçÁõ∏ÊØîÂÖ∂‰ªñbenchmarkÔºåÂèØÊìç‰ΩúÁ©∫Èó¥Á°ÆÂÆûÂ§ß
- ÂÖ¨ÂºÄÁöÑÂè™ËÉΩÂÖ®ÁúãËá™Ëßâ

- ## ‰∏≠ÊñáÂºÄÊ∫êÊ®°ÂûãËôΩÂ§öÔºåÊï∞ÊçÆÈõÜÂç¥ÂæàÂ∞ëÂºÄÊ∫ê„ÄÇ
- https://twitter.com/9hills/status/1718828132046942218
  - ÁõÆÂâçËã±Êñá 7B ËßÑÊ®°ÁöÑ SOTA Ê®°ÂûãÊòØ zephyr-7b-beta„ÄÇÂÆÉÊîæÂºÉ‰∫ÜË¥®ÈáèÂèÇÂ∑Æ‰∏çÈΩêÁöÑÂºÄÊ∫êÊï∞ÊçÆÈõÜÔºå‰ΩøÁî®ChatGPTÂíåGPT-4 ÂÖ®Êñ∞Ê†áÊ≥®‰∫Ü UltraChat Âíå UltraFeedback Êï∞ÊçÆÈõÜÔºàÂ∑≤ÂºÄÊ∫êÔºâ„ÄÇÊòØ llama-index È°πÁõÆÂÆûÊµãÂá∫Êù•ÂîØ‰∏ÄËÉΩÂ§üÊîØÊåÅ Agent ÁöÑÂ∞èÂèÇÊï∞Ê®°Âûã„ÄÇ
- ‰∏≠ÊñáÊï∞ÊçÆÈõÜÈÉΩÊòØÊãøÊù•ÂçñÈí±ÁöÑ
