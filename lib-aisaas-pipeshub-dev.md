---
title: lib-aisaas-pipeshub-dev
tags: [pipeshub]
created: 2025-11-16T15:33:44.605Z
modified: 2025-11-16T15:34:02.881Z
---

# lib-aisaas-pipeshub-dev

> extensible and explainable workplace AI platform for enterprise search and workflow automation

# guide
- pros
  - license: apache2
  - ğŸ”— insights with citations: traceable source in every answer combined with confidence scores
    - AGIå†…å®¹ä¸­åŒ…å«çŸ¥è¯†åº“ä¸­çš„å†…å®¹å¼•ç”¨ï¼Œç‚¹å‡»æ—¶èƒ½è‡ªåŠ¨æ‰“å¼€åŸæ–‡pdfå¹¶é«˜äº®åŸæ–‡ä½ç½®
  - æ”¯æŒä½¿ç”¨æœ¬åœ°llm, åŒ…æ‹¬ollama/lmstudio
  - ğŸ‡¨ğŸ‡³ å¯¹ä¸­æ–‡çš„æ”¯æŒè¾ƒå¥½
  - create custom apps and AI agents using a No-Code interface
  - graph: All data is structured into a powerful knowledge graph
  - Modular & Scalable Architecture â€“ Every service is loosely coupled to scale independently
  - æ”¯æŒä¸Šä¼ æœ¬åœ°æ–‡ä»¶: pdf, docx, xlsx, csv, markdown, images, audio, video
  - å®˜æ–¹æä¾›å¾ˆå¤šå¤–éƒ¨æ•°æ®æºçš„é›†æˆ: google-drive/gmail/docs, OneDrive, slack, notion, airtable, github
  - æ”¯æŒç›´æ¥åœ¨ç•Œé¢ä¸Šé…ç½®ç¬¬ä¸‰æ–¹ä¾èµ– redis/kafka/mongo/qdrant

- cons
  - AGIå†…å®¹åªåŒ…å«çŸ¥è¯†åº“ä¸­çš„å†…å®¹, ä¸åŒ…å«llmè‡ªèº«çŸ¥è¯†ï¼Œç»å¸¸æ‹’ç»ç”¨æˆ· I cannot answer your query
  - aiå›å¤çš„å†…å®¹ç®€çŸ­ä¸å¤Ÿä¸°å¯Œï¼Œä½“éªŒä¸å¦‚ SurfSense
  - aiæ€è€ƒæ—¶é—´è¿‡é•¿, ç•Œé¢ä¸Šæ²¡æœ‰äº¤äº’åé¦ˆ
  - ğŸ’¥ æœ¬åœ°lmstudio, æ— æ³•embeddingå¤§æ–‡æ¡£
  - æ¶æ„å¤æ‚ï¼Œä¾èµ– kafka
  - æœªå®ç°online searché›†æˆï¼Œå¦‚tavily/exa/SearxNG
  - roadmap
    - Code Search
    - Workplace AI Agents
    - MCP
    - APIs and SDKs
    - Personalized Search
    - Highly available and scalable Kubernetes deployment
    - PageRank

- features
  - ä½¿ç”¨ arangodb å®ç°å›¾ç»“æ„çš„æ•°æ®å…³ç³»é€»è¾‘
  - Real-Time or Scheduled Indexing â€“ Index data as it flows or schedule it to run exactly when you need
  - Source-level permissions ensure every document is shown only to those who are authorized
  - åŸºäºlangchain/langgraphå®ç°
  - èƒ½ä»¥ç”¨æˆ·æé—®çš„è¯­è¨€å›å¤ç”¨æˆ·

- tips ğŸ’¡
  - è™½ç„¶æ€§èƒ½åŠåŠŸèƒ½ä¸Šæœ‰ç¼ºç‚¹, ä½†åŠŸèƒ½è¾ƒä¸°å¯Œã€ä½“éªŒè¾ƒå‹å¥½ã€å¤æ‚åº¦ä¹Ÿä¸é«˜, å€¼å¾—æ·±å…¥ä¼˜åŒ–
    - å®Œå–„çš„æ–¹æ¡ˆå¾ˆéš¾æ‰¾, å¯ä»¥å°è¯•æ”¹è¿›ç°æœ‰çš„æ–¹æ¡ˆ
  - æ–‡ä»¶å†…å®¹æå–å·¥å…·æˆ–æ ¼å¼è½¬æ¢å·¥å…·ï¼Œç¼ºå°‘åº”ç”¨å±‚chunking/embedding/vectorçš„é«˜æ€§èƒ½è°ƒåº¦é€»è¾‘

- SurfSense
  - æ¶æ„ç®€å•ï¼Œä¾èµ–ä¸å¤š, ä¸»è¦ä¾èµ–PGVector/Redis
  - file formats (Documents, images, videos and supports 50+ file extensions)
  - External Sources: Tavily, SearxNG, Slack, Linear, Jira, Notion
  - Powerful Search
  - Get Cited answers just like Perplexity
  - Local LLM Support
  - æä¾›äº†æµè§ˆå™¨æ‰©å±•
  - fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)
  - Advanced RAG Techniques
    - Uses Hierarchical Indices (2 tiered RAG setup).
    - Supports all major Rerankers (Pinecode, Cohere, Flashrank etc)
    - Utilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).
  - Work together effortlessly with real-time collaboration
# not-yet â“
- ä¸ºä½•å¯åŠ¨åç«¯è¦åœ¨4ä¸ªterminalè¿è¡Œ4ä¸ªç«¯å£ä¸åŒçš„æœåŠ¡, indexing/query/docling/connectors
  - ä¸ºä»€ä¹ˆä¸æ˜¯ä¸€ä¸ªapiï¼Œç„¶åä½¿ç”¨4ä¸ªä¸åŒçš„route url
# draft
- topics
  - Wikipedia
  - developer docs: react/vue/ecmascript/python/golang/pg

- 
- 
- 

- ä¸é€‰æ‹©kbçŸ¥è¯†åº“æ—¶ï¼Œä¸æ”¯æŒéšæ„èŠå¤©

- search
  - ä¸ä»…æ”¯æŒsources, è¿˜æ”¯æŒæœç´¢ comments/discussions

- local-models
  - æœ¬åœ°çš„prompt-processingé€Ÿåº¦éå¸¸æ…¢

- å¤„ç†å¤§pdfæ—¶chunking/embeddingçš„è¿›åº¦åé¦ˆå‡ºæ¥æ›´å¥½, å¯é‡è¯•å¯æ¢å¤æ›´å¥½, ç›®å‰çš„å®ç°æ„Ÿè§‰ä¸€ç›´å¡åœ¨inprogressç„¶åå°±å¤±è´¥äº†

- åˆ é™¤æ–‡æ¡£æ—¶ï¼Œæœªå®ç°å›æ”¶ç«™

- ç»Ÿä¸€ä¸åŒæ•°æ®æºçš„æœç´¢/AGIä½“éªŒ
  - pdf
  - wikipedia-zim
  - mdn-docs-offline
  - dictionary

- 
- 
- 
- 
- 
- 

- open source alternative
  - Google just dropped the Gemini File Search API (RAG-as-a-Service)
    - It allowed me to build a RAG chatbot in 31 min. No coding

- ~~æœªå®ç°æµå¼è¾“å‡º~~

## large-pdf/docs ğŸ’¥

- æå‡å¤„ç†è¶…å¤§pdfçš„èƒ½åŠ›
  - å¯å‚è€ƒpymupdf4llm
  - å¯å‚è€ƒ https://github.com/v4ler11/llm-chat

- https://github.com/renton4code/pdf-rag /AGPL/202502/ts/inactive
  - A production-ready template for building Retrieval-Augmented Generation (RAG) applications. 
  - This template provides a complete setup for document processing, vector storage, and AI-powered question answering with kickass UI.
  - PDF document processing with OCR
  - Milvus DB with billions of vectors scale support: Vector DB for storing embeddings
  - PostgreSQL: Relational DB for metadata storage
  - Click-to-view document references with highlighting
  - Large documents processing and status updates
  - AI/ML: Google Gemini (LLM), HuggingFace Transformers (Embeddings)
  - Based on Q&A with a large document (700+ pages) in comparison to RagFlow

- [How to Summarize Large Documents with LangChain and OpenAI _202405](https://medium.com/@myscale/how-to-summarize-large-documents-with-langchain-and-openai-4312568e80b1)
  - We have over 466, 000 tokens in this book, and if we pass them all directly to the LLM, it would charge us a lot. So, to reduce the cost, we will implement K-means clustering to extract the important chunks from the book.
  - We will split the book content into documents by using the SemanticChunker utility of LangChain.
  - weâ€™ll transform the document vectors into a format compatible with Faiss, cluster them into 50 groups using K-means, and then create a Faiss index for efficient similarity searches among documents.

- [Late Chunking: Embedding First Chunk Later â€” Long-Context Retrieval in RAG Applications _202409](https://blog.stackademic.com/late-chunking-embedding-first-chunk-later-long-context-retrieval-in-rag-applications-3a292f6443bb)
  - In late chunking, the process of embedding happens first for the entire document, ensuring that every tokenâ€™s embedding retains the documentâ€™s full context. 
  - Only after this embedding process do you chunk the embeddings into smaller parts, which are contextually rich because they preserve relationships between distant parts of the document. 
  - This approach ensures that each chunk maintains the global document context, leading to more precise and meaningful retrieval results.
  - [Late_Chunking_in_Long_Context_Embedding_Models.ipynb - Colab](https://colab.research.google.com/drive/19dZeMCx-7g0kPz35e5gEjtAIama38gGI?usp=sharing)

- [Building a PDF-Powered AI: Embeddings + ChromaDB + Ollama RAG Pipeline _202507](https://medium.com/@eliyaser3121/building-a-pdf-powered-ai-embeddings-chromadb-ollama-rag-pipeline-372aaab62aa8)
# dev-xp
- ğŸ’¥ æ— æ³•embeddingå¤§æ–‡æ¡£, æœ¬åœ°lmstudioæç¤º
  - [lmstudio-llama-cpp] Error in predictTokens: The number of tokens to keep from the initial prompt is greater than the context length. Try to load the model with a larger context length, or provide a shorter input
  - libc++abi: terminating due to uncaught exception of type std::runtime_error: [METAL] Command buffer execution failed: Caused GPU Timeout Error (00000002:kIOGPUCommandBufferCallbackErrorTimeout)

- ä½¿ç”¨æœ¬åœ°modelæ—¶ï¼Œä¸è¦ä½¿ç”¨global proxy, lmstudio/langchainå­˜åœ¨é—®é¢˜

- ä¸Šä¼ å¤§æ–‡ä»¶çš„åœºæ™¯
- åœºæ™¯1, ä¸Šä¼ 7M/400é¡µçš„pdf, Content exceeds 108800 tokens (108881). Truncating to head.
  - lmstudioçš„logæ„Ÿè§‰å¡ä½, Prompt processing progress: 0.0%
  - æ•´ä¸ªmacç³»ç»Ÿéƒ½æœ‰å¡æ­»çš„æ„Ÿè§‰

- 
- 
- 

## devops

```sh
# qdrant vector, 6333
cd ~/Documents/opt/compiled/qdrant && ./qdrant

# ArangoDB
docker run --name arangodb -p 8529:8529 -e ARANGO_ROOT_PASSWORD=11111111 arangodb/arangodb:latest
docker start arangodb

docker run -d --name zookeeper -p 2181:2181 \
  -e ZOOKEEPER_CLIENT_PORT=2181 \
  -e ZOOKEEPER_TICK_TIME=2000 \
  confluentinc/cp-zookeeper:7.9.0
docker start zookeeper

docker run -d --name kafka --link zookeeper:zookeeper -p 9092:9092 \
  -e KAFKA_BROKER_ID=1 \
  -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
  -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \
  -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT \
  -e KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT \
  -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
  confluentinc/cp-kafka:7.9.0
docker start kafka

# Starting Node.js Backend Service
cd backend/nodejs/apps
cp ../../env.template .env 
npm install
# 3000
npm run dev

# Setting Up Frontend
cd frontend
cp env.template .env
# 3001
npm run dev

# Starting Python Backend Services
cd backend/python
cp ../env.template .env
# uv venv
# source .venv/bin/activate #uvä¼šè‡ªåŠ¨æ¿€æ´»ï¼Œä»…é¦–æ¬¡éœ€è¦
# uv run python -m spacy download en_core_web_sm
uv add pip
uv run python -m spacy download en_core_web_sm

uv run python -c "import nltk; nltk.download('punkt')"

# é™åˆ¶pythonç‰ˆæœ¬ `requires-python = ">=3.10,<3.11"`
# Run each service in a separate terminal
# 8088
uv run python -m app.connectors_main
# 8091
uv run python -m app.indexing_main
# 8000
uv run python -m app.query_main
# 8081
uv run python -m app.docling_main

```

- `backend/nodejs/apps/src/libs/enums/db.enum.ts`: `export const ARANGO_DB_NAME = 'es'; export const MONGO_DB_NAME = 'es';`.
  - The issue is that `MONGO_DB_NAME` is hardcoded to 'es' instead of reading from your `MONGO_DB_NAME` environment variable.

- [Arango Installation Documentation](https://docs.arango.ai/arangodb/stable/operations/installation/)
  - ArangoDB requires systems with Little Endian byte order.
  - You can run ArangoDB on Linux directly (bare metal) or in containers.
  - Starting with version 3.12, ArangoDB packages for Windows and macOS are not provided anymore. You can use the official Docker images  instead.
# more

# docs

- PipesHub AI helps you quickly find the right information using natural language searchâ€”just like Google.
- The platform not only delivers the most relevant results but also shows where the information came from, with proper citations, using Knowledge Graphs and Page Ranking. 
- Beyond search, our platform allows enterprises to create custom apps and AI agents using a No-Code interface.

- 
- 
- 

# SurfSense
- pros
  - ~~æ”¯æŒè¶…å¤§æ–‡æ¡£~~(ç³»ç»Ÿé™åˆ¶500é¡µ)çš„åˆ†æ‰¹summarizing/chunking/embedding
    - å®æµ‹ä¸Šä¼ 400é¡µæ–‡æ¡£æ—¶, å¤„ç†1håä»ç„¶å‡ºç°å¼‚å¸¸ [lmstudio-llama-cpp] Error in predictTokens: The number of tokens to keep from the initial prompt is greater than the context length. 
  - æ”¯æŒé€šè¿‡UIæ¥é…ç½®llm provider url, ä½†ä¸åŒworkspaceçš„urléå…¨å±€, ä¸èƒ½å…±äº«éœ€è¦é‡æ–°é…ç½®ä¸€æ¬¡
    - é€šè¿‡LiteLLMçš„é…ç½®æ”¯æŒæœ¬åœ°Ollama/LMStudio

- cons(bugç‰¹åˆ«å¤š)
  - chunkçš„å†…å®¹æ˜¯summaryï¼Œè€Œä¸æ˜¯åŸæ–‡ï¼Œå‡†ç¡®åº¦ä¸å¤Ÿé«˜, (â“ åŸæ–‡ä¼¼ä¹æœªåœ¨ç³»ç»Ÿä¸­æ— æ³•æŸ¥çœ‹)
    - ä¸Šä¼ pdfåä¸æ”¯æŒæŸ¥çœ‹pdfåŸæ–‡, sourcesä¸­ä¿å­˜çš„æ•°æ®æ˜¯å¤„ç†è¿‡çš„æ–‡æœ¬å†…å®¹
  - citationç‚¹å‡»åæŸ¥çœ‹çš„æ˜¯chunkæ–‡æœ¬, ä½“éªŒä¸å¦‚pdfåŸæ–‡
  - chatä¸æ”¯æŒexport
  - ä¸Šä¼ ä¸­æ–‡pdfåï¼Œchunkçš„å†…å®¹æ˜¯è‹±æ–‡summaryï¼Œè®¾ç½®äº†workspaceçº§çš„è¯­è¨€ä¸ºä¸­æ–‡åchunkä»æ˜¯è‹±æ–‡
  - æœ‰æ—¶ä¸èƒ½ä»¥ç”¨æˆ·æé—®çš„è¯­è¨€å›å¤ç”¨æˆ·
  - æœ‰æ—¶æ•´ä¸ªå›å¤å†…å®¹citationçš„ç¼–å·éƒ½æ˜¯åŒ1å¤„, ç‰¹åˆ«æ˜¯å›å¤ä¸­æ–‡å†…å®¹æ—¶
  - ç‚¹å‡»chatåˆ—è¡¨åˆ‡æ¢èŠå¤©è®°å½•æ—¶ï¼Œå®¹æ˜“å‡ºç°aié‡æ–°regenerateå†…å®¹çš„é—®é¢˜
  - ~~chatèŠå¤©å¯¹è¯ä¸æ”¯æŒæµå¼è¾“å‡ºï¼Œä½“éªŒå¾ˆæ…¢~~, 20251205å·²æ”¯æŒ

- features
  - Multiple File Formats
  - Cited Answers
  - Local LLM Support
  - External Sources: Tavily/SearxNG, slack, linear, notion, github, discord...

## not-yet

- â“æ˜¯å¦ä½¿ç”¨äº†æœ¬åœ°embedding? 
  - ä¼¼ä¹ä½¿ç”¨äº†ï¼Œä½†é…ç½®ä½¿ç”¨æœ¬åœ°lmstudioçš„embeddingæ¨¡å‹é”™è¯¯

- ä¸Šä¼ æ–‡æ¡£åï¼Œ åŸæ–‡ä¼¼ä¹æœªåœ¨ç³»ç»Ÿä¸­æ— æ³•æŸ¥çœ‹
  - RAG Pipeline Implementation for PDFs
    - 1. Full Content Processing
      - PDFs are processed through ETL services (Unstructured/LlamaCloud/Docling) in file_processors.py:131
      - The complete extracted content is stored in the `Document.content` field
      - This content is then chunked using create_document_chunks from document_converters.py
    - 2. Dual-Level Embedding System
      - Document-level: Summary embedding stored in Document.embedding 
      - Chunk-level: Each chunk gets its own embedding in Chunk.embedding 
    - Hybrid Search Implementation
      - Both levels use hybrid search combining
      - Vector similarity search (embeddings)
      - Full-text search
      - Reciprocal Rank Fusion (RRF) for result ranking
  - Document-level search uses summary embeddings (more efficient for finding relevant documents)
    - Chunk-level search provides access to the full original content through individual chunks
  - The system supports both SearchMode. CHUNKS and SearchMode. DOCUMENTS
    - The default search mode might be set to DOCUMENTS which searches document summaries instead of chunk content
  - document.content: å­˜å‚¨ LLM ç”Ÿæˆçš„æ‘˜è¦ï¼Œä¸æ˜¯åŸå§‹å†…å®¹
    - document.embedding: å­˜å‚¨æ–‡æ¡£æ‘˜è¦çš„åµŒå…¥å‘é‡
  - document.chunks: å­˜å‚¨åŸå§‹å†…å®¹çš„åˆ†å—ï¼Œæ¯ä¸ªå—éƒ½æœ‰è‡ªå·±çš„åµŒå…¥å‘é‡

## draft

- ğŸ’¥ å¤§æ–‡æ¡£é¡µæ•°é™åˆ¶, ä¼¼ä¹å¯¹summaryå»ºç«‹indexæˆåŠŸäº†ï¼Œä½†æœªå¯¹chunkå»ºç«‹indexï¼Œ åŠŸèƒ½bugå¤š
  - Failed task process_file_upload: Page limit exceeded before processing
  - [lmstudio-llama-cpp] Error in predictTokens: The number of tokens to keep from the initial prompt is greater than the context length. Try to load the model with a larger context length, or provide a shorter input
- æœªå®Œå…¨å»ºç«‹indexçš„æƒ…å†µä¸‹ä»ç„¶å¯ä»¥èŠå¤©ï¼Œä½†èŠå¤©ä¼šå‡ºç°å¦‚ä¸‹è¶…é™çš„å¼‚å¸¸
  - The number of tokens to keep from the initial prompt is greater than the context length. Try to load the model with a larger context length, or provide a shorter input
  - Token indices sequence length is longer than the specified maximum sequence length for this model (340362 > 262144). Running this sequence through the model will result in indexing errors

- 
- 
- 
- 
- 
- 
- 

## dev-xp-surfsense

- lm studioçš„é…ç½®ä¸º `lm_studio`

- 
- 
