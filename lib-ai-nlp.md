---
title: lib-ai-nlp
tags: [ai, nlp]
created: 2023-03-03T08:06:19.133Z
modified: 2023-03-03T08:06:36.592Z
---

# lib-ai-nlp

# guide

- [NLPæ–°å® â€”â€”æµ…è°ˆPromptçš„å‰ä¸–ä»Šç”Ÿ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/399295895)
- [é€šä¿—æ˜“æ‡‚åœ°ç†è§£Prompt - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/436340881)
  - Promptæ˜¯ç»™inputåŠ çš„ä¸€æ®µæ–‡å­—æˆ–ä¸€ç»„å‘é‡ï¼Œè®©æ¨¡å‹æ ¹æ®inputå’Œå¤–åŠ çš„promptåšmasked language modeling (MLM)ã€‚
# nlp-products

# translation
- https://github.com/mozilla/firefox-translations
  - a webextension that enables client side translations for web browsers.
  - ä¸æ”¯æŒä¸­æ–‡
  - [Support Chinese translations](https://github.com/mozilla/firefox-translations/issues/583)

- https://github.com/jelmervdl/translatelocally-web-ext
  - https://translatelocally.com/
  - a web-extension that enables client side in-page translations for web browsers.
  - ä¸æ”¯æŒä¸­æ–‡
  - Differences from Firefox Translations
    - Uses models from https://github.com/browsermt/students
    - Translation engine and memory is shared among all tabs and webpages
  - [Google Chrome](https://github.com/jelmervdl/translatelocally-web-ext/issues/10)

- ä½¿ç”¨Googleç¿»è¯‘ï¼ˆTranslateï¼‰çš„ç¦»çº¿ç¿»è¯‘åŠŸèƒ½ï¼Ÿæœ‰å‰æï¼šä½ å¿…é¡»å…ˆåœ¨è”ç½‘çŠ¶æ€ä¸‹å°†éœ€è¦ä¸”æ”¯æŒç¦»çº¿ç¿»è¯‘çš„è¯­è¨€ä¸‹è½½ã€‚
  - è€Œç¦»çº¿ç¿»è¯‘çš„ç»“æœä¼šä¸è”ç½‘ç¿»è¯‘çš„ç»“æœå­˜åœ¨ç»“æœå·®è·ã€‚ç‰¹åˆ«æ˜¯ç¿»è¯‘åŒä¸€ä¸ªå­—è¯è¯­å¥ä¸‹

- å…¨æ–‡ç¿»è¯‘æ¯”è¾ƒæœŸå¾…ç±»ä¼¼firefoxåšçš„è¿™ç§ç¦»çº¿æœ¬åœ°ç¿»è¯‘
  - [Firefox Translations â€“ Get this Extension for ğŸ¦Š Firefox (en-US)](https://addons.mozilla.org/en-US/firefox/addon/firefox-translations/)

## translate-api

- https://github.com/LibreTranslate/LibreTranslate
  - https://libretranslate.com/
  - Free and Open Source Machine Translation API. 
  - Self-hosted, offline capable and easy to setup.
  - its translation engine is powered by the open source Argos Translate library.
  - ä¾èµ–flask
- https://github.com/argosopentech/argos-translate
  - Open-source offline translation library written in Python
  - Argos Translate uses OpenNMT for translations
  - æ”¯æŒä¸­æ—¥éŸ©
# blogs

## [Large language model - Wikipedia](https://en.wikipedia.org/wiki/Large_language_model)

- A large language model (LLM) is a language model consisting of a neural network with many parameters (typically billions of weights or more), trained on large quantities of unlabelled text using self-supervised learning. 
  - LLMs emerged around 2018 and perform well at a wide variety of tasks. 
  - This has shifted the focus of natural language processing research away from the previous paradigm of training specialized supervised models for specific tasks.

## [Sage Gpt-4 Claude+ ChatGPT Dragonflyäº”ä¸ªAIçš„å„è‡ªç‰¹ç‚¹ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/614720305)

- çœ‹æ¥å›ç­”å’Œä¹‹å‰èŠå¤©çš„å†…å®¹ç›¸å…³æ€§éå¸¸é«˜ï¼Œå¤åˆ¶ä½ çš„é—®é¢˜é—®äº†2éï¼Œåç»™æˆ‘çš„å†…å®¹æ¯ä¸ªæ ¼å­éƒ½å¾ˆä¸ä¸€æ ·

- [Claudeç”Ÿä¸é€¢æ—¶ï¼è°·æ­Œæƒ³æ‰¶æŒè‡ªå·±çš„OpenAIå®åœ¨å¤ªéš¾äº†\_è…¾è®¯æ–°é—»](https://new.qq.com/rain/a/20230318A03HF300)
  - ç”±å‰OpenAIå‘˜å·¥åˆ›åŠçš„æ–°å…¬å¸Anthropicæ‰“é€ çš„Claudeæ­£å¼å®£å¸ƒå¼€æ”¾ç”³è¯·ï¼Œä¹Ÿæ˜¯ä¸ºæ•°ä¸å¤šèƒ½è·ŸChatGPTæ‰“ä¸ªå¹³æ‰‹çš„æ¨¡å‹
  - Claudeæ¨¡å‹å¹¶æ²¡æœ‰å›¾åƒå¤„ç†çš„èƒ½åŠ›ï¼ŒAnthropicä¹Ÿæ²¡æœ‰è¿™æ–¹é¢çš„æŠ€æœ¯ç§¯ç´¯ï¼Œè€ŒOpenAIå‘å¸ƒçš„DALL-E 2åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸä¾ç„¶æ˜¯é¢†å†›è€…ã€‚
  - Claude å’Œ ChatGPT éƒ½ä¾èµ–äºå¼ºåŒ–å­¦ä¹ (RL)æ¥è®­ç»ƒåå¥½ï¼ˆpreferenceï¼‰æ¨¡å‹ï¼Œè¢«é€‰ä¸­çš„å›å¤å†…å®¹å°†åœ¨åç»­ç”¨äºæ¨¡å‹çš„å¾®è°ƒï¼Œåªä¸è¿‡å…·ä½“çš„æ¨¡å‹å¼€å‘æ–¹æ³•ä¸åŒã€‚
  - Quora é€šè¿‡äººå·¥æ™ºèƒ½èŠå¤©åº”ç”¨ç¨‹åºPoe å‘ç”¨æˆ·æä¾›Claudeçš„å¯¹è¯èƒ½åŠ›ã€‚
  - Claude ä¸ Notion åˆä½œä»¥æé«˜å·¥ä½œå’Œå­¦ä¹ åœºæ™¯ä¸­çš„ç”Ÿäº§åŠ›ã€‚

## [é€šå‘AGIä¹‹è·¯ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯ç²¾è¦ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/597586623)

- é€šç”¨äººå·¥æ™ºèƒ½ ï¼ˆAGIï¼ŒArtificial General Intelligenceï¼‰

### èŒƒå¼è½¬æ¢1.0: ä»æ·±åº¦å­¦ä¹ åˆ°ä¸¤é˜¶æ®µé¢„è®­ç»ƒæ¨¡å‹

- è¿™ä¸ªèŒƒå¼è½¬æ¢æ‰€æ¶µç›–çš„æ—¶é—´èŒƒå›´ï¼Œå¤§è‡´åœ¨æ·±åº¦å­¦ä¹ å¼•å…¥NLPé¢†åŸŸï¼ˆ2013å¹´å·¦å³ï¼‰ï¼Œåˆ°GPT 3.0å‡ºç°ä¹‹å‰ï¼ˆ2020å¹´5æœˆå·¦å³ï¼‰ã€‚
- åœ¨Bertå’ŒGPTæ¨¡å‹å‡ºç°ä¹‹å‰ï¼ŒNLPé¢†åŸŸæµè¡Œçš„æŠ€æœ¯æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè€ŒNLPé¢†åŸŸçš„æ·±åº¦å­¦ä¹ ï¼Œä¸»è¦ä¾æ‰˜äºä»¥ä¸‹å‡ é¡¹å…³é”®æŠ€æœ¯ï¼šä»¥å¤§é‡çš„æ”¹è¿›LSTMæ¨¡å‹åŠå°‘é‡çš„æ”¹è¿›CNNæ¨¡å‹ä½œä¸ºå…¸å‹çš„ç‰¹å¾æŠ½å–å™¨ï¼›ä»¥Sequence to Sequenceï¼ˆæˆ–å«encoder-decoderäº¦å¯ï¼‰+Attentionä½œä¸ºå„ç§å…·ä½“ä»»åŠ¡å…¸å‹çš„æ€»ä½“æŠ€æœ¯æ¡†æ¶ã€‚
- åœ¨è¿™äº›æ ¸å¿ƒæŠ€æœ¯åŠ æŒä¸‹ï¼ŒNLPé¢†åŸŸæ·±åº¦å­¦ä¹ çš„ä¸»è¦ç ”ç©¶ç›®æ ‡ï¼Œå¦‚æœå½’çº³ä¸€ä¸‹ï¼Œæ˜¯å¦‚ä½•æœ‰æ•ˆå¢åŠ æ¨¡å‹å±‚æ·±æˆ–æ¨¡å‹å‚æ•°å®¹é‡ã€‚å°±æ˜¯è¯´ï¼Œæ€ä¹ˆæ‰èƒ½å¾€encoderå’Œdecoderé‡Œä¸æ–­å åŠ æ›´æ·±çš„LSTMæˆ–CNNå±‚ï¼Œæ¥è¾¾æˆå¢åŠ å±‚æ·±å’Œæ¨¡å‹å®¹é‡çš„ç›®æ ‡ã€‚è¿™ç§åŠªåŠ›ï¼Œå°½ç®¡ç¡®å®ä¸æ–­å¢åŠ äº†æ¨¡å‹å±‚æ·±ï¼Œä½†æ˜¯ä»è§£å†³å…·ä½“ä»»åŠ¡çš„æ•ˆæœè§’åº¦çœ‹ï¼Œæ€»ä½“è€Œè¨€ï¼Œä¸ç®—å¾ˆæˆåŠŸï¼Œæˆ–è€…è¯´å’Œéæ·±åº¦å­¦ä¹ æ–¹æ³•ç›¸å¯¹ï¼Œå¸¦æ¥çš„ä¼˜åŠ¿ä¸ç®—å¤§ã€‚
- æ·±åº¦å­¦ä¹ ä¹‹æ‰€ä»¥ä¸å¤ŸæˆåŠŸï¼Œæˆ‘è®¤ä¸ºä¸»è¦åŸå› æ¥è‡ªäºä¸¤ä¸ªæ–¹é¢ï¼š
  - ä¸€æ–¹é¢æ˜¯æŸä¸ªå…·ä½“ä»»åŠ¡æœ‰é™çš„è®­ç»ƒæ•°æ®æ€»é‡ã€‚éšç€æ¨¡å‹å®¹é‡çš„å¢åŠ ï¼Œéœ€è¦é æ›´å¤§é‡çš„è®­ç»ƒæ•°æ®æ¥æ”¯æ’‘ï¼Œå¦åˆ™å³ä½¿ä½ èƒ½æŠŠæ·±åº¦åšèµ·æ¥ï¼Œä»»åŠ¡æ•ˆæœä¹Ÿåšä¸ä¸Šå»ã€‚è€Œåœ¨é¢„è®­ç»ƒæ¨¡å‹å‡ºç°ä¹‹å‰ï¼Œå¾ˆæ˜æ˜¾è¿™æ˜¯NLPç ”ç©¶é¢†åŸŸä¸€ä¸ªä¸¥é‡é—®é¢˜ï¼›
  - å¦å¤–ä¸€ä¸ªæ–¹é¢æ˜¯LSTMï¼CNNç‰¹å¾æŠ½å–å™¨ï¼Œè¡¨è¾¾èƒ½åŠ›ä¸å¤Ÿå¼ºã€‚æ„æ€æ˜¯å°±ç®—ç»™ä½ å†å¤šçš„æ•°æ®ä¹Ÿæ²¡ç”¨ï¼Œå› ä¸ºä½ ä¸èƒ½æœ‰æ•ˆåœ°å¸æ”¶æ•°æ®é‡Œè•´å«çš„çŸ¥è¯†ã€‚ä¸»è¦åº”è¯¥æ˜¯è¿™ä¸¤ä¸ªåŸå› ï¼Œé˜»ç¢äº†æ·±åº¦å­¦ä¹ åœ¨NLPé¢†åŸŸçš„æˆåŠŸçªå›´ã€‚

- Bert/GPTè¿™ä¸¤ä¸ªé¢„è®­ç»ƒæ¨¡å‹çš„å‡ºç°ï¼Œæ— è®ºåœ¨å­¦æœ¯ç ”ç©¶è§’åº¦çœ‹ï¼Œè¿˜æ˜¯å·¥ä¸šåº”ç”¨è§’åº¦æ¥çœ‹ï¼Œéƒ½ä»£è¡¨äº†NLPé¢†åŸŸçš„ä¸€ä¸ªæŠ€æœ¯é£è·ƒï¼Œå¹¶å¸¦æ¥äº†æ•´ä¸ªé¢†åŸŸç ”ç©¶èŒƒå¼çš„è½¬æ¢ã€‚
- è¿™ç§èŒƒå¼è½¬æ¢å¸¦æ¥çš„å½±å“ï¼Œä½“ç°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼š
  - é¦–å…ˆï¼Œæ˜¯éƒ¨åˆ†NLPç ”ç©¶å­é¢†åŸŸçš„è¡°é€€ä¹ƒè‡³é€æ­¥æ¶ˆäº¡ï¼›
  - å…¶æ¬¡ï¼ŒNLPä¸åŒå­é¢†åŸŸçš„æŠ€æœ¯æ–¹æ³•å’ŒæŠ€æœ¯æ¡†æ¶æ—¥è¶‹ç»Ÿä¸€ï¼Œåœ¨Bertå‡ºç°åä¸€å¹´å·¦å³ï¼ŒæŠ€æœ¯æ ˆåŸºæœ¬æ”¶æ•›åˆ°ä¸¤ç§æŠ€æœ¯æ¨¡å¼ä¸­ã€‚ 

- å½±å“ä¸€ï¼šä¸­é—´ä»»åŠ¡çš„æ¶ˆäº¡
  - å…¸å‹çš„ä¸­é—´ä»»åŠ¡åŒ…æ‹¬ï¼šä¸­æ–‡åˆ†è¯ã€è¯æ€§æ ‡æ³¨ã€NERã€å¥æ³•åˆ†æã€æŒ‡ä»£æ¶ˆè§£ã€è¯­ä¹‰Parserç­‰ï¼Œè¿™ç±»ä»»åŠ¡ä¸€èˆ¬å¹¶ä¸è§£å†³åº”ç”¨ä¸­çš„å®é™…éœ€æ±‚ï¼Œå¤§å¤šæ•°æ˜¯ä½œä¸ºé‚£äº›è§£å†³å®é™…éœ€æ±‚ä»»åŠ¡çš„ä¸­é—´é˜¶æ®µæˆ–è€…è¾…åŠ©é˜¶æ®µå­˜åœ¨çš„
  - ä½†æ˜¯è‡ªä»Bertï¼GPTå‡ºç°ä¹‹åï¼Œå…¶å®å°±æ²¡æœ‰å¿…è¦åšè¿™äº›ä¸­é—´ä»»åŠ¡äº†ï¼Œå› ä¸ºé€šè¿‡å¤§é‡æ•°æ®çš„é¢„è®­ç»ƒï¼ŒBertï¼GPTå·²ç»æŠŠè¿™äº›ä¸­é—´ä»»åŠ¡ä½œä¸ºè¯­è¨€å­¦ç‰¹å¾ï¼Œå¸æ”¶åˆ°äº†Transformerçš„å‚æ•°é‡Œï¼Œæ­¤æ—¶æˆ‘ä»¬å®Œå…¨å¯ä»¥ç«¯åˆ°ç«¯åœ°ç›´æ¥è§£å†³é‚£äº›æœ€ç»ˆä»»åŠ¡ï¼Œè€Œæ— é¡»å¯¹è¿™ç§ä¸­é—´è¿‡ç¨‹ä¸“é—¨å»ºæ¨¡ã€‚

- å½±å“äºŒï¼šä¸åŒç ”ç©¶æ–¹å‘æŠ€æœ¯è·¯çº¿çš„ç»Ÿä¸€
  - å¤§å¤šæ•°NLPå­é¢†åŸŸçš„ç ”å‘æ¨¡å¼åˆ‡æ¢åˆ°äº†ä¸¤é˜¶æ®µæ¨¡å¼ï¼šæ¨¡å‹é¢„è®­ç»ƒé˜¶æ®µ+åº”ç”¨å¾®è°ƒï¼ˆFine-tuningï¼‰æˆ–åº”ç”¨Zeroï¼Few Shot Promptæ¨¡å¼ã€‚
  - æ›´å‡†ç¡®åœ°è¯´ï¼ŒNLPå„ç§ä»»åŠ¡å…¶å®æ”¶æ•›åˆ°äº†ä¸¤ä¸ªä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹æ¡†æ¶é‡Œï¼š
  - å¯¹äºè‡ªç„¶è¯­è¨€ç†è§£ç±»ä»»åŠ¡ï¼Œå…¶æŠ€æœ¯ä½“ç³»ç»Ÿä¸€åˆ°äº†ä»¥Bertä¸ºä»£è¡¨çš„â€œåŒå‘è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒ+åº”ç”¨Fine-tuningâ€æ¨¡å¼ï¼›
  - è€Œå¯¹äºè‡ªç„¶è¯­è¨€ç”Ÿæˆç±»ä»»åŠ¡ï¼Œå…¶æŠ€æœ¯ä½“ç³»åˆ™ç»Ÿä¸€åˆ°äº†ä»¥GPT 2.0ä¸ºä»£è¡¨çš„â€œè‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆå³ä»å·¦åˆ°å³å•å‘è¯­è¨€æ¨¡å‹ï¼‰+Zero /Few Shot Promptâ€æ¨¡å¼ã€‚

### èŒƒå¼è½¬æ¢2.0: ä»é¢„è®­ç»ƒæ¨¡å‹èµ°å‘é€šç”¨äººå·¥æ™ºèƒ½ ï¼ˆAGIï¼ŒArtificial General Intelligenceï¼‰

- è¿™ä¸ªèŒƒå¼è½¬æ¢æ‰€æ¶µç›–çš„æ—¶é—´èŒƒå›´ï¼Œå¤§è‡´åœ¨GPT3.0å‡ºç°ä¹‹åï¼ˆ20å¹´6æœˆå·¦å³ï¼‰ï¼Œä¸€ç›´åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åº”è¯¥æ­£å¤„äºè¿™ä¸ªèŒƒå¼è½¬æ¢è¿‡ç¨‹ä¸­ã€‚
- ChatGPTæ˜¯è§¦å‘è¿™æ¬¡èŒƒå‹è½¬æ¢çš„å…³é”®èŠ‚ç‚¹ï¼Œä½†æ˜¯åœ¨InstructGPTå‡ºç°ä¹‹å‰ï¼Œå…¶å®LLMå¤„äºè¿™æ¬¡èŒƒå¼è½¬æ¢å‰çš„ä¸€ä¸ªè¿‡æ¸¡æœŸã€‚
- è¿‡æ¸¡æœŸï¼šä»¥GPT 3.0ä¸ºä»£è¡¨çš„â€œè‡ªå›å½’è¯­è¨€æ¨¡å‹+Promptingâ€æ¨¡å¼å æ®ç»Ÿæ²»åœ°ä½
- åœ¨é¢„è®­ç»ƒæ¨¡å‹å‘å±•çš„æ—©æœŸï¼ŒæŠ€æœ¯æ¡†æ¶æ”¶æ•›åˆ°äº†Bertæ¨¡å¼å’ŒGPTæ¨¡å¼è¿™ä¸¤ç§ä¸åŒçš„æŠ€æœ¯èŒƒå‹ï¼Œè€Œä¸”äººä»¬æ™®éæ›´çœ‹å¥½Bertæ¨¡å¼ä¸€äº›
- éšç€æŠ€æœ¯çš„ç»§ç»­å‘å±•ï¼Œä½ ä¼šå‘ç°ï¼Œç›®å‰è§„æ¨¡æœ€å¤§çš„LLMæ¨¡å‹ï¼Œå‡ ä¹æ¸…ä¸€è‰²éƒ½æ˜¯ç±»ä¼¼GPT 3.0è¿™ç§â€œè‡ªå›å½’è¯­è¨€æ¨¡å‹+Promptingâ€æ¨¡å¼çš„ï¼Œæ¯”å¦‚GPT 3ã€PaLMã€GLaMã€Gopherã€Chinchillaã€MT-NLGã€LaMDAç­‰ï¼Œæ²¡æœ‰ä¾‹å¤–ã€‚
- æˆ‘è®¤ä¸ºå¯èƒ½ä¸»è¦æºäºä¸¤ä¸ªåŸå› ã€‚
- é¦–å…ˆï¼ŒGoogleçš„T5æ¨¡å‹ï¼Œåœ¨å½¢å¼ä¸Šç»Ÿä¸€äº†è‡ªç„¶è¯­è¨€ç†è§£å’Œè‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡çš„å¤–åœ¨è¡¨ç°å½¢å¼ã€‚åœ¨T5æ¨¡å‹é‡Œï¼Œè¿™äº›è‡ªç„¶è¯­è¨€ç†è§£é—®é¢˜åœ¨è¾“å…¥è¾“å‡ºå½¢å¼ä¸Šå’Œç”Ÿæˆé—®é¢˜ä¿æŒäº†ä¸€è‡´ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¯ä»¥æŠŠåˆ†ç±»é—®é¢˜è½¬æ¢æˆè®©LLMæ¨¡å‹ç”Ÿæˆå¯¹åº”ç±»åˆ«çš„å­—ç¬¦ä¸²ï¼Œè¿™æ ·ç†è§£å’Œç”Ÿæˆä»»åŠ¡åœ¨è¡¨ç°å½¢å¼å°±å®ç°äº†å®Œå…¨çš„ç»Ÿä¸€ã€‚
- ç¬¬äºŒä¸ªåŸå› ï¼Œå¦‚æœæƒ³è¦ä»¥é›¶ç¤ºä¾‹æç¤ºè¯­ï¼ˆzero shot promptingï¼‰æˆ–å°‘æ•°ç¤ºä¾‹æç¤ºè¯­ï¼ˆfew shot promptingï¼‰çš„æ–¹å¼åšå¥½ä»»åŠ¡ï¼Œåˆ™å¿…é¡»è¦é‡‡å–GPTæ¨¡å¼ã€‚
  - ç°åœ¨å·²æœ‰ç ”ç©¶ï¼ˆå‚è€ƒï¼šOn the Role of Bidirectionality in Language Model Pre-Trainingï¼‰è¯æ˜ï¼š
  - å¦‚æœæ˜¯ä»¥fine-tuningæ–¹å¼è§£å†³ä¸‹æ¸¸ä»»åŠ¡ï¼ŒBertæ¨¡å¼çš„æ•ˆæœä¼˜äºGPTæ¨¡å¼ï¼›
  - è‹¥æ˜¯ä»¥zero shot/few shot promptingè¿™ç§æ¨¡å¼è§£å†³ä¸‹æ¸¸ä»»åŠ¡ï¼Œåˆ™GPTæ¨¡å¼æ•ˆæœè¦ä¼˜äºBertæ¨¡å¼ã€‚
  - è¿™è¯´æ˜äº†ï¼Œç”Ÿæˆæ¨¡å‹æ›´å®¹æ˜“åšå¥½zero shot/few shot promptingæ–¹å¼çš„ä»»åŠ¡ï¼Œè€ŒBertæ¨¡å¼ä»¥è¿™ç§æ–¹å¼åšä»»åŠ¡ï¼Œæ˜¯å¤©ç„¶æœ‰åŠ£åŠ¿çš„ã€‚è¿™æ˜¯ç¬¬äºŒä¸ªåŸå› ã€‚

- å¦‚æœç†è§£äº†ä¸Šè¿°é€»è¾‘ï¼Œå¾ˆå®¹æ˜“å¾—å‡ºå¦‚ä¸‹ç»“è®ºï¼šfew shot promptingï¼ˆä¹Ÿè¢«ç§°ä¸ºIn Context Learningï¼‰åªæ˜¯ä¸€ç§è¿‡æ¸¡æ—¶æœŸçš„æŠ€æœ¯ã€‚å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿæ›´è‡ªç„¶åœ°å»æè¿°ä¸€ä¸ªä»»åŠ¡ï¼Œè€Œä¸”LLMå¯ä»¥ç†è§£ï¼Œé‚£ä¹ˆï¼Œæˆ‘ä»¬è‚¯å®šä¼šæ¯«ä¸çŠ¹è±«åœ°æŠ›å¼ƒè¿™äº›è¿‡æ¸¡æœŸçš„æŠ€æœ¯ï¼ŒåŸå› å¾ˆæ˜æ˜¾ï¼Œç”¨è¿™äº›æ–¹æ³•æ¥æè¿°ä»»åŠ¡éœ€æ±‚ï¼Œå¹¶ä¸ç¬¦åˆäººç±»çš„ä½¿ç”¨ä¹ æƒ¯ã€‚
- è¿™ä¹Ÿæ˜¯ä¸ºä½•æˆ‘å°†GPT 3.0+Promptingåˆ—ä¸ºè¿‡æ¸¡æœŸæŠ€æœ¯çš„åŸå› ï¼ŒChatGPTçš„å‡ºç°ï¼Œæ”¹å˜äº†è¿™ä¸ªç°çŠ¶ï¼Œç”¨Instructå–ä»£äº†Promptingï¼Œç”±æ­¤å¸¦æ¥æ–°çš„æŠ€æœ¯èŒƒå¼è½¬æ¢ï¼Œå¹¶äº§ç”Ÿè‹¥å¹²åç»­å½±å“ã€‚
# discuss
- ## 

- ## If I were to learn about convolutional networks, residual networks, and automatic differentiation, how much do those relate to large language models?
- https://twitter.com/fabiospampinato/status/1637835546738212867
- Convolutions(å·ç§¯): they seem to be mainly used for vision, as they are kinda based on the assumption that closer together "pixels" are related, not much to do with LLMs. Although where vision ends and language starts is blurry.
- ResNet: I don't know how this works, but I don't think it's used in LLMs much, if at all, the main building block should be the "Transformer".
- Autodiff: I think that's basically how ~all these neural networks are trained, propagating inputs forward all the way, then at each layer from the output layer calculating the error, and nudging weights and biases toward what you want.
- After that I'm not sure, because that's more or less where I'm at at the moment ğŸ¤£ It may be worth checking out LSTMs, maybe RNNs... at some point transformers... generally trying to reproduce existing tiny things, building intuition, should be would be super useful for learning.

- ## I just used OpenAI embeddings to embed half a million tweets and ended up paying ... $3!!!
- https://twitter.com/fleuria__/status/1634125805717704704
- æœ¬ä¹‰æ˜¯è¯´æŠŠè¿‡å¾€åšæ³•ä¸­ç»´åº¦æˆåƒä¸Šä¸‡çš„ç‰¹å¾ï¼ˆæ¯ä¸ªè¯é¡¹ä»¥åŠæ•°åç§äººå·¥ç‰¹å¾æ„é€ ä¸€ä¸ªå·¨å¤§çš„one-hotï¼‰åƒé•¶é’»ä¸€æ ·åµŒå…¥(embed)ä¸€ä¸ªä½ç»´æµå½¢ï¼ˆæƒ³è±¡ä¸€ä¸ª2ç»´æè¿°çš„ä¸‰ç»´çƒé¢ï¼‰ï¼Œä¼˜åŒ–ç®—æ³•ä¼šæƒ³åŠæ³•è°ƒæ•´è¿™äº›ç‰¹å¾åœ¨çƒé¢ä¸ŠåµŒå…¥çš„ä½ç½®ã€‚æ¯ä¸ªè¯é¡¹å°±éœ€è¦å¯¹åº”å®ƒçš„ä½ç»´è¡¨ç¤ºï¼Œå­˜ä¸‹æ¥çš„ä¸œè¥¿å®è´¨ä¸Šå°±æ˜¯ä¸ªKVæŸ¥è¯¢è¡¨(embedding)ã€‚
  - åªä¸è¿‡å‘å±•åˆ°ç°åœ¨ä¸ä¸€å®šè¿™ä¸ªembeddingå°±çœŸçš„æ˜¯ä½ç»´åº¦äº†ï¼ˆ
  - å‡ åä¸ªè¯æŠŠembeddingè®¾æˆ256ç­‰ç­‰ä¹‹ç±»çš„ä¹Ÿå¾ˆå¸¸è§ï¼ˆ
  - ç”šè‡³æœ‰ç ”ç©¶è¡¨ç¤ºover-parameterizeæ‰æ˜¯æ­£é“ï¼ˆ
