---
title: lib-utils-codesandbox-community-runtime
tags: [codesandbox, community, docker]
created: 2024-05-12T17:19:42.440Z
modified: 2024-05-12T17:20:03.132Z
---

# lib-utils-codesandbox-community-runtime

# guide

# discuss-stars
- ## 

- ## 

- ## ğŸ§©ğŸš€ Today we open sourced Localsandbox, a lightweight library that gives agents a sandboxed filesystem with safe bash and code execution. _202601
- https://x.com/vimota/status/2014009354614132912
  - [Localsandbox: A Lightweight Agent Sandbox _202601](https://coplane.com/perspectives/localsandbox)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Your Agent                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Localsandbox API              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ just-bashâ”‚ Pyodide  â”‚                   â”‚
â”‚  (bash)  â”‚ (python) â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚
â”‚      AgentFS        â”‚    SQLite DB      â”‚
â”‚  (virtual filesystem)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- As part of that, I wanted to cover the recent history of agent harnesses and why we might want to use agent sandboxes at all.
- We can think of the evolution of agent techniques in three stages:
- Prompt engineering and RAG
  - Before  there were agents, tools, harnesses, and sandboxes, there were just  LLMs with a simple API: text in â†’ text out.
  - You'd pass in some context  and instructions and the LLM would generate a response. 
  - Products that  used this needed to carefully craft the model's prompt, often using  techniques like RAG to inject the relevant context. It wasn't long until  folks found it limiting to have to provide all the information the LLM needed upfront.
- Tool calling (the start of "agents")
  - Why couldn't the LLM request the information it needed? And even moreâ€”why can't the LLM take action based on my request? That quickly led to the  idea of tools: let's inform the LLM that it has a set of methods it could call if it needs to fetch information or take action to fulfill the request.
  - But LLMs fundamentally can only output text, so calling these tools, of  course, needs to be done through text as the interface. We developed  techniques for how the LLM could signal to the system that it wanted to  make a tool callâ€”a (typically JSON) spec that agents could use to call a  tool and pass parameters, which the harness could parse, perform the  action, and then return the result to the agent.
  - This was really powerful. We could now design custom tools that an agent  would need to perform a particular task. We could wrap APIs, call  databases, perform computation, and even allow the agent to request human-in-the-loop interventions.
  - This is also how MCP emerged: why have every AI engineer write the same  tools wrapping APIs when we can expose APIs through a standard that  automatically generates tools the agents know how to call. Security and  privacy concerns emerged ("waitâ€”the agent can call these APIs to do  something I didn't intend!") but since you were writing the tools  yourself, you could largely control the blast radius during tool development.
- Filesystem and code execution
  - More improvements emerged: instead of writing custom tools for file search, just let the agent run grep; rather than testing the program and pasting in errors, let the agent  run the linting, tests, and the code directly. 
  - Giving the coding agent the ability to run bash meant that it could be the developer, including  writing and running its own tools.
  - You no longer needed to think ahead of every possible tool the agent could  needâ€”the agent could come up with those tools just-in-time. Now... that  also means it could run `rm -rf /` on your system and cause a lot of trouble, so coding agents make sure  to ask you for permission any time they want to run something (unless you're brave enough to use YOLO mode).
  - From these simple primitives and the models being trained on their usage, emerged incredibly powerful coding agents that could run for hours to  complete long horizon tasks. 
  - But why should they be limited to software  development? Couldn't other tasks benefit from these powerful harnesses?
- Beyond software development
  - It turns out thatâ€”yes!â€”all sorts of tasks benefit from the agent being  able to read and write files, including programs that it could then run.
  - This lets agents be armed with just a few very general tools, rather  than requiring custom-designed tools per task or domain.
- The sandboxing problem
  - since coding agents ran natively on the  user's machine, the users were responsible for the risk of unsafe  operations. 
  - Prior to AI, software had a very similar problem: untrusted code execution  and isolation. The browser (i.e., V8, WASM), containers (i.e., Docker), and sandboxed VMs (i.e., gVisor, Firecracker) were different sandboxing  solutions to this class of problems. 
  - The same technologies can be used  to sandbox agents, and various providers are already focusing on this  particular space: Modal, Azure Container Apps Dynamic Sessions, Google Cloud Run, and AWS AgentCore Code Interpreter.
- Localsandbox
  - Localsandbox is a lightweight alternative to these that can be used if you're  developing locally or don't need the full guarantees of these providers.
  - Its foundation is AgentFS, a virtual  filesystem backed by SQLite, making it portable and easy to backup and resume.
  - On top of that, we get the ability to run bash on the filesystem through @vercel 's just-bash projectâ€”a TypeScript interpreter of a subset of bash over a virtual filesystem like AgentFS.
  - Finally, we want the agent to be able to write and execute code, so we use Pyodide (run through Deno) to execute a WASM-sandboxed version of Python with the AgentFS filesystem mounted to it so that the Python code can read  and write to files.
  - All of this together gives you the ability to expose a pseudo-VM to your  agent in a lightweight and highly portable way.

- ## âœ¨ I'm introducing my holiday project: just-bash _20251228
- https://x.com/cramforce/status/2004992618913251786
  - just-bash is a pretty complete implementation of bash in TypeScript designed to be used as a bash tool by AI agents. Because it turns out agents love exploring data via shell scripts, even beyond coding.
  - It comes with grep, sed, awk and the 99th percentile features that an agent like Claude Code or Cursor would use. In fact, Claude Code can use it for secure bash execution.
  - A bash-tool for @aisdk
  - An overlay filesystem to feed files to your agent securely
  - A Vercel Sandbox compatible API, so you can quickly upgrade to a real VM if you need to run binaries
  - It was essentially entirely written by Opus 4.5. Coding agents love bash and they are good at reproducing it. They are also great at text-book recursive descent parsers and AST tweet-walk interpreters. 
  - It has cURL already
- why is this better than letting the agents just use bash?
  - If it is your agent that you operate in the cloud then you don't need extra VMs per agent.
  - For the local case: it provides a sandbox so your agent doesn't get prompt injected to mine Bitcoin and steal your passwords

- when does your agent actually need bash logic versus a simpler structured tool? I've found the shell flexibility becomes a debugging nightmare once my team tries to maintain it three months later.
  - don't think there is a steadfast rule. It generally works well when you see a lot of emergent behavior. That is both more powerful and inherently harder to maintain. Trade-offs

- While the filesystem is undoubtedly a core element of advanced AI agents, a Bash-like wrapper isn't enough if you need a remote AI agent that can interact with external apps, services, and devices, including the need for OAuth or simply going beyond the context of a single user. - in other words, where do you think limitations are here? 
  - Daytona or E2B seem to be aiming to address "bash is all you need" at a production scale. What do you think about them? These sandboxes can easily add significantly to your bills, especially when considering AI agents that already require many tokens to be paid for.
- There is a class of very advanced agents that don't actually need much else. That agent has exactly 2 tools: - the bash tool - an executeSQL tool
  - "All you need" is a metaphor not meant literally. just-bash comes with an @aisdk tool to integrate it into a bigger picture.
  - It also comes with a Vercel Sandbox compatible API. So, if you later notice that you need more power you can switch to a real VM with no refactoring.
- In my experience:
  - agents that do analysis and then use custom tools actually just need just-bash
  - the VMs are mostly needed if you actually want to run sophisticated agent-developed programs
  - The latter is a currently important category but your everyday business agent isn't doing that

- If you are looking for a sandbox which runs locally on your mac, check coderunner out which is based on apple containers, runs your AI generated code on your machine but safely inside a VM 
- https://github.com/instavm/coderunner /apache2/202512/python
  - CodeRunner is an MCP (Model Context Protocol) server that executes AI-generated code in a sandboxed environment on your Mac using Apple's native containers.

- Reminds me of some of your former projects like ampproject/worker-dom, reimplement something to use in a sandbox and make it safe. Interactive email // bash access to agents use cases unlocked. Used to be so tedious and now so fast, perfect use case for agents!

- Any way to combine this with something like pyodide and provide python scripting support along with bash support?
  - Yeah, that should just work. The thing you may need to investigate is how this python interacts with the filesystem. But in terms of running ad hoc scripts, it should just work

- This is awesome. I was working on something that helps creating/wrapping CLI commands as tools.

- ## ğŸ†š Manus æŠŠ Ubuntu Linux æ²™ç®±ç¯å¢ƒæ”¾åœ¨äº‘ç«¯ï¼ŒClaude æŠŠ Code ç¯å¢ƒæ”¾åœ¨æœ¬åœ°ã€‚è¿™ä¸¤ä¸ªä¸åŒçš„æŠ€æœ¯è·¯å¾„åœ¨äº§å“ä½“éªŒä¸Šå¸¦æ¥äº†éå¸¸å¤§çš„å·®åˆ«ã€‚
- https://x.com/xiao_zcloak/status/2006280637922394241
  - Manus é‚£è¾¹ï¼ŒåŸºæœ¬ä¸€ä¸ªè¦æ±‚æè¿‡å»å°±å¯ä»¥èµ°äººäº†ï¼Œä¸€ä¼šå›æ¥çœ‹ç»“æœã€‚å› ä¸º AI æœ‰æ“ä½œæ²™ç®±çš„å…¨éƒ¨æƒé™ã€‚
  - è€Œ Claude é‚£è¾¹åˆ™éœ€è¦ä½ ä¸€ç›´é™ªç€å®ƒï¼Œåšå„ç§æƒé™è®¸å¯ï¼šâ€œæˆ‘å¯ä»¥è®¿é—®è¿™ä¸ªè·¯å¾„å—ï¼Ÿæˆ‘å¯ä»¥ä¸‹è½½è¿™ä¸ªè„šæœ¬å—ï¼Ÿæˆ‘å¯ä»¥æ‰§è¡Œè¿™ä¸ªç¨‹åºå—ï¼Ÿâ€ 

- Claude å…¶å®æœ‰ä¸¤ä¸ªæ¨¡å¼è€Œä¸”å¾ˆæ— ç¼ 
  - ä½ çœ‹åˆ°çš„Claude Code åªæ˜¯ä¸€éƒ¨åˆ† 
  - åœ¨ Claude Desktop è¿™è¾¹å°±æ˜¯äº‘ç«¯çš„è™šæ‹Ÿæœºâ€”ä½ å¯ä»¥ç”¨å®ƒå…ˆ research ç„¶åç”Ÿæˆ PDF æŠ¥å‘ŠéªŒè¯ä¸€ä¸‹ï¼šå®ƒä¼šå…ˆå†™ä¸ª Python ç„¶åè¿è¡Œè¿™ä¸ª Python ç”Ÿæˆ pdf. è¿™éƒ½æ˜¯äº‘ç«¯çš„æ“ä½œ
  - ç„¶è€ŒåŒæ—¶å®ƒä¹Ÿå¯ä»¥è°ƒç”¨æœ¬åœ° MCP. æ¯”å¦‚åœ¨æœ¬åœ°è°ƒæ•´ MacOS ä¸€äº›è®¾ç½®â€”å®ƒåœ¨æœ¬åœ°å†™ä¸ª script ç„¶åäº¤ç»™MCP è¿è¡Œ
  - ç°é˜¶æ®µå¦‚æœè°ˆå–ä»£ç™½é¢†å·¥ä½œçš„è¯ï¼Œ Claude åœ¨æŠ€æœ¯æ•´åˆä¸Šæœ€æœ‰æ¦‚ç‡çš„ ï¼Œè€Œ Gemini åœ¨ç”Ÿæ€ä¸Šæ˜¯æœ€æœ‰æ¦‚ç‡çš„ã€‚å…¶ä»–ç©å®¶è§£å†³æ–¹æ¡ˆéƒ½æ²¡æœ‰åœ¨ distribution + comprehensiveness Pareto frontier.

- é•¿è¿œçœ‹ï¼Œäº‘ç«¯æ²™ç®±ä»£ç†ï¼ˆManusï¼‰å’Œæœ¬åœ°ä»£ç ç¯å¢ƒï¼ˆClaude Code è¿™ä¸€ç±»ï¼‰ç¡®å®éƒ½ä¼šé•¿æœŸå…±å­˜ï¼Œè€Œä¸”ä¼šè¶Šæ¥è¶Šæ¸…æ™°åœ°åˆ†å·¥ã€‚
  - æœªæ¥å¯èƒ½æ›´æˆç†Ÿçš„å½¢æ€ï¼šæ··åˆç¼–æ’

- github copilot agentè°ƒç”¨claudeçš„modelä¹Ÿæ˜¯ä¸¢äº‘ç«¯æ²™ç®±ï¼Œç„¶åé€šè¿‡æprçš„æ–¹å¼è®©ç”¨æˆ·ç¡®è®¤æ‰§è¡Œç»“æœã€‚ è¿™ä¸ªåŠŸèƒ½æ ¹æœ¬å°±ä¸æ˜¯manusç‹¬æœ‰çš„ï¼Œç®—ä¸ä¸Šä»€ä¹ˆå¾ˆç‹¬ç‰¹çš„ä¸œè¥¿

- ## Why are VMs still not as convenient as Docker containers to work with?
- https://x.com/eatonphil/status/1910391779834372356
  - Installing Docker is hard (podman is easier) but once you have it it's this easy standard.
  - Whereas for VMs it's a hodgepodge across platforms that no-one has built a good frontend for.
- Multipass is the closest I've seen to a Docker-like VM experience. Unfortunately it only supports Ubuntu as a guest OS, but if you're ok with Ubuntu (I usually am!) it's pretty good.

- We tried so hard to buy in to the HashiCorp Packer story. It's a nightmare. We ended up building snapshotting in-house.
  - We(cloudflare) avoid containers (rather treat them as a cloud target, similar to an AWS/GCP image) because it's an unnecessary layer for what we do.

- VM DX is so far off Docker/OCI, and I don't know if there is a fundamental reason why. At least one problem is the mess of machine image formats. Here's a compatibility table

- historically, getting files in and out of a VM, modifying networking settings, building images, were much harder with VMs because of the hard isolation.

- VMs contain an entire system, whereas OCI is focused on running a single process.

- ## ğŸ†šï¸ğŸ³ [Docker vs. containerd vs. Nabla vs. Kata vs. Firecracker and more | by Benjamin | Medium _202303](https://benriemer.medium.com/docker-vs-containerd-vs-nabla-vs-kata-vs-firecracker-and-more-108f7f107d8d)
- Containerd is a container runtime originally part of Docker but has since been spun off into its project. 
  - It provides a powerful and flexible way to manage containers at scale, with features such as image distribution, container execution, and network and storage management. 
  - In addition, Containerd is designed to be lightweight and fast, making   it a good choice for large-scale deployments.

- Nabla is a secure container runtime designed to provide isolation and protection for containerized workloads. 
  - It uses a microkernel-based architecture to minimize attack surfaces and prevent container escape. 
  - As a result, Nabla is ideal for running untrusted code or sensitive workloads that require high security

- Kata Containers is a container runtime that uses hardware virtualization to provide substantial isolation between containers. 
  - It combines the lightweight and fast nature of containers with the security and isolation of virtual machines. 
  - As a result, Kata Containers is a good choice for running multi-tenant applications or workloads that require strict isolation.

- Firecracker is a lightweight, fast virtual machine manager designed to run serverless workloads. 
  - It uses a minimalist approach to virtualization, creating microVMs that are lightweight and efficient. 
  - Firecracker is ideal for running serverless functions in a secure and isolated environment.

- Whether you need scalability, security, isolation, or efficiency, a container runtime can meet your needs. By understanding the strengths of each runtime, you can choose the one best suited for your specific use case.

- ## ğŸ†šï¸ [My VM is lighter (and safer) than your container (2017) | Hacker News _202209](https://news.ycombinator.com/item?id=32764501)
- stuff like Kata Containers exist exactly because containers alone is not enough. WebAssembly on the cloud is only re-inventing what other bytecode based platforms have been doing for decades, but VC need ideas to invest into apparently.

- I would say that firecracker VMs are not more lightweight than Linux containers.
  - Linux containers are essentially the separation of Linux processes via various namespaces e.g. mount, cgroup, process, network etc. Because this separation is done by Linux internally there are not that many overheads.
  - VMs provide a different kind of separation one that is arguably more secure because it is backed up hardware -- each VM thinks it has the whole hardware to itself. When you switch between the VM and the host there is quite a heavyweight context switch (VMEXIT/VMENTER in Intel parlance). It can take a long time compared to just the usual context switch from one Linux container (process) to another host (process) or another Linux container (process).
  - But coming back to your point, no firecracker VMs are not lighter/lightweight than a Linux container. They are quite heavyweight actually. But the firecracker VMM is probably the most nimble(è¿…é€Ÿçš„ï¼›æ•æ·çš„) of all VMMs.

- This reminds me: in 2015 I went to Dockercon and one booth that was fun was VMWare's. Basically they had implemented the Docker APIs on top of VMWare so that they could build and deploy VMs using Dockerfiles, etc. I've casually searched for it in the past and it seems to not exist anymore. 
  - For me, one of the best parts of Docker is building a docker-image (and sharing how it was done via git). It would be cool to be able to take the same Dockerfiles and pivot them to VMs easily.
- https://github.com/weaveworks/ignite can apparently do this. It looks exciting!

- Isn't that essentially what Vagrant and Vagrantfiles do?
  - Vagrant images are always huge. But docker images are small. You can deliver and share docker images with ease.
  - Vagrant never targeted production so not much money/resource/interest invested?

- ğŸ› The issue with unikernels and things like Firecracker are that you can't run them on already-virtualized platforms
  - (Yes, I know about Firecracker-in-Docker, but I mean real production use)
- This is a limitation of whatever virtualized instance you're running on, not Firecracker itself. Firecracker depends on KVM, and AWS EC2 virtualized instances don't enable KVM. But not all virtualized instance services disable KVM.
  - Google Cloud, for instance, allows nested virtualization, IIRC.
  - Azure and Digital Ocean allowed nested virt as well!
- ğŸ’¡ Firecracker runs in hosts that support nested virtualization. GCP and Github Codespaces do, but unfortunately EC2 and Macs don't.

- Firecrackers being originally based on a fork of crosvm might even benefit from this if they ever decide to play around this space, but also it'd need to be enabled on the platform level.

- Not surprising that VMs running unikernels are as nimble as containers, but not quite useful either, at least in general. Much easier to just use a stock docker image.

- Kubernetes says no... The article is light on detail. Containers and VMs have different use cases. If you self host lightweight VMs is likely the better path, however, once you in the cloud most managed services only provide support for containers.
  - You can also use a firecracker runner in k8s to wrap each container in a VM for high isolation and security.
- There is a huge difference running on VMs that you have zero access to, and actually owning your own VM infrastructure. Yes AWS Lambda runs on Firecracker, however, it could as well running on a FireCheese VM platform and you would be none the wiser, unless AWS publishes this somewhere.
  - There is a huge difference running on VMs that you have zero access to, and actually owning your own VM infrastructure. Yes AWS Lambda runs on Firecracker, however, it could as well running on a FireCheese VM platform and you would be none the wiser, unless AWS publishes this somewhere.

- Containers are great because they are small and fast, low overhead etc. And that is why they replaced virtual machines. This article however proves that this is also achievable with virtual machines.
- VMs don't need a full OS. You can run a single process directly from the kernel with no init system or other userland
- VMs can have private networks between each other just as containers do. That's pretty much what EC2 is about.

- 
- 
- 

- ## ğŸ†šï¸ [è™šæ‹ŸåŒ–è½¯ä»¶Dockerã€Wineã€Qemuã€KVMæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/540942002)
- ä½ æŠŠæ¨¡æ‹Ÿå’Œè™šæ‹Ÿæ··æ·†æ‰ï¼ŒOSçº§åˆ«å’Œè½¯ä»¶çº§åˆ«ä¹Ÿæ··æ·†äº†ï¼Œå½“ç„¶å‚»å‚»åˆ†ä¸æ¸…äº†ã€‚
- Dockerä¸å­˜åœ¨æ¨¡æ‹Ÿï¼Œä¹Ÿä¸å­˜åœ¨è™šæ‹Ÿã€‚
  - å®ƒæ˜¯éš”ç¦»å·¥å…·ï¼Œéœ€è¦è¿è¡Œç‰¹å®šçš„imageï¼Œå°±æ˜¯ä½ æƒ³è¿è¡Œçš„è½¯ä»¶å¤§é›†åˆã€‚ä¸åŒimageå¸®ä½ éš”ç¦»å¼€äº’ç›¸ä¸å¯è§ã€‚ 
  - è¿™ä¸€åˆ‡è®©ä½ è¯¯ä»¥ä¸ºæ˜¯OSçº§åˆ«ï¼Œ**å…¶å®æ˜¯è½¯ä»¶çº§åˆ«**ã€‚
- **Wineä¹Ÿæ˜¯è½¯ä»¶çº§åˆ«çš„æ¨¡æ‹Ÿ**ï¼Œåªæ¨¡æ‹Ÿwindowsï¼Œè®©Linuxä¹Ÿèƒ½è¿è¡Œwinç¨‹åºï¼Œ
  - è¿™ä¸ªå’Œå¾®è½¯æä¾›WSLæä¾›windowsä¸Šæ¨¡æ‹ŸLinuxé“ç†æ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯æ­£å¥½ç›¸åã€‚
- KVMæ˜¯è™šæ‹Ÿç¡¬ä»¶çš„ï¼Œéå¸¸åº•å±‚ã€‚ä½†æ˜¯å®ƒæ˜¯å†…æ ¸çš„ä¸€éƒ¨åˆ†ï¼Œæ‰€ä»¥å¼ºç»‘Linuxå¹³å°ã€‚
  - Qemuåœ¨ä¸­å±‚é…åˆKVMä½¿ç”¨ã€‚ä¸Šå±‚çš„å®¢æˆ·OSå¯ä»¥å®Œå…¨ä¸çŸ¥é“è‡ªå·±åœ¨å“ªï¼Œæ˜¯è°ã€‚
- **KVMå’ŒQemuéƒ½æ˜¯ç”¨åœ¨OSçº§åˆ«çš„æ¨¡æ‹Ÿæˆ–è€…è™šæ‹Ÿ**ã€‚ä»–ä»¬éƒ½æ”¯æŒã€‚ åˆ°åº•æ˜¯å“ªä¸ªï¼Œå†³å®šåœ¨äºä½ æ€ä¹ˆä½¿ç”¨ä»–ä»¬ã€‚
  - ä½ å¯ä»¥å•ç‹¬ä½¿ç”¨Qemuè¿›è¡ŒOSæ¨¡æ‹Ÿã€‚è¿™ä¸ªæ—¶å€™å°±æ— æ‰€è°“åº•å±‚æ˜¯ä»€ä¹ˆæ“ä½œç³»ç»Ÿäº†ã€‚ä½†æ˜¯å®ç°ä¸äº†å®Œå…¨è™šæ‹ŸåŒ–ã€‚
- ğŸ‘‰ğŸ» Linuxç³»åˆ—ï¼Œkvmè´Ÿè´£cpuè™šæ‹ŸåŒ–+å†…å­˜è™šæ‹ŸåŒ–ï¼Œå®ç°äº†cpuå’Œå†…å­˜çš„è™šæ‹ŸåŒ–ï¼Œä½†kvmä¸èƒ½æ¨¡æ‹Ÿå…¶ä»–è®¾å¤‡ï¼›qemuæ˜¯æ¨¡æ‹ŸIOè®¾å¤‡ï¼ˆç½‘å¡ï¼Œç£ç›˜ï¼‰ï¼ŒkvmåŠ ä¸Šqemuä¹‹åå°±èƒ½å®ç°çœŸæ­£æ„ä¹‰ä¸ŠæœåŠ¡å™¨è™šæ‹ŸåŒ–ã€‚å› ä¸ºç”¨åˆ°äº†ä¸Šé¢ä¸¤ä¸ªä¸œè¥¿ï¼Œæ‰€ä»¥ä¸€èˆ¬éƒ½ç§°ä¹‹ä¸ºqemu-kvmã€‚
  - libvirtåˆ™æ˜¯è°ƒç”¨kvmè™šæ‹ŸåŒ–æŠ€æœ¯çš„æ¥å£ç”¨äºç®¡ç†çš„ï¼Œç”¨libvirtç®¡ç†æ–¹ä¾¿ï¼Œç›´æ¥ç”¨qemu-kvmçš„æ¥å£å¤ªç¹çã€‚
  - å¯¹åº”Windowsçš„ï¼Œå¤§æ¦‚å°±æ˜¯Hyper-Vï¼Œè¿˜æœ‰ä¸€ä¸ªå°±æ˜¯å¼€æºçš„VirtualBoxï¼Œæ¯”è¾ƒè½»é‡ï¼›å¦å¤–ä¸€ä¸ªç‹¬ç«‹å‘å±•çš„å°±æ˜¯VMWare ESXiã€‚
- wineå’Œwsl1å·®åˆ«è¿˜æŒºå¤§çš„ï¼Œwslæ˜¯è‡ªå·±æäº†å¥—å’Œwindowsç³»ç»Ÿè°ƒç”¨å¹³çº§çš„linuxç³»ç»Ÿè°ƒç”¨ï¼Œwineæ˜¯åŠ äº†ä¸ªapiä¸­é—´å±‚ï¼Œç”¨çš„ç³»ç»Ÿè°ƒç”¨è¿˜æ˜¯linuxè‡ªå·±çš„ã€‚

- ğŸ‘‰ğŸ» å‡†ç¡®æ¥è¯´ï¼Œqemuæ˜¯æ¨¡æ‹Ÿå™¨ï¼Œæ¢å¥è¯è¯´qemuæ¨¡æ‹Ÿçš„ç¯å¢ƒé‡Œä¸ä¾èµ–ç¡¬ä»¶å¹³å°ã€‚
  - kvmæ˜¯è™šæ‹Ÿæœºï¼Œç”¨æ¥åœ¨æŸä¸€ä¸ªæ“ä½œç³»ç»Ÿä¸‹ï¼Œæä¾›ä¸€ä¸ªè™šæ‹Ÿçš„ç¡¬ä»¶ç¯å¢ƒï¼ˆCPUå’Œå†…å­˜ï¼‰

- Dockerä¸æ˜¯è™šæ‹ŸåŒ–è½¯ä»¶ï¼Œå®ƒæ˜¯ä¸€ä¸²cgroup çš„å‘½ä»¤é“¾ï¼Œæ”¶çª„ä»¥æ­¤è¿è¡Œçš„è½¯ä»¶çš„æƒé™
- Wineæ˜¯ä¸€å¥—æä¾›Windows API çš„è½¯ä»¶
- Qemuæ˜¯ç”µè„‘å¤„ç†å™¨æ¨¡æ‹Ÿå™¨ï¼ŒåŒ…æ‹¬è€Œä¸é™äºx86, arm, mips...ï¼Œè€Œä»¥æ­¤åŸºç¡€ä¸Šæ„æˆçš„ç”µè„‘æ¨¡æ‹Ÿå™¨
- KVMä¸æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå™¨ï¼Œæ˜¯ä¸€ä¸ªå¯ä»¥è®©ä¸Šé¢è½¯ä»¶ç»•è¿‡kernel è®¿é—®ç¡¬ä»¶(ä¸»è¦æ˜¯cpu å’Œpci)çš„kernel æ¨¡ç»„

- Dockerä¸èƒ½è¿è¡Œwindowsç¨‹åºå§ï¼Œä¹Ÿå¹¶ä¸æ˜¯å®Œå…¨çš„è™šæ‹ŸåŒ–ï¼Œåªæ˜¯å®ç°äº†èµ„æºé™åˆ¶å’Œéš”ç¦»ã€‚
- Qemuæ˜¯æ¨¡æ‹Ÿå™¨ï¼ŒKVMæ˜¯å†…æ ¸æ¨¡å—ï¼Œä¸€èˆ¬æ˜¯ä¸€èµ·ç”¨æ¥è¾¾åˆ°è™šæ‹ŸåŒ–çš„åŠŸèƒ½ã€‚

- åœ¨Linuxä¸Šçš„Dockerå’ŒWineä¸æä¾›ä»»ä½•è™šæ‹ŸåŒ–ã€‚
  - Dockeræ˜¯ä¸€æ¬¾å®¹å™¨è½¯ä»¶ï¼Œé‡Œé¢è·‘çš„ç¯å¢ƒæ˜¯æ²¡æœ‰å†…æ ¸çš„ï¼Œä¹Ÿä¸è¿è¡Œåœ¨è™šæ‹Ÿæœºå™¨ä¸Šã€‚
  - Wineæ˜¯ä¸€æ¬¾ç”¨äºåœ¨Linuxä¸Šè¿è¡ŒWindowsç¨‹åºçš„è¿è¡Œæ—¶ï¼Œæ—¢ä¸æä¾›è™šæ‹ŸåŒ–ä¹Ÿä¸æä¾›å®¹å™¨åŒ–ï¼ˆé™¤éä½ ä½¿ç”¨åŒ…å«å®¹å™¨åŠŸèƒ½çš„ç¬¬ä¸‰æ–¹wineå‘è¡Œç‰ˆï¼‰ã€‚
- KVMæ˜¯Linuxçš„ä¸€ä¸ªå†…æ ¸æ¨¡å—ï¼Œæä¾›å†…æ ¸çº§çš„è™šæ‹ŸåŒ–æ”¯æŒã€‚
- Qemuæ˜¯ä¸€æ¬¾è™šæ‹Ÿæœºè½¯ä»¶ã€‚åœ¨Linuxä¸Šé€šå¸¸å’ŒKVMé…åˆä½¿ç”¨ã€‚

- ## ğŸ¤” è¯·é—® StackBlitz å’Œ CodeSandBox è¿™ç±» Web IDE çš„æŠ€æœ¯æ ˆæ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿ
- https://www.zhihu.com/question/279268026/answers/updated
- æˆ‘æƒ³æä¾›ä¸€ä¸ªæ€è·¯ï¼šåˆ©ç”¨ es module + service worker + browser file system + æµè§ˆå™¨ç«¯å³æ—¶ç¼–è¯‘å®ç°ä¸ä¾èµ–æœåŠ¡ç«¯èƒ½åŠ›çš„ Web IDE.
  - ç±»ä¼¼äº CodeSandBox çš„æ–¹æ¡ˆæ˜¯åœ¨æµè§ˆå™¨ç«¯å®ç°äº†ä¸€å¥—å…¼å®¹äº webpack çš„æ‰“åŒ…ç³»ç»Ÿï¼Œè¿™ä¸»è¦æ˜¯ä¸ºäº†è§£å†³å‰ç«¯æ„å»ºè¿‡ç¨‹ä¸­å¯¹æ¨¡å—ç³»ç»Ÿçš„ä¾èµ–ã€‚
  - è€Œç›®å‰å¸‚é¢ä¸Šæ‰€æœ‰ä¸»æµæµè§ˆå™¨ï¼ˆæ’é™¤æ‰ IEï¼‰éƒ½å·²ç»å¾ˆå¥½çš„æ”¯æŒäº† es moduleï¼Œæˆ‘ä»¬å…¶å®å¯ä»¥å……åˆ†çš„åˆ©ç”¨è¿™ä¸ªæŠ€æœ¯ç‚¹æ¥æ›¿ä»£ webpackï¼ˆå³ bundless æ„å»ºï¼‰ã€‚
  - åœ¨æµè§ˆå™¨åŠ è½½ es module çš„è¿‡ç¨‹ä¸­ä¼šä¸æ–­çš„å‘å‡º http è¯·æ±‚å»è·å–è¢«ä¾èµ–çš„ es moduleï¼Œå¯¹äºä¸ä¾èµ–æœåŠ¡ç«¯èƒ½åŠ›çš„ Web IDEï¼Œæˆ‘ä»¬éœ€è¦è¡¥å…¨è¿™éƒ¨åˆ†æœ¬è¯¥ç”±é™æ€èµ„æºæœåŠ¡å™¨æä¾›çš„èƒ½åŠ›ã€‚
  - æ‰€ä»¥ï¼Œå¯ä»¥æ‹¦æˆªè¯·æ±‚å¹¶å®šåˆ¶å“åº”çš„ service worker æ­£å¥½å¯ä»¥æä¾›è¿™ä¸ªèƒ½åŠ›ã€‚
  - æˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰çš„ä»£ç èµ„æºå­˜å‚¨åœ¨æµè§ˆå™¨ç«¯ï¼Œä¹Ÿå°±æ˜¯è¯´éœ€è¦å®ç°ä¸€å¥— browser file systemã€‚
- es moduleå‘å‡ºçš„httpè¯·æ±‚ï¼Œæ˜¯ä»¥scriptsè„šæœ¬åŠ è½½çš„å½¢å¼ã€‚è€Œservice workeråªèƒ½æ‹¦æˆªfetchã€‚
  - service worker ä¸­ç›‘å¬ fetch äº‹ä»¶å½“ç„¶æ˜¯å¯ä»¥æ‹¦æˆªè„šæœ¬çš„è¯·æ±‚çš„ï¼Œä¸ä¿¡ä½ å¯ä»¥è¯•è¯•
  - åŒ…æ‹¬esmåŠ è½½æ—¶çš„è¯·æ±‚éƒ½å¯ä»¥è¢«æ‹¦æˆª

# discuss-wasm/wasi
- ## 

- ## 

- ## [Kubernetes reinvented virtual machines in a good sense | Hacker News _202207](https://news.ycombinator.com/item?id=32298058)
- "Containers" are just a pattern, the real technology behind it is kernel namespaces (plus ordinary process boundaries). (For that matter container security is still half-baked in many ways. The kernel namespacing has been added after-the-fact and it can't really be trusted to make an effective "sandbox" on its own.)

- I know most configurations don't use it, but k8s supports different runtimes and today you can use Kata or firecracker to run each pod in it's own "microvm."
  - You then get the security benefits of a "real" VM while still distributing your software as an OCI (Docker) container, and can operate it like any other k8s cluster.
- Launching a container is implementation detail for Kubernetes. I could totally see Kubernetes which runs some pods in containers and some pods in VMs with tiny kernel. May be it even exists, I'm too lazy to google it.

- Did JVM reinvent VM? Is it a VM or a container?
  - No one (not enough people?) asked (cared?) for this about the JVM for like 25 years till Linux Cgroups came around and Docker capitalized on it. Its not VM v/s container question that we care about. (I am aware this is a very poor comparison but so is comparing a vm to a container, which is my point)
  - K8S brings complexity yes, and in a consistent, predictable way. The biggest difference between a k8s based shop vs a DIY VM/Container shop is the former can (in theory) hire some people with intermediate K8S experience, and have them sort out the mess that the people without that experience (or more typically the ones who don't have clear requirements) may have created. With k8s it becomes more predictable to find the boundaries and start pulling things apart in production and make incremental changes. Without k8s, making changes to a production setup is a nightmare that folks who have done that sort of thing don't ever want to repeat.

- ## [WASIX, the Superset of WASI Supporting Threads, Processes and Sockets | Hacker News _202305](https://news.ycombinator.com/item?id=36126032)
- I think server side WASM has the potential to be the next thing after containers.
- Since I've been web applications and distributed systems, the evolution has been something like this.
  1. Copy files around, into the web or application servers directory. This was simple but isolation, repeatability and testability was hard. The application dependencies were directly tied to the host, so you had to be very careful about your upgrades breaking applications. The lack of isolation is not suitable for multi-tenant environments for security and capacity planning reasons. It's pretty hard to run multiple-applications
  2. Things like bundler, and other language specific tools that let you bundle your application dependencies together. Avoids some of the host OS related upgrade issues, but again, no real isolation.
  3. VMs. The whole cloud industry started on this. You get true isolation of your whole application at the cost of complexity and weight. Scheduling new VMs is not a fast process because you're copying around or at least booting a whole OS. You still have to manage OS images. There is also generally a tax on memory that you have to pay, so you waste some resources with VMs to get the isolation. Amazon invested in Nitro to avoid paying a lot of this tax.
  4. Containers. Lighter than VMs, good enough isolation in a lot of cases, especially inside of an organization. Lower tax than VMs. More testable. But containers are still fairly heavy, startup time can be slowish. So you have some limitations on how fast you can scale up and down in response to load. Also isolation isn't good enough for some workloads. So we see things like firecracker VMs being used to improve isolation.
  5. Wasm on the server side. Much lighter than containers. Less memory overhead, way less startup time. Probably faster startup time than even firecracker. So for lambda like workloads you could could have a lot better cold start latency. You probably have good enough isolation that you don't need to resort to things like firecracker to isolate customer workloads. Because of the fast startup time you could probably pack your long tail of workloads even tighter to get better effective utilization out of your server infrastructure.

- Excited for threads, sockets, futexes, but not excited for fork, signals and setjmp. They didn't include POSIX stuff like shared memory, select(2), unix sockets, and had to draw a line somewhere. Why include error-prone cruft?
- A few points there:
  - A big goal here is to make existing software compile to Webassembly, with only a relatively small amount of changes.
  * classical shared memory (mapped into the main address space) is very hard to do in the context of Webassembly, especially in a performant way. Not impossible, but complicated. It would be valuable to add, but there are different approaches as well (like sharing additional Webassembly-level memories between instances, for example)
  * WASI already has a concept similar to select
  * unix sockets would be a pretty straight-forward addition

- WASI is not meant for browsers, it's for using WebAssembly as platform-independent bytecode in non-browser environments.
# discuss-cloud-env
- ## 

- ## 

- ## [Cloud, why so difficult? (2022) | Hacker News _202306](https://news.ycombinator.com/item?id=36488733)
- Cloud is difficult because it stiched together from fragments of old applications and approaches.
  - Cloud will be easy with consistent approach. One or very few languages for everything, from configuring resources, to configuring policies and writing code.
  - Cloud will be easy with opinionated tooling. No Dockerfile and supporting of miriads languages. Just file with dependencies and folder with source code.
  - Cloud will be easy with opinionated approach to configuration. No need to support PASSWORD env, PASSWORD_FILE env, some secret access service. Only one secure and simple way.

- 
- 

- ## [Cloud desktops aren't as good as you'd think | Hacker News _202210](https://news.ycombinator.com/item?id=33106234)
- Latency for every keystroke, your brain starts to add latency to latency that is not there to compensate for a life lived wearing citrix latency goggles.

- Nothing beats working directly on a fast but quiet workstation sitting next to my table.

- With cloud gaming you can stream 4K games at 60FPS, with clarity and quality for fast moving objects.
  - Why does remote desktop still shit itself when I move around MS Word with a few pictures?

- 
- 

# discuss-runtime-vm
- ## 

- ## 

- ## 

- ## 14/15 å¹´çš„æ ·å­å¼€å§‹æå®¹å™¨ï¼Œè¿›ç¨‹çº§åˆ«éš”ç¦»+æ— çŠ¶æ€å¤§è¡Œå…¶é“ï¼Œç°åœ¨å†å²çš„è½¦è½®åˆå›åˆ°å¼ºéš”ç¦»+æœ‰çŠ¶æ€, ä»¥è™šæ‹Ÿæœºä¸ºè½½ä½“çš„ sandbox éœ€æ±‚ â€¦ å¤©å¤©å°±çŸ¥é“é€ åè¯ï¼Œåœ°åŸºä¸è¿˜æ˜¯é‚£ä¸ªåœ°åŸºçœŸæ˜¯â€¦â€¦
- https://x.com/CMGS1988/status/2017142326829990201
  - åº•å±‚æ²¡å•¥å˜åŒ–ï¼Œhypervisor å·²ç»å®šå‹äº†â€¦â€¦
  
- æˆ‘ä»¬éƒ½ç”¨å®¹å™¨è·‘sandboxçš„ï¼Œå› ä¸ºä¸éœ€è¦kernelçš„æ“ä½œï¼Œç‚¸äº†å°±ç‚¸äº†
  - å¯¹å†…å¯¹å¤–éœ€æ±‚ä¸ä¸€æ ·â€¦å…¬æœ‰å¤šç§Ÿæˆ·å®¹å™¨å°±æœ‰ç‚¹ä¸å¤ªè¡Œ
- åŠ ä¸Š gVisor å‘¢ï¼Ÿ

- ## [I'm building a Self-Hosted Alternative to OpenAI Code Interpreter, E2B : r/LocalLLaMA _202505](https://www.reddit.com/r/LocalLLaMA/comments/1kxlx46/im_building_a_selfhosted_alternative_to_openai/)
- What's the security model - looks like using a full VM but maybe pooling them?
  - They are lightweight VMs like the one Firecracker uses. They boot fast and have low memory footprints. As low as a few mbs depending on the image you are running.

- does it use firecracker?
  - it uses libkrun
- how does that work, compared to firecracker?
  - it is probably not that different from firecracker. personally i like that it bundles the kernel. i have not tried firecracker extensively btw.
- libkrun shares code with firecracker and uses crates from https://github.com/rust-vmm

- How would you suggest embedding microsandbox in some fashion so it can be distributed as part of a larger project?

- ## [microsandbox: I'm building a Self-Hosted Alternative to OpenAI Code Interpreter, E2B : r/LangChain _202505](https://www.reddit.com/r/LangChain/comments/1kxlsq5/im_building_a_selfhosted_alternative_to_openai/)
- This is cool, I built a solution as part of my previous startup, and I've been thinking about extracting it out as its own service.
  - That said it requires Kubernetes, as it spins up a new pod for each connected session, then proxies requests to it based on headers.
  - One down side to this, is the start up time is about 20-30 seconds.
  - I'm curious to see how you proxy requests and handle issues with sandbox escape attempts. I'll dig through the code when I have a chance.
- Right now, the sandboxes are based on custom python image with a server program that accepts code, runs code in repl and spits back the result. It is a temp setup that I will be changing soon to allow any image. I don't understand what you mean by sandbox escape attempts. The received code runs in VMs.
- I'm digging through the code now and I see it's a true VM that requires KVM, for nested virtualization. Which means you don't have the same security issues as containers do
  - That said, this is awesome and looks more like a replacement for docker, than anything else. Awesome stuff, and again thanks for sharing.
- You are right. The goal is to build Docker for microvms

- ## [microsandbox: A secure environment for running untrusted code : r/rust _202505](https://www.reddit.com/r/rust/comments/1kz7tw6/microsandbox_a_secure_environment_for_running/)
- That's an exciting project! While similar tech exists in the form of Firecracker and Hyperlight, there are nowhere near this easy to use. I'm happy to see a solution with a ready-to-use API and CLI
  - There is more to virtualization on Linux than just KVM. There is a fair bit of userspace code too, either in the form of qemu-kvm or something from the crosvm lineage (firecracker, cloud-hypervisor, etc). I don't see any references to any of these in the code. I wonder how microsandbox deals with that?

- ## [Is there an open-source alternative to e2b (e2b.dev. Code interpreting for your AI app)? : r/LocalLLaMA _202405](https://www.reddit.com/r/LocalLLaMA/comments/1chsx7z/is_there_an_opensource_alternative_to_e2b_e2bdev/)
- e2b is fully open source!
  - The reason we aren't using containers for code execution is because they aren't secure. We're using Firecracker micro VMs under the hood instead.
- When I am executing the code, where is the code getting executed - locally using Firecracker or one of the cloud services?
  - Our cloud service. We'll make it easier for people to run E2B locally but it's tricky. For example, Firecracker doesn't run natively on macOS, another layer of virtualization is required.

- If you are still looking (or maybe anyone else is), microsandbox is what you want. It is the same true VM-level isolation you'd get from E2B but has the easiest path to self hosting.

- ## Vercel is now consistently handling over 1M builds a day every day.
- https://x.com/rauchg/status/1902347649275068858
  - Thatâ€™s 1M microVMs (virtual mini cloud computers) instantly booting, running user code in a secure sandbox, and being destroyed every day.
  - To meet our scale and flexibility needs, we had to build our own custom compute platform powered by Firecracker called Hive. 
  - We orchestrate Hive clusters in multiple regions worldwide, we handle automatic failover, QoS policies across customer and plan tiers, multiple classes of hardware, pre-warming and snapshotting of containers, custom networking overlays, security patching, Dependabot thundering herdsâ€¦ We even carefully synchronize OS and Node runtime flavors with your Functions to prevent unsavory production surprises. 

- https://x.com/rauchg/status/1906814800426086861
  - We're building an API to run arbitrary compute, targeting agentic AI usecases and long-running tasks. Yes, it can run servers.

- Predicted it in one my replies couple of weeks before. Essentially firecracker as an api.

- would love a e2b like experience for ephemeral sanbox / is this similar to that?
  - yes

- What will the memory and CPU limits look like? Cloudflareâ€™s worker isolates can be nice for off-loading tasks, but 128mb is very limiting.
  - Muti-core / multi-GB. Open to adjusting based on customer needs

- 
- 
- 

- ## ğŸš€ğŸ§Š I'm excited to launch Arrakis: an open-source and self-hostable sandboxing service designed to let AI Agents execute code and operate a GUI securely. _20250403
- https://x.com/abshkbh/status/1907480355529203809
  - AI agents become incredibly capable when given access to a full Linux VM environment.
  - Self-hostable: Run it on your own infra or Linux server.
  - Secure by Design: Uses MicroVMs for strong isolation between sandbox instances.
  - Snapshotting & Backtracking: First-class support allows AI agents to snapshot-and-restore a running sandbox.
  - Ready to Integrate: Comes with a Python SDK py-arrakis and an MCP server arrakis-mcp-server out of the box.
  - Customizable: Docker-based tooling makes it easy to tailor sandboxes to your needs.

- When's the typescript SDK dropping?
  - One of the top things on my plate. But it has a REST API so you can use whatever for the time being.

- firecracker support when? Also, any guide on how to deploy it?
  - Arrakis uses `cloud-hypervisor` underneath. Both Firecracker and chv are based on `crosvm`.
- ğŸ†š any reason why you chose hypervisor over firecracker?
- When I started the project -
  1. Chv had stable snapshot/restore support. Firecracker didn't
  2. Hotplugging of memory that I thought would help with memory management
  3. I didn't want to be at the mercy of Amazon close sourcing Firecracker

- I can't really deploy your project on aws because EC2 doesn't support virtualization
  - Yes, exactly why I also went with GCP/GCE. AWS does have a bare metal offering, it's just expensive.

- would work on aws ec2?
  - Any Linux machine with kvm access. That rules out EC2 but GCE VMs work

- ## same.dev literally hosts a vm for your project
- https://x.com/aidenybai/status/1901019106234732938
  - you get basically a full agent + operating system (which you can play doom / minecraft on if you want)

- surely it's docker; no way youre giving 32gb ram vms to agents lol
  - docker
- interesting that you didnâ€™t go for a pre-made solution like e2b or scrapy though
  - figured we can just built it ourselves at 50% the costs

- Few crypto miners and your AWS account is cooked, just fyi
- Can't wait for you to see your first AWS bill

- ## ğŸ‘·ğŸ» I 100% agree with this. We needed an orchestrator that understands VM snapshotting/cloning and can spin up a new VM extremely quickly.
- https://x.com/CompuIves/status/1853478202569617647
  - We tried different solutions, but ultimately we had to build our own because nothing fit the bill.
  - This was also a big challenge for us, so we had to customize how VM snapshots are created and loaded.

- ## æµè§ˆå™¨ä¸­è¿è¡Œçš„ Linux è™šæ‹Ÿæœºç¯å¢ƒ https://webvm.io
- https://x.com/geekbb/status/1816265787113627786
  - é€šè¿‡ HTML5 å’Œ WebAssembly æŠ€æœ¯å®ç°å®¢æˆ·ç«¯æ‰§è¡Œ x86 äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œæ”¯æŒç½‘ç»œåŠŸèƒ½ï¼Œå¹¶æä¾›äº†éƒ¨ç½²å’Œå®šåˆ¶çš„æŒ‡å—ã€‚

- ## [csb: We clone a running VM in 2 seconds | Hacker News _202312](https://news.ycombinator.com/item?id=38651805)
- Firecracker is the same VM technology used by Fly.io although they donâ€™t have memory cloning as seen here. 
  - A VM can be cloned and the clone started in seconds though this heavily depends on image / rootfs size so itâ€™s likely more in the order of 30 seconds or so (number pulled out of thin air).
- Can't you use CoW to make the clone essentially instant? (I thought BTRFS supported it, but I'm not sure)
  - I did a bit of searching and if the image is a file (qcow2, say) I'm pretty sure you could just cp --reflink=always old.qcow2 new.qcow2 and it would take way less than 30 seconds. (Again, assuming BTRFS; I guess ex. XFS has some sort of reflink support these days but I don't know)
- This isnâ€™t about cloning an image, itâ€™s the full running VM state, including memory.

- Super cool technology, but when cloning entire VMs, one needs to be very careful to not accidentally leak crypto material.
  - If you run server A, give me the copy Server B of it and Alice logs into B, I could log into Server A as Alice. Same thing if we both run copies of the same original VM.

- Very cool. Seems like this could also be great for an integration test runner when the test environment requires a lot of setup. I thought this might use some of the underlying page tracking support for live network migration that nearly every hypervisor has now, but it doesn't seem like they needed it!

- They're using firecracker, which is a KVM-backed VM manager.

- ## ğŸ“ [Why We Replaced Firecracker with QEMU | Hocus Blog _202307](https://hocus.dev/blog/qemu-vs-firecracker/)
- 
- 

- ğŸ“ [Virtualizing Development Environments in 2023 | Hocus Blog _202308](https://hocus.dev/blog/virtualizing-development-environments/)
  - There was no silver bullet we could use to virtualize development environments. VMs are not very memory-efficient, and containers can't isolate all workloads. 
  - Many of our users would not need the virtualization capabilities of VMs, and could save costs by putting more containers on a single machine. 
  - However, Hocus itself depends on low-level kernel features, and can't be fully developed inside a container. We wanted to use Hocus to develop Hocus as soon as we could, so the first version of Hocus uses VMs ğŸ¯. 
  - However, we designed the system in a way that allows us to add container support later.
  - Hocus is a work in progress, a proof of concept, and we want to finish it in collaboration with people who need it.
  - if you're just interested in what we've built so far, you can check out the alpha version on GitHub.

- ## ğŸ¤¼ğŸ» [hocus: Why We replaced Firecracker with QEMU | Hacker News _202307](https://news.ycombinator.com/item?id=36666782)
- ğŸ‘·ğŸ» At CodeSandbox we use Firecracker for hosting development environments, and I agree with the points. Though I don't think that means you should not use Firecracker for running long-lived workloads.
  - We reclaim memory with a memory balloon device, for the disk trimming we discard (& compress) the disk, and for i/o speed we use io_uring (which we only use for scratch disks, the project disks are network disks).
  - It's a tradeoff. It's more work and does require custom implementations. For us that made sense, because in return we get a lightweight VMM that we can more easily extend with functionality like memory snapshotting and live VM cloning.
- Firecracker has a balloon device you can inflate (ie: acquire as much memory inside the VM as possible) and then deflate... returning the memory to the host. You can do this while the VM is running.
  - The first footnote says If you squint hard enough, you'll find that Firecracker does support dynamic memory management with a technique called ballooning. However, in practice, it's not usable. To reclaim memory, you need to make sure that the guest OS isn't using it, which, for a general-purpose workload, is nearly impossible
- Yes, we use this at CodeSandbox for reclaiming memory to the host (and to reduce snapshot size when we hibernate the VM).
- for many mostly "general purpose" use cases it's quite viable, or else ~fly.io~ AWS Fargate wouldn't be able to use it
- A bit disingenuous to make a broad sweeping claim, then have a footnote which contradicts that claim, and upon closer inspection even that claim is incorrect. It's absolutely usable in practice, it just makes oversubscription more challenging.
- That and the fact that this was after "several weeks of testing" tells me this team doesn't have much virtualization experience. Firecracker is designed to quickly virtualize 1 headless stateless app (like a container), not run hundreds of different programs in a developer environment.

- while Firecracker was designed for thing running just a few seconds, there are many places running it with jobs running way longer then that
  - the problem is if you want to make it work with long running general purpose images you don't control you have to put a ton of work into making it work nicely on all levels of you infrastructure and code ... which is costly ... which a startup competing on a online dev environment compared to e.g. a vm hosting service probably shouldn't wast time on
  - So AFIK the decision in the article make sense the reasons but listed for the decision are oversimplified to a point you could say they aren't quite right. Idk. why, could be anything from the engineer believing that to them avoiding issues with some shareholder/project lead which is obsessed with "we need to do Firecracker because competition does so too".
- That being said, firecracker also runs long-running tasks on AWS in the form of Fargate

- yes, it was created originally for AWS Lambda. 
  - mainly it's optimized to run code only shortly (init time max 10s, max usage is 15min, and default max request time 130s AFIK)
  - also it's focused on thin server less functions, like e.g. deserialize some request, run some thin simple business logic and then delegate to other lambdas based on it. This kind of functions often have similar memory usage per-call and if a call is an outlier it can just discard the VM instance soon after (i.e. at most after starting up a new instance, i.e. at most 10s later)

- No mention of Cloud Hypervisorâ€¦perhaps they donâ€™t know about it? Itâ€™s based in part on Firecracker and supports free page reporting, virtio-blk-pci, PCI passthrough, and (I believe) discard in virtio-blk.
  - We do, and we'd love to use it in the future. We've found that it's not ready for prime time yet and it's missing some features. The biggest problem was that it does not support discard operations yet.

- Fly uses Firecracker, and they host long-running processes. I wonder what's their opinion about it.
  - I think their usecase makes a lot of sense as their workloads consume a predefined amount of ram. As a customer you rent a VM with a specified amount of memory so fly.io does not care about reclaiming it from a running VM.

- I came to the same conclusion as OP. QEMU is the most stable, hackable, well-supported VM hypervisor on the market. Setting it up is a pain, but once you get it set up with all your custom scripts, you never have to do it again. Ever. Even in your next project.

- I know that Firecracker does not let you bind mount volumes, but QEMU does. So, we changed to QEMU from Firecracker. If you run the workloads in Kubernetes, you just have to change a single value in a yaml file to change the runtime. 
  - I would be scared to let unknown persons use QEMU that bind mounts volumes as that is a huge security risk. 
  - Firecracker, I think, was designed from the start to run un-sanitized workloads, hence, no bind mounting.

- don't use virtualization.
  - In this context (the blog post) and the reason firecracker was created, was to isolate workloads.
  - And if youre running untrusted code, then using a virtualized environment is the easiest (id even say best) way to go about it.

- Listen people, Firecracker is NOT A HYPERVISOR. A hypervisor runs right on the hardware. KVM is a hypervisor. Firecracker is a process that controls KVM. If you want to call firecracker (and QEMU, when used in conjunction with KVM) a VMM ("virtual machine monitor") I won't complain. But please please please, we need a word for what KVM and Xen are, and "hypervisor" is the best fit. Stop using that word for a user-level process like Firecracker.

- 
- 
- 

- ## [Firecracker internals: Inside the technology powering AWS Lambda (2021) | Hacker News _202302](https://news.ycombinator.com/item?id=34964197)
- A large part of the Cloud Hypervisor code is based on either the Firecracker or the crosvm project's implementations. Both of these are VMMs written in Rust with a focus on safety and security, like Cloud Hypervisor.
  - The goal of the Cloud Hypervisor project differs from the aforementioned projects in that it aims to be a general purpose VMM for Cloud Workloads and not limited to container/serverless or client workloads.
- They're both built on the same rust-vmm crates, the code is very similar, as are the interfaces. Both very cool projects.

- We're using Firecracker at our company to allow API companies build interactive demos like this one we built for Prisma

- At CodeSandbox we use Firecracker to run our VMs 
- What do you use to build the 'micro' images
  - We created a CLI that creates a rootfs from a Docker image. It pulls the image, creates a container and then extracts the fs from it to an ext4 disk. For the init, we forked the open sourced init from the Fly team
- You guys don't happen to have a public writeup about how this works, do you? Maybe it's as simple as it sounds, but Fly and CodeSandbox both have some magic to turn Docker images into VM disks that I'd like to know how to build
  - Fly is doing fancy stuff to avoid using docker entirely, but with docker you can just run "docker export" to dump an image to a .tar file that contains the whole filesystem. Built-in feature. I use this as a convenient way to grab a foreign platform sysroot for clang cross-compilation; just pick a Docker image and rip the filesystem out.
  - There's been a writeup on this topic by the Fly team -- https://fly.io/blog/docker-without-docker/

- As an old school unix sysadmin thatâ€™s aware of the â€œcontainers donâ€™t containâ€, people ditching proper isolation for containers everywhere for performance reasons has been alarming. now with Firecracker we have isolation and performance.

- Iâ€™ve always thought Firecracker would be great for CI runners.
  - This is basically what CodeBuild does.
  - The default Docker containers that CodeBuild uses (you can create your own) and the shell script it uses to parse the yaml configuration file (mostly a list of shell scripts) are all open source and the entire process can be run locally.

- Iâ€™ve been playing with Firecracker on a Raspberry Pi 4, but never could get Docker inside a Firecracker uVM to work. Should this be supported at all?
  - Depends how your starting the VM. Iâ€™ve run Docker on Firecracker with a Raspberry PI 4 before but it needed some fixes.
  - One possibly is if your running directly from a Initramfs without a block device then docker needs DOCKER_RAMDISK set as a environment variable.

- Any tooling like Docker/Podman to make it as easy to kick off Firecracker VMs as it is to kick off containers?
  - Kata containers https://katacontainers.io/

- ## [csb: We clone a running VM in 2 seconds | Hacker News _202209](https://news.ycombinator.com/item?id=32683834)
- 
- 
- 
- The bit about turning a dockerfile into a rootfs. A docker image is just a tarball of tarballs. We do something like this:
  - you can dump the image using `docker save <name>`. - you can then get a list of the tarballs in this image by extracting this tarball and reading the file manifest.json; Config -> Layers will give you a list of tarballs (see undocker for how to do this: https://github.com/larsks/undocker) - Untar these in a directory and use linux tools to convert this dir to a rootfs.

- also interested in the upper limit of a micro vm, like how big can it get? 64gb memory? not really micro any more and maybe a traditional VM would be a better fit.
  - AWSâ€™s serverless Docker solution - Fargate - based on Firecracker supports up to 30GB of RAM and 4 vCPUs.

- 
- 
- 

- ## [Ignite â€“ Use Firecracker VMs with Docker images | Hacker News _202209](https://news.ycombinator.com/item?id=32990127)
- This is a great idea and something I would love to use, but it's a lot less docker compatible and user friendly than it may seem. 
  - For instance there is no way to automatically run the image's specified command, effectively leaving you with a dead VM
  - You also simply can not share directories with the host (out of scope for Firecracker).
- microVMs for running containers are definitely a great idea; 
  - another project aiming to do it (a little differently) is Kata Containers. 
  - It has a lot more industry support, can also run on Firecracker (though QEMU microVMs are just as good) and can (theoretically, I haven't gotten it to work) interact with many container runtimes that are not Docker (the focus appears to be on Kubernetes).
  - While more powerful, Kata Containers are also much, much more complicated (and honestly under- and misleadingly documented). 

- Firecracker has a number of limitations, however. Firecracker is tailored to the highly vertical 'lambda' use case and too much of the power of the kernel and userspace is stripped away. You can't even live migrate a firecracker VM right now.
  - VMs are great and will ultimately resurge to the detriment of 'everything is a container' serverless orchestration, but Firecracker in its current form won't cut it. 

- Running Docker in a VM is not new at all. This is even how Docker for Windows/Mac works. The GitHub page states that Ignite does more than just "wrapping a container in a VM layer" but I'm not sure how that matters, since the notion of a "container" is purely about configuration. It seems like a distinction without a difference.
  - Docker for Windows/Mac uses a single VM for all your containers. Firecracker/Ignite has a dedicated micro-VM for each container - a completely different architecture, with much better isolation between containers. And the 125ms boot time is not something you get with a typical VM.
  - But I do agree that they don't explain how Ignite is different from "wrapping a container in a VM layer" that Kata/gVisor does.
- Ignite is running a VM per container. Docker for Windows/Mac runs all containers in a single VM.

- ## [Firecracker: Start a VM in less than a second | Hacker News _202101](https://news.ycombinator.com/item?id=25883253)
- 
- 
- 

- ## [å¦‚ä½•è¯„ä»·awså¼€æºçš„firecrackerï¼Œå®ƒå¯¹æ¥ä¸‹æ¥çš„è™šæ‹Ÿæœºã€å®¹å™¨å‘å±•æ–¹å‘å¸¦æ¥ä»€ä¹ˆå˜åŒ–å—ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/303920344)
- åˆä¸€ä¸ªé’ˆå¯¹ container åº”ç”¨åœºæ™¯çš„è½»é‡çº§ device modelï¼Œå’Œåˆ«äººå®¶æœ€å¤§çš„ä¸åŒä¹‹ä¸€å°±æ˜¯ä¸º AWS åšäº†æ›´å¥½çš„é€‚é…ã€‚
  - åˆ©ç”¨ä¼ ç»Ÿç¡¬ä»¶è™šæ‹ŸåŒ–çš„å¼ºéš”ç¦»ä¿æŠ¤ container åº”ç”¨å¹¶ä¸æ˜¯ä¸€ä¸ªå¤ªæ–°çš„æƒ³æ³•ï¼Œä¹Ÿæ˜¯ä¸šç•Œä¸€ç›´åœ¨å°è¯•çš„æ–¹å‘ä¹‹ä¸€ã€‚ç¨å¾®è¿œä¸€ç‚¹çš„æœ‰ hyper.shï¼Œè¿‘çš„æœ‰ Kata Containers (The speed of containers, the security of VMs)ï¼Œæ›´ä¸è¦è¯´å¸¸è§çš„ container æœ¬èº«å°±å¯ä»¥ç›´æ¥è·‘åœ¨è™šæ‹Ÿæœºä¸­ã€‚
  - æ›¿ä»£ QEMU çš„è½»é‡çº§çš„ device model ä¹Ÿæœ‰ä¸å°‘ï¼Œä¾‹å¦‚ Kata Containers ä¸­ä½¿ç”¨çš„é’ˆå¯¹ container åº”ç”¨åœºæ™¯çš„è¿½æ±‚æé™å¯åŠ¨é€Ÿåº¦å’Œä½ memory footprint çš„ QEMU-liteï¼Œé’ˆå¯¹ç°ä»£äº‘è®¡ç®—ç¯å¢ƒçš„ NEMU (intel/nemu)ï¼Œä»¥åŠä½¿ç”¨ Rust å¼€å‘çš„ Google Chrome OS ä¸­çš„ crosvm 
  - å®é™…ä¸Šï¼Œå„å¤§äº‘è®¡ç®—å‚å•†éƒ½åœ¨æˆ–è€…è®¡åˆ’ä½¿ç”¨ QEMU ä»¥å¤–çš„ device model æ”¯æ’‘å®ƒä»¬çš„å¹³å°ã€‚ä½œä¸ºæœ€å¤§å’ŒæŠ€æœ¯å®åŠ›æœ€é¡¶å°–çš„äº‘è®¡ç®—å‚å•†ä¹‹ä¸€ï¼ŒAmazon åœ¨ä» Xen è½¬å‘ KVM çš„è¿‡ç¨‹ä¸­ï¼Œé€‰æ‹©åœ¨éƒ¨åˆ†ä¸šåŠ¡ä¸­ä½¿ç”¨è‡ªç ”çš„è½»é‡çº§ device model ä¹Ÿä¸æ˜¯ä¸€ä»¶ä»¤äººæ„å¤–çš„äº‹æƒ…ã€‚

- è¿™æ˜¯ Serverlessï¼ˆæ— æœåŠ¡å™¨ï¼‰ å‰è¿›çš„ä¸€æ­¥ï¼Œä¹Ÿæ˜¯å¼€æºå‰è¿›çš„ä¸€æ­¥ã€‚125ms å¯åŠ¨æ—¶é—´ã€5 MiB å†…å­˜å ç”¨çš„ microVM ï¼Œè®©å®¹å™¨/å‡½æ•°è½»æ˜“è·å¾—éš”ç¦»æ€§çš„åŠ æŒï¼ˆç°é˜¶æ®µå®¹å™¨å’Œå‡½æ•°çš„å®‰å…¨æ€§è¿˜æ˜¯é è™šæ‹ŸåŒ–äº†ï¼‰ï¼Œè€Œ AWS å¼€æºæ­¤é¡¹æŠ€æœ¯ï¼Œè¡¨æ˜åŸºç¡€æŠ€æœ¯çš„æœªæ¥è¿˜æ˜¯å¼€æºã€‚

- 2å¹´å‰å¤§å®¶åšFaaSçš„æ—¶å€™ï¼Œå°±å·²ç»åœ¨çŒœæµ‹å’Œè¿½èµ¶AWSåœ¨è½»é‡çº§VMè¿™å—çš„æŠ€æœ¯ã€‚åˆ°ç°åœ¨å¼€æºï¼Œå¤§å®¶å·²ç»æˆ–å¤šæˆ–è€…å½¢æˆäº†ç±»ä¼¼çš„æŠ€æœ¯ï¼Œä¾‹å¦‚Kata Containerï¼Œgoogle gVisorç­‰ã€‚ è€Œä¸”åœ¨17å¹´é€æ­¥å½¢æˆäº†ä¸€å®šçš„å½±å“åŠ›ã€‚ä½†æ˜¯Lambdaå’ŒFargateå·²ç»æˆä¸ºServerlessçš„äº‹å®ä¸Šçš„æ ‡å‡†çš„æ—¶å€™ï¼ŒæŠŠè¿™é¡¹æŠ€æœ¯å¼€æºå‡ºæ¥, ä¸€æ–¹é¢æ˜¯æ˜¾ç¤ºAWSæœ¬èº«åœ¨serverless infrastructureæ–¹é¢çš„é©æ–°èƒ½åŠ›ï¼Œå¦å¤–ä¸€æ–¹é¢ä¹Ÿæ˜¯å¯¹é€æ­¥ç«çƒ­çš„è½»é‡çº§VMæŠ€æœ¯çš„ç»Ÿæ²»åŠ›çš„äº‰å¤ºã€‚è‡³äºå¯¹VMå’ŒcontaineræŠ€å‘å±•æ–¹å‘çš„å˜åŒ–ï¼Œæˆ‘ç†è§£å˜åŒ–å¹¶ä¸æ˜¯firecrackerå¸¦æ¥çš„ï¼Œè€Œæ˜¯Lambdaï¼ŒFargateç­‰å¸¦æ¥çš„ï¼Œè€Œè¿™ä¸ªå˜åŒ–æ—©å·²ç»å‡ºç°ã€‚
# discuss-e2b-sandbox
- ## 

- ## 

- ## Traditional containers like Docker were too slow for Manus. 
- https://x.com/dobrac8/status/1928111487211512231
  - E2B's Firecracker microVMs offer faster startup times and full OS capabilities

- ## We run close to 1 million sandboxes a day _20250314
- https://x.com/mlejva/status/1900275519943282781
  - It's not so long ago that running this many sandboxes took us a month

- ## [Notes on the new Claude analysis JavaScript code execution tool | Hacker News _202410](https://news.ycombinator.com/item?id=41943662)
- That's an interesting idea to generate javascript and execute it client side rather than server side. I'm sure that saves a ton of money for Anthropic not by not having to spin up a server for each execution.
  - I imagine this is a broader push to be able to have Claude pilot in your browser (and other applications) in the future. This is the right way to go about it versus having a headless agent: users can be in the loop and you can bootstrap and existing environment.
  - Otoh itâ€™s going to be a security nightmare.

- Google Collab are all individual VMs. It seems Anthropic doesnâ€™t want to be in the â€œhost a VM for every single userâ€ business.

- ğŸ¤” I've been trying to figure out the right pattern for running untrusted JavaScript code in a browser sandbox that's controlled by a page for a while now, looks like Anthropic have figured that out. Hoping someone can reverse engineer exactly how they are doing this - their JavaScript code is too obfuscated for me to dig out the tricks, sadly.
  - The key is running the untrusted code in a cross-origin iframe so you can rely on the same-origin policies and `sandbox`.
  - You can control the code in a number of ways - loading a trusted shim that sets up a postMessage handler is pretty common. You can be careful and do that in a way that untructed code can't forge messages to look like their from the trusted code.
  - ğŸ’¡ Another way is to use two iframes to the untrusted origin. One only loads untrusted code, the other loads a control API that talks to the trusted code. You can then to the loading into the iframe with a service worker. This is how the Playground Elements work (they're a set of web components that let you safely embed a mini IDE for code samples) https://github.com/google/playground-elements
- The cross origin iframe method is the same Iâ€™ve employed in A few browser extensions Iâ€™ve built

- You should check out how Figma plugins work. They have blog posts on all the tradeoffs they considered. What I believe they settled on was a JS interpreter compiled to WASM -- it can run arbitrary JS but with very well-defined and restricted interfaces to the outside world (the browser's JS runtime environment).
  - > We now use QuickJS, a JavaScript VM written in C and cross-compiled to WebAssembly.

- Much easier in the browser that has V8 isolate, however even with webworkers you still want to control CPU/network hijacking which is not ideal.
  - If it's only the user's own code it's fine but if they can run code from others it's a massive pain indeed.
  - On the server it's still not easy in 2024, even with Firecracker (doesn't work on mac), Workerd (is a subset of NodeJS),  `isolated-vm` (only pre-compiled code, no modules).

- What are the attack vectors for a web browser js environment to do malicious things? All browser code is sandboxed via origin controls, and process isolation. It canâ€™t even open an iframe and read the contents of that iframe.
  - It's a fine place to run code trusted by the server (or code trusted by the client within the scope of the app).
  - But for code not trusted by either, it's bad -- user data in the app can be compromised/exfiltrated.
  - Hence for third-party plugins for a web app, the built-in JS runtime doesn't have sufficient trust management capability.

- duckdb-wasm would be a good addition here. We use it in Definite and I can't say enough good things about duckdb in general.

- ## [Show HN: Riza - Safely run untrusted code from your app | Hacker News _202404](https://news.ycombinator.com/item?id=40159630)
  - weâ€™ve been working on Riza (https://riza.io), a project to make WASM sandboxing more approachable. Weâ€™re excited to share a developer preview of our code interpreter API with HN.
  - The Riza Code Interpreter API is an HTTP interface to various dynamic language interpreters, each running inside a WASM sandbox without access to the outside world (for now). We modeled the API to align with a POSIX shell-style interface.

- Been using Riza for the past few months at our startup for executing code generated by GPT4.
- We use it for local dev, running model eval (when changing prompts), in CI and production work loads.
  - It's the easiest to setup. It took us just a few minutes to execute our first function call.
  - Multiple languages support - we use both JS and Python for code generated by LLM , Riza works great with both out of the box.
  - No cold start - this is important because latency matters in our product.
  - No infra management - even if we use AWS lambda or similar serverless product we felt like we still needed to a bunch of setup to make sure its fast + secure.

- ## [Spin 3.0 â€“ open-source tooling for building and running WASM apps | Hacker News _202411](https://news.ycombinator.com/item?id=42118496)
- everything, including the infra is open-source (below), but it currently requires more than just your laptop (gcp, nomad, firecracker, postgres, etc.)
  - this way, we're able to run millions secure sandbox environments

- GCP supports nested virtualisation

- wasmtime works as long as you make sure to include the lib directory
  - This is just a limitation of the wasmtime CLI. The full Rust API let's you mount filesystems as read-only. Not sure why it's not exposed as an argument.

- WASM is basically similar to JVM bytecode. So the comparison would be like using compiled code from Java, Scala and/or Kotlin for example. The source language only determines how the code is expressed in WASM and whether or not it also needs to bundle / compile-in some runtime code baggage for it to work.

- I develop the Scala-to-Wasm compiler, and also maintain the JVM backend of Scala. I can tell you that Wasm is very different from JVM bytecode.
  - The fundamental difference is that the JVM bytecode has an object model. When they talk to each other, Java, Scala and Kotlin do so at the abstraction level of the JVM object model. You can directly call methods between them because virtual dispatch of methods is a concept with semantics in the bytecode.
  - There's no such thing in Wasm, even with the GC extension. You get structs and arrays, but nothing like methods. If you want virtual dispatch, you encode it yourself using your own design of virtual method tables. That means Java, Scala and Kotlin, despite all having their Wasm GC backend at this point, cannot call each other's methods in Wasm.

- https://news.ycombinator.com/item?id=38712521
  - Our sandboxes are isolated microVMs (Firecrackers) that you can quickly spawn (~1s) and control with our SDK. It's like a playground for AI agents.

- [Ask HN: AI agent devs, how do you sandbox LLM generated code? | Hacker News](https://news.ycombinator.com/item?id=42132329)
  - There are a few tools here like e2b and modal 

- ## [Show HN: Desktop Sandbox for Secure Cloud Computer Use | Hacker News _202410](https://news.ycombinator.com/item?id=41938888)
  - We're using Firecrackers to power our sandboxes. Funnily enough, we had this repo sitting on our GitHub for about 6 months. We originally made this for one of our customers because they were running evals on the desktop-like environment with GUI for their model.
  - You can use PyAutoGUI to control the whole environment programmatically.
  - The desktop-like environment is based on Linux and Xfce at the moment. We chose Xfce because it's a fast and lightweight environment that's also popular and actively supported. However, this Sandbox template is fully customizable and you can create your own desktop environment.

- We are building a similar thing but with Windows environments. If you need Windows, shoot me an email to victor+borg.games No public SDK yet, but our client is a fork of old Parsec codebase. It is relatively straightforward: you connect using WebRTC, and send your actions over a datachannel.

- ## [Show HN: Open-source SDK for creating custom code interpreters with any LLM | Hacker News _202404](https://news.ycombinator.com/item?id=40093257)
  - The way our code interpret SDK works is by spawning an E2B sandbox with Jupyter Server. We then communicate with this Jupyter server through Jupyter Kernel messaging protocol 
- I have something similar but with a different philosophy. Basically, a docker container running and you can execute code against with ability to set timeouts, auto install and uninstall dependencies and a bunch of other cool stuff. The pain point of all of this is dependencies and making sure someone doesnâ€™t use your infrastructure to DDOS other folks.  And crypto mining!

- Another challenge is how to make the whole product cost efficient for our users. You don't want them to pay for idle sandboxes running on E2B but only when they actually need them for serverless execution. Firecracker and its snapshots are really helpful with this.

- ## [e2b - Show HN: We are building an open-source IDE powered by AI | Hacker News _202304](https://news.ycombinator.com/item?id=35440552)
  - Our new project - e2b - is using some of the technology we built for Devbook in the past. Specifically the secure sandbox environments where the AI agents run are our custom Firecracker VMs that we run on Nomad.

- I have question for these AI powered editors: what advantage would a dedicated editor like this have over just using an AI plugin for VsCode? How do you fundamentally build the editor differently if you are thinking about AI from the ground up?
  - Our editor isn't a regular coding editor. You don't actually write code with e2b. You write technical specs and then collaborate with an AI agent. Imagine it more like having a virtual developer at your disposal 24/7. It's built on a completely new paradigms enabled by LLMs. This enables to unlock a lot of new use cases and features but at the same time there's a lot that needs to be built.

- I built this https://github.com/campbel/aieditor to test the idea of programming directly with the AI in control. Long story short, VS Code plugin is better IMO.

- My biggest concern with tools like this is reproducibility and maintainability. How deterministically can we go from the 'source' (natural language prompts) to the 'target' (source code)? Assuming we can't reasonably rebuild from source alone, how can we maintain a link between the source and target so that refactoring can occur without breaking our public interface?

- 
- 
- 

# discuss-firecracker
- ## 

- ## 

- ## 

- ## ğŸŒ° Added an example of how to run Firecracker VM on top of AgentFS with copy-on-write overlay filesystem to the git repository
- https://x.com/penberg/status/2003522274004206057
- I know this was a lot of effort but Iâ€™ve been getting to work with microvm in QEMU and experience has been much better than firecracker. Look at the virtme-ng project by Andrea Righi, really nifty optimizations and recently heâ€™s added mcp support for running it via Claude code
  - Makes sense, have personal preference for QEMU too

- I thought you had issues with firecracker working with NFS or not needed for this?
  - The default kernel image of Firecracker has only NFS version 4 support, but -- of course -- the nfsserve crate we have in agentfs is v3. But yeah, lots of tokens and brain cells spent today, but got it finally working!

- I know rust doesnâ€™t have a nice server for it, but nfs v4.1 is wayyyy better (perf and correctness)

- ## Why on earth does Firecracker not support NFS, 9p, or even virtio-fs out of the box? Sad.
- https://x.com/MarcJBrooker/status/2003630318939701306
- "Have fewer features" is one of the goals of Firecracker, and maybe the single largest motivation behind investing in building it (versus, say, embracing QEMU).
  - So why not these ones? Well, filesystems are complex and large surface areas.
  - Compare the complexity of the host-side implementation between, say, virtio-fs and virtio-blk. The latter is way simpler, which means a smaller surface to reason about, easier formal verification, easier to reason about performance isolation, etc.
  - You can do NFS over TCP (through the net device), of course, but that requires trusting the NFS implementation.
  - In our NSDI'20 paper we wrote: "Minimizing the feature set of the VMM helps reduce surface area, and controls the size of the TCB. Firecracker contains approximately 50k lines of Rust code (96% fewer lines than QEMU), "

- I was genuinely surprised that Firecracker does not support accessing shared host files with virtio-fs, for example, because it's such an important part of light-weight virtualization. I am not sure I buy the security argument as it could be opt-in. 

- the reason why i'm using qemu microvm. at least i can use block device.

- Firecracker only supports the bare minimum to get Linux up and running. 

- Isn't Firecracker whole stick that they sacrificed almost all Linux modules on the altar of fast startup?

- ## ntroducing foreverVM, a Python REPL-as-a-service. 
- https://x.com/JamsocketHQ/status/1884660472076513468
  - Under the hood, we swap between memory and durable storage
  - Giving an LLM a code sandbox massively increases its capabilities, but managing VM lifecycles is a pain.

- ## [ForeverVM: Run AI-generated code in stateful sandboxes that run forever | Hacker News _202502](https://news.ycombinator.com/item?id=43184686)
- ğŸ”’ why are you allowing network requests in the VM? (Tested in the python REPL which is available on your homepage) What are you doing to prevent the abuse?
  - We allow outgoing requests because a common use case of ForeverVM is making API calls or fetching data files.
  - We give every repl its own network namespace and virtual ethernet device. We also apply a set of firewall rules to lock it out from making non-public-internet requests.

- Does this use CRIU? https://criu.org/Main_Page
  - I don't think so. This looks like to be using an actual VM instead of container tech.

- ğŸ¤” Itâ€™s trivial to build something that does what this describes. Iâ€™m sure thereâ€™s more too it, but based on the description the pieces are already there under permissive open source licenses. ğŸ’¡ For a clean implementation Iâ€™d look at socket-activated rootless podman with a wasi-sdk build of Python.
  - It was an afternoon to prototype, followed by a lot of work to make it scale to the point of giving everyone who lands from HN a live CPython process
- This is the sort of thing that would touch a lot of my data so Iâ€™d much prefer to have it self hosted but you mention Claude rather than deepseek or mistral so know your audience I guess.
  - Fair enough. Our audience is businesses rather than consumer, so our equivalent to self-hosting is that we can run it in a customer's cloud.
  - We mention Claude a lot because it is a good general coding model, but this works with any LLM trained for tool calling. Lately I've been using it as much with Gemini Flash 2.0, via Codename Goose.
- Is it possible to run cython code with this as well? Since you can run a setup.py script could you compile cython and run it? Looking at the docs, it seems only suited for interpreted code
  - We are working now on support for arbitrary imports of public packages from PyPi, which will include cython support, but only for public pypi packages. Soon after that we'll be working on a way to provide proprietary packages (including cython).

- I tried to do this myself about ~1.5 years ago, but ran into issues with capturing state for sockets and open files (which started to show up when using some data science packages, jupyter widgets, etc.)
  - Good insight! We also initially tried to use Jupyter as a base but found that it had too much complexity (like the widgets you mention) for what we were trying to do and settled on something closer to a vanilla Python repl. This really simplified a lot.

- ğŸ†šï¸ How is this different than chatgpt's python code execution?
  - ChatGPT's code interpreter is mostly used as a calculator / graphing calculator. It can run arbitrary Python code, but it is limited in practice because it can't (e.g.) make external web requests or install arbitrary packages.

- Fun fact: this is very similar to how Smalltalk works. Instead of storing source code as text on disk, it only stores the compiled representation as a frozen VM. Using introspection, you can still find all of the live classes/methods/variables. 
  - Is this the best way to build applications? Almost assuredly not. But it does make for an interesting learning environment, which seems in line with what this project is, too.

- it swaps idle sessions to disk, so that they don't consume memory. From what I read, apparently "traditional" code interpreters keep sessions in memory and if a session is idle, it expires. This one will write it to disk instead, so that if user comes back after a month, it's still there.

- Is it possible to reuse the same paused VM multiple times from the same snapshot?
  - Check out why Togerther. AI acquired CodeSandbox.

- The API could be used for non-AI use cases if you wanted to, but itâ€™s built to be integrated with an LLM through tool calling. We provide an MCP (model context protocol, for integration in Claude, Cursor, Windsurf etc.) server.

- ## [Fast CI with MicroVMs | Hacker News _202211](https://news.ycombinator.com/item?id=33656767)
- Firecracker is nice but still very limited to what it can do.
  - My gripe with all CI systems is that an an industry standard we've universally sacrificed performance for hermeticity and re-entrancy, even when it doesn't really gives us a practical advantage. Downloading and re-running containers and vms, endlessly checking out code, installing deps over and over is just a waste of time, even with caching, COW, and other optimizations.
- The perceived practical advantage is the incremental confidence that the thing you built won't blow up in production.
  - Many CI systems do employ caching. For example, Circle.

- Hermeticity is precisely what allows you to avoid endlessly downloading and building the same dependencies. Without hermeticity you can't rely on caching.

- 

- ## ğŸ¤” [Deno raises $21M | Hacker News _202206](https://news.ycombinator.com/item?id=31827387)
- Lambda and Fargate use the Firecracker VM
  - The Lambda service team always emphasizes the level of isolation that Firecracker VMs gives you that containers donâ€™t.

- And Google Cloud Run use gVisor, which is also not a container.

- ## Docker is not the optimal way to run serverless workloads; hence, AWS Lambda or even an online judge like CodeChef, uses Firecracker as its execution environment.
- https://x.com/arpit_bhayani/status/1852550914638537196
  - This week, Iâ€™m reading about Firecracker which is Amazonâ€™s lightweight VM built specifically for serverless applications like AWS Lambda and Fargate. The paper covers how Firecracker supports thousands of short-lived containers and functions on shared hardware without sacrificing security or efficiency.
  - I skimmed the paper once, and it is pretty interesting to understand the story behind how Amazon found the right trade-offs between traditional hypervisors and containers and balanced both really well. 

- It is also interesting to read how Aws  fast load docker containers on lambda. 
- Docker has added lot of features, and continously evolving, abd has become bit heavy, so if you just need containers, then all features are not needed. That's the main reason kubernetes also uses 'containerd' which is mini version of docker, and extracted from docker.
# discuss-runtime-docker
- ## 

- ## 

- ## Docker Sandboxes are now available _202602
- https://x.com/Docker/status/2018368918612492466
  - They use isolated microVMs so agents can install packages, run Docker, and modify configs - without touching your host system. 

- Nice to see microVM sandboxes becoming mainstream for coding agents. We have been running agents inside isolated microVMs since day one at Umans, on our infra. What we are exploring with envs: one ready microVM per repo, so nothing touches your laptop.

- I build a version of this using docker-in-docker. Of course thatâ€™s not the same level of hard isolation as the VM approach, but it gives the agent a well isolated workspace where it can use docker.

- can you fix wsl support and testcontainers
# discuss-kata-containers
- ## 

- ## [Kata Containers: Virtual Machines that feel and perform like containers | Hacker News _202307](https://news.ycombinator.com/item?id=36757688)
- Kata used â€œLinux kernel Direct Access filesystem (DAX)â€ to directly share access of the host filesystem to the guest kernel. I thought this was pretty interesting, but it sounds like a possible spot to start a jailbreak. Iâ€™m guessing these kinds of optimizations along with using super simple virtualized devices is what gives Kata its almost-cgroups-like performance.

- Is it possible to add kata containers in eks or gke?
  - Not in EKS. EC2 doesn't support nested virtualization today.
- Peer pods are meant to solve this (one day) by running the "containers" (ie VMs) to the side as regular AWS instances and peering the communications
  - AWS already has an implementation of this, namely, Nitro Enclaves. A Nitro Enclave is an isolated VM used for confidential computing purposes. You provide the Enclave an EIF file, which is basically an OCI image with cryptographic measurements. The Nitro hypervisor launches the VM and allows communications with its parent VM only over a Linux vsock connection.
  - Also, AWS has Fargate, which is basically containers-as-VMs. Every ECS task or EKS Pod is launched as a separate EC2 instance under the hood. This obviates a lot of the need for a solution like Kata there.

- So why would you use this over regular containers? Is this for people who think containers are insecure?
  - Containers are not appropriate for some scenarios. If your threat model is "attacker has arbitrary code execution" you should be very wary of a container holding them for long - you're one kernel privesc away from a container escape.
  - By contrast, running attacker code in Firecracker is much safer - for one thing, the attacker needs to escalate within the VM in order to then expose the necessary primitives to then escape the VM. So it immediately adds an additional required vuln. But also, instead of the entire Linux kernel being the attack surface (it is for the first vuln, but...) you have to attack a much smaller codebase that implements the VM.

- ## [Kata Containers: The speed of containers, the security of VMs | Hacker News _202310](https://news.ycombinator.com/item?id=37762380)
- What I don't like about Docker is that it spews stuff all over the place. After installation, it is constantly running.
  - I was on a train in Germany recently and could not use the Wifi because of that. Turned out the docker daemon occupied the IP range the Train Wifi uses. While I was not using docker at all.
- Podman is probably close to what you want. It runs "daemonless" - while no containers are running, podman doesn't have a running process either. Also, as long as the containers are run in rootless mode, podman creates no virtual network interfaces. Rootless Podman makes use of network namespaces instead to separate the processes in the container from the host network. Processes on the host cannot see Podman's network namespaces and therefore are completely unaffected by any IP configuration therein.
- Podman is what Docker should have been, for me. Security first, no daemon, more Linux-like behavior (You can manage them with SystemD unit files if you wish) and it supports the same, usual container images you build/use with Docker. The main part it was lacking is the compose equivalent, but that too is coming along.
- You can use docker-compose with podman using podman's docker compatibility API.

- Is it coming along? Last I saw they were moving away from the compose Schema to a k8s manifest and... Those are absolutely disgusting.
- Podman has docker-compatible CLI, Dockerfiles and even docker-compose compatibility. It won't be as painful as you worry. So just give it a try. And besides the advantages already mentioned (rootless, no-daemon and isolated network namespaces), podman has some additional nifty features like pods (like in K8s), quadlets (containers managed by systemd), compatibility with k8s manifests, etc.

- Isnâ€™t podman just IBMs version of docker?

- We use Kata Containers to create Firecracker VMs from Kubernetes. Works really well for us. Though I am hoping there will be a more specific solution for Firecracker, as we don't need any other runtimes (which kind of ruins the purpose of Kata).

- Is there a good reason why kata containers don't seem widely adopted?
  - TBH their docs aren't that great. There should probably be a 'curl | sh' solution to install it at the top of the readme followed by a `<run this command and you're in an ubuntu shell in kata!>` command right after.
  - Another issue is the lack of nested virtualization in EC2 instances that aren't the very expensive i3 metals. That turns this from a "it's a drop in replacement" to "I'm spending thousands of dollars on this".
- The proposed solution to the nested virtualization problem (apart from somehow persuading Amazon to switch that on) is something called peer pods, where the containers run in separate AWS instances. Arranging the traffic between the peer pods and the instance which is acting as host is quite challenging and I've never seen this successfully deployed in production

- Amazonâ€™s Firecracker, another project with similar goals, has seen a lot of adoption however.
  - To add, firecracker is an alternative to qemu like katacontainers are an alternative to containerd or runc. But both focus of security by isolation, as you mentioned.

- "Like VMs but containers" doesnt tell me anything
  - By running the containers in VMs, and attempting to make the VMs boot very quickly by carefully tuning qemu (or Firecracker) and the guest kernel. The main problem with this approach is surprisingly not the time overhead - Kubernetes is fscking slow at scheduling regular containers - but the memory overhead, since you need to allocate sufficient memory up front for the largest possible memory usage of the container. Most containers expect to get more memory from the system simply by doing sbrk/mmap, and VMs simply don't work this way.

- LXC does it better, you can even have virtualization
  - The difference here is that katacontainers are OCI and CRI complaint - meaning that it can immediately be used with K8s, Nomad and possibly others. You get all the features of these orchestration platforms. LXC doesn't have that (it actually predates OCI and CRI).
  - There are other orchestration systems that can use LXC - LXD, libvirt, Proxmox, and may be others. Also, LXC doesn't have traditional virtualization - that's a feature of LXD using KVM. (Do you mean system containers, as opposed to regular app containers?)
# discuss-apple-container ğŸ
- ## 

- ## 

- ## 

- ## NanoClaw moved from Apple Containers to Docker _202602
- https://x.com/Gavriel_Cohen/status/2025603982769410356
  - When I built NanoClaw, it was my personal project. I use a Mac, I loved the idea of using lightweight Apple Containers that are optimized for Apple Silicon, so that's what I shipped. 
  - But NanoClaw isn't just my personal project anymore. Thousands of people are using it. 
  - The right thing for the project is to default to @Docker, supported universally, battle-tested, runs everywhere. So that's what we've done.

- Went through the same debate. Apple Containers are beautiful in isolation, but the moment you need CI/CD or a teammate on Linux, Docker's ecosystem gravity wins. Pragmatism > purity.

- Can't you use the same OCI containers in both worlds? If Docker is not installed + macOS 26 â¡ï¸ use Apple Containerization framework.

- docker's boring but it doesn't surprise you at 2am. that's worth a lot
# discuss-computer-use
- ## 

- ## 

- ## Launching public preview of C/ua Cloud Containers today - Docker for Computer-Use Agents. _202505
- https://x.com/trycua/status/1928173355347365942
  - Zero local setup. Same Computer and Agent interfaces. Scale 1-100 agents instantly.
  - First cloud platform built for Computer-Use Agents.
  - Linux/Windows/macOS desktops in your browser. 
  - Works with OpenAI, Anthropic, or any LLM. Pay only for compute time.
  - not open source yet - but if there's enough interest, we can cover it in a blog post

# discuss-sandbox-ai ğŸ‘¾
- ## 

- ## 

- ## 

- ## Your agents are useless without a sandbox. Sandbox is the primitive
- https://x.com/Rasmic/status/2005953095243596175
- Hard agree. but the real constraint is how much isolation you can afford before agents lose access to the context that makes them useful. The interesting work is finding the boundary where safety doesnâ€™t kill leverage.

- why? i just run them all in yolo mode. its fine.
# discuss-qemu
- ## 

- ## 

- ## 

- ## 

- ## I got the boot time for our little Linux sandbox down to <0.5s with QEMU and it now has TLS MITM. 
- https://x.com/mitsuhiko/status/2018805803239800874
  - Kinda shocked how easy this is to do armed with Opus and Codex. Userland sandbox plumbing is Zig, host is just node. (And I built the FS abstraction on @matteocollina 's new node VFS)
  - The sandbox is unaware of the funky business. It just talks ethernet. There is a JS implementation of Ethernet that handles DNS and TCP and re-encrypts TLS so you can do full interception of all HTTP requests which is super cool because you can just replace fetch.
- Tiago Freitas in founder mode @tiagoefreitas Â· 14h I wish it had native virtiofs on macos, only old QEMU patch series enable vhost-user backends that could enable VirtioFS. Use case is using macos apps on the same folders available inside the sandbox with native performance. Maybe the clankers can finish that
  - Oh my shit is way worse : D The entire file system is JavaScript 

- So now you can do something like after having mounted the VFS with the eventual thing to run ? For the network part did you attached to a socket ?
  - Sort of, but you register a fetch function instead, and the file system is a full posix like VFS.
# discuss
- ## 

- ## 

- ## 

- ## 

- ## [How safe is Docker compared to a VM for running ComfyUI and similar programs? : r/StableDiffusion _202503](https://www.reddit.com/r/StableDiffusion/comments/1j90nmg/how_safe_is_docker_compared_to_a_vm_for_running/)
- Short answer is that a VM is substantially more secure than a container. 
  - VM's use the CPU's virtualization functionality to isolate workloads while containers use the kernel to do the same. 
- Isolation: The VM runs it's own OS and does not rely on resources of the host OS. Docker uses a shared host kernel, which increases the risk of container escape vulnerabilities if the container is compromised. VM wins

- The root user in a container is not equivalent to root on the host, it by default is limited in capabilities. You have to actively relax that security in order to pull off exploits.

- ## I rebuilt @Replit â€” Sandbox is an open-source cloud code editing environment with an AI copilot and multiplayer collaboration, made with @Nextjs + @CloudflareDev Workers _202405
- https://x.com/ishaandey_/status/1796338262002573526
  - https://github.com/ishaan1013/sandbox
  - Collaborate in real-time on the Monaco editor, powered by @liveblocks and Y.js

- 
- 

- ## same.dev: after spending $5k+ running automated browsers:
- https://x.com/aidenybai/status/1907111968521023659
  - hosting browsers is annoying asf
  - existing hosting providers are EXPENSIVE
  - non-deterministic memory usage leads to over/under provisioning
  - sometimes just randomly crashes???

- cloudflare worker browsers any good? probably not as cheap or flexible though
- Stop using cloud run and dynamically spin up containers on a kubernetes cluster. Way cheaper.

- ## ğŸ†šï¸ [Ask HN: Pros and cons of V8 isolates? | Hacker News _202206](https://news.ycombinator.com/item?id=31740885)
  - I was reading this article on Cloudflare workers https://blog.cloudflare.com/cloud-computing-without-containe... and seemed like isolates have significant advantage over serverless technology like lambda etc.
  - What are the downsides of v8? Is it poor security isolation?

- The downside of v8 isolates is: you have to reinvent a whole bunch of stuff to get good isolation (both security and of resources).
  - Here's an example. Under no circumstances should CloudFlare or anyone else be running multiple isolates in the same OS process. They need to be sandboxed in isolated processes. Chrome sandboxes them in isolated processes.
  - Process isolation is slightly heavier weight (though forking is wicked fast) but more secure. Processes give you the advantage of using cgroups to restrict resources, namespaces to limit network access, etc.
  - My understanding is that this is exactly what Deno Deploy does (https://deno.com/deploy).
  - Once you've forked a process, though, you're not far off from just running something like Firecracker. This is both true and intense bias on my part. I work on https://fly.io, we use Firecracker. We started with v8 and decided it was wrong. So obviously I would be saying this.
  - Firecracker has the benefit of hardware virtualization. It's pretty dang fast. The downside is, you need to run on bare metal to take advantage of it.
  - My guess is that this is all going to converge. v8 isolates will someday run in isolated processes that can take advantage of hardware virtualization. They already _should_ run in isolated processes that take advantage of OS level sandboxing.
  - At the same time, people using Firecracker (like us!) will be able to optimize away cold starts, keep memory usage small, etc.
  - The natural end state is to run your v8 isolates or wasm runtimes in a lightweight VM.

- The future of compute is fine-grained. Cloudflare Workers is all about fine-grained compute, that is, splitting compute into small chunks -- a single HTTP request, rather than a single server instance. This is what allows us to run every customer's code (no matter how little traffic they get) in hundreds of locations around the world, at a price accessible to everyone.
  - The finer-grained your compute gets, the higher the overhead of strict process isolation gets. At the point Cloudflare is operating at, we've measured that imposing strict process isolation would mean an order of magnitude more overhead, in terms of CPU and memory usage. It depends a bit on the workload of course, but it's big. Yes, this is with all the tricks, zygote processes, etc.
  - We have plenty of defense-in-depth that we can do instead of process isolation, that doesn't have such enormous cost.

- > containers aren't a mechanism to increase compute granularity
  - Yes they are. Instead of thinking in terms of a whole running operating system with dozens of services, now you are thinking in terms of individual (micro?)services that are relatively isolated from each other. We stuff a lot more containers per box than we used to stuff VMs per box.
  - But it's true containers (of the namespace/cgroup/seccomp variety) have failed to be a sufficiently secure isolation mechanism to use them for multi-tenant scenarios, so instead we mostly pack containers from the same owner together.
  - I'd sort of argue that Firecracker and gVisor are actually container engines that happen to use CPU features meant for hardware VMs for additional security hardening. The granularity of compute that you put in them is more container-ish than traditional VM-ish.

- Answering the security question specifically: v8 is a runtime and not a security boundary. Escaping it isn't trivial, but it is common. You should still wrap it in a proper security boundary like gVisor.
- V8 is still very hard to escape -- an escape is a remote hole in Chrome isn't it? To be honest, I'm happy to base my security on "as safe as Chrome".
  - ğŸ’¡ Chrome runs each origin in a different process. Chrome also runs all the privileged code in separate processes, so like file, network, GPU, and many OS calls are not actually happening from within the same process as v8 executing web JS code. Process isolation is also necessary to mitigate CPU and OS level timing attacks between origins (and the privileged process).
  - v8 is a pretty good security boundary, but to be as secure as Chrome you need the other layers too.

- ğŸ› We did pretty much the same thing as Cloudflare does for workers in the Parse Cloud Code backend many years ago--when possible, we ran multiple V8 isolates in the same OS process. There are certain issues we ran into:
  - Security-wise, isolates aren't really meant to isolate untrusted code. That's certainly the way Chrome treats them, and you would expect that they would know best. For instance, if you go to a single webpage in Chrome that has N different iframes from different origins, you will get N different Chrome renderer processes, each of which contain V8 isolates to run scripts from those origins.
  - Isolation is not perfect for resources either. For instance, one big issue we had when running multiple isolates in one process is that one isolate could OOM and take down the all the isolates in the process. This was because there were certain paths in V8 which basically handled hitting the heap limit by trying to GC synchronously N times, and if that failed to release enough space, V8 would just abort the whole process. Maybe V8 has been rearchitected so that this no longer occurs.
  - Basically, the isolation between V8 isolates is pretty weak compared to the isolation you get between similar entities in other VMs (e.g. Erlang processes in the BEAM VM). And it's also very weak when compared to the isolation you get from a true OS process.
- > isolates aren't really meant to isolate untrusted code.
  - They totally are meant for that. That's arguably the main thing they are designed to do. The fact that people add defense-in-depth layers on top of it doesn't mean that the first layer isn't expected to be effective.

- wasm has its place, especially for contained workloads that can be wrapped in strict capability boundaries compile-time (think, file-encoding jobs that shouldn't access anything else but said files)
- > Containers are still the defacto standard.
  - afa FaaS is concerned, wasmedge [0], atmo [1], tarmac [2], krustlet [3], blueboat [4] and numerous other projects are turning up the heat

- ğŸ¤” Containers do not provide sufficient isolation to run untrustedd binaries. That's why aws built and uses firecracker for lambda.
  - VMs are also full of side channels. Depending on how much isolation is a concern, you need to own the host.
  - I don't trust VMs particularly more than containers in this respect: Containers have a lot of attack surface, but VMs also have a lot of complicated in the code in the kernel, in addition to having complicated emulated device drivers and a large silicon-based attack surface.

- There is also an issue of compatibility.
  - Underneath the hood other serverless technologies like lambda are running lightweight VMs running linux. Therefore they can easily accept any linux compatible container and they can run it for you in a serverless way.
  - Cloudflare Workers are running modified Node.js runtimes. You can only run code that is compatible with their modified Node.js runtime. For Cloudflare to be able to offer a product that runs arbitrary linux compatible containers they would have to change their tech stack to start using lightweight VMs.
  - If you want to run Node.js, then Cloudflare Workers probably works fine. But if you want to run something else (that doesn't have a good WASM compatibility story) then Cloudflare Workers won't work for you.

- The JVM has a similar feature since in the early 2000s. I don't know how popular it was in Java's heydays, but it doesn't seem used today. Being tied to a handful of languages may have been an issue.
  - As far as I know, the JSR-121 proposal was never actually implemented (or if it was, it was never publicly released). The OpenJDK issue is marked as "Won't Fix".
  - Not really a surprise after the `SecurityManager` was not very successful in managing security. It turned out to be a huge undertaking and a constant burden.
- Sounds similar to . Net's AppDomains which were deprecated in . Net Core

- ğŸ¤” if a process is running multiple Isolates simultaneously, they're multi threaded and still require context switching? (accepted, thread switches are less resource intensive than process switches). Interestingly when Chrome runs on Windows desktops it seems to allocate separate processes for each Isolate anyway, but I'm guessing this is not baked into V8?
  - Thread switching is much cheaper than process switching.
  - Threads are just stacks. Processes are whole address spaces. All kinds of caches need to be flushed when you switch processes (especially if you have more processes than you have PCID entries), but not when switching threads.

- For me, Cloudflare's decision to go with V8 Isolates was really smart and that blog post is a brilliant explanation of how one looks at trade-offs in engineering. Truly amazing work.
  - That said, no I don't believe V8 Isolates are the future of computing - and I think I'll explain why by comparing it to shared PHP hosting.
  - PHP became so big because it solved something that was very important for a lot of people 
  - If you deployed Python, your app would be running the whole Python stack just for you. You'd need resources for a python interpreter, all the HTTP libraries, all your imports, and then your code. 
  - On the other hand, if you ran PHP, you'd be sharing the PHP interpreter, the PHP standard library (which was written in C and not in PHP), and the server would only need to run your code. If your code was basically `mysql_select($query); foreach ($result in $results)...`, then the server was executing basically nothing. All the hard work was in C and all the request/response stuff was in C and all of that code was being shared with everyone else on the box so there was almost not RAM usage.
  - V8 Isolates are similar in a lot of ways. You get to share the V8 runtime and pay its cost once and then just run the code needed for the user. It makes sharing space really good.
  - So how isn't this the future? Not to knock PHP, but PHP isn't dominating the world. Most of the time, you're working at a place you have services that will be getting consistent traffic and you can put lots of endpoints into a service and you just run that service. There are things like having well-JIT'd code that can be good with long-running processes, you can use local caches with long-running processes, you pay the startup costs once (even if they might be tiny in some cases).

- Does dart isolate have any similarity to this ?
  - Would say, super generally and speaking loosely, yes: simply because I knew "isolate" from Dart, not V8, and didn't miss a beat with the article: in a sense, I'm surprised find the exact same concept in V8. I assume it was in V8 first, IIRC Dart/Flutter started from Chrome.

- ğŸ†šï¸ It seems to me that WASM is clearly a better suited technically as the core runtime of the future for serverless platforms... but the question is are isolates the VHS and WASM the Betamax in this story?
  - They're not comparable in that way. WASM runs inside a v8 runtime environment (or in other places), I'm pretty sure you can run wasm inside an isolate. You can basically think of WASM and JS as two different interpreters you can run inside of V8's little mini operating system.
- Actually that's incorrect, there are a number of wasm runtimes that don't use v8. Here's links to a couple: wasmtime, fastly-edge-compute
