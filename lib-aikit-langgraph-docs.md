---
title: lib-aikit-langgraph-docs
tags: [docs, langgraph]
created: 2025-08-11T08:47:23.340Z
modified: 2025-08-11T08:47:32.124Z
---

# lib-aikit-langgraph-docs

# guide

# overview
- To create an agent, use `create_react_agent`
- To configure an LLM with specific parameters, such as temperature, use `init_chat_model`

- Static prompt: A string is interpreted as a system message.
- Dynamic prompt: A list of messages generated at runtime, based on input or configuration.

- When you enable the `checkpointer`, it stores agent state at every step in the provided checkpointer database (or in memory, if using `InMemorySaver`).
  - when the agent is invoked the second time with the same `thread_id`, the original message history from the first conversation is automatically included, together with the new user input.

- To produce structured responses conforming to a schema, use the `response_format` parameter. 
  - The schema can be defined with a `Pydantic` model or `TypedDict`.

- 
- 
- 
- 
- 
- 
- 
- 

# docs

# blogs

## [LangGraph çš„ å†…å­˜/è®°å¿† æœºåˆ¶æ€»ç»“ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1915604388443031303)

- æ—¨åœ¨ä¸º AI ä»£ç†æä¾›åœ¨ä¸åŒäº¤äº’ä¸­å­˜å‚¨ã€æ£€ç´¢å’Œåˆ©ç”¨ä¿¡æ¯çš„èƒ½åŠ›
- çŸ­æœŸè®°å¿†æ˜¯é’ˆå¯¹å•ä¸ªå¯¹è¯çº¿ç¨‹ï¼ˆæˆ–ä¼šè¯ï¼‰çš„ï¼Œå¯ä»¥åœ¨è¯¥çº¿ç¨‹å†…çš„ä»»ä½•æ—¶é—´è¢«å›å¿†ã€‚å®ƒç±»ä¼¼äºäººç±»åœ¨ä¸€æ¬¡å¯¹è¯ä¸­è®°ä½çš„å†…å®¹ã€‚
  - LangGraph å°†çŸ­æœŸè®°å¿†ä½œä¸ºä»£ç†çŠ¶æ€ï¼ˆStateï¼‰çš„ä¸€éƒ¨åˆ†è¿›è¡Œç®¡ç†ã€‚è¿™ä¸ªçŠ¶æ€ä¼šé€šè¿‡æ£€æŸ¥ç‚¹ï¼ˆCheckpointerï¼‰æŒä¹…åŒ–åˆ°æ•°æ®åº“ä¸­ï¼Œè¿™æ„å‘³ç€å³ä½¿ä¼šè¯ä¸­æ–­ï¼Œä¹Ÿå¯ä»¥éšæ—¶æ¢å¤ã€‚
  - æ›´æ–°æœºåˆ¶ï¼š çŸ­æœŸè®°å¿†åœ¨å›¾è¢«è°ƒç”¨æˆ–æŸä¸ªæ­¥éª¤å®Œæˆæ—¶æ›´æ–°ï¼Œå¹¶åœ¨æ¯ä¸ªæ­¥éª¤å¼€å§‹æ—¶è¯»å–ã€‚
  - å…¸å‹åº”ç”¨ï¼š æœ€å¸¸è§çš„çŸ­æœŸè®°å¿†å½¢å¼æ˜¯å¯¹è¯å†å²ï¼ˆconversation historyï¼‰ã€‚
  - é’ˆå¯¹é•¿å¯¹è¯å¯èƒ½å¯¼è‡´çš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶å’Œæ€§èƒ½ä¸‹é™é—®é¢˜ï¼ŒLangGraph æä¾›äº†ä»¥ä¸‹ç­–ç•¥ï¼š
    - ç¼–è¾‘æ¶ˆæ¯åˆ—è¡¨ï¼ˆEditing message listsï¼‰
    - æ€»ç»“è¿‡å»å¯¹è¯ï¼ˆSummarizing past conversationsï¼‰ï¼š é€šè¿‡è°ƒç”¨ LLM å¯¹å†å²å¯¹è¯è¿›è¡Œæ‘˜è¦ï¼Œå¹¶å°†æ‘˜è¦ä½œä¸ºæ–°çš„ä¸Šä¸‹æ–‡ä¼ å…¥ï¼Œä»¥å‡å°‘æ¶ˆæ¯æ•°é‡åŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯
  - LangGraph å°†çŸ­æœŸè®°å¿†ä½œä¸ºå…¶æ ¸å¿ƒå›¾çŠ¶æ€ï¼ˆGraph Stateï¼‰çš„ä¸€éƒ¨åˆ†è¿›è¡Œç®¡ç†ï¼Œå¹¶é€šè¿‡å†…ç½®çš„â€œreducerâ€å‡½æ•°å®ç°çŠ¶æ€çš„åŸå­æ€§æ›´æ–°ã€‚è¿™ç§åŸºäºå›¾çš„çŠ¶æ€æœºæ¨¡å¼ä½¿å¾—ç®¡ç†å¯¹è¯å†å²å’Œç›¸å…³ä¸Šä¸‹æ–‡ï¼ˆå¦‚ä¸Šä¼ æ–‡ä»¶ã€æ£€ç´¢æ–‡æ¡£ï¼‰å˜å¾—éå¸¸è‡ªç„¶å’Œå¼ºå¤§ã€‚æ£€æŸ¥ç‚¹æœºåˆ¶ä¿è¯äº†ä¼šè¯çš„å¯æ¢å¤æ€§ã€‚

- é•¿æœŸè®°å¿†æ˜¯è·¨å¯¹è¯çº¿ç¨‹å…±äº«çš„ï¼Œå¯ä»¥åœ¨ä»»ä½•æ—¶é—´ã€ä»»ä½•çº¿ç¨‹ä¸­è¢«å›å¿†ã€‚å®ƒä¸åƒçŸ­æœŸè®°å¿†é‚£æ ·å±€é™äºå•ä¸ªä¼šè¯ã€‚
  - LangGraph å°†é•¿æœŸè®°å¿†å­˜å‚¨ä¸º JSON æ–‡æ¡£ï¼Œå¹¶ä½¿ç”¨å­˜å‚¨ï¼ˆStoreï¼‰ï¼ˆå¦‚ `InMemoryStore` æˆ–ç”Ÿäº§ç¯å¢ƒä¸­çš„æ•°æ®åº“æ”¯æŒçš„å­˜å‚¨ï¼‰è¿›è¡Œç®¡ç†ã€‚
  - æ¯ä¸ªè®°å¿†éƒ½ç»„ç»‡åœ¨ä¸€ä¸ªè‡ªå®šä¹‰çš„å‘½åç©ºé—´ï¼ˆnamespaceï¼‰å’Œä¸€ä¸ªå”¯ä¸€çš„é”®ï¼ˆkeyï¼‰ä¸‹ã€‚å‘½åç©ºé—´é€šå¸¸åŒ…å«ç”¨æˆ· IDã€ç»„ç»‡ ID æˆ–å…¶ä»–æ ‡ç­¾ï¼Œä¾¿äºåˆ†å±‚ç»„ç»‡ä¿¡æ¯ï¼Œå¹¶æ”¯æŒè·¨å‘½åç©ºé—´çš„æœç´¢ï¼ˆé€šè¿‡å†…å®¹è¿‡æ»¤å™¨ï¼‰ã€‚
  - é•¿æœŸè®°å¿†çš„ç±»å‹
    - è¯­ä¹‰è®°å¿†ï¼ˆSemantic Memoryï¼‰ï¼š å­˜å‚¨äº‹å®å’Œæ¦‚å¿µã€‚
    - æƒ…æ™¯è®°å¿†ï¼ˆEpisodic Memoryï¼‰ï¼š å­˜å‚¨è¿‡å»çš„äº‹ä»¶æˆ–è¡Œä¸ºç»éªŒã€‚å¸¸
    - ç¨‹åºè®°å¿†ï¼ˆProcedural Memoryï¼‰ï¼š å­˜å‚¨æ‰§è¡Œä»»åŠ¡çš„è§„åˆ™æˆ–æŒ‡ä»¤ã€‚
  - è®°å¿†æ›´æ–°æ—¶æœºï¼š
    - çƒ­è·¯å¾„ï¼ˆHot Pathï¼‰ï¼š åœ¨ä»£ç†çš„åº”ç”¨ç¨‹åºé€»è¾‘è¿è¡Œæ—¶å®æ—¶åˆ›å»ºè®°å¿†ã€‚ä¼˜ç‚¹æ˜¯å®æ—¶æ›´æ–°å’Œé€æ˜åº¦ï¼Œä½†å¯èƒ½å¢åŠ å¤æ‚æ€§ã€å»¶è¿Ÿï¼Œå¹¶å½±å“è®°å¿†çš„æ•°é‡å’Œè´¨é‡ã€‚
    - åå°ï¼ˆBackgroundï¼‰ï¼š ä½œä¸ºå•ç‹¬çš„å¼‚æ­¥ä»»åŠ¡åˆ›å»ºè®°å¿†ã€‚ä¼˜ç‚¹æ˜¯é¿å…ä¸»åº”ç”¨å»¶è¿Ÿã€åˆ†ç¦»é€»è¾‘ï¼Œå¹¶å…è®¸æ›´èšç„¦çš„ä»»åŠ¡å®Œæˆ

- LangGraph æ˜ç¡®åŒºåˆ†äº†çŸ­æœŸè®°å¿†ï¼ˆä¼šè¯çº§ï¼‰å’Œé•¿æœŸè®°å¿†ï¼ˆè·¨ä¼šè¯ï¼‰ï¼Œå¹¶è¿›ä¸€æ­¥å°†é•¿æœŸè®°å¿†ç»†åŒ–ä¸ºè¯­ä¹‰è®°å¿†ï¼ˆäº‹å®ï¼‰ã€æƒ…æ™¯è®°å¿†ï¼ˆç»éªŒï¼‰å’Œç¨‹åºè®°å¿†ï¼ˆè§„åˆ™ï¼‰ã€‚è¿™ç§ç»†ç²’åº¦çš„åŒºåˆ†å’Œç®¡ç†æ–¹å¼åœ¨ä¸»æµæ¡†æ¶ä¸­ç›¸å¯¹ä¸å¸¸è§ã€‚
- LangGraph æä¾›äº†ä¸€ä¸ªæ˜ç¡®çš„ Store æŠ½è±¡ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢é•¿æœŸè®°å¿†ï¼ˆJSON æ–‡æ¡£ï¼‰ã€‚å®ƒæ”¯æŒå‘½åç©ºé—´å’Œå†…å®¹è¿‡æ»¤/å‘é‡ç›¸ä¼¼æ€§æœç´¢ï¼Œå¹¶æä¾›äº†â€œprofileâ€å’Œâ€œcollectionâ€ä¸¤ç§ä¸åŒçš„è¯­ä¹‰è®°å¿†ç®¡ç†æ¨¡å¼ã€‚è¿™ç§æŠ½è±¡ä½¿å¾—é•¿æœŸè®°å¿†çš„é›†æˆæ›´ä¸ºç›´æ¥ã€‚

## â³ [The Evolution of Graph Processing: From Pregel to LangGraph _202507](https://medium.com/@pur4v/the-evolution-of-graph-processing-from-pregel-to-langgraph-6e8c2063df98)

- The story of large-scale graph processing is fundamentally about solving one of computer scienceâ€™s most challenging problems: how do you efficiently compute on massive, interconnected datasets that donâ€™t fit on a single machine
  - This journey from makeshift MapReduce solutions to Googleâ€™s revolutionary Pregel system, and ultimately to modern applications like LangGraphâ€™s multi-agent AI coordination
- Googleâ€™s 2010 Pregel paper didnâ€™t just introduce a new framework â€” it established an entirely new way of thinking about distributed graph computation that has influenced everything from social network analysis to modern AI agent coordination. 

- Life Before Pregel: The Dark Ages of Graph Processing
  - Single-machine approaches were simpler to program but hit hard physical limits. 
  - The dominant approach for distributed processing was MapReduce, but graph algorithms created a perfect storm of incompatibility. MapReduce required data chunks to be processed independently, while graph algorithms inherently needed information from neighboring vertices. This violated MapReduceâ€™s core assumption of data locality and independence.
  - The performance implications were devastating. A typical PageRank computation on large graphs required hundreds of chained MapReduce jobs, with processing times measured in hours to days. 
  - Each iteration involved moving the complete graph state across the network â€” a massive waste of resources where disk I/O time dominated computation time by factors of 5â€“20x.
  - The fundamental problems were clear: excessive data movement violated principles of data locality, synchronization overhead made coordinating distributed graph state expensive, and poor resource utilization meant massive clusters were required for basic graph analytics. 
  - Academic benchmarks from 2009â€“2010 showed MapReduce implementations were often 10â€“100x slower than theoretical optimal performance, with up to 90% of execution time spent on data movement rather than computation.

- What is Pregel: Thinking Like a Vertex
  - how Pregel works â€” it transforms graph problems into a series of coordinated communication rounds called â€œsupersteps.â€
  - Pregel builds on Leslie Valiantâ€™s Bulk Synchronous Parallel (BSP) model from the 1980s. 
  - Pregelâ€™s genius lies in its â€œthink like a vertexâ€ programming model. Instead of reasoning about the entire graph, you write code from a single vertexâ€™s perspective
  - Each vertex maintains state that persists across supersteps, can modify its own values and outgoing edges, and communicates through explicit message passing. 
  - This abstraction makes complex distributed graph algorithms surprisingly intuitive to implement.

- After Pregel: The New Graph Processing Era
  - Pregelâ€™s 2010 publication triggered an explosion of innovation in graph processing frameworks. 
  - The core insight â€” that vertex-centric programming with BSP synchronization could make distributed graph algorithms both intuitive and scalable â€” inspired dozens of systems that extended, optimized, and adapted the original design.
  - Apache Giraph emerged as the dominant open-source implementation, originally developed at Yahoo and later adopted by Facebook as their primary graph processing platform. Giraphâ€™s technical innovations included memory optimizations (serializing edges into byte arrays rather than Java objects), flexible input models supporting multiple data sources, and enhanced parallelization with multi-threading within workers.
  - Apache Sparkâ€™s GraphX took a fundamentally different approach by embedding graph processing within a general-purpose data processing framework. Instead of creating another standalone graph system, GraphX extends Spark RDDs with graph abstractions, enabling the same data to be viewed as both graphs and collections.
  - PowerGraph (Carnegie Mellon) introduced the Gather-Apply-Scatter (GAS) programming model specifically designed for power-law graphs with highly skewed degree distributions. By using vertex-cut partitioning â€” distributing edges rather than vertices â€” PowerGraph could handle high-degree vertices more efficiently than traditional approaches.

- These post-Pregel systems demonstrated several key evolution patterns:
  - Architectural Improvements: Memory management moved from object-based to byte-array representations, communication was optimized through message combining and sharded aggregators, and fault tolerance was enhanced with better checkpointing mechanisms.
  - Programming Model Extensions: Multi-phase algorithms gained support through master computation between supersteps, while higher-level abstractions and domain-specific languages made graph programming more accessible.
  - Specialization: Different systems optimized for specific use cases â€” social networks (Giraph), machine learning (PowerGraph), integrated analytics (GraphX), or research flexibility (GPS).

- The real test of Pregelâ€™s impact lies in its production applications at web-scale companies
  - Beyond PageRank, Google employs Pregel-based systems for web spam detection, analyzing link patterns to identify manipulative linking schemes, link farms, and other attempts to game search rankings.
  - Twitter developed GraphJet, a specialized real-time graph processing engine that maintains a bipartite interaction graph between users and tweets entirely in memory. This system achieves 1 million edge ingestions per second while serving 500 recommendations per second with sub-second latency.

- LangGraph and Pregel: Coordinating AI Agents in the Modern Era
  - The emergence of large language models and multi-agent AI systems has created an unexpected new application for Pregelâ€™s BSP model. 
  - LangGraph, developed by LangChain, demonstrates how the same principles that revolutionized web-scale graph processing now enable sophisticated coordination of AI agents in production systems.
- Why Traditional Approaches Fall Short
  - Conversational AI and multi-agent systems expose fundamental limitations in traditional DAG-based workflows. Linear processing pipelines cannot handle the iterative, cyclical reasoning required by intelligent agents. 
  - Traditional frameworks are stateless, processing each interaction independently without maintaining context across conversations or sessions.
  - Traditional approaches cannot maintain shared state across these agents or recover gracefully when individual agents fail.
- The core problems mirror pre-Pregel graph processing challenges: rigid execution paths without dynamic decision-making, poor state management across distributed components, and limited fault tolerance for long-running processes.
  - LangGraph addresses these limitations by implementing Pregelâ€™s BSP model specifically for AI agent coordination. 
  - The system models agent workflows as graphs with shared state, executable nodes, and conditional edges, organizing execution into discrete supersteps where agents that can run in parallel execute simultaneously.
- Agent Communication follows Pregelâ€™s message-passing paradigm:
  - Direct messaging: Agents send specific updates to target agents
  - Broadcast communication: Agents send messages to multiple recipients
  - State-mediated interaction: Communication through shared state channels
  - Conditional routing: Dynamic message routing based on current state
- Each superstep follows the familiar BSP pattern: agents process their current state in parallel, send messages to other agents, and synchronize at a barrier before the next superstep begins.
- Think of LangGraph like coordinating actors in a complex theatrical performance. 
  - Each actor (AI agent) has their role and script, but they must coordinate their actions with other actors on stage. 
  - Supersteps are like synchronized scenes where all actors perform their parts simultaneously, then pause for stage direction (synchronization) before continuing to the next scene.
- Unlike improvisation where actors might interrupt each other or miss cues, the BSP model ensures every actor completes their lines before the scene transitions.

- Production Applications and Success Stories
  - LinkedInâ€™s SQL Bot demonstrates LangGraphâ€™s production capabilities, using coordinated agents for natural language to SQL conversion. 
  - Uberâ€™s code migration system employs multiple agents for large-scale automated refactoring across repositories. 
  - Replitâ€™s coding assistants use collaborative agent architectures for complex programming tasks.

- Technical Architecture and Fault Tolerance
  - LangGraph implements comprehensive checkpoint systems that save state after each superstep, enabling powerful fault tolerance capabilities. Unlike traditional systems where failures require complete restarts, LangGraph can recover from any checkpoint and even support â€œtime travelâ€ functionality â€” rolling back and forking execution from previous states.
  - Memory management operates at multiple levels: working memory for current superstep reasoning, conversation memory for multi-turn interactions, long-term memory for cross-session knowledge retention, and shared memory accessible to all agents in the system.
  - The system provides human-in-the-loop integration with interrupt functions that pause execution for human input

- ğŸ¤” Why BSP Works Better Than Event-Driven Models
  - Event-driven architectures create race conditions where unpredictable message ordering leads to inconsistent agent behavior. Agents may operate on stale information, debugging becomes extremely difficult due to non-deterministic execution paths, and the system lacks built-in persistence mechanisms.
  - LangGraphâ€™s BSP approach provides deterministic execution with predictable superstep-based processing, global synchronization ensuring all agents reach consistent states, and built-in checkpoint/recovery mechanisms. The synchronous nature provides consistent timing characteristics and enables reliable resource management across distributed agent networks.
  - Performance characteristics demonstrate BSPâ€™s advantages: predictable latency through synchronous supersteps, resource efficiency via parallel node execution, scalability through framework-managed distribution, and reliability with comprehensive fault tolerance.

- The Assembly Line Analogy (for BSP)
  - Think of LangGraphâ€™s BSP coordination like a sophisticated assembly line where each station (agent) performs specialized work on shared products (state). 
  - Every station completes its work before the conveyor belt moves to the next position (superstep synchronization). If any station encounters problems, the entire line can pause, fix the issue, and resume from exactly where it left off (checkpointing).
  - This coordination model ensures `quality` control (no agent operates on inconsistent state),  `efficiency` (maximum parallelism within each superstep), and `reliability` (comprehensive error recovery mechanisms).

- Why does BSPâ€™s synchronous model persist despite its apparent inefficiency? 
  - The answer lies in the fundamental tension between performance and predictability. 
  - While asynchronous systems can achieve higher throughput by eliminating global barriers, they sacrifice deterministic execution and introduce complex debugging challenges.
  - The emergence of â€œchaos engineeringâ€ in distributed systems suggests that even modern cloud-native architectures struggle with the complexity introduced by fully asynchronous coordination.

- Pregel and Real-Time Streaming: An Unexplored Frontier
  - Can BSP adapt to real-time streaming scenarios where data arrives continuously rather than in discrete batches? Traditional stream processing frameworks like Apache Storm or Kafka Streams use event-driven models, but recent research suggests hybrid approaches might be possible.
  - The challenge lies in checkpointing and fault tolerance for rapidly changing state. How do you implement recovery mechanisms when the system state changes thousands of times per second? This represents a fundamental research opportunity at the intersection of stream processing and graph algorithms.

- Modern federated learning systems face coordination challenges remarkably similar to early graph processing. 
  - Multiple parties need to collaboratively train machine learning models while maintaining data privacy and handling network partitions or device failures.
  - Could Pregel-style coordination improve federated learning? The BSP modelâ€™s deterministic execution and built-in fault tolerance seem well-suited to scenarios where participating devices may join or leave unpredictably. Each device could be modeled as a vertex in a learning graph, with model updates passed as messages between supersteps.

- Privacy-preserving graph algorithms represent another frontier. Techniques like differential privacy or secure multi-party computation could potentially be integrated with BSP coordination to enable privacy-preserving social network analysis or collaborative filtering.

- Will the rise of large language models fundamentally change how we think about graph processing? 
  - LLM-guided graph partitioning could potentially outperform traditional hash-based or even sophisticated algorithms by understanding semantic relationships between vertices. 
  - Conversely, will graph processing techniques enhance LLMs? The success of retrieval-augmented generation (RAG) suggests that combining structured knowledge graphs with language models creates capabilities neither approach achieves alone. Graph neural networks applied to LLM architectures represent an active research area with significant potential.

- How will specialized hardware affect graph processing architectures? 
  - Modern GPUs excel at parallel computation but struggle with the irregular memory access patterns common in graph algorithms. 
  - Graph processing units (GPUs specifically designed for graph workloads) are emerging from research labs, potentially requiring fundamental changes to software architectures.

## [Building LangGraph: Designing an Agent Runtime from first principles _202509](https://blog.langchain.com/building-langgraph/)

- LangGraph builds upon feedback from the super popular LangChain framework, and rethinks how agent frameworks should work for production. 
  - We aimed to find the right abstraction for AI agents, and decided that was little to no abstraction at all. 
  - Instead, we focused on control and durability. 
- This post shares our design principles and approach to designing LangGraph based on what weâ€™ve learned about building reliable agents.

- We started LangGraph as a reboot of LangChainâ€™s super popular chains and agents more than two years ago. 
  - We decided that starting fresh would give us the most leeway to address all the feedback we had received since the launch of the original langchain 
- The main thing we heard was that langchain was easy to get started but hard to customize and scale.
- When we had to make tradeoffs, we prioritized production-readiness over how easy it would be for people to get started.

- â€œDo we actually need to build LangGraph?â€ And, â€œWhy canâ€™t we use an existing framework to put agents in production?â€ 
  - we had to define what makes an agent different (or similar) to previous software. 
  - More latency makes it harder to keep users engaged
  - Retrying long-running tasks when they fail is expensive and time-consuming
  - The non-deterministic nature of AI requires checkpoints, approvals, and testing

- ğŸ› The first defining quality and challenge of LLM-based agents is latency. We used to measure the latency of our backend endpoints in milliseconds. Now, we need to measure agent run times in seconds, minutes, or soon hours.
  - This is because LLMs themselves are slow and are becoming slower with test-time compute. Itâ€™s also because we often need multiple LLM calls to achieve our desired results, with looping agents, and chaining
  - So, we identified two features that would come in handy when building agents: Parallelization, Streaming ğŸ’¡

- ğŸ› Long-running agents fail more often because, the longer they run, the more opportunity there is for something to go wrong.
  - When traditional software bugs out, you usually want to retry. With AI agents? That may not be the best approach. 
  - going back to the beginning is pretty time consuming and also expensive.
  - So we knew we had to add two more features to the list: Checkpointing, Task queue ğŸ’¡

- ğŸ› the non-deterministic nature of LLMs creates two more challenges.
  - With LLMs, both input and their output is open-ended. 
  - two more features then become very useful when going to production: Human-in-the-loop, Tracing ğŸ’¡

- If the agent youâ€™re building doesnâ€™t need most of these features (eg. because itâ€™s a very short agent without tools and a single prompt), then you might not need LangGraph, or any other framework.

- Why was a new framework needed?
- Existing frameworks were mostly split between two categories:
  - DAG frameworks (made popular by Apache Airflow and many others): These we had to exclude just based on the name, as LLM agents really benefit from looping, ie. the computation graph for an LLM agent is cyclical, and thus cannot be handled by DAG algorithms.
  - Durable execution engines (made popular by Temporal and others): they were lacking some of those specific features â€” namely streaming. In addition, these engines introduced latency between steps which would have been noticeable to chatbot developers. Lastly, due to their design, the performance degrades the more steps there are in the history

- We designed LangGraph with two leading principles.
  - We donâ€™t know what the future holds for AI. The only assumptions we wanted to bake into it were the realizations we talked about above, i.e. that LLMs are slow, flaky, and open-ended.
  - It should feel like writing code. The public API for the framework should be as close as possible to writing regular framework-less code
- These principles impacted a few key design decisions that have stayed with us ever since.
  - First, the runtime of the library is independent from the developer SDKs. The runtime (which we call PregelLoop) implements each of the features listed earlier, plans the computation graph for each agent invocation, and executes it. This design enables us to evolve the developer APIs and the runtime independently. 
  - Second, we wanted to provide each of the 6 features as building blocks, with the developer free to pick which to use in their agent at any point in time.

- âœ¨ letâ€™s look at how LangGraph implements each of the 6 features we wanted to have (as a reminder, these are parallelization, streaming, checkpointing, human-in-the-loop, tracing and a task queue).
- If there is one idea that informs every other architectural decision weâ€™ve made, it is the idea of structured agents. 
- Once you make the choice to structure agents into multiple discrete steps, you need to choose some algorithm to organize its execution. 
  - Even if itâ€™s a naive one that feels like â€œno algorithm, â€ which is where LangGraph started before launch. 
  - The problem with using â€œno algorithmâ€ is, while it may seem simpler, youâ€™re making it up as you go along, and end up with unexpected results. (For instance, an early version of a precursor to LangGraph suffered from non-deterministic behavior with concurrent nodes). 
  - The usual DAG algorithms (topological sort and friends) are out of the picture, given we need loops. We ended up building on top of the BSP/ Pregel algorithm, because it provides deterministic concurrency, with full support for loops (cycles).

- Our execution algorithm works like this:
  - Channels contain data (any Python/JS data type), and have a name and current version (a monotonically increasing string)
  - Nodes are functions to run, which subscribe to one or more channels, and run whenever they change
  - One or more channels are mapped to input, ie. the starting input to the agent is written to those channels, and therefore triggers any nodes subscribed to them
  - One or more channels are mapped to output, ie. the return value of the agent is the value of those channels when execution halts
  - The execution proceeds in a loop
  - The execution loop stops when there are no more nodes to run (ie. after comparing channels with their subscriptions we find all nodes have seen the most recent version of their subscribed channels), or when we run out of iteration steps (a constant the developer can set).
# more
