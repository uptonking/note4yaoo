---
title: spec-format-apache-arrow-community
tags: [apache-arrow, community]
created: 2021-07-24T08:16:02.068Z
modified: 2021-07-24T08:16:41.076Z
---

# spec-format-apache-arrow-community

# guide

# discuss-stars
- ## 

- ## 

- ## 
# discuss-parquet
- ## 

- ## 

- ## ClickBench keeps me convinced that  Parquet can be quite fast. There is only a 2.3x performance difference vs @duckdb 's own format and unoptimized parquet
- https://x.com/andrewlamb1111/status/1925537738360504663
  - I am surprised that the (closed source) Umbra only reports 3.3x faster than DuckDB on parquet
  - BTW the delta between DuckDB and DataFusion is almost entirely explained by filter pushdown and late materialization, which we are working on
- Rather than twitter speculation, here is a fun project @MOVNTDQ filed about actually measuring how much better performance you can get using just parquet by simply sorting / clustering
- Agree. I believe it's possible to 2x or even 5x parquet or certain workloads, but 10x is impossible with an optimized parquet reader . Across all workloads even 3x is hard.

# discuss
- ## 

- ## 

- ## 

- ## Vortex is an Apache Arrow-compatible toolkit for working with compressed array data. 
- https://x.com/fredine/status/1803525967174049962
  - We are using Vortex to develop a next-generation columnar file format for multidimensional arrays.

- ## Arrow æ¯æ¬¡å‘ç‰ˆéƒ½è·³ä¸€ä¸ªå¤§ç‰ˆæœ¬
- https://twitter.com/OnlyXuanwo/status/1675803368856633345
- ä½†è¿™æ ·ç†è®ºä¸Šä½ å¾—å¯¹æ‰€æœ‰ç‰ˆæœ¬æµ‹è¯•å§
- ä¸»è¦é—®é¢˜æ˜¯ç‰ˆæœ¬ä¸åŒçš„è¯æ²¡æ³•äº’ç›¸äº¤äº’ï¼Œæ¯”å¦‚ darabend ç”¨ arrow 40ï¼Œä½†æ˜¯ ice lake å†™äº† arrow 42ï¼Œé‚£å°±å¯„äº†
- æœ¬è´¨æ˜¯ arrow å¤ªæ‘†çƒ‚äº†ï¼Œä¸€ç‚¹ API stability éƒ½ä¸æ„¿æ„ä¿è¯ï¼Œæ°¸è¿œæ˜¯åœ¨ bump major version
  - è¿™å¯èƒ½éƒ¨åˆ†ä¹Ÿå› ä¸º ASF é¡¹ç›®æ¯æ¬¡å‘ç‰ˆéƒ½éœ€è¦æŠ•ç¥¨ï¼Œä¸çŸ¥é“ OpenDAL æ€ä¹ˆè§£å†³è¿™ä¸ªé—®é¢˜
  - è¿™å…¶å®å¯¹åˆ«çš„åº“æ²¡å•¥é—®é¢˜ï¼Œä¸»è¦æ˜¯arrowæ¯”è¾ƒç‰¹æ®Šï¼Œè¦è¢«å¤šä¸ªåº“è·¨ç‰ˆæœ¬ç”¨

- ## Streamlit is now more performant thanks to @ApacheArrow !
- https://twitter.com/streamlit/status/1418637045468000256
  - Using Arrow for data serialization helped us delete over 1, 000 lines of code 
- In general, the performance improvements come from reduced data serialization time (i.e. sending data from Python to the browser).
  - But pandas itself is also becoming faster due to Apache Arrow, so having both pandas and Streamlit using the same format should yield improvements
  - The app creator still needs to think about their workflow, use caching as appropriate, etc. **Apache Arrow can't fix algorithmic complexity of complex transformations**

# discuss-showcase
- ## 

- ## çœ‹äº†ä¸€å¤©çš„ datafuselabs/databend ä»£ç æ‰å‘ç°å…¶èŠ‚ç‚¹é—´æ•°æ®äº¤æ¢åŸºäºApache Arrow Flightï¼Œå’Œ apache/arrow-ballista ç¥ä¼¼ï¼Œä¸è¿‡åè€…READMEä¸€ç›®äº†ç„¶ã€‚ä¸çŸ¥é“è¿™ä¸ªä¸¤ä¸ªé¡¹ç›®æœ‰å•¥å…³ç³»
- https://twitter.com/yanchanglu/status/1578758012994433030
- https://github.com/apache/arrow-ballista
  - Ballista is a distributed SQL query engine powered by the Rust implementation of Apache Arrow and DataFusion.
  - Ballista implements a similar design to Apache Spark (particularly Spark SQL)
  - Ballista is designed from the ground up to use columnar data, enabling a number of efficiencies such as vectorized processing (SIMD) and efficient compression. Although Spark does have some columnar support, it is still largely row-based today.
  - The use of Apache Arrow as the memory model and network protocol means that data can be exchanged efficiently between executors using the Flight Protocol, and between clients and schedulers/executors using the Flight SQL Protocol

- ## "InfluxDB uses DataFusion for its Query Execution and Arrow as its internal data representation"
- https://twitter.com/gunnarmorling/status/1683184256011366400
  - Enjoyed reading this article about the system architecture of #InfluxDB 3.0
  - [InfluxDB 3.0: System Architecture](https://www.influxdata.com/blog/influxdb-3-0-system-architecture/)

- ## ç”¨ Go è¯­è¨€å¼€å‘çš„å¼€æºæ—¶åºå‹æ•°æ®åº“ InfluxDB å®£å¸ƒäº†æœªæ¥çš„å¼€å‘è®¡åˆ’ï¼šInfluxDB IOxï¼Œç”¨ Rust è¯­è¨€å¼€å‘ï¼Œä½¿ç”¨ Apache Arrow å’ŒæŸ±åˆ—å¼æ•°æ®ç»“æ„ã€‚
- https://twitter.com/wllcjd/status/1326999282105286656
  - InfluxDB é¡¹ç›®åˆ›å§‹äºº Paul Dix ç§°ï¼ŒRust èƒ½ç»™äºˆè¿è¡Œæ—¶è¡Œä¸ºå’Œå†…å­˜ç®¡ç†æ›´ç»†ç²’åº¦æ§åˆ¶ï¼Œå®ƒå¸¦æ¥çš„é¢å¤–å¥½å¤„æ˜¯å¹¶å‘ç¼–ç¨‹æ›´å®¹æ˜“æ¶ˆé™¤äº†æ•°æ®ç«äº‰ã€‚

- ## [Apache Arrow, Parquet, Flight and Their Ecosystem are a Game Changer for OLAP | InfluxData_202004](https://www.influxdata.com/blog/apache-arrow-parquet-flight-and-their-ecosystem-are-a-game-changer-for-olap/)
  - Apache Arrow, a specification for an in-memory columnar data format, and associated projects: Parquet for compressed on-disk data, Flight for highly efficient RPC, and other projects for in-memory query processing will likely shape the future of OLAP and data warehousing systems.

# discuss-roadmap-changelog
- ## 

- ## 

- ## ğŸ¯ [Apache Arrow 3.0 | Hacker News_202102](https://news.ycombinator.com/item?id=26018187)
- Arrow is the most important thing happening in the data ecosystem right now. 
  - It's going to allow you to run your choice of execution engine, on top of your choice of data store, as though they are designed to work together. 
  - It will mostly be invisible to users, the key thing that needs to happen is that all the producers and consumers of batch data need to adopt Arrow as the common interchange format.
- Google BigQuery recently implemented the storage API, which allows you to read BQ tables, in parallel, in Arrow format
  - Snowflake has adopted Arrow as the in-memory format for their JDBC driver, though to my knowledge there is still no way to access data in parallel from Snowflake, other than to export to S3.
  - As Arrow spreads across the ecosystem, users are going to start discovering that they can store data in one system and query it in another, at full speed, and it's going to be amazing.

- The premise around arrow is that when you want share data with another system, or even on the same machine between processes, most of the compute time spent is in serializing and deserializing data. 
  - Arrow removes that step by defining a common columnar format that can be used in many different programming languages. 
  - Theres more to arrow than just the file format that makes working with data even easier like better over the wire transfers (arrow flight). 
- ğŸ‘‰ğŸ» Not only in between processes, but also in between languages in a single process. In this POC I spun up a Python interpreter in a Go process and pass the Arrow data buffer between processes in constant time

- ğŸ†šï¸ How is this any better than something like flatbuffers though?
  - For one, Arrow is columnar. ğŸ‘‰ğŸ» The idea of zero-copy serialization is shared between Arrow and FlatBuffers.
  - This is explained in the FAQS for Arrow, saying they use flatbuffers internally. Arrow supports more complex data types.
  - Only for IPC support - Arrow data format does not use flatbuffers.

- ğŸ†šï¸ The big thing is that it is one of the first standardized, cross language binary data formats. 
  - CSV is an OK text format, but parsing it is really slow because of string escaping. The files it produces are also pretty big since it's text.
  - Arrow is really fast to parse (up to 1000x faster than CSV), supports data compression, enough data-types to be useful, and deals with metadata well. 
  - The closest competitor is probably protobuf, but protobuf is a total pain to parse.

- ğŸ˜© Until arrow has proper support for multidimensional arrays, its not really appropriate for many use cases.
  - FWIW, you can support tensors etc. natively today by using Arrow's structs
- Almost no database systems support multidimensional arrays. So they are not appropriate for many use cases?
  - * BigQuery: no * Redshift: no * Spark SQL: no * Snowflake: no * Clickhouse: no * Dremio: no * Impala: no * Presto: no ... list continues
  - We've invited developers to add the extension types for tensor data, but no one has contributed them yet. I'm not seeing a lot of tabular data with embedded tensors out in the wild.
- ClickHouse has support for multidimensional arrays with arbitrary types and number of dimensions. They are stored in tables in efficient column-oriented format.
- The only database I know of that natively supports multidimensional arrays is SciDB (one of Stonebraker's products)
- TileDB supports both sparse and dense multidimensional arrays. We support returning data in arrow arrays or an arrow table as one of many ways we interoperate with computational tools. You can also access the data directly via numpy array, pandas, R data.frames and a number of integrations with MariaDB, Presto, Spark, GDAL, PDAL and more.

- What are the use cases where multi-dimensional arrays are important and why?
  - R&D/manufacturing of consumer facing sensors, in my case.
  - Astronomy, seismology, microscopy. There's an interesting query language, SciQL, built to support such use cases. It can be used with MonetDB

- 
- 
- 
