---
title: lib-db-app-community-etl
tags: [community, database, etl]
created: 2023-09-17T18:10:19.845Z
modified: 2023-09-17T18:10:33.050Z
---

# lib-db-app-community-etl

> about data sources and integration

# guide

# discuss-stars
- ## 

- ## [æœ‰äººè§£ç­”ä¸‹dataxã€talendã€etlcloudå„è‡ªçš„ä¸åŒå—ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/603875795)
- dataxæ˜¯eltå·¥å…·æ²¡æœ‰webç•Œé¢ï¼Œæ€§èƒ½ä¹Ÿä¸€èˆ¬
  - talendæ˜¯å•†ç”¨etlå·¥å…·ä¹Ÿæœ‰ç¤¾åŒºç‰ˆæœ¬æ˜¯c/sç»“æ„çš„ï¼Œå›½å†…ç”¨æˆ·ç¾¤ä½“æ¯”è¾ƒå°‘å‡ºäº†é—®é¢˜ä¸çŸ¥é“æ‰¾è°
  - etlcloudåº”è¯¥æ˜¯å›½å†…æœ€å¥½çš„ä¸€æ¬¾etlå·¥å…·äº†ï¼Œå…¨webç•Œé¢åŠŸèƒ½ä¹Ÿå¾ˆå¼ºå¤§ï¼Œè¿˜æœ‰cdcå®æ—¶æ•°æ®å¤„ç†èƒ½åŠ›

- ## ğŸ†šï¸ [Otter, Canal, DataXè¿™å‡ ä¸ªå¼€æºçš„æ•°æ®åŒæ­¥å·¥å…·å·®åˆ«åœ¨å“ª? ç©¶ç«Ÿè¯¥ç”¨å“ªä¸ª? - çŸ¥ä¹](https://www.zhihu.com/question/392983860)
- cannalã€otteréƒ½æ˜¯é˜¿é‡Œå·´å·´æ——ä¸‹å¼€æºçš„äº§å“ï¼Œå…¶ä¸­canalæ˜¯åªæœ‰æ•°æ®å¢é‡è®¢é˜…çš„åŠŸèƒ½, ä¸èƒ½å®ç°æ•°æ®åŒæ­¥ï¼Œotteræ˜¯å†…åµŒäº†canalçš„ï¼Œåˆ©ç”¨å…¶å¢é‡è®¢é˜…ï¼Œæ¥è¿›è¡Œæºæ•°æ®åº“å’Œç›®æ ‡æ•°æ®åº“çš„ç»‘å®šï¼Œæ”¯æŒåŒå‘ç»‘å®šå’Œå•å‘ç»‘å®šï¼Œè¿™ä¸¤ä¸ªè¯¥ç”¨å“ªä¸ªå°±çœ‹éœ€æ±‚äº†ï¼Œdataxæ²¡ç”¨è¿‡ä¸åšè¯„ä»·

- Otterã€Canalè”åˆèµ·æ¥ä½¿ç”¨ï¼Œç”¨äºä¸¤ä¸ªmysqlæ•°æ®åº“çš„å®æ—¶åŒæ­¥ï¼Œç®€å•é€šä¿—åœ°è¯´å°±æ˜¯ï¼Œæºmysqlä¸€æ—¦æœ‰å˜æ›´åŠ¨ä½œï¼ˆä¹Ÿå°±æ˜¯æ“ä½œæ—¥å¿—ï¼‰ï¼Œå°±ä¼ é€’ç»™Canalï¼ŒCanalå†ä¼ é€’ç»™Otterï¼ŒOtterå†åœ¨ç›®æ ‡mysqlé‡Œé‡åšè¯¥åŠ¨ä½œï¼Œç”±äºåŒæ–¹åšäº†ç›¸åŒåŠ¨ä½œï¼ŒåŒæ–¹å°±ä¸€è‡´ã€‚è¿™ç§ä¼ é€’-é‡åšçš„è¿‡ç¨‹ä¸€ç›´åœ¨è¿›è¡Œï¼ŒåŒæ–¹å°±ä¸€ç›´ç›¸åŒã€‚
  - DataXåˆ™ä¸åŒï¼Œå®ƒæ˜¯ç›´æ¥è¯»å–æºåº“çš„æ•°æ®ï¼Œå¹¶å†™å…¥ç›®æ ‡åº“ï¼Œä»è€Œä½¿åŒæ–¹ä¸€è‡´ã€‚è¿™ç§åŠ¨ä½œæ‰§è¡Œå®Œæˆåï¼Œç¨‹åºå°±é€€å‡ºäº†ï¼Œå¦‚æœä»¥åéœ€è¦å†åŒæ­¥ï¼Œéœ€è¦å†æ‰§è¡Œä¸€æ¬¡ã€‚

- ## ğŸ†šï¸ How are you propagating changes from your operational database to your data warehouse / analytics system?
- https://twitter.com/gunnarmorling/status/1718681535803449394
  - batch vs streaming  =  0.52: 0.48  /145votes
- It is a tough question. A lot of factor should be considered. Such as how the change impacts reports, is there any realtime report got impacted â€¦ if all is â€œyesâ€, you will need realtime pipeline, else, batch processing is preferable.
- Streaming is much higher than expected and seen in the wild in this poll.  I suspect itâ€™s influenced by your audience.
- Both!

# discuss
- ## 

- ## 

- ## Why is it that there's so little code reuse in the data transformation layer/ETL?
- https://x.com/mistercrunch/status/1841875802092405059
- I've been involved in several data interoperability standards committees over the years. In my experience (which is admittedly outdated) only power ($, legislation, etc..) drives adoption (e.g. medicare, walmart)

- ## ğŸ“¥ Question for data people, how do you track files ingested, do you create a specific table or just a simple log like CSV etc ?
- https://twitter.com/mim_djo/status/1755004742005387534
- Snowflake automatically create a metadata table for that purposeâ€”no need for any manual stuff. Back in the good old Hadoop days, we created a simple table with the date that has been ingested.
- but how do you make sure only 1 file is copied?
  - I think the uniqueness is the combination of filename, checksum and modified date

- In the @dagster , it is given out-of-box with the sensors whereby the run_key is unique and not run two times. You leverage metadata in the background
  -  we write the checkpoint next to the source folder in the same structure and write files that it was processed

- Normally, I have a table in the database where I log the pipeline ingestion and also track errors in the data processing that could be helpful for troubleshooting later

- We create a lineage table with a row per file or group of files, then this becomes a key on every row in every table within staging, any integration tables and final star schema. In Fabric we also stamp filename on table, if a single notebook loaded multiple files in one go.

- ## If you fix your Data, you have solved half of your problems. And using SQL Constraints is the perfect way to start.
- https://twitter.com/RaulJuncoV/status/1735285206179860639
  - PRIMARY Key Constraint
  - FOREIGN Key Constraint
  - UNIQUE Constraint
  - CHECK Constraint
  - NOT NULL Constraint
  - DEFAULT Constraint: useful when you want to provide a default value instead of leaving it NULL.

- ## DuckDB don't support RAM cache when querying Parquet files(to be fair none of the OSS engine support it), 
- https://twitter.com/mim_djo/status/1725848821681541295
  - current workaround is to load the whole tables into Memory, this is basically what #PowerBI did with import mode, at least before 2021
  - [Announcing on-demand loading capabilities for large models in Power BI_202112](https://powerbi.microsoft.com/en-us/blog/announcing-on-demand-loading-capabilities-for-large-models-in-power-bi/)
- so in a sense the biggest innovation of #Fabric direct lake is the capability to load and transcode from Parquet to the in-memory format that PowerBI understand, that's none trivial at all 

- ## Do you want a #database that can:
- https://twitter.com/OnlyXuanwo/status/1725494564755325036
  - SELECT * FROM 'gdrive://*.csv'
  - COPY FROM 'dropbox://*.xlsx' INTO table
- It seems that @duckdb supports both features. It may not support `gdrive://` or `dropbox://` scheme, but with the help of fuse i think it is possible to perform IO directly over these webdrivers
  - Yep, many databases support load data from services like s3. As known as data lake house 
- Taking a look at the popularity of projects like dsq, clickhouse-local, duckdb, sqlite-utils, datafusion-cli, etc.
- TiDB does support importing data from S3/GCS, but I agree GDrive/Dropbox sounds much cooler for personal usage. 

- ## ğŸ”¥ [Show HN: Stein â€“ Use Google Sheets as a No-Setup Database | Hacker News_201907](https://news.ycombinator.com/item?id=20426682)
- 
- 
- 

- ## [SQL should be the default choice for data transformation logic | Hacker News_202301](https://news.ycombinator.com/item?id=34578324)

> æŠ€æœ¯é€‰å‹è¦æ ¹æ®ä½¿ç”¨åœºæ™¯ï¼Œæ•°æ®ç±»æ“ä½œå’Œè®¡ç®—å¤§å¤šç”¨python

- To me, data pipelines are about moving data from one place to another. and that's not really covered by the SQL spec. The article doesn't specify what constitutes a "data engineering pipeline", but for me there's three components to it:
  - orchestration, or what happens when
  - data manipulation, aka the T in ETL
  - data movement, getting data from A to B
- Orchestration? Please do not use dynamic SQL, you're digging a hole you'll never get out of. Moreover, SQL has zero support for time-based triggers so you'll need a secondary system anyway.
- Manipulating the data? Sure, use SQL, as long as you're operating on data from one database. Trying to collate(æ”¶é›†, æ•´ç†; æ ¡å¯¹, æ ¸å¯¹ ) data from multiple databases may have unpredictable performance issues even in the engines that support it.
- Moving data? There are much better options. And the same caveat as for orchestration applies here: every RDBMS vendor has its own idea of what external query's should look like (openrowset, external tables, linked servers) with their own performance considerations.
- Even on the manipulation side I tend to disfavor sql.
  - Even though itâ€™s improved a lot the testing situation in sql is still not where a more traditional programming language is, modularity comes either in the form of ever nested cte, stored procs or dbt style templates. And **sql types are wholly dependent on the sql engine, which can lead to wacky transforms**.
  - Sql is great for adhoc analysis. If something isnâ€™t adhoc, there is almost always a better tool.

- How do you test some SQL logic in isolation?
  - I do this using sql
  1. Extracting an 'ephemeral model' to different model file
  2. Mock out this model in upstream model in unit tests https://github.com/EqualExperts/dbt-unit-testing
  3. Write unit tests for this model.
  - This is not different than regular software development in a language like java.
  - I would argue its even better better because unit tests are always in tabular format and pretty easy to understand. Java unit tests on other hand are never read by devs in practice.

- As someone who works with 10s to 100s of TB of data from SQL and SQL-like DBs on a daily basis, writes lots of analytical code, I can say, emphatically, BS. Unless your analyses are trivial (column mean, max, etc.), chances are you need a real programming language behind you. 
  - And when you are working with dataframes that are 300+ million rows (glances over to his work machine, yup, that's a smaller analysis I need to run), you need a compiled and fast language for this, which can run in parallel on multiple threads and machines. 
  - You aren't using SQL for this. You aren't likely using pandas for this, or pyspark.

- I'm fine with this, but SQL has some lingering(ç»§ç»­å­˜åœ¨çš„) shortcomings:
  - Inability to express recursive/nested datasets directly (tree-like).
  - General inability to express structural sharing and graphs directly.
  - Inability to express associative (map-like) relationships directly.
  - Some of these are solved in the SQL standard, but not universally adopted. Others are solved by particular databases, but also not universally adopted. For the rest, of course you can solve it if we add the condition "some assembly required" when you get the data.
  - But if you have to assemble and disassemble the data at every step, it stops being a viable pipeline choice. It's as good as serializing into CSV, JSON, or whatever. In fact, JSON at least can nest.

- I would strongly recommend reconsidering the suggestion that SQL serve as a data engineering pipeline default. We use SQL heavily at our organization, and use it for broad, significant ELT workloads. 
  - In essence, we use SQL, largely via DBT, where appropriate.
  - However, everywhere else, which comprises(åŒ…å«ï¼›æ„æˆï¼›ç»„æˆ) a significant collection of data pipeline services, uniform where possible but definitely heterogenous, we default to Clojure; we also utilize Python where more appropriate. Of these 3 languages **SQL clearly has the least generality, least testability, least API integration capability**, and definitely the most awkward data processing capability. I feel it is naive to suggest it could serve as a default language for data engineering pipelines, particularly given the need to interact with cloud services and third parties.

- Iâ€™ve found that sql pipelines end up depending heavily on a specific database (ssis packages for sqlserver) so really suck when you donâ€™t want to use that database any more, or still need something to coordinate and run all the sql.

- Most SQL centric projects I've touched had a severe lack of automated tests.
  - Most projects I've touched had a severe lack of automated tests.

- SQL is fine. But as an engineer, I much prefer getting a notebook over a 1000-line plate of SQL spaghetti.

- I learnt this when I worked at www.carto.com. In Geo Analysis, you start with Raw data and then apply a series of transformations to it. The idea that all these transformations could be summarized in chain of SQL commands fascinated me.

- 
- 
- 

- ## dbt (data build tool) enables analytics engineers to transform data in their warehouses by simply writing select statements. 
  - dbt handles turning these select statements into tables and views.
  - dbt does the T in ELT (Extract, Load, Transform) processes â€“ it doesnâ€™t extract or load data
  - A dbt project is a directory of .sql and .yml files. 
