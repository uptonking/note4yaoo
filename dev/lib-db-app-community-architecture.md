---
title: lib-db-app-community-architecture
tags: [architecture, community, database]
created: 2023-09-17T17:37:06.649Z
modified: 2023-09-17T17:37:19.913Z
---

# lib-db-app-community-architecture

> about database and application architecture

# guide
- who is using #db-per-user
  - bluesky
# discuss-stars
- ## 

- ## 

- ## 

- ## Poll: When your automated tests run on your CI server, what database does your app call?
- https://x.com/housecor/status/1955624930487497212
- Why run these tests at all? I ran them on my machine, we're good, right?
- I truncate all tables between every test case, but I guess its technically only a unique db per build
- I wonder if the size of the test data would matter & what is the purpose of that test?

- ## [Synchronizing local state with the database : r/rust _202306](https://www.reddit.com/r/rust/comments/14jfcen/synchronizing_local_state_with_the_database/)
- I'm working on a websocket server too, and I'm completely forgoing the DB for real time updates.
  - Roughly, each thing a user might open a WS connection for is managed in-memory by a tokio task. This tokio task pulls in input, applies it to its local copy of the state, and then pushes the updates to other connections. This keeps everything entirely in-memory to avoid as much latency as possible.
  - To persist things to the DB, there is a separate task running that writes to the DB. Depending on what data you're storing, you can either communicate via a channel that your state has changed and should be read or you can just send the updates via the channel directly. In the first case, you'll want to store your document in something like a left-right. In the second case, your persistence task can probably write directly to the DB.

- Meilisearch does something extremely similar; they call it autobatching requests. 

- 
- 

- ## ğŸ” at Facebook, our experience was that clientâ€“server roundtrips are expensive and serverâ€“DB roundtrips are mostly fine; 
- https://twitter.com/sophiebits/status/1728536177110905215
  - this is reflected in the design of both Relay and XHP/RSC
  - (whether this is applicable outside FB depends on how your DB is set up)
- Does relay hoist fragments to the top or no?
  - relay does (to avoid clientâ€“server waterfalls); xhp doesnâ€™t

- https://twitter.com/OlegLustenko/status/1728498563292811385
- 90% of time we do need top-level fetching. Waterfall is a real perf bottleneck for the first meaningful screen.

- ## Data Pipelines Overview
- https://twitter.com/huangyun_122/status/1725519643580760501
  - åŠ¨ç”»ç¤ºæ„å›¾

- ## ğŸ¤”ğŸ”¥ [Databases = Frameworks for Distributed Systems | Hacker News_202205](https://news.ycombinator.com/item?id=31459745)
- I was watching a video by one of Amazonâ€™s distinguished engineers a while ago and for the life of me I canâ€™t find it now but the thing that stuck with me from it is, 
  - â€œThere are only three modes for a distributed system - it implements paxos/raft itself, it relies on a data store that implements paxos/raft or itâ€™s wrong.â€
  - Look at k8s and etcd for a very widely used example of this approach as well.

- While databases having been converging on composability for a long time, the model presented here is not the correct one and causes problems for many important and emerging use cases.
  - As an elementary observation, isolating the designs of storage, compute, and networking as separate modules is a reliable way to achieve poor performance and resource efficiency. Some of the most important scalability and performance optimizations in modern database architecture are schedule-driven, which requires tight coupling across the design of storage, compute, and networking. Ignoring this class of optimizations produces very large reductions in real-world workload performance, and feeds a narrative that the software is wasteful and not environmentally friendly.
  - Most of our distributed systems are modeled as giant distributed file systems, and that fits okay with the idea of horizontally partitioning compute, storage, and network. This is easy for developers to reason about because they are familiar with file systems. However, it is difficult to express important database-y operations in this model. For example, if you want to do ad hoc joins in a scale-out environment, you need mechanics like decentralized parallel orchestration which isn't really a concept in a "distributed file system" model of "distributed".

- I would say the author just tried to redefined "distributed", and ignored those real distributed systems, e.g. Paxos/Raft-based ones.
  - You are thinking of fully decentralised systems. A system where there is a centralised leader is still a distributed system so long as at least some of the processing takes place on other nodes.

- ## ğŸ’¡ğŸ”¥ [Immutable Databases | Hacker News_202005](https://news.ycombinator.com/item?id=23290769)
- 
- 
- 

- ## ğŸ¤”ğŸ”¥ [Ask HN: Do you use foreign keys in relational databases? | Hacker News_202209](https://news.ycombinator.com/item?id=32731916)
- 
- 
- 

- ## ğŸ”¥ [The growing pains of database architecture | Hacker News_202306](https://news.ycombinator.com/item?id=36208420)
- 
- 
- 

- ## I think I want to change the way I write code from â€˜projectsâ€™ to a library of code that is tagged in a database and queryable. 
- https://twitter.com/JungleSilicon/status/1713939838624501941
  - The queries could compose software dynamically or even be used as part of static site generation.
- Are there many examples of similar efforts? It would work really well with sync & user defined behaviours.
  - Check out @ValDotTown and @observablehq . Both store code in a database and let you import between the database entries. Observable has real-time sync, http://val.town does not.
  - That's very similar to what @ValDotTown is doing, a database of code snippets.

- Some former teammates of mine used https://glean.software to query code at the level of language constructs. IIRC you can extend it to include additional information in the code index.

- ## [General-purpose databases that never delete or update data in-place - Stack Overflow](https://stackoverflow.com/questions/13508035/general-purpose-databases-that-never-delete-or-update-data-in-place)
- I'm very much inspired by the approach to data management advocated by Rich Hickey, and implemented in Datomic, where the data is never mutated in-place, all the versions are always preserved and query-able, and the time is a first-class concept.

- I think both the BerkeleyDB Java Edition and CouchDB work like that internally
- Noms is versioned, forkable, syncable, append-only database. It is possible to see the entire history of the database
- LiteTree SQLite with Branches

- ## ğŸ¤”ğŸ”¥ [How not to structure database-backed web apps: performance bugs in the wild | Hacker News_201806](https://news.ycombinator.com/item?id=17414383)
- 
- 
- 

- ## ğŸ§­ğŸ›¢ï¸ [ä¸ºä»€ä¹ˆ JS ä¸èƒ½ç»•è¿‡åç«¯ä»£ç ç›´æ¥è°ƒæ•°æ®åº“ï¼Œæœ‰å“ªäº›åç«¯å¤„ç†çš„é€»è¾‘ï¼ŒJS ä¸èƒ½å†™ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/307461543)

- ä¹‹æ‰€ä»¥ä¸èƒ½ç›´æ¥å¯¹æ¥åˆ°æ•°æ®åº“ï¼Œè¦ä¸­é—´æœåŠ¡ä»£ç†ä¸€ä¸‹ï¼ˆgraphql ä¹Ÿéœ€è¦â€œä»£ç†â€ï¼‰ï¼Œä¸»è¦æœ‰å‡ ä¸ªåŸå› ï¼š
  - å®‰å…¨é—®é¢˜ï¼šç›´æ¥æš´éœ²æ•°æ®åº“è¿æ¥åœ°å€ã€è¿æ¥ tokenï¼ˆä¸ç®¡æ˜¯å¯†ç è´¦å·è¿˜æ˜¯ä¸­é—´tokenï¼‰ï¼Œéƒ½å¯èƒ½å¯¼è‡´æ•°æ®åº“è¢«è„±åº“ï¼Œå®¹æ˜“è¢«çˆ†ç ´ã€ç¯¡æ”¹
  - æ€§èƒ½é—®é¢˜ï¼šæ•°æ®åº“ SQL çš„ä¼˜åŒ–å¾ˆå¤æ‚ï¼Œä¸æ˜¯çœ‹åˆ°çš„ç®€å• select è¯­å¥ã€CRUDï¼Œåˆ†åº“åˆ†è¡¨ã€ç´¢å¼•ä¼˜åŒ–éƒ½éš¾ä»¥åœ¨å‰ç«¯å®ç°ï¼Œå³ä½¿ GraphQLï¼Œä¹Ÿéœ€è¦ db çš„ä¸­é—´å±‚

- åœ¨å‰ç«¯ç”¨å­—ç¬¦ä¸²æ‹¼æ¥SQLè¯­å¥ï¼Œåç«¯æ•°æ®åº“ç›´æ¥æ‰§è¡Œï¼Œç„¶åæŸ¥è¯¢ç»“æœé€å›å‰ç«¯å¤„ç†ã€‚å†™èµ·æ¥æ˜¯æŒºçˆ½çš„ï¼Œå¼€å‘æ•ˆç‡å¥‡é«˜ï¼Œå› ä¸ºåŸºæœ¬ä¸éœ€è¦å†™åç«¯äº†ã€‚
  - ä½†æ˜¯è¿™æ ·ä¼šå¸¦æ¥å¾ˆå¤§çš„å®‰å…¨æ€§é—®é¢˜ï¼Œå¾ˆå¿«æˆ‘å°±å†³å®šå…¨éƒ¨é‡å†™ã€‚
- ä¸»æµçš„å…³ç³»å‹æ•°æ®åº“ï¼Œæ¯”å¦‚MySQL, åªèƒ½æ”¯æŒåˆ°è¡¨çº§åˆ«çš„é‰´æƒã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€ä¸ªç”¨æˆ·å¦‚æœè¢«æˆæƒè®¿é—®ä¸€å¼ è¡¨ï¼Œé‚£ä¹ˆä»–å°±èƒ½è®¿é—®è¿™å¼ è¡¨é‡Œçš„æ‰€æœ‰æ•°æ®ã€‚
  - ä½†æ˜¯ç°å®ä¸­çš„ä¸šåŠ¡é€»è¾‘ï¼Œé€šå¸¸ä¸å¸Œæœ›ç”¨æˆ·å¯ä»¥è®¿é—®è¡¨é‡Œçš„æ‰€æœ‰æ•°æ®ã€‚
  - ç”¨æˆ·å¯ä»¥ç¯¡æ”¹ä¸€äº›ä¿¡æ¯
- å‰ç«¯å‘æ¥çš„è¯·æ±‚éƒ½æ˜¯ä¸å—ä¿¡ä»»çš„ã€‚åç«¯è¦å¯¹å®ƒè¿›è¡Œå„ç§æ ¡éªŒå®¡æŸ¥ï¼Œæœ€åæ‰èƒ½æ‰§è¡Œå¹¶è¿”å›ç»“æœã€‚
  - æ¯”å¦‚è¦æ£€æŸ¥ç”¨æˆ·çš„ç™»å½•çŠ¶æ€ï¼Œè¾“å…¥çš„å‚æ•°æ˜¯å¦åˆæ³•æœ‰æ•ˆï¼ŒSQLè¯­å¥è¦ç”¨ä¸“é—¨çš„é€»è¾‘å»æ¸…æ´ï¼Œè€Œä¸èƒ½ç”¨ç®€å•çš„å­—ç¬¦ä¸²æ‹¼æ¥ï¼Œä»¥é¿å…SQLæ³¨å…¥è¿™ç§æ”»å‡»ã€‚
  - JavaScriptè¿è¡Œåœ¨æµè§ˆå™¨é‡Œï¼Œå¾ˆå®¹æ˜“è¢«ç¯¡æ”¹ã€‚ç½‘ç»œè¯·æ±‚ä¹Ÿå¯ä»¥åœ¨æµè§ˆå™¨é‡Œè¢«æŸ¥çœ‹å’Œä¿®æ”¹ã€‚
- å½“ç„¶ï¼Œä¸€äº›ä¼ ç»Ÿçš„æ¡Œé¢ç‰ˆä¿¡æ¯ç®¡ç†ç³»ç»Ÿè½¯ä»¶ï¼Œå‰ç«¯ä¸æ˜¯JavaScriptå†™çš„ï¼Œæ¯”å¦‚ç”¨C#, ç”šè‡³C++å®ç°çš„ã€‚è¿™äº›è½¯ä»¶éƒ¨ç½²åœ¨ä¼ä¸šå†…éƒ¨çš„ç³»ç»Ÿé‡Œï¼Œç½‘ç»œç¯å¢ƒç›¸å¯¹æ›´åŠ å®‰å…¨å¯é ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå…¶å®æ˜¯å¯ä»¥åœ¨â€œå‰ç«¯â€ç›´æ¥æ“çºµæ•°æ®åº“çš„ã€‚ä¼ ç»Ÿçš„ERPè½¯ä»¶å¾ˆå¤šéƒ½æ˜¯è¿™ä¹ˆåšçš„ã€‚

- ç›´æ¥ç”¨ SQL ä¹Ÿä¸æ˜¯ä¸èƒ½ä¿éšœå®‰å…¨ï¼Œæœ‰å¤§æ¦‚ä¸¤ç§æ–¹æ³•ï¼š
  - 1ã€ç”¨æˆ·ä¸æ•°æ®åº“ç”¨æˆ·ç»Ÿä¸€ï¼Œåœ¨åˆ›å»ºç½‘ç«™ç”¨æˆ·æ—¶åˆ›å»ºæ•°æ®åº“ç”¨æˆ·å¹¶è®¾ç½®æƒé™ï¼›
  - 2ã€ä»£ç†å±‚è§£æå‰ç«¯çš„ SQLï¼Œæ£€æµ‹æ˜¯å¦è¶…è¿‡å¿…è¦é™å®šï¼Œæˆ–è€…é™„åŠ é™åˆ¶æ¡ä»¶ç­‰ã€‚
  - æ–¹æ³• 1 ä¸å¤ªå¥½å®æ–½ï¼Œä¸ºäº†ç®¡ç†å’Œç»Ÿè®¡æ–¹ä¾¿ï¼Œé€šå¸¸ä¼šæŠŠæ•°æ®é›†ä¸­åˆ°ä¸€èµ·ï¼Œå³ä½¿ç»†åˆ†äº†è´¦å·ä¹Ÿéš¾ä»¥é™å®šè¡Œçº§çš„æ“ä½œï¼›
  - æ–¹æ³• 2 è¦å…ˆå°† SQL è§£ææˆå¦ä¸€ä¸ªæ–¹ä¾¿æ£€æµ‹å’Œå¤„ç†çš„æ•°æ®ç»“æ„ï¼Œåœ¨æ£€æµ‹å’Œé™„åŠ é™å®šä¹‹åå†æ‹¼è£…å› SQLï¼Œéš¾åº¦æœ‰ç‚¹å¤§ã€‚ 

- æŠ€æœ¯ä¸Šæ ¹æœ¬æ²¡æœ‰éšœç¢ã€‚JSå¯ä»¥åšå‡ ä¹ä»»ä½•äº‹ï¼Œä½†æ˜¯å¾ˆå¤šäº‹æƒ…ä¼šè¢«æµè§ˆå™¨ç»™æ‹¦ä¸‹æ¥ã€‚
  - æ¯”å¦‚ä½ ç”¨jsä¸å€ŸåŠ©fileæ§ä»¶ç›´æ¥è¯»å†™ä¸€ä¸ªæœ¬åœ°æ–‡ä»¶ä½ è§‰å¾—å¯èƒ½å—ï¼Ÿå¦‚æœçœŸçš„å¯ä»¥åå°ç›´æ¥æ“ä½œï¼Œé‚£å½“ä½ æ‰“å¼€æŸä¸ªç½‘é¡µæ—¶ï¼Œä½ çš„æ‰€æœ‰ç”µè„‘ä¿¡æ¯éƒ½å¯ä»¥è¢«æ‹·è´èµ°
  - ä½†å‡¡æœ‰ç‚¹ç»éªŒçš„å¼€å‘éƒ½çŸ¥é“ï¼Œæ‰€æœ‰æ”¾åˆ°å‰ç«¯çš„ä¸œè¥¿éƒ½æ˜¯ä¸å®‰å…¨çš„ã€‚
# discuss-arch-per-user ğŸ‘£
- ## 

- ## 

- ## [Plane: Per-user backends for web apps | Hacker News _202210](https://news.ycombinator.com/item?id=33178797)
  - Plane came from our desire to build tools that have the low friction to use of running in the browser, but use more memory and compute than the browser will allocate. The basic idea is to run a remote background process, connect to it over WebSocket, and stream data.
- A related and also very useful usage pattern: "backend instance per customer". 
  - Becauseâ€¦ There are many ways to implement multi-tenant SaaS; but a highly underrated approach is to write a single tenant app, then use infrastructure to run an instance of it per (currently logged in) customer. Plus a persistent database per customer of course.
  - This has the tremendous advantage that you can add another column to your pricing page easily: the "bring a truckload of money and we will set you up to run this behind your firewall" tier. There are still a lot of orgs out there, large ones with considerable financial capacity, who really want this.

- The implementation makes some weird choices like rebuilding a bunch of services like DNS, cert, weird dependency on SQLite. Wish people would stop reimplementing Kubernetes and just build on top of it.
  - I think "per-user" is probably the wrong killer feature for something like this. Much more potential in shared distributed processes that support multiple users (chat, CRDT/coauthoring). Appears that the underlying layer can probably do that.

- This seems similar to an Elixir/Phoenix use case where you have a GenServer per user. At first glance, it seems like that approach would be functionally equivalent.
  - Yes, the BEAM/OTP/{Erlang/Elixir} stack is unique in that it provides similar primitives as part of the runtime.
  - My impression of that approach is that itâ€™s good for IO-bound work and stateful business logic, but less so for the CPU/memory-bound applications that weâ€™re targeting. Iâ€™d love to know if there are counterexamples to that though. Itâ€™s admittedly been over a decade since I touched Erlang and Iâ€™m not up to date, only peripherally familiar with Elixir and Phoenix.
- Yes, for CPU bound processed on the BEAM you'll want to use a NIF (native implemented function) but that leaves you open to taking down the entire VM with bad NIF code (segfaults, infinite loops, etc). 

- I think CouchDB kind of has a db per user kind of model. I was wondering, what's the best way to persist a session. This kind of per-user model I think is super useful for local-first, decentralized apps.

- This is the same concept that Cloudflare has with Durable Objects. 
  - Yes, DOs have a similar approach. Plane is sort of like DOs but using containers instead of V8 isolates, allowing you to run code that couldnâ€™t run in the browser (either for cpu/memory needs, or because it just wonâ€™t compile to JS/wasm).
- If I remember correctly, CF liked isolates because they have a superb cold start time. How does Plane do there?
  - On the order of a few seconds, which is certainly slower than isolates, but fast enough to e.g. spin up a Jupyter backend in a reasonable amount of time. We're playing around with some ideas to make it even faster, like snapshotting.

- most messaging apps and almost all bi-directional websocket based apps do something very similiar. mobile apps/webapps become thin clients with a web socket.
  - that said, it's nice to abstract it away, and with a container - makes it very generic which is nice.

- 
- 

# discuss-db-per-user ğŸ‘£
- ## 

- ## 

- ## 

- ## Apps used to have one database for multiple users. Now many have one database per user. Soon we will have multiple databases for each user.
- https://x.com/jamespearce/status/1943114966162379234
  - This is a pattern I find myself using increasingly with @tinybasejs : some of a userâ€™s app data is private. Some is local. Some is in the cloud. Some is shared. Some is synced. 
  - Makes sense to shard it out into different databases altogether, each with different behaviors.

- ## We are writing a massive multitenant database at Turso. A node is capable of running hundreds of thousands of databases, concurrently. 
- https://x.com/iavins/status/1900220354985169332
  - We also decided to write our own asynchronous runtime implementation (instead of using `Tokio` ) for reasons. Now this bad boy is all bare bones, we don't use Rust's `async` yet.
  - For disk or network we use io_uring (of course). Since it's a database, that's pretty much all it does: talk with io. And that requires a state machine. You submit a request, wait for some time to poll or for callback to trigger. That means, a function doesn't always have a result ready; sometimes it says, my friend wait for sometime, I don't have result ready yet: `Ok(None)` . When it's done you get: `Ok(Some(T))` .
  - The entire codebase is pretty much `Result<Option<T>>`

- Fallible Iterators also use Result`<Option<T>>. Ok(Some(T))`: when there's valid next item. Ok(None) when iteration's complete.

- ## The idea of having a separate database per tenant in a multi-tenant application always felt way too complicated to me.
- https://x.com/marcelpociot/status/1802942305143296136 
  - But now I'm thinking: doesn't SQLite make this a lot easier?

- We use separate DBs per tenant and it's a lifesaver, for a couple reasons:
  - you can bill tenants for their DB Usage
  - you don't have noisy neighbours
- The core issues with it (we use it for HelpSpot for legacy reasons) is schema changes are burdensome as things may go wrong in one account, but not another etc. On the other hand, it does mean things only go wrong with one customer in that case and not all the customers! So overall for our scale and desired stress level I do like it. 

- ## ğŸ› read this before you go do the â€œdb per userâ€ pattern
- https://twitter.com/thdxr/status/1766450800425865279
  - it makes a lot of things harder
  - you cannot easily query across customers and while your primary application might not need to do this, internal functions often will
  - any analytics questions you have, any admin or operations workflows require extra special architecture
  - it forces replicating your data into a central place way earlier (we used bigquery) but that only works for reads
  - schema and data migrations need to be fanned out N times - lot of stuff you have to invent for this
  - it was worth it for us given the specific circumstances but i doubt iâ€™d really ever do it again
- Not to mention the problems you hit if/when you need a product feature to share data between users.
  - What is the problem you're really trying to solve with db per user? Authorization. This is why I'm a fan of things like row level security.

- Whatâ€™s your opinion on something like ATTACH in SQLite. Allows you to connect to multiple DBs in a transaction. Would something like that have solved your issues?
  - that maybe could have been helpful but probably still needs a few exotic things
  - we used lmdb (similar, single file on disk) and the problem was not all files were on the same machine

- Was doing this with Replicache + Turso and it definitely makes some things a bit more annoying
  - I asked yesterday about the schema and data migration issue and they said they will have something to solve that in their launch week soon

- Had to go in this direction of multi-tenancy two years back and instead chose to use RLS with custom policies in PostgreSQL to achieve the logical isolation.

- ## [Apple built iCloud to store billions of databases | Hacker News_202401](https://news.ycombinator.com/item?id=39028672)
- I leveraged FoundationDB and RecordLayer to build a transactional catalog system for all our data services at a previous company, and it was honestly just an amazing piece of software. 
  - Adding gRPC into the mix for the serving layer felt so natural since schemas / records are defined using Protobuf with RecordLayer.
  - ğŸ› The only real downside is that the onramp for running FoundationDB at scale is quite a bit higher than a traditional distributed database.

- Given that FoundationDB is built on top of SQLite, I wonder if that team is eyeing the HCTree engine for it.
  - It's still in experimental mode but provides literally 10x improvement on read/writes to SQLite.
- They have built their own storage engine named Redwood, which has some very FoundationDB-specific optimizations (like prefix compression). 

- How would you handle schema migrations in a system like this?
  - It depends on the layer, some of the layers might be able to take advantage of how the data is persisted. For example, if you use avro/protobuf, the decoder will handle it for you. If that's not the case, you would have to implement the migration by yourself.

- CouchDB implements a DB per user approach. Personally, I've found it much easier to use than an SQL DB for web apps
- 
- 
- 
- 
- 

- ## ğŸ”¥ [Database of Databases | Hacker News_202009](https://news.ycombinator.com/item?id=24494403)
- 
- 
- 

- ğŸ”¥ [Database of Databases | Hacker News](https://news.ycombinator.com/item?id=37314622)

- ## ğŸ”¥ [The magic of small databases | Hacker News_202301](https://news.ycombinator.com/item?id=34558054)
- 
- 
- 

- ## ğŸ”¥ [Millions of Tiny Databases [pdf] | Hacker News_202002](https://news.ycombinator.com/item?id=22329256)
- 
- 

- ## ğŸ”¥ [Millions of Tiny Databases | Hacker News_202003](https://news.ycombinator.com/item?id=22481612)
- 
- 
- 

# discuss-db-in-memory
- ## 

- ## 

- ## âŒ›ï¸ In-memory database systems were a game changer when they first came out in the early 2010s. But it looks like everyone moved back to persistent storage.
- https://x.com/cedar_db/status/1863986711845306799
- ğŸ“ [The History of the Decline and Fall of In-Memory Database Systems | CedarDB - The All-In-One-Database _202412](https://cedardb.com/blog/in_memory_dbms/)
  - So what did we learn from the story of in-memory database systems? 
  - For one thing, main memory capacity is now enough to fit a significant share of data, allowing us to rethink data processing algorithms. 
  - Also, it is important to try to take advantage of new trends in computer hardware so as not to get stuck with the performance and capabilities of the current generation. 
  - However, the painful lesson is that committing to a single technology is a huge gamble that is unlikely to pay off. 
  - So it is important to be prepared for future changes in all components of the system, from the storage layer to address the cloud, to join algorithms for time series, to completely different data models.
  - Regardless of whether CXL takes off, and whatever other trends the future brings, building a modular system that allows components like the storage tier to be easily swapped out seems like the best way forward.

# discuss-db-cloud
- ## 

- ## 

- ## [Repl.it Database | Hacker News _202009](https://news.ycombinator.com/item?id=24472941)
- Can I ask what you're using under the hood to power this?
  - It's Postgres under the hood. The key-value service itself is Go and we run it on GKE alongside some of our other services.
- For filtering/search queries, is the idea to use the prefix system to group related items together, and then run matches using that?
  - Yep, that's the idea. If you had a user with ID 1, one scheme to get all her info could be storing attributes in keys. Then a prefix search for "user:1:" would get you the relevant keys for that user.
  - But you could also go another way and drop a JSON-encoded object in a single key and work with it inside your app, since that might be the fastest way to get something off the ground!
- I didn't look super closely, but it looks like your database works similarly to DynamoDB, in which case it could use similar data modeling. Here is a great post on single table design for DynamoDB that would probably be mostly applicable here: https://www.alexdebrie.com/posts/dynamodb-single-table/ The post author actually wrote an entire book on data modeling with a single key/value table.

- This sounds like a database-per-tenant architecture, which is something I'm interested in. Are you having to build a lot of custom tooling to manage that many databases? Does the existing repl.it code make it easy to just spin up an extra docker container for each session? Or are there off-the-shelf tools that make this easy?

- Did you consider SQLite for this?
  - We did! Our infrastructure has some unique constraints around filesystem persistence. We use btrfs and regularly take snapshots of the repl's filesystem while someone is editing the repl. However, we don't take snapshots when a repl is only acting as a web server without anyone editing it, meaning that we would need a solution that lives outside of the repl's filesystem.
  - Keeping each database outside of the filesystem also gives us the flexibility to let people access the same database across multiple repls in the future.
  - I have lots of love for SQLite; early prototypes of Database were backed by it!
- Totally makes sense - SQLite is very dependent on a traditional filesystem. I've been figuring out how to run backups recently and grabbing a copy of the file isn't enough - you need to be sure there are no transactions going on, so you need to use the SQLite backup API or run VACUUM INTO a separate copy, which then doubles the amount of disk space you need.

- Having a simple database it a great feature for prototyping, but I think having custom Dockerfile support would be even better.
# discuss-storage-compute-separation
- åˆ†ç¦»æ¡ˆä¾‹
  - known: clickhouse, quickwit, kafka, neondatabase, yugabytedb, Doris v3+, xtdb v2, OrioleDB, PolarDB
  - å¯å‚è€ƒå‰ç«¯çŠ¶æ€ç®¡ç†/dbçš„æŒä¹…åŒ–æ–¹æ¡ˆ
- tips
  - [Ramifications and status of 'Separation of Storage and Compute' (mentioned in 2022 Roadmap) Â· Issue Â· ClickHouse/ClickHouse](https://github.com/ClickHouse/ClickHouse/issues/41791)
  - [A Comparative Study of Storage and Compute Decoupling - RisingWave: Real-Time Event Streaming Platform _202407](https://risingwave.com/blog/a-comparative-study-of-storage-and-compute-decoupling/)

- ## 

- ## 

- ## 

- ## 

- ## 

- ## [Ramifications and status of 'Separation of Storage and Compute' (mentioned in 2022 Roadmap) Â· Issue Â· ClickHouse/ClickHouse _202209](https://github.com/ClickHouse/ClickHouse/issues/41791)
- ClickHouse already support the usage of S3 and HDFS as table engine. By using them, you are effectively separating compute and storage, and you can scale either of them independently.

- [Clickhouseè®¡ç®—ä¸å­˜å‚¨åˆ†ç¦»è°ƒç ” Â· Issue Â· cloudnativecube/octopus _202103](https://github.com/cloudnativecube/octopus/issues/48)
- mergetreeå¼•æ“æ”¯æŒS3
  - replicatedmergetree å¼•æ“æ”¯æŒS3
- insertï¼š
  - ç”¨æˆ·ç›´æ¥å°†æ•°æ®æ’å…¥åˆ°æŸä¸€ä¸ªreplicaåï¼Œå…¶ä»–replicaä¼šé€šè¿‡zookeeperè¿›è¡Œæ„ŸçŸ¥ï¼Œå°†å…ƒæ•°æ®ä»è¯¥replica fetchè¿‡æ¥åŠ è½½åˆ°å†…å­˜å’Œå†™å…¥æœ¬åœ°ä»¥ä¿æŒåŒæ­¥ï¼ˆåŒç†ï¼Œæ–°åŠ å…¥replicaä¹Ÿä¼šå¦‚æ­¤è€Œè¿›è¡ŒåŒæ­¥ï¼‰ï¼Œå…ƒæ•°æ®æ–‡ä»¶å­˜å‚¨åœ¨æœ¬åœ°ç£ç›˜ï¼Œä½†æ˜¯æ–‡ä»¶é‡Œçš„å†…å®¹å…¶å®æ˜¯s3çš„æ–‡ä»¶åï¼ŒçœŸå®çš„æ•°æ®ç»Ÿç»Ÿå­˜å‚¨åœ¨s3ä¸Šï¼Œæ‰€æœ‰replicaå…±äº«è¿™äº›æ•°æ®ï¼Œå½“æœ‰å¤šä¸ªreplicaåŒæ—¶æ“ä½œå…±äº«æ–‡ä»¶æ—¶ä¼šå¯¹æ–‡ä»¶è¿›è¡ŒåŠ é”ï¼Œä»è€Œä¿è¯æ•°æ®çš„ä¸€è‡´æ€§ã€‚
- merge:
  - å¤šä¸ªreplicaå¯ä»¥è®©å…¶ä¸­ä¸€ä¸ªreplicaåšmergeæ“ä½œï¼Œå…¶ä»–replicaç­‰ä»–å…¶mergeæ“ä½œå®Œæˆåï¼Œå‘å…¶ç´¢è¦mergeåçš„å…ƒæ•°æ®æ–‡ä»¶ä¿å­˜åˆ°æœ¬åœ°ï¼Œä»è€Œå‡å°‘æ€§èƒ½æŸè€—ã€‚
- select:
  - ç»è¿‡éªŒè¯ç›®å‰æ˜¯å¯ä»¥æ”¯æŒåˆ†å¸ƒå¼è¡¨æŸ¥è¯¢å¤šä¸ªshards(å‡å…±ç”¨åº•å±‚s3å­˜å‚¨)ï¼Œä»è€Œæ”¯æŒå¹¶å‘æŸ¥è¯¢ï¼ŒæŸ¥è¯¢è¯·æ±‚å¯ä»¥è½åˆ°å¤šä¸ªreplicas(æ°´å¹³æ‰©ç¼©)ï¼Œåº•å±‚å…±ç”¨s3å­˜å‚¨ã€‚
- delete:
  - åœ¨è¿›è¡Œåˆ é™¤æ“ä½œæ—¶ï¼Œclickhouseå†…éƒ¨ä¼šæœ‰share data lockæœºåˆ¶ï¼Œè¯¥æœºåˆ¶ç¡®ä¿æ‰€æœ‰çš„replicaçš„å…ƒæ•°æ®å…¨éƒ½åˆ é™¤åæ‰ä¼šçœŸæ­£çš„å»åˆ é™¤s3å¯¹è±¡å­˜å‚¨ä¸Šçš„æ•°æ®ã€‚

- cloud.tencent.com/developer/article/1759109 è…¾è®¯äº‘CKè®¡ç®—å­˜å‚¨åˆ†ç¦»çš„ç»éªŒ

- ## It's wild that @databricks chose to pay $1 billion (with a 200x multiple) for the slowest Postgres, @neondatabase _202506
- https://x.com/pucchkaa/status/1933650926365257993
- I think this is because of their decoupled architecture where they separate the storage and the compute. This gives them benefits like autoscaling, serverless, branching etc. but comes with an unavoidable network hop from compute to pageservers. They use WAL for decoupling.
  - This does remind me of @isamlambert tweet where he mentioned why it might be bad to separate storage and compute. Planetscale got some really good raw performance with NVMes.

- But it's serverless

- https://x.com/nikitabase/status/1725394868619342198
  - It's MUCH easier to scale stateless systems - so let's separate storage and compute. Storage is a glorified key value store (okay a bit more than that but close). So now we can turn Postgres on and off and move it from one machine to another and not lose state. This is key!
  - When a query comes in it hits our proxy which forwards it to the right Postgres process. But where should the Postgres process live? VM, container, bare metal?
  - There are many reasons to use VMs. Primarily due to security boundary and live VM migrations (more on that later)
  - You can also use containers. In this case you need to be sure that an attacker won't break out of the process. Postgres and its extensions are NOT secure by default and hence container didn't work for us.

- [Why separating storage and compute can be a problem for OLTP workloads. | PlanetScale posted on the topic | LinkedIn _202506](https://www.linkedin.com/posts/planetscale_while-separating-storage-and-compute-has-activity-7333155501230243843-ONWt)

- ## Every time we separate compute from storage, we bring it back together it seems. And then we do it again a decade later. _202503
- https://x.com/kellabyte/status/1900649147234955489
- is this about the PlanetScaleâ€™s?
  - Yup but also just a general observation
- I know the planetscale folks are pushing hard on Metal, but Aurora/AlloyDB/SQL Hyperscale are all tiered systems already.  They cache hot data on fast local NVMe disks on the same machine as the SQL compute ("SSD Cache" in the diagram here)

- That's a natural cycle, I think. 
  - 1) Separate to get perf from parallel access, then 
  - 2) combine again when HW has caught up and remove complexity.

- IMO, all the approaches have pros and cons. It is about tradeoffs and the problems you are trying to solve. Vendors make it very hard to have this discussion and don't acknowledge the tradeoffs. There are legit benefits for storage/compute decoupling for OLTP, and there are definite benefits of putting every component of an OLTP on one machine.

- With @spacetime_db we're skipping right to the next decade.
# discuss-scaling
- ## 

- ## 

- ## 

- ## Scaled from 1, 000 to 100, 000 users. Here's what broke.
- https://x.com/brankopetric00/status/2011561755927826910

At 5, 000 users:
- Single database became the bottleneck
- Added read replicas

At 20, 000 users:
- Session storage overwhelmed Redis
- Switched to JWT tokens

At 50, 000 users:
- File uploads killed our servers
- Moved to S3 with presigned URLs

At 75, 000 users:
- Search became unusable
- Implemented Elasticsearch

At 100, 000 users:
- DNS became single point of failure
- Multi-region with Route53 failover

Every stage felt like the final architecture.

None of them were.

Scaling isn't a destination. It's a continuous series of bottleneck discoveries.

- your DB broke at 5k users what the fuck was it a json file?
- At 5, 000 users: - Single database became the bottleneck - Added read replicas This seems unlikely - We have 25k users on a single db and it works flawlessly.

- A single DB became a bottleneck at 300k users for us, but we could solve it with partition schemes and scale to millions. Just added some read replicas for dashboard and report generation.

- I think most people scale too early.
  - That's also true, wast on cloud resources is crazy.
# discuss
- ## 

- ## 

- ## 

- ## 

- ## Easier to think of a database = storage layer + data model + query layer (language + optimizer)
- https://x.com/redixhumayun/status/1919669471377310166
- I don't think query layer in necessary. AFAIK rocks doesn't have one. API is all you need.
  - Query layer makes a lot of things easier though. Also hard to write performant queries without an optimizer Data model is the logical structure of the data - relational or document etc

- Same spirit pursued by Apache datafusion project. 
  - Query + data model + storage. 
  - Datafusion gives these components as composable pieces => acts as LLVM for databases .

- ## ç°åœ¨æ•°æ®åº“ç”¨ Rust å†™ extensions è¶Šæ¥è¶Šæµè¡Œäº†
- https://x.com/yihong0618/status/1860542339267330185
  - pg -> pgrx
  - sqlite-> sqlite-loadable-rs
  - duckdb -> extension-template-rs

- ## ğŸï¸ How Zoom supports 300 million video calls a day:
- https://x.com/systemdesign42/status/1806551695331139635
  - They use scalable video coding (SVC) to stream video
  - SVC scales well because it sends only a single video stream
  - They separate video stream processing from stream routing
  - The client processes the video streams to reduce server load
  - They send separate video streams from each participant to the client
  - They do multimedia routing to send streams at low latency
  - They set up the client to check the stream quality
  - They use UDP to send video data

- ## æ ¹æ®æˆ‘ä¸ªäºº 10 å‡ å¹´çš„ä¼ä¸šåº”ç”¨ç»éªŒï¼Œæ€»ç»“å‡ºçš„ä¼ä¸šä¿¡æ¯åŒ–ç³»ç»Ÿçš„è®¾è®¡è§„èŒƒæè¿°æ–¹æ³•è®ºï¼š
- https://twitter.com/xqliu/status/1789127844435521880
* ä¸šåŠ¡æ¨¡å‹è®¾è®¡ï¼ˆç›¸å½“äºæ•°æ®åº“è®¾è®¡ï¼‰
é€šä¿—çš„è¯´ï¼Œå°±æ˜¯æœ‰å“ªäº›æ•°æ®åº“è¡¨ï¼Œæœ‰å“ªäº›å­—æ®µï¼Œè¡¨ä¹‹é—´çš„å…³è”æ˜¯ä»€ä¹ˆæ ·çš„
* è¡¨å•è®¾è®¡ï¼ˆåŒ…æ‹¬ CRUD å’Œå¤šæ­¥éª¤è¡¨å•ï¼‰
å„ä¸ªå¯¹è±¡çš„ CRUD è¡¨å•éƒ½æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œå­—æ®µæœ‰å“ªäº›éœ€è¦è®¾ç½®é»˜è®¤å€¼ã€éœ€è¦ç‰¹æ®Šæ ¡éªŒã€å­—æ®µä¹‹é—´éœ€è¦è”åŠ¨ã€åŠ¨æ€æ˜¾å½±ã€å­—æ®µå¦‚ä½•åˆ†ç»„ã€å„ä¸ªè¡¨å•éƒ½å¼€æ”¾ç»™ä»€ä¹ˆç”¨æˆ·ç­‰ã€‚
* ä»ªè¡¨ç›˜è®¾è®¡, ä»ä¸šåŠ¡éœ€æ±‚è§’åº¦ï¼Œéœ€è¦åˆ›å»ºå“ªäº›å¤§å±ä»ªè¡¨ç›˜ï¼Œæ¯ä¸ªä»ªè¡¨ç›˜æœ‰å“ªäº› Widget ç­‰ã€‚
* å¯¹è±¡ç”Ÿå‘½å‘¨æœŸè®¾è®¡ ï¼ˆCRUD æ¯ä¸ªèŠ‚ç‚¹çš„é€»è¾‘è®¾è®¡ï¼‰, æ¯ä¸ªå¯¹è±¡çš„ CRUD ç”Ÿå‘½å‘¨æœŸæœ‰å“ªäº›æ ¡éªŒã€é›†æˆã€é¢å¤–çš„ä¸šåŠ¡é€»è¾‘ç­‰ã€‚
* æ ¸å¿ƒæµç¨‹è®¾è®¡ï¼ˆæµç¨‹å®¡æ‰¹ç­‰ï¼‰ã€‚ å¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸï¼Œæ˜¯å¦æœ‰ç›¸å…³æµç¨‹å…³è”ã€‚
* åŠ¨æ€æ“ä½œè®¾è®¡ï¼ˆå¯¹äºç±»åŠå¯¹è±¡æœ‰å“ªäº›æ“ä½œï¼‰, å¯¹äºæ¯ä¸€ç±»å¯¹è±¡ï¼Œéƒ½æœ‰å“ªäº›ä¸šåŠ¡å±‚é¢çš„æ“ä½œï¼Œè¿™äº›æ“ä½œæ˜¯å¯¹å“ªäº›ç”¨æˆ·å¯ç”¨ï¼Œå¯ç”¨çš„é€»è¾‘æ˜¯ä»€ä¹ˆï¼Œå…·ä½“æ“ä½œçš„é€»è¾‘åˆæ˜¯ä»€ä¹ˆã€‚
* ç³»ç»Ÿé—´é›†æˆè®¾è®¡ã€‚ æœ‰å“ªäº›å¯¹æ¥çš„ç¬¬ä¸‰æ–¹ç³»ç»Ÿï¼Œå…·ä½“é›†æˆçš„æ–¹å‘ã€æ–¹å¼å„æ˜¯ä»€ä¹ˆï¼Œå°¤å…¶ç‰¹æ®Šçš„å‡ ç±»æ˜¯ï¼šSSO å•ç‚¹ç™»å½•å’Œç”¨æˆ·ã€ç»„ç»‡æ¶æ„åŒæ­¥ï¼Œè¿™å¾€å¾€åœ¨é¡¹ç›®å®æ–½ä¸­æ˜¯å¿…é€‰é¡¹ã€‚
* å®šæ—¶ä»»åŠ¡è®¾è®¡ã€‚æ ¹æ®ä¸šåŠ¡éœ€æ±‚ï¼Œéœ€è¦åšå“ªäº›å®šæ—¶ä»»åŠ¡çš„è®¾è®¡ï¼ŒåŒ…æ‹¬ç³»ç»Ÿå¯åŠ¨æ—¶è¿è¡Œã€å•æ¬¡è¿è¡Œã€å‘¨æœŸè¿è¡Œç­‰ç­‰ã€‚
* æŠ¥è¡¨åŠå•æ®æ‰“å°è®¾è®¡ã€‚ä¸šåŠ¡ä¸Šæœ‰å“ªäº›æŠ¥è¡¨éœ€è¦è®¾è®¡ï¼Œæœ‰å“ªäº›å•æ®éœ€è¦è®¾è®¡æ‰“å°ç‰ˆå¼ã€‚
* ç•Œé¢äº¤äº’è®¾è®¡ã€‚éœ€æ±‚æ–¹æœ‰æ²¡æœ‰ç‰¹æ®Šçš„ç•Œé¢äº¤äº’éœ€æ±‚ï¼Œå¦‚æœæœ‰ï¼Œå…ˆä½¿ç”¨åŸå‹å·¥å…·ç”»å‡ºäº¤äº’å…¨è¿‡ç¨‹ã€‚
- åŸºæœ¬ä¸Šä»¥ä¸Šçš„åå°é€»è¾‘ç†æ¸…äº†ä¹‹åï¼Œä¼ä¸šåº”ç”¨å¼€å‘éœ€æ±‚åå°é€»è¾‘çš„ 80% - 90% å¤§æ¦‚å·²ç»ç†æ¸…äº†ã€‚
- å…¶ä¸­æ¯”è¾ƒæ ¸å¿ƒçš„å‡ ç‚¹æ˜¯
  1. æ•´ä½“çš„ä¸šåŠ¡æ¨¡å‹è®¾è®¡
  2. å¯¹è±¡ç”Ÿå‘½å‘¨æœŸç®¡ç†åŠè®¾è®¡
  3. åŠ¨æ€æ“ä½œè®¾è®¡
- æˆ‘ä»¬çš„ç³»ç»Ÿæ˜¯çœŸæ­£å°†ä¼ä¸šä¿¡æ¯åŒ–æ•°å­—åŒ–è¿‡ç¨‹çš„æœ€ä½³å®è·µï¼Œé€šè¿‡ç³»ç»Ÿè®¾è®¡çš„æ–¹å¼ï¼Œå›ºåŒ–åˆ°æˆ‘ä»¬çš„ä½ä»£ç å¹³å°ä¸­äº†ã€‚åŒæ—¶ï¼Œé’ˆå¯¹å…¶ä¸­æ¯”è¾ƒå®¹æ˜“è·‘åï¼Œå‘æ¯”è¾ƒå¤šçš„ç‚¹ï¼Œä¹Ÿåˆ©ç”¨ç³»ç»Ÿçš„æ ¸å¿ƒæ¶æ„ï¼Œåšäº†ç›¸åº”çš„é˜²å¾¡å’Œåº”å¯¹ã€‚
  - æŒ‰ç…§æˆ‘ä»¬ç³»ç»Ÿçš„èŒƒå¼æ¥è¿›è¡Œä¼ä¸šä¿¡æ¯åŒ–ç³»ç»Ÿçš„è®¾è®¡ï¼Œä¸€å®šé”™ä¸åˆ°å“ªé‡Œå»ã€‚ä½¿ç”¨æˆ‘ä»¬çš„ç‰§è¨€ä½ä»£ç å¹³å°è¿›è¡Œä¼ä¸šä¿¡æ¯åŒ–ç³»ç»Ÿå¼€å‘ï¼Œä¸ç®¡æ˜¯å¯¹å®¢æˆ·ï¼Œè¿˜æ˜¯å¯¹å†…éƒ¨ä¸šåŠ¡éƒ¨é—¨ï¼Œä¸€å®šå¯ä»¥æˆåŠŸå¿«é€Ÿäº¤ä»˜ã€‚

- ## ç®€æ˜æ˜“æ‡‚çš„åˆ†ç‰‡åŸç†æ•™ç¨‹ï¼Œæ¥è‡ªArchitecture Notesâ€”â€”è¿™å®¶ç½‘ç«™çœŸçš„æ˜¯å®è—ï¼Œé‡Œé¢æœ‰ä¸å°‘å¸¦æœ‰æ¼‚äº®æ‰‹ç»˜å›¾çš„å¥½æ–‡ç« ã€‚
- https://twitter.com/huangzworks/status/1776087981214101659

- ## Use synchronous by default, asynchronous when necessary.
- https://twitter.com/RaulJuncoV/status/1770789583375974471
- Sync is more convenient in most cases. But can lead to low scalability, reliability, and other undesirable characteristics.
- Async can provide performance benefits and scale. But can present a host of headaches:
  â€¢ data synchronization
  â€¢ deadlocks
  â€¢ Race conditions
  â€¢ Debugging, and so on

- ## 4 must-know strategies to build high-availability systems:
- https://twitter.com/ProgressiveCod2/status/1768176860037517417
- [1] Replication
  - Involves duplicating data on servers spread across different locations & data centers  
  - If one server goes down, the other can support the workload  
  - Also, protects against data loss by increasing durability

- [2] Load Balancing
  - Distribute traffic across multiple servers to share the load  
  - This ensures no single server gets overwhelmed by incoming requests  
  - Also, if certain servers go down, other servers are present to take over

- [3] Auto Scaling
  - Sometimes a dramatic increase in load can result in a loss of availability  
  - Auto-scaling helps a system scale out based on demand
  - This ensures that the overall availability is not impacted

- [4] Rate Limiting
  - Denial-of-Service attacks can cripple a systemâ€™s availability
  - Rate limiting based on IP addresses or users can ensure that the system is not abused by a malicious actor  
  - This ensures that genuine users are not impacted

- Backups and disaster recovery plans
- Auto Scaling is critical for elasticity! Just make sure to set your Rate Limiter to avoid a surprising bill

- ## ğŸŒ° Memory allocation by layers in YugabyteDB:
- https://twitter.com/FranckPachot/status/1767219717037490550
  - YSQL (PostgreSQL backend), visible in pg_stat_activity.allocated_mem_bytes
  - DocDB Block Cache for reads (Block Based Table)
  - DocDB MemTable for writes (Tablet Overhead)
  - goes to SST Files when flushed

- ## Database-per-Service said each service should have its own private database. You can only access the data through its API.
- https://twitter.com/RaulJuncoV/status/1766092301955113113
  - In monolithic architectures, different parts of the application share data schemas.
  - Imagine you need to remove the Foreign keys and Views from your database.
  - That's what you have to do if you want a Database-per-Service pattern in microservices.

- ## I am wrapping my head around Snapshot Isolation (SI) with a minimal computational model
- https://twitter.com/DominikTornow/status/1760054290146873397
  - SI presents a transaction with a consistent snapshot of the database at the start of the transaction and allows to commit only if there are no conflicts with other concurrent transactions' writes

- ## If SQL is considered a programming language, then relational databases function as virtual machines that execute SQL, similar to how the JVM executes Java.
- https://twitter.com/criccomini/status/1759375042847314000
- SQL-speaking databases are both query compilers and runtimes for the resulting execution plan.

- ## 9 strategies to make your system more scalable in the long run.
- https://twitter.com/ProgressiveCod2/status/1754827378436841922
  [1] Horizontal Scalability
  [2] Load Balancing
  [3] Caching
  [4] Database Replication
  [5] Database Sharding
  [6] Stateless Services
  [7] Containerization
  [8] Async Processing
  [9] Adopting Serverless

- Separate WRITE and READ workloads gives good scalability as well.

- ## ğŸ˜ï¸ It's hard to design a software system that can work on a large scale.
- https://twitter.com/Franc0Fernand0/status/1750801088310095905
1. Adding server clones  
- This is the quickest and cheapest way to scale a system from scratch. 
- This method works best for services that don't keep any state, so there is nothing to sync. 
- Whenever the load balancer receives a request, it should be able to send it to any server without knowing where the previous request went.  

1. Partitioning servers  
- A more complex approach is dividing the system into groups of servers with different roles. 
- Ideally, each group of servers is a self-contained application that can make decisions independently. 
- This separation allows each part to be scaled on its own, depending on its needs. It promotes efficient management, as teams can work on different parts of the system simultaneously without interfering with each other. 
- However, it requires more initial effort, and there's a limit to how much you can partition a system before it becomes too complex.  

1. Partitioning data  
- A third approach is dividing and distributing the entire dataset across multiple machines, each having a subset of the data. 
- This setup speeds up data processing and storage as each server only has to deal with a smaller amount of it at a time. 
- It also makes the system scalable, so as the amount of data grows, you can add more computers and change how the data is distributed between them.
- The partitioning strategy, however, makes things more complicated.
  - You need a system to track where each piece of data is stored to direct queries to the correct server. 
  - Also, it can be hard to make queries that cover more than one data partition work well.

- ## ğŸ—³ï¸ What's your favorite way to work with a database?
- https://twitter.com/jamesqquick/status/1730592106517790752
  - raw sql?
  - ORM (prisma, drizzle, etc.)?
  - custom product SDK?
- In an application, ORM for sure. Anything else makes little sense.
- I was was always fan of ORM. IMHO there just be a clear data/logic separation. Plus I wouldn't trust most of BE devs to secure raw queries properly. I also come from the world of Laravel and Eloquent is best data layer abstraction I've used.

- Raw SQL for anything more than the simplest queries, otherwise a query builder.
- Keep trying ORM approach and always come back to compiled raw SQL for anything even moderately complex (Personally enjoy Kysely). Still use query builder for simple requests. So much easier to iterate, adapt, and find help online with. Will sometimes re-factor into built queries
- Query builder (personally node + knex). When the application/queries get too complicated for ORMs, you always resort to sql anyways

- ## Amazon has joined the shared-nothing relational databases club by introducing Amazon Aurora Limitless, 
- https://twitter.com/denismagda/status/1729552540784586867
  - a service capable of scaling write throughput and storage capacity beyond the limits of a single Aurora writer instance.

- ## æŠŠå…¬å¸é‡Œçš„ä¸€ä¸ªå­˜ç®—å¼ºè€¦åˆçš„ AP ç³»ç»Ÿå°è¯•æ”¹é€ æˆå­˜ç®—åˆ†ç¦»ï¼Œå­˜å’Œç®—ä¹‹é—´ç”¨ RPC é€šè®¯ã€‚å‘ç°æ”¹é€ å®Œåæ€§èƒ½ä¸ä½†æ²¡æœ‰æŸå¤±å¤ªå¤šï¼Œåœ¨æŸäº›åœºåˆè¿˜æ”¹è¿›ä¸å°‘ã€‚
- https://twitter.com/r0_ayanami/status/1728979172373193155
- ä¸€èˆ¬å†…ç½‘å¸¦å®½æ˜¯æå¤§å¯Œè£•çš„ï¼Œååæˆ‘æ„Ÿè§‰è‚¯å®šæ˜¯å­˜ç®—åˆ†ç¦»çš„ä¼˜åŠ¿ï¼Œä¸è¿‡éœ€è¦è€ƒè™‘å»¶è¿Ÿçš„å˜åŒ–
- å­˜ç®—ä¸€æ—¦å½»åº•è§£è€¦äº†ï¼Œç®—çš„é‚£éƒ¨åˆ†å¯ä»¥æ— é™å¼¹æ€§æ‰©å®¹äº†ï¼Œæ€§èƒ½ç«‹é©¬åŸåœ°èµ·é£
  - IOä¸æ˜¯ç“¶é¢ˆçš„è®¡ç®—ä»»åŠ¡çœŸæ˜¯å¤ªçˆ½äº†
- ä¸å¤Ÿå°±ä¸Šfiberï¼ŒRoCE

- å¥½å¥‡ç»†èŠ‚ï¼Œä¸€èˆ¬å­˜ç®—è€¦åˆçš„ ap ç³»ç»Ÿä¸€åˆ†ç¦»éƒ½å»¶è¿Ÿçˆ†ç‚¸äº†
  - å…¶å®æˆ‘ä»¬æ—©å°±æŠŠæ•°æ®å­˜åœ¨ S3 é‡Œäº†ã€‚è¿™é‡Œçš„ä¸Šä¸‹æ–‡æ˜¯æŠŠ MemTable ç‹¬ç«‹å‡ºæ¥ã€‚AP ç³»ç»Ÿé‡Œ in-memory çš„ MemTable çš„æ•°æ®é‡çº§ä¸€èˆ¬è‚¯å®šä¼šæ¯” on-disk çš„æ•°æ®å°å¾ˆå¤šï¼Œæ‰€ä»¥æ€§èƒ½æŸå¤±ä¸€èˆ¬ä¸ä¼šå¤ªç¦»è°±ã€‚å®ç°ä¸ŠæŠ“äº†ä¸å°‘ç«ç„°å›¾åšäº†ä¸€äº›å°çš„ä¼˜åŒ–ï¼Œæ¯”å¦‚ key ä¸‹æ¨ï¼Œå‡å° proto ç¼–ç è§£ç çš„ä»£ä»·ä¹‹ç±»çš„ã€‚
  - å…¶å®è¿™ä¸ªé¡¹ç›®çœŸæ­£éš¾çš„åœ°æ–¹ä¸åœ¨äºæ€§èƒ½ï¼Œè€Œåœ¨äºå¦‚ä½•ä»å­˜ç®—è€¦åˆçš„æ¶æ„ä¸å®•æœºåœ°è¿ç§»åˆ°å­˜ç®—åˆ†ç¦»çš„æ¶æ„ä¸Š
- ç°åº¦åˆ‡æµå¾€å¾€ä¼šæå‡é¡¹ç›®çš„å®ç°éš¾åº¦
  - æ˜¯çš„ï¼Œè¿™ä¸ªä¸œè¥¿ä¸»è¦æ˜¯æ–°æ—§ç³»ç»Ÿè¿”å›ä¸ä¸€è‡´çš„è¯ï¼Œå®šä½ä¹Ÿå¾ˆå›°éš¾ã€‚

- ## Separating storage and compute is cool, but separating data and metadata is cooler
- https://twitter.com/richardartoul/status/1726642180473909752
  - [Unlocking Idempotency with Retroactive Tombstones - WarpStream](https://www.warpstream.com/blog/unlocking-idempotency-with-retroactive-tombstones)
  - WarpStream is an Apache Kafka protocol compatible data streaming system built directly on top of object storage. 
  - It has zero local disks, and incurs zero inter-zone bandwidth costs. 
  - many people arenâ€™t aware that Kafka supports more advanced features like idempotent produce requests which allows a Kafka client to produce the same batch of data multiple times, but ensure itâ€™s only appended to the log once.
  - it was one of the primary reasons why we created our own metadata store instead of reusing something off the shelf.
  - In this post, weâ€™ll go over in detail how we added support for the idempotent producer functionality to WarpStreamâ€™s storage engine.

- ## Been thinking about S3 as primary storage for serverless infra. 
- https://twitter.com/criccomini/status/1725183323474190704
  - For OLTP, a transactional KV store on S3 would id a necessary building block. @PingCAP â€™s TiKV is one such example. 
  - [Production Flink Usage, Key-Value Stores on S3, Titan is Terraform for Data, and more...](https://materializedview.io/p/flink-usage-kv-store-on-s3-terraform-for-data)

- ## ğŸ› The biggest problem with shared-storage OLTP is that the data volume is limited by single machine. 
- https://twitter.com/YingjunWu/status/1723832509098766801
  - Some big customers moved away from Aurora to CRDB/TiDB because they felt frustrated when hitting the storage limit. 
  - But most companies donâ€™t store so much data in OLTP DBs and wonâ€™t hit the limit.

- ğŸ¤” This is not a problem for shared storage, you can split the data into different storage nodes via sharding. Where each shard is a shared storage architecture (multi tenant) and lives on its own machine - CRDB does this.
  - Aurora could theoretically do this as well. The storage limit has nothing to do with the shared storage architecture.

- But what you said is not quite true -- Oracle RAC is there. Also we need to look and see whether the issue is raw storage capacity or or read througput/tps or write throughput/tps, etc., each has its own answers.
- Havenâ€™t studied Oracle RAC before, but the 128 TB hard limit is a pain for Aurora.
  - Aurora has limitations for sure but most workloads donâ€™t need a lot and secondly you can add more clusters. But RAC I believe scales to multiple PBs.
- The storage capacity is not a problem for those who run on Aurora. The last time I checked it was possible to scale beyond 60 TB. The inability to scale write workloads across zones and regions is what shared-storage architecture doesnâ€™t deal with. Thatâ€™s why Amazon turns to DynamoDB to scale both reads & writes. But that should change soon once Amazon introduces a distributed shared-nothing version of Aurora

- ## what is the reason in your mind that none of the database vendors are able to crack the moat(åŸå£•ï¼›æŠ¤åŸæ²³) of Oracle and RAC, in a meaningful way?  I am curious to hear your thoughts.
- https://twitter.com/sv_techie/status/1723777534406332517
- Snowflake and Databricks in the analytics space. OLTP is tricky, hard to build, hard to earn developer trust. That's why @neondatabase bets on Postgres and strive to become default cloud Postgres

- ## There has been several startups building an operational relational databases focused on OLTP with a shared nothing architecture. 
- https://twitter.com/nikitabase/status/1723579854266966399
  - @neondatabase is using a different approach - shared storage.  What's the difference?
- In a shared nothing architecture you take a cluster of machines and assign a subset of data (shard) to each machine using range or hash partitioning.
  - Incoming SQL queries are split into parts that can be  run on each machine. If need be the results of those queries are reshuffled, potentially multiple times, and finally collected on one node to generate the final result.
  - While this architecture allows you to scale your TPS for simple reads and writes it has several disadvantages:
  - 1. Some write transactions are only touching one shard and some multiple. So some write queries scale MUCH (10-100x) worse than others.
  - 2. To achieve consistency you need to implement 2 phase commit (or other distributed protocols) to preserve transactional semantics, which requires two network roundtrips with each participant so you transaction latency is inconsistent.
  - 3. Cross shard foreign keys and uniqueness are super hard to implement in a shared nothing system. You need to go over the network to validate presence of foreign keys and uniqueness. Radical performance implications.
  - The list can go on. Mature shared nothing systems achieved an incredible engineering feat of supporting all the query processing use cases, but even the best ones sacrifice surface area and DEFINITELY break compatibility with the single node system they are based upon.
  - Because even if in fullness of time you implement all the surface area it's impossible to preserve performance compatibility. In addition to having to go over the network for some queries, you also have a different query optimizer which breaks performance compatibility.
  - The result is that most shared nothing systems support a subset of features of single node systems and try to convince their users that those don't matter or don't scale.
  - This means you can't migrate back and forth from Postgres or MySQL to a shared nothing system. Most developers start on Postgres or MySQL, so companies building shared nothing systems need to find workloads that ran out of scale on single node databases.
  - Attempts to build a popular shared nothing system have been here since the 80ies. None of the shared nothing OLTP systems took off. Microsoft had two attempts with SQL Server - both failed.
- Lets now look at shared storage systems. In a shared storage system there is a separate storage: either page based or row based. 
  - E.g. Postgres stores everything in 8k pages.
  - Storage systems like this is MUCH easier to make distributed - it's a glorified key value store.
  - Then you can "attach" compute - a single node database system to the distributed storage. Which has the following benefits.
  - 1. I/O is infinite. You can still run out of network for pushing writes into the system, but you won't run out of iops.
  - 2. 100% compatibility with the single node system. Which means your apps don't need to change. Just change the connection string.
  - 3. Compute is now stateless - you can move it around nodes in a multi-tenant system. And this allows you to build serverless compute on top of a traditional SQL database
  - 4. Read replicas don't need to copy data to start replicating. They attach to the same storage.
  - The most successful systems are shared storage: Oracle RAC, SQL Server Hyperscale, and AWS Aurora. @neondatabase is shared storage which lets us be serverless.
- Oracle RAC is different from others as it provides HA (resilience to node failure, online patching). All cluster nodes are full-feature read-write, sharing not only the storage but also the buffer cache, the prepared statements and the distributed locks
  - I was about to reply that Oracle RAC has been around in this segment for a while now until I read this last tweet. Even RAC had its own issues which shared nothing databases actually solved. Even Oracle introduced the concept of sharding recently.
  - RAC is only one part of the Maximum Availability Solution and is about global cache and lock manager. They distribute storage with ASM, they replicate with Data Guard, they shard on top of it. It is not binary like shared nothing or shared storage

- Shared storage systems are not reliable: you canâ€™t have efficient replication and assume storage is â€œlocalâ€ so no way to allow entire continents to lose connection. Any system which needs more than one machine is niche

- nice write-up! shared storage also means some of your data will come from over the network everytime, depending on how you've distributed your data across the shared storage -- if you have fast drives and good bandwidth on the network this becomes less of concern.. how are failover to DR sites handled in this architecture?

- There is also a combined approach where you build a fully distributed database on shared storage. In this design, the database nodes share the storage, while the storage nodes share nothing. This approach is used at @YDBPlatform

- Detached compute from storage is the right approach when possible.

- ## ğŸ’¡ People always talk about Create, Read, Update and Delete.
- https://twitter.com/andrewingram/status/1722291782564602202
  - They never talk about Duplicate, Split and Merge. The far more gnarly actions that are commonly required in any real system.
- CRUD erases any intent in our system. Why did the address change ? Has the user moved ? Was he correcting a typo ? I do not understand why CRUD is a thing. Any CRUD system becomes unmaintainable when we need to implement workflows such as duplicating, etc as you mentioned
  - Yeah, I prefer my "write" operations to be centred around use cases (Jobs To Be Done) rather than just plain old entity manipulation. I tend to enjoy the CQRS-type approach that GraphQL gives me when set up effectively.

- Don't forget reorder!

- Duplicate = Read + Create
  - Split = Read + Create
  - Merge = Read + Read + Create + Delete

- But we don't have HTTP verbs for them, making them un-possible.

- ## ğŸ”¥ [Oops, You Wrote a Database | Hacker News_202302](https://news.ycombinator.com/item?id=34941650)
- 
- 
- 

- ## ğŸ”¥ [Feature Casualties of Large Databases | Hacker News_202012](https://news.ycombinator.com/item?id=25270107)
- 
- 
- 

- ## ğŸ”¥ [It's not Ruby that's slow, it's your database | Hacker News_202211](https://news.ycombinator.com/item?id=33524039)
- 
- 
- 

- ## ğŸ”¥ [Ask HN: Server-side web apps â€“ is the database still a bottleneck? | Hacker News_201906](https://news.ycombinator.com/item?id=20123659)
- 
- 
- 

- ## ğŸ”¥ [Database Lab â€“ Full-size staging databases as a service | Hacker News_202002](https://news.ycombinator.com/item?id=22258806)
- 
- 
- 

- ## ğŸ”¥ [The program is the database is the interface | Hacker News_202302](https://news.ycombinator.com/item?id=34761768)
- You nailed it, it's a spreadsheet - but one that doesn't constraint all data to live on the same application window, but could access and transform it throughout the whole system - just like copy/paste can transfer data from one program to another, where it can be transformed with a different toolset.
  - Copy/paste is the most fundamental tool in the toolbox of the end user, since it's the only universal* way to transfer data between apps without accessing an API programmatically, so I think it's undeservedly(ä¸è¯¥å¾—åˆ°çš„; ä¸è¯¥å—çš„) neglected as a tool to build real life practical workflows.
  - Intents on mobile devices are similar, but they are not universal - many times the app to which you want to transfer data is not recognized as a valid target for the shared data.
- Incidentally, spreadsheets are one of the most versatile and successful End-User Development tools, allowing users to build information systems tailored to their business purposes. A system combining those design capabilities with the flexibility of building system-wide workflows could transcend the current model of siloed apps.

- The Unix pipe seems like scriptable copy & paste.
  - It's not, because you can't easily spawn transient pipes between running processes. The OS probably allows this, but I've never seen it exposed in the UI layer.
  - Pipes are primarily used to build ad-hoc pipelines that run in batch mode. Copy&paste is a tool for moving specific bits of data on demand between two running programs, or within the same program. Copy&paste interaction model is "please take this and put it there", and the ability to select the this and the there is just as important as transferring the data itself.

- ğŸ¤” Whenever I stumble on an article like that*, I can't stop wondering why do so many developers think that SQL is hard/clunky in comparaison?
- > A database introduces a different data model - I have to translate between database data-types and my programming languages native data-types.
  - It doesn't matter how difficult or simple it is; the problem is that it's a different model, so it creates translation inefficiencies and makes it more difficult to reason about.
  - The object-relational impedance mismatch is a known problem; having data in one place and format, accessed through a single programming model will typically be easier than a separate service with a different language and model, no matter how efficient.
- I wonder about going the other way, and turning SQL into a full scripting language.
  - That's what PL/SQL and T-SQL basically are. They work, but they have a somewhat weird programming model, halfway between programming and declarative sql but being neither, with table cursors and stateful SQL queries. And of course, their scope is limited to interacting with the database itself, not the outer world.
- SQL is... clunky, but it's still a wonderful way of working with data, compared to hand-rolling imperative loops or functional transforms.

- ## ğŸ”¥ [We are splitting our database into Main and CI | Hacker News_202207](https://news.ycombinator.com/item?id=31956871)
- 
- 
- 

- ## I think one of the most important problems in databases right now is figuring out how to keep data really really close to users.
- https://twitter.com/ankrgyl/status/1717190980905189772
  - In OLAP this is all about loading and caching data in the browser, and then letting @duckdb rip.
- Figuring out which data youâ€™ve loaded and knowing when to load more is a Really Hard Problem. Ideally you can also push filters or full queries to the backend depending on their shape/size. But if you solve it â€” the UX is truly magical
- Keeping it in the browser is an orthogonal problem, but yes once itâ€™s in the browser Duckdb is magic

- ## ğŸ¤” [Ask HN: Has anybody shipped a web app at scale with 1 DB per account? | Hacker News_202005](https://news.ycombinator.com/item?id=23305111)
- My startup currently does just this 'at scale', which is for us ~150 b2b customers with a total database footprint of ~500 GB. We are using Rails and the Apartment gem to do mutli-tenancy via unique databases per account with a single master database holding some top-level tables.
  - This architecture decisions is one of my biggest regrets, and we are currently in the process of rebuilding into a single database model.
  - However as we have grown this has become a huge headache. It is blocking major feature refactors and improvements. It restricts our data flexibility a lot. Operationally there are some killers. Data migrations take a long time, and if they fail you are left with multiple databases in different states and no clear sense of where the break occurred.
  - Lastly, if you use the Apartment gem, you are at the mercy of a poorly supported library that has deep ties into ActiveRecord. The company behind it abandoned this approach as described here

- Echoing this as well, I worked for Influitive and was one of the original authours of apartment (sorry!)
- There are **a lot of headaches involved with the "tenant per schema" approach**. Certainly it was nice to never have to worry about the "customer is seeing data from another customer" bug (a death knell if you're in enterprisish B2B software), but it added so many problems:
  - Migrations become a very expensive and time-consuming process, and potentially fraught with errors. Doing continious-deployment style development that involves database schema changes is close to impossible without putting a LOT of effort into having super-safe migrations.
  - You'll run into weird edge cases due to the fact that you have an absolutely massive schema (since every table you have is multiplied by your number of tenants). We had to patch Rails to get around some column caching it was doing.
  - Cloud DB hosting often doesn't play nice with this solution. We continually saw weird performance issues on Heroku Postgres, particularly with backup / restores (Heroku now has warnings against this approach in their docs)
  - It doesn't get you any closer to horizontal scalability, since connecting to a different server is significantly different than connecting to another schema.
  - It will probably push the need for a dedicated BI / DW environment earlier than you would otherwise need it, due to the inability to analyze data cross-schema.
- I still think there's maybe an interesting approach using partioning rather than schemas that eliminates a lot of these problems, but apartment probably isn't the library to do it (for starters, migrations would be entirely different if partioning is used over schemas)

- Can confirm, here be dragons. I did a DB per tenant for a local franchise retailer and it was the worst design mistake I ever made, which of course seemed justified at the time (different tax rules, what not), but we never managed to get off it and I spent a significant amount of time working around it, building ETL sync processes to suck everything into one big DB, and so on.
  - **Instead of a DB per tenant, or a table per tenant, just add a TenantId column on every table from day 1**.

- Nutshell does this! We have 5, 000+ MySQL databases for customers and trials. Each is fully isolated into their own database, as well as their own Solr "core."
  - We've done this from day one, so I can't really speak to the downsides of not doing it. The piece of mind that comes from some very hard walls preventing customer data from leaking is worth a few headaches.
  - We don't split ALBs / ASGs / application servers per customer. It's only the MySQL / Solr layer which is multi-tenant. Memcache and worker queues are shared.
  - We do a DB migration every few weeks. Like a single-tenant app would, we execute the migration under application code that can handle either version of the schema. Each database has a table like ActiveRecord's migrations, to track all deltas. We have tooling to roll out a delta across all customer instances, monitor results.
  - All of this makes it really easy to backup, archive, or snapshot a single customer's data for local development.

- ## ğŸ¤” [Ask HN: Is there a way to efficiently subscribe to an SQL query for changes? | Hacker News_202104](https://news.ycombinator.com/item?id=26901352)
- Source: I worked on Google Cloud Firestore from it's launch until 2020 and was the on responsible for the current implementation and data structures of how changes get broadcasted to subscribed queries.
  - Without joins Google Cloud Firestore does exactly what you're describing.
  - With joins (or any data dependant query where you can't tell a row is in the result set without looking at other data) you need to keep the query results materialized otherwise you can't have enough information without going back to disk or keeping everything in memory, which isn't really feasible in most cases.

- I like this idea a lot. In a sense it's thinking of your application as a spreadsheet, where the database is a data tab and the frontend is the summary tabs. If there's a change to the data tab, the "spreadsheet engine" (or "materialized view engine" in your case) walks the dependency graph and updates all the relevant parts of the summary tabs.
  - The closest thing I'm aware of to this is BigQuery's materialized views, which take care of making otherwise expensive queries cheap fast, but they are rather limited (i.e. no subqueries or joins), and don't have the "streaming output of changes" you describe.

- This is the exact problem that we are solving here at Materialize! 
  - we (Materialize) have the TAIL operator, which was built to allow users to subscribe to changes
