---
title: lib-aikit-ollama-dev
tags: [large-language-model, ollama]
created: 2026-01-14T18:56:51.610Z
modified: 2026-01-14T18:57:01.673Z
---

# lib-aikit-ollama-dev

# guide

- tips-ai-tools
  - lm studioåº•å±‚ç”¨çš„ä¹Ÿæ˜¯llama.cpp, ä¸å¿…å¯»æ‰¾æ›¿ä»£ï¼Œæ·±å…¥åº•å±‚æ›´å®¹æ˜“æ›¿ä»£å’Œæ‰©å±•

- mlx
  - mlxçš„ä¼˜ç‚¹ä¹‹ä¸€æ˜¯æ–¹ä¾¿æ”¯æŒåœ¨iphoneä¸Šè¿è¡Œ

- ollama-pros
  - stable and rich models
  - å¯¹ embedding æ¨¡å‹é¢æ”¯æŒæ›´å¥½

- ollama-cons
  - å¾ˆå¤šæ¨¡å‹æ¨ç†é€Ÿåº¦æ¯” llama.cpp æ…¢
  - å¯¹ mlx æ¨¡å‹çš„æ”¯æŒè½å

- lmstudio-pros
  - uiæ“ä½œä½“éªŒå‹å¥½

- lmstudio-cons
  - ä¸æä¾›apiæ¥æ”¯æŒremote control
  - å¯¹mlxæ ¼å¼çš„ embedding æ¨¡å‹é¢æ”¯æŒæœ‰é—®é¢˜, ä½†ggufæ ¼å¼æ¨¡å‹æ”¯æŒå¥½
# draft
- ğŸ¤” ä¸€ç§æ€è·¯: tool-callæ—¶ä½¿ç”¨æ“…é•¿tool-callçš„æ¨¡å‹ï¼Œåˆ†ææ—¶ä½¿ç”¨å…¬ç›Šç«™çš„èŠå¤©ä¼˜è´¨ä½†æ— æ³•tool-callçš„æ¨¡å‹
  - æ”¯æŒç±»ä¼¼ roocode çš„ model profile åˆ‡æ¢

- toolåº”è¯¥æ”¯æŒåœ¨uiä¸Šé…ç½®å‚æ•°
  - toolçº§åˆ«çš„prompt
  - stable-diffusionæ–‡ç”Ÿå›¾çš„å·¥å…·åº”è¯¥æ”¯æŒé…ç½®steps/é€‰æ‹©ä¸åŒæ•ˆæœçš„æ¨¡å‹

- è‡ªåŠ¨æ¸…ç†
  - æ­¤ç±»å·¥å…·çš„runtime/æ¨¡å‹æ–‡ä»¶ä¼šå ç”¨å¤§é‡ç£ç›˜ç©ºé—´, éœ€è¦æä¾›æ–¹ä¾¿ç®¡ç†ç©ºé—´çš„å·¥å…·

- æ”¯æŒé€šè¿‡uiå¿«é€Ÿåœ¨æç¤ºè¯æœ«å°¾æ·»åŠ `/non_think`çš„å¼€å…³
# ui/server
- llm-ui/wrapper
  - janai(llama.cpp)
  - klee(ollama)

- llm-ui/client
  - å¯å‚è€ƒ ollama/lmstudio/janai å°è£… llama.cpp/mlx-lm/vlm çš„é€»è¾‘
  - è¿˜å¯å‚è€ƒLocalAiå°è£…ollama/comfyuiçš„é€»è¾‘
  - ç±»ä¼¼lmstudioçš„æ¨¡å‹å¸‚åœº, ä½†æ”¯æŒmodelscope, åŒæ—¶è‡ªåŠ¨æ‰«ææœ¬åœ°å·²æœ‰çš„ollama/lmstudioæ¨¡å‹
  - ç”¨æˆ·è‡ªå·±ä¸Šä¼ çš„pdfï¼Œå°±ç±»ä¼¼è¯å…¸è½¯ä»¶çš„è¯åº“
  - llama.cpp ui + mlx/vllm
  - huggingface-cli, ä¸‹è½½æ¨¡å‹ä¼˜å…ˆæœç´¢modelscope
- ğŸ˜ï¸ å‚è€ƒä¸»æµå‚å•† coding-agent-cli çš„å®ç°, æœ‰ç¬¬ä¸‰æ–¹åœ¨cliä¸Šå®ç°web/electron/tauri, å¦‚aionui
  - é‡‡ç”¨aionuiå°è£…cc/gemini-cliçš„æ€è·¯æ¥å°è£…llama.cpp/mlx
  - åŒæ—¶éœ€è¦æ”¯æŒ openai-compatible api, è¿™æ ·æ–¹ä¾¿ä½æ€§èƒ½çš„ç”µè„‘/å®¢æˆ·ç«¯ä½¿ç”¨

- é’ˆå¯¹mlxä¼˜åŒ–çš„ç‰ˆæœ¬ï¼Œä¸»è¦æ˜¯åç«¯å¤šæ¨¡å‹åŠåˆ‡æ¢æ¨¡å‹çš„ä¼˜åŒ–
  - ä¸»è¦å¼€å‘å‰ç«¯ï¼Œæ˜¯å¦æœ‰å¿…è¦åšåç«¯?
  - å¯å‚è€ƒ jan.ai

- model-apiä½œä¸ºäº§å“çš„éä¸»è¦ç‰¹æ€§æ—¶, æ²¡å¿…è¦å¯¹æ ‡lm-studio/ollamaå°†apiæš´éœ²å‡ºå»
  - å‚è€ƒcline/roocode/librechatæ”¯æŒé…ç½®ä¸åˆ‡æ¢agentå³å¯
# ollama-xp

```sh
# ollamaåŸå§‹çš„ç‰ˆæœ¬æ˜¯å¹¶ä¸å…¼å®¹ OpenAI æ ¼å¼çš„è¯·æ±‚
curl http://localhost:11434/api/chat -d '{
  "model": "gemma3:4b",
  "messages": [
    { "role": "user", "content": "why is the sky blue?" }
  ]
}'
# æ¨¡å‹å“åº”è¾“å‡ºå¦‚ä¸‹ 
{"model":"gemma3:4b","created_at":"2025-07-31T16:54:00.804772Z","message":{"role":"assistant","content":"That"},"done":false}

```

# llama.cpp/ollama/lmstudio ğŸ”§ğŸ¤”
- ç”¨ä¸åŒlmå·¥å…·é“¾(llama.cpp/mlx-lm/mlx-vlm)å¤„ç†ä¸åŒæ¨¡å‹æ–‡ä»¶(gguf/mlx)çš„é€»è¾‘
  - å¯å‚è€ƒvscodeç”¨ä¸åŒç¼–è¾‘å™¨æ‰“å¼€ä¸åŒæ–‡ä»¶çš„é€»è¾‘
  - è¿˜å¯å‚è€ƒLocalAiå°è£…å¤šç§cli
  - è¿˜å¯å‚è€ƒæ–‡ä»¶ç®¡ç†å™¨çš„é€»è¾‘

- å·¥å…·é“¾åŠŸèƒ½
  - model management: Ollama(æ”¯æŒapi), ç›´æ¥importæ–‡ä»¶
  - frontend: openwebui, librechat
  - backend/api: Koboldcpp(åŒ…æ‹¬fe)
  - all-in-one: lmstudio, janai

- ollama-pros
  - ollamaå®ç°çš„apiå¯¹openai apiçš„å…¼å®¹æ€§æ›´å¥½ï¼Œå¦‚æ”¯æŒparse

- ollama-cons
  - macä¸Šæ¨¡å‹è¿è¡Œæ—¶å ç”¨å†…å­˜å¾ˆå¤§ï¼Œ4bæ¨¡å‹éƒ½ä¼šå æ»¡32gbå†…å­˜
  - ä¸æ”¯æŒmlx

- lmstudio-pros
  - æ¨¡å‹è¿è¡Œæ—¶å ç”¨å†…å­˜æ¯”ollamaå°å¾ˆå¤š
  - å¯¹mlxçš„æ”¯æŒæ¯”è¾ƒå®Œå–„ï¼Œè€Œollamaè¿˜ä¸æ”¯æŒ
  - ä½¿ç”¨æœ¬åœ°å°æ¨¡å‹æ—¶ï¼Œtool callæˆåŠŸç‡lmstudioæ¯”ollamaæ„Ÿè§‰æ›´é«˜

- lmstudio-cons
  - å¯ä»¥å•ç‹¬æ›´æ–°mlx-engine, ä½†ä¸å¯ä»¥å•ç‹¬æ›´æ–°llama.cpp
  - prompt-processingçš„é€Ÿåº¦æ¯”ollamaæ…¢å¾ˆå¤š, å…¶å®ollamaé€Ÿåº¦ä¹Ÿä¸å¿«

- è‡ªå®šä¹‰æ¨¡å‹å·¥å…·é“¾çš„ä¼˜ç‚¹
  - åå°è¿è¡Œllmã€è‡ªåŠ¨åˆ‡æ¢llmã€åŠ å…¥task queueåŠå¼‚å¸¸æ¢å¤ éƒ½éœ€è¦è‡ªå®šä¹‰å·¥å…·é“¾çš„æ”¯æŒ
  - é’ˆå¯¹ç¡¬ä»¶è‡ªåŠ¨é€‰æ‹©åˆé€‚å‚æ•°
  - æ”¯æŒé’ˆå¯¹æ¨¡å‹æ·»åŠ è‡ªå®šä¹‰å¯åŠ¨å‚æ•°
  - å®šåˆ¶åˆ‡æ¢æ¨¡å‹ã€æ¨¡å‹ä¿ç•™ç¼“å­˜çš„é€»è¾‘ï¼Œé¿å…æ¯æ¬¡éƒ½é‡æ–°åŠ è½½æ¨¡å‹
  - è‡ªå®šä¹‰æ¨¡å‹è·¯ç”±ï¼Œç±»ä¼¼openrouter, æ ¹æ®ä½“ç§¯/costè‡ªåŠ¨é€‰æ‹©æ¨¡å‹
  - èƒ½ä¼˜åŒ–è½¯ä»¶å ç”¨çš„å†…å­˜, lmstudioçš„å†…å­˜å ç”¨å¾ˆå°‘, janaiä»·å€¼qwen3-4bæ¨¡å‹ggufå ç”¨å†…å­˜ä¸Š10GB
  - é’ˆå¯¹å¤šgpuè¿›è¡Œä¼˜åŒ–
  - é’ˆå¯¹appleè®¾å¤‡è¿›è¡Œä¼˜åŒ–
  - æ”¯æŒç”¨æˆ·æ›¿æ¢æœ€æ–°çš„llamacppæ¥æµ‹è¯•æœ€æ–°æ¨¡å‹

- ollama/lmstudioå°è£…äº†llama.cpp, janai forkäº†llama.cpp

- æ”¯æŒä½¿ç”¨å·²æœ‰modelæ–‡ä»¶.safetensorsçš„å·¥å…·åŒ…æ‹¬
  - janai, é€šè¿‡llama.cppæ–¹å¼çš„provider
  - Oobabooga

- ai-chatçš„å®¢æˆ·ç«¯å°è£…å¯å‚è€ƒ
  - janai, ollama-ui
  - gradio: Oobabooga, sd-webui

- llama.cppçš„å°è£…å¯å‚è€ƒ
  - janai, Oobabooga

- guiå°è£…åç«¯é€»è¾‘çš„å‚è€ƒ
  - comfyui, invokeai, aionui

- local-model-features

- lmstudio for image generation

- 
- 
- 
- 

# gpustack
- features
  - OpenAI-Compatible APIs: Fully compatible with OpenAIâ€™s API specifications for seamless integration.
  - Broad GPU Compatibility: Seamlessly supports GPUs from various vendors across Apple Macs, Windows PCs, and Linux servers.
  - Supports a wide range of models including LLMs, VLMs, image models, audio models, embedding models, and rerank models.
  - Flexible Inference Backends: Flexibly integrates with multiple inference backends including vLLM, Ascend MindIE, llama-box (llama.cpp & stable-diffusion.cpp) and vox-box.
  - Multi-Version Backend Support: Run multiple versions of inference backends concurrently to meet the diverse runtime requirements of different models.
  - Distributed Inference: Supports single-node and multi-node multi-GPU inference
  - Scalable GPU Architecture: Easily scale up by adding more GPUs or nodes to your infrastructure.
  - Robust Model Stability: Ensures high availability with automatic failure recovery, multi-instance redundancy
  - Intelligent Deployment Evaluation: Automatically assess model resource requirements, backend and architecture compatibility, OS compatibility, and other deployment-related factors.
  - Automated Scheduling: Dynamically allocate models based on available resources.
  - Lightweight Python Package: Minimal dependencies and low operational overhead.
  - User & API Key Management: Simplified management of users and API keys.
  - Real-Time GPU Monitoring: Track GPU performance and utilization in real time.
  - Token and Rate Metrics: Monitor token usage and API request rates.
  - Supported Platforms: Linux macOS Windows
  - Supported Accelerators: nvidia cuda, apple metal, AMD ROCm, Ascend CANN, Moore Threads MUSA
  - GPUStack uses vLLM, Ascend MindIE, llama-box (bundled llama.cpp and stable-diffusion.cpp server) and vox-box as the backends and supports a wide range of models.

- 
- 
- 
- 
- 
- 
- 
- 
- 
- 

## docs-gpustack

- [Architecture](https://docs.gpustack.ai/0.7/architecture/)
- GPUStack server
  - API Server: Provides a RESTful interface for clients to interact with the system. It handles authentication and authorization.
  - Scheduler: Responsible for assigning model instances to workers.
  - Model Controller: Manages the rollout and scaling of model instances to match the desired model replicas.
  - HTTP Proxy: Routes inference API requests to workers.
  - ğŸ›¢ï¸ uses `SQLite` by default, but you can configure it to use an external PostgreSQL or MySQL as well.
- GPUStack worker
  - Running inference servers for model instances assigned to the worker.
  - Reporting status to the server.
  - Routes inference API requests to backend inference servers.
- Inference Server
  - the backends that performs the inference tasks. GPUStack supports vLLM, Ascend MindIE, llama-box and vox-box as the inference server.
- RPC Server
  - enables running llama-box backend on a remote host. The Inference Server communicates with one or several instances of RPC server, offloading computations to these remote hosts. This setup allows for distributed LLM inference across multiple workers, enabling the system to load larger models even when individual resources are limited.
- Ray Head/Worker
  - Ray is a distributed computing framework that GPUStack utilizes to run distributed vLLM. Users can enable a Ray cluster in GPUStack to run vLLM across multiple workers. 
  - By default, it is disabled.

- 
- 
- 
- 
- 

# more
