---
title: lib-ai-app-community-model-dev-xp
tags: [large-language-model, stable-diffusion]
created: 2025-11-18T13:21:29.771Z
modified: 2025-11-18T13:22:22.078Z
---

# lib-ai-app-community-model-dev-xp

# guide

- models-variants
  - watching: openai, claude, qwen, deepseek, gemini/gemma, mistral/codestral, glm, kimi, minimax
  - variants: mlx, unsloth, quants
  - æµ‹è¯•æ¨¡å‹æ—¶å¯èƒ½æ›´å¸Œæœ›é€Ÿåº¦å¿«(å°å‚æ•°)ï¼Œä½†åšä»»åŠ¡æˆ–è§„åˆ’æ—¶æ›´å¸Œæœ›è´¨é‡å¥½(å¤§å‚æ•°)ï¼Œè¦å–èˆ
  - ğŸ“± ç«¯ä¾§æ¨¡å‹è¿˜è¦è€ƒè™‘ç”µæºåŠåŠŸè€—é—®é¢˜, å®æµ‹macbook-airåœ¨è·‘æ¨¡å‹æ—¶æ‰ç”µå¾ˆå¿«
    - ç«¯ä¾§æœ€å¥½ç”¨ api-key + tiny-local-llm
  - unslothå›¢é˜Ÿæä¾›çš„é‡åŒ–ç‰ˆæ¨¡å‹ä¼šæ ¹æ®åé¦ˆä¸æ–­æ›´æ–°ï¼Œç»å¸¸é’ˆå¯¹çƒ­é—¨æ¨¡å‹æ›´æ–°fixå’Œä¼˜åŒ–, è€Œlmstudio-communityçš„æ¨¡å‹å¾ˆå°‘ä¼šæ›´æ–°

- models-choices
  - ğŸ¤” LMs are tools. Describe your use cases.
    - åˆ†ææ¸…æ¥šæ ¸å¿ƒéœ€æ±‚: éœ€è¦reasoning/coding/large/faster
  - moeæ¨¡å‹çš„å®é™…æ•ˆæœå¤§æ¦‚åªæœ‰denseæ¨¡å‹çš„ä¸€åŠï¼Œå¦‚qwen3-30B-A3B ç›¸å½“äº Qwen3-14b
  - æ¨¡å‹å ç”¨VRAMä¸èƒ½å¤ªå¤§ï¼Œè¿˜è¦ä¸ºcontextå¤„ç†ã€åº”ç”¨ç¨‹åºå¦‚nextjs/comfyuié¢„ç•™RAM/VRAM
  - é€‰æ‹©æ¨¡å‹æ—¶å¤šç”¨å®˜æ–¹ç‰ˆ/ä¸»æµç‰ˆï¼Œå°ä¼—å¾®è°ƒçš„ç‰ˆæœ¬å¯èƒ½å­˜åœ¨tool-call/overthink/å¤šè¯­è¨€multilingual/å¯¹è¯é£æ ¼/llama.cppä¸æ”¯æŒç­‰é—®é¢˜/loop
    - é€‰ç”¨ä¸»æµç‰ˆè¿˜æ–¹ä¾¿ä¸å…¶ä»–ç”¨æˆ·å¯¹æ¯”é€Ÿåº¦/é…ç½®
    - éä¸»æµç‰ˆå¯èƒ½å‡ºç°vision/ragç­‰è¢«å»æ‰çš„é—®é¢˜
  - å¤šagentæ¶æ„æ—¶ï¼Œå¯ä½¿ç”¨ä¸åŒæ¶æ„çš„agentç›¸äº’éªŒè¯
  - non-thinkingæˆ–è¾“å‡ºç®€æ´çš„æ¨¡å‹é€‚åˆcoding
  - æ¨¡å‹çš„å‡çº§æ¢ä»£å¾ˆå¿«ï¼Œå–œæ–°ä¸å–œæ—§ï¼Œæ–°æ¨¡å‹å¯¹ tool-call/attention/é‡åŒ–çš„æ”¯æŒä¸€èˆ¬éƒ½æ›´å¥½
  - å¤§å‚çš„æ¨¡å‹ä¸€èˆ¬éƒ½ä¼šå®šæœŸå‡çº§ï¼Œä¼šæœ‰ç¤¾åŒºç”¨æˆ·å¯¹æ¯”è¯„æµ‹ï¼Œæ›´å¯é ã€æ›´æœ‰å‚ä¸æ„Ÿ

- tool-call/use
  - é’ˆå¯¹tool-useå¾®è°ƒä¼˜åŒ–çš„å°æ¨¡å‹æ…ç”¨, å› ä¸ºmcpçš„toolæè¿°å¯èƒ½å¯¼è‡´contextè¶…é•¿

- structured-output

- donts
  - å¾ˆå¤šå¸¦thinkingçš„å¤§æ¨¡å‹ä¸æ“…é•¿è®¡æ•°ï¼Œå¦‚within 18 wordsï¼Œ æœ‰çš„æ¨¡å‹çœŸçš„ä¼šé€ä¸ªtokenæ‰“å°å‡ºæ¥é€ä¸ªæ•°ä¸€é

- mac ğŸ
  - ğŸ‘€ åœ¨low power modeçœç”µæ¨¡å¼ä¸‹, æ¨¡å‹çš„è¾“å‡ºé€Ÿåº¦ä¼šæ¯”éçœç”µæ¨¡å¼æ…¢2-3å€
# draft
- ç°åœ¨çš„æ¨¡å‹ç¼ºå°‘ 15b-23b é—´çš„å°æ¨¡å‹ï¼Œå¯é€‰æ‹©ç¬¬ä¸‰æ–¹é­”æ”¹æˆ–æ‰©å®¹åçš„ç‰ˆæœ¬
  - è¿˜ç¼ºå°‘ 40b-71b é—´çš„ä¸­æ¨¡å‹

- task-specific leaderboard
  - rag
  - community users can upload benchmark results with device-specs from apps like lmstudio
    - è¯„æµ‹è€—æ—¶è€—åŠ›ï¼Œç¤¾åŒºå‚ä¸æ›´å¿«æ·ï¼Œä½†ä¼šé™ä½å‡†ç¡®åº¦
    - åŒæ ¼å¼ã€åŒé…ç½® çš„æ¨¡å‹æ¯”è¾ƒæ›´æœ‰æ„ä¹‰

- 
- 
- 
- 
- 
- 

# usage-xp
- ğŸ§ª test-cases
  - Which of these objects is not like the others: orange, banana, potato, chair
  - I have 7 apples. Yesterday I ate one apple. how many apples do i have now?
  - I have 2 apples, then I buy 2 more. I bake a pie with 2 of the apples. After eating half of the pie how many apples do I have left?
  - If we lay 5 shirts out in the sun and it takes 4 hours to dry, how long would 20 shirts take to dry? Explain your reasoning step by step
  - convert inches to cm: 15.4 x 7.3 x 13.5 inches
  - what is the latest version of Node.js?
  - å¤§æ¼ å­¤çƒŸç›´ å‡ºè‡ªå“ªé‡Œ? ä¸Šä¸‹å¥æ˜¯ä»€ä¹ˆï¼Ÿ è¡¨è¾¾ä»€ä¹ˆæ„æ€ï¼Ÿ æœ‰å…¶ä»–åœ°æ–¹å¼•ç”¨è¿‡è¿™å¥å—?

- cowork-model
  - macos model
  - shell model

- qwen3 ğŸŒ¹ /èƒ½åŠ›å…¨/thinkingå¼€å…³/å†…å®¹ä¸°å¯Œ
  - 4båŠ14bçš„è¾“å‡ºå†…å®¹éƒ½æ¯”è¾ƒè¯¦ç»†ï¼Œç»å¸¸åŒ…å«è¡¨æ ¼ğŸ“ˆ

- gpt-oss-20B-A3.6B ğŸ‘€ /ä¸šç•Œæ ‡æ†/è¾“å‡ºå¿«
  - è¾“å‡ºçš„å†…å®¹ç‰¹åˆ«å–œæ¬¢ç”¨è¡¨æ ¼ğŸ“ˆ, è®¨è®ºä»£ç ç›¸å…³é—®é¢˜ä¹Ÿå–œæ¬¢ç”¨è¡¨æ ¼
  - unsloth-Q5çš„è¾“å‡ºé€Ÿåº¦ä¸º 11.8 tops, offcial-Q4çš„è¾“å‡ºé€Ÿåº¦ä¸º 11.2 tops, é€Ÿåº¦æ¯”qwen3-14bæ›´å¿«

- glm-4.7-flash-30b-a3b
  - GLM-4.7-Flash-MLX-4bit çš„ç‰ˆæœ¬å®¹æ˜“é™·å…¥thinking loop
    - loopæ—¶çš„thinkingå†…å®¹æ²¡æœ‰éµå¾ªå¸¦stepçš„thinkingè¿‡ç¨‹
    - ğŸ’¡ å®æµ‹å®˜æ–¹å‚æ•°é…ç½®ä¸å¥½ç”¨, mlx-4bité€‚åˆ temperature-0.7
  - ğŸŒ¹thinkingçš„å†…å®¹éå¸¸agentic, å¸¦æœ‰æ˜ç¡®çš„step
    - request analysis > Brainstorm > draft res > refine res > revise > polish > response
  - [DeepSeek architecture?](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/discussions/2)
    - [DeepseekV3ForCausalLM](https://huggingface.co/zai-org/GLM-4.7-Flash/discussions/5)
    - the modular file does inherit the DSv3 module
    - And it looks like the only thing added on top of DSv3 is to ignore the MTP layer.

- nemotron-3-nano-30b-a3b
  - ä¸­æ–‡å†…å®¹ä¸°å¯Œ

- gemma3 ğŸŒ¹ /å¤šè¯­è¨€/åˆ›æ„æ–‡æœ¬/vision
  - 27b å’Œ 12b éƒ½èƒ½è¾ƒå¥½éµå¾ªå¸¦ç»“æ„çš„instructè¾“å‡ºï¼Œ 27bèƒ½ä¸»åŠ¨ç»™å‡ºæ›´å¤šå¤–éƒ¨ç½‘é¡µé“¾æ¥è€Œ12bç»™çš„é“¾æ¥å¾ˆå°‘

- magistral-small-2509-24b  ğŸ‘€ /å¯ä»¥ç”¨/think+vision/æ¬§æ´²å¤šè¯­è¨€/äº§å“çº¿ä¸°å¯Œ/censorå¼±
  - å›å¤ä¸€èˆ¬å¾ˆçŸ­ï¼Œæ„Ÿè§‰è´¨é‡ä¸é«˜
  - mistralç³»åˆ—æ¨¡å‹çš„çŸ¥è¯†ä¸°å¯Œåº¦å¾ˆé«˜, å¯ä»¥é™ä½å¯¹RAGçš„ä¾èµ– ğŸ¤”
  - thinkingæ—¶é—´åœ¨~~3-10~~min(2509å·²æ”¹è¿›)å·¦å³ï¼Œæˆ–è®¸å¯¹äºplanåˆ¶å®šè®¡åˆ’æœ‰ç”¨
  - è¾“å‡ºå†…å®¹å‡ ä¹ä¸æä¾›å¤–éƒ¨é“¾æ¥ï¼Œ2507ä¸ä¹Ÿæä¾›å¤–éƒ¨é“¾æ¥
  - è¾“å‡ºå†…å®¹ä¸­å‡ ä¹ä¸æä¾›è¡¨æ ¼
  - å¸¦thinkingçš„æ¨¡å‹ä¸æ“…é•¿è®¡æ•°ï¼Œå¦‚ within 18 words

- glm4 ğŸ‘€ /å¯ä»¥ç”¨/æ˜¯å¦å–„é•¿htmlä»£ç ?
  - ğŸ–¼ï¸ htmlä¸­æ”¯æŒæ˜¾ç¤ºå¤–éƒ¨å›¾ç‰‡ï¼Œéœ€è¦å½¢çŠ¶ç±»å›¾æ ‡çš„ä½ç½®èƒ½å‡†ç¡®ç”Ÿæˆsvg
  - glm4ä¸ä¼šthinkï¼Œè¾“å‡ºå†…å®¹è´¨é‡æ„Ÿè§‰ä¸€èˆ¬
  - è¾“å‡ºçš„é•¿åº¦å¤§æ¦‚åœ¨30-60è¡Œï¼Œç®€æ´æ˜¯ç‰¹è‰²ï¼Œå¯¹ä»£ç æœ‰ç”¨?
  - åœ¨å¤šè½®èŠå¤©æ—¶ï¼Œè¾“å‡ºå†…å®¹ä¹Ÿä¼šé€æ¸å˜é•¿?
  - è¾“å‡ºhtmlé¡µé¢æ—¶èƒ½æ·»åŠ å¤æ‚çš„svgä»£ç ï¼Œå½¢çŠ¶ç±»å›¾æ ‡çš„ä½ç½®èƒ½å‡†ç¡®ç”Ÿæˆsvg
  - ç”Ÿæˆçš„htmlé¡µé¢é£æ ¼æœ‰ç‚¹tailwindï¼Œä¹Ÿæœ‰ç‚¹bootstrap
  - ç”Ÿæˆçš„slider/carouselæ¯”qwen3æ›´å‡†ç¡®

- glm-z1 ğŸ‘€ /æ€è€ƒéå¸¸ä¹…/ä¸æ“…é•¿ä»£ç 
  - z1ä¼šthink5-15minï¼Œthinkä¸æ”¯æŒdisableï¼Œè¾“å‡ºå†…å®¹çš„é•¿åº¦ä¼šæ¯”glm4å¤š20è¡Œå·¦å³ï¼Œå¤šä¸€äº›å¤–éƒ¨é“¾æ¥ï¼Œå¤šç”¨å¾ˆå¤šè¡¨æ ¼ï¼Œè´¨é‡è¾ƒå¥½
  - z1çš„thinkæ—¶é—´æ¯”qwen3é•¿å¾ˆå¤šï¼Œ
  - è¾“å‡ºå†…å®¹çš„é•¿åº¦æ¯”qwen3æ›´å°‘, è¾“å‡ºå†…å®¹ä¼šæœ‰è¡¨æ ¼ğŸ“ˆ

## mobile ğŸ“±

- lfm
  - å¯¹ chat template å’Œ prompt çš„è¦æ±‚ä¸ä¸¥æ ¼ï¼Œæ–¹ä¾¿æµ‹è¯•
    - ä¸è¾“å…¥system promptä¹Ÿèƒ½å·¥ä½œ

## coding ğŸ”¡

- tips
  - âš–ï¸ codingçš„promptä¹Ÿå°½é‡éµå¾ª plan + act çš„ç»“æ„
  - å¯¹äºaiæŒ‰ç”¨æˆ·æä¾›çš„æ¨¡ç‰ˆè¾“å‡ºhtmlçš„åœºæ™¯ï¼Œç”¨æˆ·æä¾›å’Œaiè¾“å‡ºçš„ä»£ç é€šå¸¸éƒ½æ˜¯åçŸ­çš„ã€åé™æ€çš„; 
    - å¦ä¸€ç§æ€è·¯æ˜¯æŒ‰æ¨¡ç‰ˆè¾“å‡ºmarkdownæ‰‹åŠ¨æ‹¼æ¥ï¼Œé€Ÿåº¦å¯èƒ½æ›´å¿«
  - codingæ¨¡å‹å¿…é¡»è¦ç”¨æ–°ç‰ˆæ‰èƒ½ä½¿ç”¨æœ€æ–°æ¡†æ¶çš„æ¶æ„å†™æ³•ï¼Œå¦‚tailwind.v4, reactjs.v19

- ğŸ§ª test-cases
  - landing-page + tailwind: åŸºæœ¬éƒ½èƒ½å®ç°é¡µé¢, ğŸŒ¹ glmæ“…é•¿å›¾æ ‡å’Œå›¾ç‰‡
    - kat-devå’Œqwen3éƒ½ä¸æ“…é•¿å›¾æ ‡å’Œå›¾ç‰‡
  - landing + threejs: åŸºæœ¬éƒ½èƒ½å®ç°, 
    - qwen3-32bæœ‰æ—¶æ— æ³•è¿è¡Œdemo
  - ğŸ¤” game-reaction-for-click: kat-devèƒ½æ­£ç¡®å®ç°ï¼Œ qwen3-thinkèƒ½å®ç°
    - glmå¼‚å¸¸ï¼Œqwen3å¼‚å¸¸
  - game-typing: glm/kat-devè¡Œ, qwen3éthinkä¹Ÿè¡Œ
  - ğŸ¤” dashboard-crud: qwen3å°bugï¼Œglmèƒ½åšuiï¼Œkat-devå¤±è´¥
  - ğŸ¤” weather: glmå¤§å¤šèƒ½å®ç°
    - kat-devéƒ¨åˆ†å¼‚å¸¸ï¼Œqwen3å¼‚å¸¸
  - slider: glmæ“…é•¿cssåŠ¨ç”»æ•ˆæœï¼Œå…¶ä»–aiçš„uxæ•ˆæœä¸€èˆ¬
  - threejs-earth: qwen3è¡Œ, glméƒ¨åˆ†å¼‚å¸¸, kat-devå¼‚å¸¸
  - vocabulary-card: åŸºæœ¬éƒ½èƒ½æ­£ç¡®è°ƒæ•´é¡µé¢, 
    - kat-devæœ‰æ—¶æ ·å¼å¼‚å¸¸

- qwen3-coder-30b-a3b ğŸŒ¹ /é€Ÿåº¦å¿«
  - ç”Ÿæˆå•é¡µé¢çš„æ•ˆæœå¥½é€Ÿåº¦å¿«
  - æ“…é•¿ç”¨æ¸å˜è‰²å—ä»£æ›¿å›¾ç‰‡å ä½ç¬¦
  - å†™å®Œä»£ç åä¸€èˆ¬è¿˜ä¼šè®²è§£è¯´æ˜ä¸€æ®µ

- devstral-2512-24b
  - å‰ç«¯landing-pageæ˜¯tailwindé£æ ¼, ä½†æ•´ä½“ç™½è‰²ç®€çº¦ï¼Œè‰²å½©ä¸ä¸°å¯Œ
- devstral-2507-24b /æ¬§æ´²å¤šè¯­è¨€/instruct

- qwen2.5-coder-32b /å¾®è°ƒå¤š

- qwen3-32b /thinkingå¼€å…³/èƒ½åŠ›å…¨
  - æ“…é•¿æ¸å˜è‰²æ–‡å­—
  - ç”Ÿæˆå•é¡µhtmlæ—¶ï¼Œ éœ€è¦å½¢çŠ¶ç±»å›¾æ ‡çš„åœ°æ–¹ä¼šä¹±ç 
  - ä¸æ“…é•¿æ˜¾ç¤ºå¤–éƒ¨å›¾ç‰‡ï¼Œéœ€è¦å›¾ç‰‡çš„åœ°æ–¹ä¼šæ˜¾ç¤ºç¼ºçœå ä½ç¬¦

- kat-dev-32B
  - é¡µé¢ç®€æ´
  - æ¯”qwen3æ›´æ“…é•¿æ ·å¼ã€èƒ½æ˜¾ç¤ºéƒ¨åˆ†å›¾æ ‡
  - é¡µé¢ä¸æ˜¯å…¸å‹çš„tailwindé£æ ¼ï¼Œé£æ ¼æœ‰ç‚¹é™ˆæ—§
  - ä¸æ“…é•¿æ˜¾ç¤ºå¤–éƒ¨å›¾ç‰‡
  - ä¸æ“…é•¿ç”Ÿæˆå½¢çŠ¶ç±»å›¾æ ‡ï¼Œç»å¸¸ç”Ÿæˆé‡å¤å›¾æ ‡ï¼Œæœ‰æ—¶ä¼šç¼ºå¤±éƒ¨åˆ†å›¾æ ‡

- uigen-fx-4b /æ“…é•¿uiæ¡†æ¶/èƒ½å†™js
  - æ“…é•¿æ¸å˜è‰²æŒ‰é’®
  - ä¸æ“…é•¿ç”¨æ¸å˜è‰²å—ä»£æ›¿å›¾ç‰‡å ä½ç¬¦
  - æœ‰æ—¶èƒ½å†™å¾ˆå¤šjsä»£ç 

- webgen-4b /æ“…é•¿htmlé¡µé¢ä¸æ“…é•¿æ¡†æ¶å’Œjs
  - webgenç”Ÿæˆå•é¡µé¢çš„æ•ˆæœè¿œä¸å¦‚uigen/qwen3-coder
  - ä¼¼ä¹ä¸æ“…é•¿tailwind, ç”Ÿæˆé¡µé¢çš„é£æ ¼åétailwindæ ·å¼çš„ä¼ ç»Ÿç½‘é¡µ
  - ç»å¸¸å‡ºç°éƒ¨åˆ†å…ƒç´ æ ·å¼é”™ä¹±çš„é—®é¢˜

- [ruv/ruvltra-claude-code Â· Hugging Face](https://huggingface.co/ruv/ruvltra-claude-code)
  - The World's First LLM Optimized for Claude Code
  - Self-Learning Intelligence (SONA): The model continuously improves from interactions, learning your coding patterns, preferences, and project-specific conventions.

## data-viz/charting ğŸ“Š

- ç”Ÿæˆå›¾è¡¨çš„æ–¹æ¡ˆ(åˆ†æéœ€æ±‚: åå‰ç«¯å±•ç¤º/åæ•°æ®åˆ†æ)
  - æ€è·¯1: llmè¿”å›æ ¸å¿ƒæ•°æ®ï¼Œå®¢æˆ·ç«¯tool callç»˜å›¾
  - æ€è·¯2: llmç›´æ¥è¿”å›ä»£ç ï¼Œä½¿ç”¨codingæ–¹æ¡ˆå®ç°ç»˜å›¾
  - æ€è·¯3: ç®€å•åœºæ™¯è€ƒè™‘ä½¿ç”¨md-tableæ›¿ä»£ï¼Œç„¶åå‰ç«¯æ‰‹åŠ¨æ¸²æŸ“å›¾è¡¨æˆ–å¼•å¯¼ç”¨æˆ·åˆ›å»ºå›¾è¡¨
  - ğŸ’¡ æ€è·¯4: ä½¿ç”¨codeblockä¼ è¾“å›¾è¡¨ç›¸å…³æ•°æ®ï¼Œå‚è€ƒstreamdownæµå¼è§£æmermaidçš„æ–¹æ¡ˆ
  - æ€è·¯5: data > sql > chart, ç±»ä¼¼chat2dbçš„æ–¹æ¡ˆ

- [mermaidchart/merged-mermaid-7b _202509](https://huggingface.co/mermaidchart/merged-mermaid-7b/tree/main)
  - æ— æ–‡æ¡£è¯´æ˜

- [TroyDoesAI/BlackSheep-MermaidMistral-22B Â· Hugging Face](https://huggingface.co/TroyDoesAI/BlackSheep-MermaidMistral-22B)
  - /202409
  - merge: TroyDoesAI/Mermaid-22B, TroyDoesAI/BlackSheep-Mistral-22B
  - [TroyDoesAI/Mermaid-Llama-3-8B Â· Dataset](https://huggingface.co/TroyDoesAI/Mermaid-Llama-3-8B/discussions/1)
    - Is the dataset publicly available?
    - Yeah sure, make your own using my toolkit and my model to distill its knowledge: https://github.com/Troys-Code/AI_Research
    - [TroyDoesAI/MermaidMistral Â· Dataset?](https://huggingface.co/TroyDoesAI/MermaidMistral/discussions/1)
    - I do not plan to release my ~500 example hand curated dataset as that is used for my evaluation dataset in training now as I believe these are the highest of quality compared to Generated by GPT4 and do not want that public as this is my benchmarker now for how the model is improving as it is incredibly hard to evaluate a models performance on subjective things like story flow.

- [octadion/deepseek-coder-6.7b-pandas Â· Hugging Face](https://huggingface.co/octadion/deepseek-coder-6.7b-pandas)
  - /202403

- [pipizhao/Pandalyst-7B-V1.2 Â· Hugging Face](https://huggingface.co/pipizhao/Pandalyst-7B-V1.2)
  - /202310
  - https://github.com/pipizhaoa/Pandalyst
  - [ã€Pandalyst-7B-V1.2ã€‘ Now we can plot and much more powerful ! : r/LocalLLaMA _202310](https://www.reddit.com/r/LocalLLaMA/comments/178uxtf/pandalyst7bv12_now_we_can_plot_and_much_more/)
  - Pandalyst-7B-V1.2, which was trained on CodeLlama-7b-Python and it surpasses ChatGPT-3.5 (2023/06/13), Pandalyst-7B-V1.1 and WizardCoder-Python-13B-V1.0 in our PandaTest_V1.0.
  - 7b version is trained based on Codellama-python-7B, and achieve competitive performance with our 13B !!! (witch was trained on wizardcoder-python-13B)

## image ğŸ–¼ï¸

- z-image-turbo ä¸æ“…é•¿512å°ºå¯¸çš„å›¾ç‰‡ï¼Œæ„Ÿè§‰å¾ˆç³Š
  - https://www.modelscope.cn/studios/Tongyi-MAI/Z-Image-Gallery
  - åŒæ ·prompt+stepç”Ÿæˆ1024å°ºå¯¸å°±æ¯”è¾ƒæ¸…æ™°
  - ä¼¼ä¹åªèƒ½ç”Ÿæˆé•¿å®½1:1æ¯”ä¾‹çš„å›¾ç‰‡, éšæœºæ¯”ä¾‹ä¼šæŠ›å‡ºå¼‚å¸¸
  - ç”Ÿæˆ 2048x2048 çš„å›¾ç‰‡æ—¶, macbookå†…å­˜çˆ†ç‚¸ï¼Œç”µè„‘é‡å¯
  - ä¸æ“…é•¿æ¸²æŸ“ä¸­æ–‡å­—ä½“

- https://huggingface.co/AIDC-AI/Ovis-Image-7B
  - https://github.com/AIDC-AI/Ovis-Image
  - a 7B text-to-image model specifically optimized for high-quality text rendering

## translation ğŸŒ

- é€šè¿‡ promptfoo æµ‹è¯•å¤šä¸ªæ¨¡å‹çš„ç¿»è¯‘èƒ½åŠ›, é¼ æ ‡hoveråœ¨è¡¨å¤´åˆ—å¯ä»¥çœ‹åˆ°æ¨¡å‹ä¿¡æ¯, promptä¸å˜æ—¶åªä¼šå¢é‡è¯·æ±‚æ–°åŠ çš„prompt
  - hy-mtä¼šé‡‡ç”¨æ›´å¤šæ„è¯‘ï¼Œå°†ä¸­æ–‡çš„ä¸€å¥ç¿»è¯‘ä¸ºå¤šå¥è‹±æ–‡ï¼Œæ‰€ä»¥ç¿»è¯‘è¾“å‡ºå†…å®¹æœ€é•¿
  - å¯¹ä¸­æ–‡äººåç§°å‘¼ã€ä¸­æ–‡ä¿—è¯­ç¿»è¯‘è¾ƒå‡†ç¡®çš„æ˜¯hy-mt1.5, å©¶å©¶/è„‚ç²‰å †
  - rivaå¯¹ 4% çš„æ•°å­—ç¿»è¯‘ä¸º 4 percent, è€Œqwen3-4b/hy-mtèƒ½ç›´æ¥ç”¨åŸæ–‡çš„ 4%
    - rivaå¯¹äºåæ–‡æœ¬çš„é“¾æ¥åˆ—è¡¨ï¼ŒèŒƒå›´åä»ä¸º1è¡Œï¼Œä½†qwen3/hy-mtèƒ½è¾“å‡ºå¤šè¡Œè€Œå‹å¥½
    - ç¿»è¯‘è¾“å‡ºå†…å®¹æœ€çŸ­
    - ğŸ¤” ä¼¼ä¹ç›¸å¯¹äºqwen3-4bæ²¡æœ‰ä»»ä½•ä¼˜åŠ¿

- [tencent/HY-MT1.5-7B Â· Hugging Face _202512](https://huggingface.co/tencent/HY-MT1.5-7B)
  - /freeTill100mMAU
  - ZH<=>XX
  - > ğŸ”  å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘ä¸º{target_language}ï¼Œæ³¨æ„åªéœ€è¦è¾“å‡ºç¿»è¯‘åçš„ç»“æœï¼Œä¸è¦é¢å¤–è§£é‡Š: {source_text}
  - XX<=>XX Translation, excluding ZH<=>XX
  - > ğŸ”  Translate the following segment into {target_language}, without additional explanation: 
  - > ğŸ”  å°†ä»¥ä¸‹<source></source>ä¹‹é—´çš„æ–‡æœ¬ç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œæ³¨æ„åªéœ€è¦è¾“å‡ºç¿»è¯‘åçš„ç»“æœï¼Œä¸è¦é¢å¤–è§£é‡Šï¼ŒåŸæ–‡ä¸­çš„<sn></sn>æ ‡ç­¾è¡¨ç¤ºæ ‡ç­¾å†…æ–‡æœ¬åŒ…å«æ ¼å¼ä¿¡æ¯ï¼Œéœ€è¦åœ¨è¯‘æ–‡ä¸­ç›¸åº”çš„ä½ç½®å°½é‡ä¿ç•™è¯¥æ ‡ç­¾ã€‚è¾“å‡ºæ ¼å¼ä¸ºï¼š<target>str</target>
  - âš–ï¸ [Why such an absurdly restrictive license?](https://huggingface.co/tencent/HY-MT1.5-7B/discussions/3)
  - https://github.com/Tencent-Hunyuan/HY-MT/blob/main/License.txt
    - If, on the Tencent HY version release date, the monthly active users of all products or services made available by or for Licensee is greater than 100 million monthly active users in the preceding calendar month, You must request a license from Tencent

- [google/translategemma-4b-it Â· Hugging Face _202601](https://huggingface.co/google/translategemma-4b-it)
  - 4b, 12b, 27b
  - supports direct translation of a text input, or text-extraction-and-translation from an image input. 
  - The models were fine-tuned from the original Gemma 3 checkpoints using parallel data from a wide variety of sources. The TranslateGemma models used 4.3 billion tokens during SFT and 10.2 million tokens during the reinforcement learning phase. 
  - Training was done using JAX and ML Pathways. 
  - å¯¹æ¨¡ç‰ˆè¦æ±‚ä¸¥æ ¼ï¼Œä¼¼ä¹ä¸èƒ½åœ¨lmstudioç›´æ¥å¯¹è¯

- [nvidia/Riva-Translate-4B-Instruct Â· Hugging Face _202506](https://huggingface.co/nvidia/Riva-Translate-4B-Instruct)
  - /free
  - mlxæ¨¡å‹åœ¨lmstudioæµ‹è¯•en2cnæ—¶ï¼Œåªä¼šç¿»è¯‘ç¬¬ä¸€æ®µenè‹±æ–‡æ–‡å­—
  - > ğŸ”  translating text from English to Simplified Chinese
  - It is a fine-tuned version of a 4B Base model that was pruned and distilled from nvidia/Mistral-NeMo-12B-Base(202405) using our LLM compression technique.
  - [Any-to-any](https://huggingface.co/nvidia/Riva-Translate-4B-Instruct/discussions/1)
  - The model currently supports the following combinations (non-English to non-English is not supported):
  - English to non-English
  - Non-English to English
  - German(de), European Spanish(es-ES), LATAM Spanish(es-US), France(fr), Brazillian Portugese(pt-BR), Russian(ru), Simplified Chinese(zh-CN), Traditional Chinese(zh-TW), Japanese(ja), Korean(ko), Arabic(ar).

## rag

- [nvidia/NVIDIA-Nemotron-Parse-v1.1 Â· Hugging Face](https://huggingface.co/nvidia/NVIDIA-Nemotron-Parse-v1.1)
  - designed to understand document semantics and extract text and tables elements with spatial grounding
  - Given an image, NVIDIA Nemotron Parse v1.1 produces structured annotations, including formatted text, bounding-boxes and the corresponding semantic classes, ordered according to the document's reading flow.

- [inference-net/Schematron-8B Â· Hugging Face](https://huggingface.co/inference-net/Schematron-8B)
  - Schematron series, Inference.net's longâ€‘context extraction models specialized in converting noisy HTML into clean, typed JSON that conforms to your custom schema. 
  - åŸºäº meta-llama/Llama-3.1-8B-Instruct
  - https://x.com/samhogan/status/2018838862123196455
    - I found out today that two of the largest web scraping companies in the world are using a custom Llama 3 model we released last year to process millions of webpages per day. 
    - Schematron-3b: HTML -> JSON parsing
    - Custom LLMs for data extraction are GOATed dude! We built one for a special use case and it outperforms NLP by far.

- [inference-net/OSSAS-Qwen3-14B Â· Hugging Face](https://huggingface.co/inference-net/OSSAS-Qwen3-14B)
  - part of Project OSSAS, developed in collaboration with LAION and Wynd Labs to democratize access to scientific knowledge by creating structured summaries of research papers at scale.

- [zilliz/semantic-highlight-bilingual-v1 Â· Hugging Face](https://huggingface.co/zilliz/semantic-highlight-bilingual-v1)
  - We kept running into this problem: when we retrieve documents in our RAG system, users can't find where the relevant info actually is.
  - This work draws its core ideas and theoretical underpinnings from Provence (https://arxiv.org/abs/2501.16214)

- [apple/CLaRa-7B-Instruct Â· Hugging Face _202511](https://huggingface.co/apple/CLaRa-7B-Instruct)
  - instruction-tuned unified RAG model with built-in semantic document compression (16Ã— & 128x).
  - It supports instruction-following QA directly from compressed document representations.

- [CohereLabs/c4ai-command-r7b-12-2024 Â· Hugging Face](https://huggingface.co/CohereLabs/c4ai-command-r7b-12-2024)
  - /cc-nc
  - [How was r7b?](https://huggingface.co/CohereLabs/c4ai-command-r7b-12-2024/discussions/3)

- [jinaai/ReaderLM-v2 _1.5BÂ· Hugging Face _202503](https://huggingface.co/jinaai/ReaderLM-v2)
  - /cc-nc
  - converts raw HTML into beautifully formatted markdown or JSON with superior accuracy and improved longer context handling.

- [inference-net/Schematron-8B Â· Hugging Face](https://huggingface.co/inference-net/Schematron-8B)
  - specialized in converting noisy HTML into clean, typed JSON that conforms to your custom schema. 

## vlm

- [stepfun-ai/Step3-VL-10B Â· Hugging Face](https://huggingface.co/stepfun-ai/Step3-VL-10B)

- [apple/FastVLM-0.5B Â· Hugging Face _202412](https://huggingface.co/apple/FastVLM-0.5B)
  - We introduce FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images.
  - Our smallest variant outperforms LLaVA-OneVision-0.5B with 85x faster Time-to-First-Token (TTFT) and 3.4x smaller vision encoder.
  - Our larger variants using Qwen2-7B LLM outperform recent works like Cambrian-1-8B while using a single image encoder with a 7.9x faster TTFT.
  - [riddhimanrana/fastvlm-0.5b-captions Â· Hugging Face](https://huggingface.co/riddhimanrana/fastvlm-0.5b-captions)
    - a finetuned version of FastVLM-0.5B Stage 3 
    - built for efficient structured image captioning on mobile devices.

## omni

- [openbmb/MiniCPM-o-4_5 Â· Hugging Face _202602](https://huggingface.co/openbmb/MiniCPM-o-4_5)
  - The first full-duplex omni-modal LLM 
  - Full-duplex Omni-modal Live Streaming: The model can see, listen, and speak simultaneously in a real-time conversation without mutual blocking
  - https://x.com/OpenBMB/status/2018741614257307678
    - Leading Performance: Scoring 77.6 on OpenCompass, it outperforms GPT-4o & Gemini 2.0 Pro in vision-language tasks with 9B params
# model-wiki/bookmarks
- è¶…å¤§æ¨¡å‹çš„æå°é‡åŒ–ç‰ˆ
  - Llama-3.3-70B-Instruct-abliterated-Q2-mlx  22.07gb
  - Qwen3-Next-80B-A3B-Instruct-q2-mlx  24.95gb
  - Mistral-Large-Instruct-2411-Q2-MLX  45.99gb
  - gpt-oss-120b-mlx-2Bit(116.8B A5.1B)  36.61gb
  - GLM-4.5-Air-2bit(106b A12B)  33.45gb
  - GLM-4.5-Air-4bit(106b A12B)  62gb
  - DeepSeek-V3.1-Terminus-mlx-2Bit  209.89gb
  - DeepSeek-R1-2bit  251.82gb
# models-features/variants
- ä¸“ç”¨æ¨¡å‹
- ocr
- tool-calling
  - lfm2-1.2b-tool
- edit-apply
- devops
- graphics
- computer-use
- cpu
  - [NanoAgent â€” A 135M Agentic LLM with Tool Calling That Runs on CPU : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1oomy4t/nanoagent_a_135m_agentic_llm_with_tool_calling/)

- reap
- https://huggingface.co/cerebras/GLM-4.6-REAP-268B-A32B
  - [I think the dataset is gonna have to be more diverse.](https://huggingface.co/cerebras/GLM-4.6-REAP-268B-A32B/discussions/1)
    - The method looks promising but as in other prunes, it may have REAPed too much to be viable.

- [starvector/starvector-1b-im2svg Â· Hugging Face _202503](https://huggingface.co/starvector/starvector-1b-im2svg)
  - StarVector is a foundation model for generating Scalable Vector Graphics (SVG) code from images and text
  - It utilizes a Vision-Language Modeling architecture to understand both visual and textual inputs, enabling high-quality vectorization and text-guided SVG creation.
  - https://github.com/joanrod/star-vector /apache2/202504/python
    - StarVector Accepted at CVPR 2025
  - [æ„Ÿè§‰ä¸å¤ªè¡Œã€‚ã€‚](https://huggingface.co/starvector/starvector-1b-im2svg/discussions/2)
    - This checkpoint is designed for converting images into SVGs. While we do have a text-to-SVG model, we plan to release it at a later stage.
    - The current model performs well with simple icons but has limitations with more complex images. Improving performance on complex cases is a key focus of our ongoing work.

- [lakhera2023/devops-slm-v1 Â· Hugging Face _202509](https://huggingface.co/lakhera2023/devops-slm-v1)
  - Based on Qwen2.5
  - a specialized language model specifically for DevOps tasks and operations only.
  - designed EXCLUSIVELY for DevOps-related tasks. It has robust filtering that will NOT respond to general questions about movies, weather, cooking, sports, music
  - [Meet the first Small Language Model built for DevOps : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ndm44z/meet_the_first_small_language_model_built_for/)

- https://huggingface.co/Alibaba-EI/SmartResume /apache2/202510
  - ä¸€ä¸ªé¢å‘ç‰ˆé¢ç»“æ„çš„æ™ºèƒ½ç®€å†è§£æç³»ç»Ÿï¼Œç³»ç»Ÿæ”¯æŒ PDFã€å›¾ç‰‡åŠå¸¸è§ Office æ–‡æ¡£æ ¼å¼ï¼Œèåˆ OCR ä¸ PDF å…ƒæ•°æ®å®Œæˆæ–‡æœ¬æå–
  - æœ¬ä»“åº“åŒ…å« SmartResume é¡¹ç›®æ‰€éœ€çš„ä¸¤ä¸ªæ ¸å¿ƒæƒé‡æ–‡ä»¶ï¼Œç”¨äºç®€å†ä¿¡æ¯æå–å’Œç‰ˆé¢åˆ†æã€‚
  - Qwen3-0.6B: ç®€å†æ–‡æœ¬ä¿¡æ¯æå–å’Œç»“æ„åŒ–å¤„ç†
  - YOLOv10: ç®€å†ç‰ˆé¢å¸ƒå±€æ£€æµ‹å’ŒåŒºåŸŸåˆ†å‰²
# models-exploring
- [æ ç²¾å¤§æ¨¡å‹ _202406](https://modelscope.cn/models/maomoa/GangLLM)
  - ä¸€ä¸ªä¸“é—¨ä¸ç”¨æˆ·æŠ¬æ çš„å¤§æ¨¡å‹ï¼Œå®ƒçš„åå­—å«â€œé’¢è›‹å„¿â€ï¼Œå®ƒå¯ä»¥æ ¹æ®ç”¨æˆ·çš„è¾“å…¥ä¼šè¿›è¡Œæ ç²¾å¼çš„å›å¤ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆ°ç”¨æˆ·è¨€è¾ä¸­çš„ç»†å¾®æ¼æ´ï¼Œå¹¶æ®æ­¤å±•å¼€çŠ€åˆ©çš„åé©³ï¼Œ
  - è¯¥æ¨¡å‹åœ¨å¼€æºå¤§æ¨¡å‹ä¸Šå¾®è°ƒè€Œæ¥ï¼ŒåŸºç¡€æ¨¡å‹é‡‡ç”¨çš„ InternLLM-Chat-7B æ¨¡å‹ï¼Œé‡‡ç”¨1680æ¡æ ç²¾å¼å¯¹è¯æ•°æ®ï¼Œå¤–åŠ 100æ¡è‡ªæˆ‘è®¤çŸ¥æ•°æ®è¿›è¡Œçš„å¾®è°ƒè®­ç»ƒã€‚

- [SicariusSicariiStuff/Assistant_Pepe_8B Â· Hugging Face](https://huggingface.co/SicariusSicariiStuff/Assistant_Pepe_8B)
  - é€‚åº¦æ ç²¾ + é€‚åº¦ç”Ÿäº§åŠ›
  - å›ç­”æ¯”åŸºæ¨¡llama3-8bè¦çŸ­ï¼Œ ä¸å–œæ¬¢ç”¨åˆ—è¡¨ï¼Œä¸å–œæ¬¢ç”¨è¡¨æ ¼

- [LiquidAI/LFM2-1.2B-RAG Â· Hugging Face _202509](https://huggingface.co/LiquidAI/LFM2-1.2B-RAG)
  - specialized in answering questions based on provided contextual documents, for use in RAG
  - We recommend using greedy decoding with a `temperature=0`.
  - The system prompt is optional. By default, the output's language follows the user prompt's language.
  - âš ï¸ The model supports both single-turn and multi-turn conversations.
  - [Correct chat template ?](https://huggingface.co/LiquidAI/LFM2-1.2B-RAG/discussions/3)
    - You can use any. As mentioned, the system prompt is optional.
    - "[...]" symbolizes a truncation, which is why there's no question in the chat template example.
- [LiquidAI/LFM2-1.2B-Extract Â· Hugging Face _202508](https://huggingface.co/LiquidAI/LFM2-1.2B-Extract)
  - designed to extract important information from a wide variety of unstructured documents (such as articles, transcripts, or reports) into structured outputs like JSON, XML, or YAML.
  - We strongly recommend using greedy decoding with a `temperature=0`.
  - If no system prompt is provided, the model will default to JSON outputs. We recommend providing a system prompt with a specific format (JSON, XML, or YAML) and a given schema to improve accuracy 
  - âš ï¸ The model is intended for single-turn conversations.
- [LiquidAI/LFM2-1.2B-Tool Â· Hugging Face _](https://huggingface.co/LiquidAI/LFM2-1.2B-Tool)
  - designed for concise and precise tool calling.
  - We recommend using greedy decoding with a `temperature=0`.

- [apple/CLaRa-7B-Instruct Â· Hugging Face](https://huggingface.co/apple/CLaRa-7B-Instruct)
  - our instruction-tuned unified RAG model with built-in semantic document compression (16Ã— & 128x).
  - It supports instruction-following QA directly from compressed document representations.
  - âš ï¸ The model supports both single-turn and multi-turn conversations.

- [nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct Â· Hugging Face](https://huggingface.co/nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct)
  - a series of ultra-long context language models designed to process extensive sequences of text (up to 1M, 2M, and 4M tokens) while maintaining competitive performance on standard benchmarks. 

- [Otakadelic/phi-4-abliterated-Orion-18B Â· Hugging Face](https://huggingface.co/Otakadelic/phi-4-abliterated-Orion-18B)
  - Orion-zhen's original wasnâ€™t just abliteration. it is capable of portraying complex personalities, delicate emotions, sharp thoughts and inner conflict, it felt more like a katana than a model: slicing through responses with cold precision.
  - This version shifts the focus slightly toward storytelling. Still emotionally complex, still powerful under duress, captivity, or obedienceâ€”but with a more narrative-friendly tone.
  - Based on Phi-4 Abliterated (40-layer model)
  - Inspired by @mlabonne's BigQwen2.5-Echo-47B-Instruct, added exact same layers to original Phi-4 14B(40-layer model) to the middle part while keep in tact first and last parts. Though its parameter count increased(14B --> 18B), it is a structural duplicate.

- [RUC-DataLab/DeepAnalyze-8B Â· Hugging Face _202510](https://huggingface.co/RUC-DataLab/DeepAnalyze-8B)
  - the first agentic LLM for autonomous data science. It can autonomously complete a wide range of data-centric tasks without human intervention
  - https://github.com/ruc-datalab/DeepAnalyze

- [DatarusAI/Datarus-R1-14B-preview Â· Hugging Face _202508](https://huggingface.co/DatarusAI/Datarus-R1-14B-preview)
  - a 14B-parameter open-weights language model fine-tuned from Qwen2.5-14B-Instruct, designed to act as a virtual data analyst and graduate-level problem solver.
  - Unlike traditional models trained on isolated Q&A pairs, Datarus learns from complete analytical trajectoriesâ€”including reasoning steps, code execution, error traces, self-corrections, and final conclusionsâ€”all captured in a ReAct-style notebook format.
  - https://github.com/DatarusAI/Datarus-JupyterAgent /MIT/python
  - [Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis | Abstract](https://www.arxiv.org/abs/2508.13382)

- [Ellbendls/Qwen-3-4b-Text_to_SQL Â· Hugging Face _202509](https://huggingface.co/Ellbendls/Qwen-3-4b-Text_to_SQL)
  - a fine-tuned version of Qwen/Qwen3-4B designed to convert natural language queries into SQL statements
  - In scenarios where table schema context is missing, the model is trained to generate schema definitions along with the SQL query

- [jupyter-agent/jupyter-agent-qwen3-4b-instruct Â· Hugging Face _202509](https://huggingface.co/jupyter-agent/jupyter-agent-qwen3-4b-instruct)
  - a fine-tuned version of Qwen3-4B-Thinking-2507 specifically optimized for data science agentic tasks in Jupyter notebook environments
  - This model can execute Python code, analyze datasets, and provide step-by-step reasoning with intermediate computations to solve realistic data analysis problems.
  - Dataset domains: Primarily trained on Kaggle-style data science tasks
  - https://github.com/huggingface/jupyter-agent /202509
  - https://huggingface.co/spaces/lvwerra/jupyter-agent-2
  - Context window: Limited to 32K tokens, may struggle with very large notebooks
  - Tool calling format: Requires specific scaffolding for optimal performance
  - [jupyter-agent/jupyter-agent-qwen3-4b-thinking Â· Hugging Face](https://huggingface.co/jupyter-agent/jupyter-agent-qwen3-4b-thinking)
  - [Jupyter Agents: training LLMs to reason with notebooks](https://huggingface.co/blog/jupyter-agent-2)

- [mlfoundations/tabula-8b Â· Hugging Face _202406](https://huggingface.co/mlfoundations/tabula-8b)
  - model for prediction (classification and binned regression) on tabular data.
  - fine-tuned from the Llama-3 8B model
  - [TabuLa-8B: a foundation model for prediction on tabular data : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1dmnlv0/tabula8b_a_foundation_model_for_prediction_on/)

- [defog/llama-3-sqlcoder-8b Â· Hugging Face _202405](https://huggingface.co/defog/llama-3-sqlcoder-8b)
  - model for text to SQL generation for Postgres, Redshift and Snowflake 
  - Finetuned from model: [Meta-Llama-3-8B-Instruct]
  - ğŸ†š [Difference between the llama-3-sqlcoder-8b model vs. sqlcoder-7b-2 model.](https://huggingface.co/defog/sqlcoder-7b-2/discussions/47)
    - æ— å›å¤
  - https://x.com/rishdotblog/status/1788650171246551086 _202405
    - Llama-3 based SQLCoder 8b is out! Open weights with a commercially friendly cc-by-sa license. Probably the best <10B param model for Postgres text to SQL right now.
    - Our previous small model (sqlcoder-7b-2) was good at generating 0-shot SQL, but did terribly at following instructions. So while it was great in our evals, it was lacking in real-world use-cases where instruction following is much more important.
    - To address this, we trained this model with much more instruction data. We also made our original eval much harder to make sure we stayed on the right track.
    - Changes to prompt: You previously had to use our slightly idiosyncratic prompt for best results. Now, you can just use the standard Llama-3 instruct prompt.

- [defog/sqlcoder-7b-2 Â· Hugging Face _202402](https://huggingface.co/defog/sqlcoder-7b-2)
  - model for natural language to SQL generation.
  - Finetuned from model: [CodeLlama-7B]
  - [Use ReAct prompting with this model](https://huggingface.co/defog/sqlcoder-7b-2/discussions/37)
    - unfortunately our model is not trained for such chain-of-thought (CoT) prompting; it is only meant to be used as a direct interpreter from your question + instructions + schema to the final SQL query. This is partly due to the small size of the model

- [motherduckdb/DuckDB-NSQL-7B-v0.1 Â· Hugging Face _202401](https://huggingface.co/motherduckdb/DuckDB-NSQL-7B-v0.1)
  - NSQL is a family of autoregressive open-source large foundation models (FMs) designed specifically for SQL generation tasks
  - based on Meta's original Llama-2 7B model and further pre-trained on a dataset of general SQL queries and then fine-tuned on a dataset composed of DuckDB text-to-SQL pairs.
  - 200k DuckDB text-to-SQL pairs, synthetically generated using Mixtral-8x7B-Instruct-v0.1, guided by the DuckDB v0.9.2 documentation

- [distil-labs/distil-qwen3-0.6b-SHELLper Â· Hugging Face](https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper)
  - [SHELLper ğŸš: Multi-Turn Function Calling with a <1B model : r/LocalLLM](https://www.reddit.com/r/LocalLLM/comments/1qnjiym/shellper_multiturn_function_calling_with_a_1b/)
  - Small models struggle with multi-turn tool calling - out of the box, Qwen3-0.6B achieves 84% accuracy on single tool calls, which drops to just 42% over 5 turns. Our tuning brings this to 100% on the test set, delivering robust multi-turn performance.
# more
- https://github.com/NVIDIA/RULER /æ—§æµ‹è¯„æœªæ›´æ–°
  - Whatâ€™s the Real Context Size of Your Long-Context Language Models?
  - RULER generates synthetic examples to evaluate long-context language models with configurable sequence length and task complexity.

- [Qwen3: How to Run & Fine-tune | Unsloth Documentation](https://docs.unsloth.ai/models/qwen3-how-to-run-and-fine-tune)
