---
title: lib-ai-app-community-model-dev-xp
tags: [large-language-model, stable-diffusion]
created: 2025-11-18T13:21:29.771Z
modified: 2025-11-18T13:22:22.078Z
---

# lib-ai-app-community-model-dev-xp

# guide

- models-variants
  - watching: openai, claude, qwen, deepseek, gemini/gemma, glm, mistral/codestral
  - variants: mlx, unsloth, quants
  - æµ‹è¯•æ¨¡å‹æ—¶å¯èƒ½æ›´å¸Œæœ›é€Ÿåº¦å¿«ï¼Œä½†åšä»»åŠ¡æˆ–è§„åˆ’æ—¶æ›´å¸Œæœ›è´¨é‡å¥½ï¼Œæ‰€ä»¥åå‘é€‰æ‹©å¤§Bå‚æ•°çš„æ¨¡å‹
  - ğŸ“± ç«¯ä¾§æ¨¡å‹è¿˜è¦è€ƒè™‘ç”µæºåŠåŠŸè€—é—®é¢˜, å®æµ‹macbook-airåœ¨è·‘æ¨¡å‹æ—¶æ‰ç”µå¾ˆå¿«
    - ç«¯ä¾§æœ€å¥½ç”¨ api-key + tiny-local-llm

- models-choices
  - ğŸ¤” LMs are tools. Describe your use cases.
    - åˆ†ææ¸…æ¥šæ ¸å¿ƒéœ€æ±‚: éœ€è¦reasoning/coding/large/faster
  - moeæ¨¡å‹çš„å®é™…æ•ˆæœå¤§æ¦‚åªæœ‰denseæ¨¡å‹çš„ä¸€åŠï¼Œå¦‚qwen3-30B-A3B ç›¸å½“äº Qwen3-14b
  - æ¨¡å‹å ç”¨VRAMä¸èƒ½å¤ªå¤§ï¼Œè¿˜è¦ä¸ºcontextå¤„ç†ã€åº”ç”¨ç¨‹åºå¦‚nextjs/comfyuié¢„ç•™RAM/VRAM
  - é€‰æ‹©æ¨¡å‹æ—¶å¤šç”¨å®˜æ–¹ç‰ˆ/ä¸»æµç‰ˆï¼Œå°ä¼—å¾®è°ƒçš„ç‰ˆæœ¬å¯èƒ½å­˜åœ¨tool-call/overthink/å¤šè¯­è¨€multilingual/å¯¹è¯é£æ ¼/llama.cppä¸æ”¯æŒç­‰é—®é¢˜/loop
    - é€‰ç”¨ä¸»æµç‰ˆè¿˜æ–¹ä¾¿ä¸å…¶ä»–ç”¨æˆ·å¯¹æ¯”é€Ÿåº¦/é…ç½®
    - éä¸»æµç‰ˆå¯èƒ½å‡ºç°vision/ragç­‰è¢«å»æ‰çš„é—®é¢˜
  - å¤šagentæ¶æ„æ—¶ï¼Œå¯ä½¿ç”¨ä¸åŒæ¶æ„çš„agentç›¸äº’éªŒè¯
  - non-thinkingæˆ–è¾“å‡ºç®€æ´çš„æ¨¡å‹é€‚åˆcoding

- donts
  - å¾ˆå¤šå¸¦thinkingçš„å¤§æ¨¡å‹ä¸æ“…é•¿è®¡æ•°ï¼Œå¦‚within 18 wordsï¼Œ æœ‰çš„æ¨¡å‹çœŸçš„ä¼šé€ä¸ªtokenæ‰“å°å‡ºæ¥é€ä¸ªæ•°ä¸€é

- mac ğŸ
  - ğŸ‘€ åœ¨low power modeçœç”µæ¨¡å¼ä¸‹, æ¨¡å‹çš„è¾“å‡ºé€Ÿåº¦ä¼šæ¯”éçœç”µæ¨¡å¼æ…¢2-3å€
# draft
- ç°åœ¨çš„æ¨¡å‹ç¼ºå°‘ 15b-23b é—´çš„å°æ¨¡å‹ï¼Œå¯é€‰æ‹©ç¬¬ä¸‰æ–¹é­”æ”¹æˆ–æ‰©å®¹åçš„ç‰ˆæœ¬
  - è¿˜ç¼ºå°‘ 40b-71b é—´çš„ä¸­æ¨¡å‹

- 
- 
- 
- 
- 

# usage-xp
- ğŸ§ª test-cases
  - Which of these objects is not like the others: orange, banana, potato, chair
  - I have 7 apples. Yesterday I ate one apple. how many apples do i have now?
  - I have 2 apples, then I buy 2 more. I bake a pie with 2 of the apples. After eating half of the pie how many apples do I have left?
  - If we lay 5 shirts out in the sun and it takes 4 hours to dry, how long would 20 shirts take to dry? Explain your reasoning step by step
  - convert inches to cm: 15.4 x 7.3 x 13.5 inches
  - å¤§æ¼ å­¤çƒŸç›´ å‡ºè‡ªå“ªé‡Œ? ä¸Šä¸‹å¥æ˜¯ä»€ä¹ˆï¼Ÿ è¡¨è¾¾ä»€ä¹ˆæ„æ€ï¼Ÿ æœ‰å…¶ä»–åœ°æ–¹å¼•ç”¨è¿‡è¿™å¥å—?

- gemma3 ğŸŒ¹ /å¤šè¯­è¨€/åˆ›æ„æ–‡æœ¬/vision
  - 27b å’Œ 12b éƒ½èƒ½è¾ƒå¥½éµå¾ªå¸¦ç»“æ„çš„instructè¾“å‡ºï¼Œ 27bèƒ½ä¸»åŠ¨ç»™å‡ºæ›´å¤šå¤–éƒ¨ç½‘é¡µé“¾æ¥è€Œ12bç»™çš„é“¾æ¥å¾ˆå°‘

- qwen3 ğŸŒ¹ /èƒ½åŠ›å…¨/thinkingå¼€å…³/å†…å®¹ä¸°å¯Œ
  - 4båŠ14bçš„è¾“å‡ºå†…å®¹éƒ½æ¯”è¾ƒè¯¦ç»†ï¼Œç»å¸¸åŒ…å«è¡¨æ ¼ğŸ“ˆ

- gpt-oss-20B-A3.6B ğŸ‘€ /ä¸šç•Œæ ‡æ†/è¾“å‡ºå¿«
  - è¾“å‡ºçš„å†…å®¹ç‰¹åˆ«å–œæ¬¢ç”¨è¡¨æ ¼ğŸ“ˆ, è®¨è®ºä»£ç ç›¸å…³é—®é¢˜ä¹Ÿå–œæ¬¢ç”¨è¡¨æ ¼
  - unsloth-Q5çš„è¾“å‡ºé€Ÿåº¦ä¸º 11.8 tops, offcial-Q4çš„è¾“å‡ºé€Ÿåº¦ä¸º 11.2 tops, é€Ÿåº¦æ¯”qwen3-14bæ›´å¿«

- magistral-small-2509-24b  ğŸ‘€ /å¯ä»¥ç”¨/think+vision/æ¬§æ´²å¤šè¯­è¨€/äº§å“çº¿ä¸°å¯Œ/censorå¼±
  - å›å¤ä¸€èˆ¬å¾ˆçŸ­ï¼Œæ„Ÿè§‰è´¨é‡ä¸é«˜
  - mistralç³»åˆ—æ¨¡å‹çš„çŸ¥è¯†ä¸°å¯Œåº¦å¾ˆé«˜, å¯ä»¥é™ä½å¯¹RAGçš„ä¾èµ– ğŸ¤”
  - thinkingæ—¶é—´åœ¨~~3-10~~min(2509å·²æ”¹è¿›)å·¦å³ï¼Œæˆ–è®¸å¯¹äºplanåˆ¶å®šè®¡åˆ’æœ‰ç”¨
  - è¾“å‡ºå†…å®¹å‡ ä¹ä¸æä¾›å¤–éƒ¨é“¾æ¥ï¼Œ2507ä¸ä¹Ÿæä¾›å¤–éƒ¨é“¾æ¥
  - è¾“å‡ºå†…å®¹ä¸­å‡ ä¹ä¸æä¾›è¡¨æ ¼
  - å¸¦thinkingçš„æ¨¡å‹ä¸æ“…é•¿è®¡æ•°ï¼Œå¦‚ within 18 words

- glm4 ğŸ‘€ /å¯ä»¥ç”¨/æ˜¯å¦å–„é•¿htmlä»£ç ?
  - ğŸ–¼ï¸ htmlä¸­æ”¯æŒæ˜¾ç¤ºå¤–éƒ¨å›¾ç‰‡ï¼Œéœ€è¦å½¢çŠ¶ç±»å›¾æ ‡çš„ä½ç½®èƒ½å‡†ç¡®ç”Ÿæˆsvg
  - glm4ä¸ä¼šthinkï¼Œè¾“å‡ºå†…å®¹è´¨é‡æ„Ÿè§‰ä¸€èˆ¬
  - è¾“å‡ºçš„é•¿åº¦å¤§æ¦‚åœ¨30-60è¡Œï¼Œç®€æ´æ˜¯ç‰¹è‰²ï¼Œå¯¹ä»£ç æœ‰ç”¨?
  - åœ¨å¤šè½®èŠå¤©æ—¶ï¼Œè¾“å‡ºå†…å®¹ä¹Ÿä¼šé€æ¸å˜é•¿?
  - è¾“å‡ºhtmlé¡µé¢æ—¶èƒ½æ·»åŠ å¤æ‚çš„svgä»£ç ï¼Œå½¢çŠ¶ç±»å›¾æ ‡çš„ä½ç½®èƒ½å‡†ç¡®ç”Ÿæˆsvg
  - ç”Ÿæˆçš„htmlé¡µé¢é£æ ¼æœ‰ç‚¹tailwindï¼Œä¹Ÿæœ‰ç‚¹bootstrap
  - ç”Ÿæˆçš„slider/carouselæ¯”qwen3æ›´å‡†ç¡®

- glm-z1 ğŸ‘€ /æ€è€ƒéå¸¸ä¹…/ä¸æ“…é•¿ä»£ç 
  - z1ä¼šthink5-15minï¼Œthinkä¸æ”¯æŒdisableï¼Œè¾“å‡ºå†…å®¹çš„é•¿åº¦ä¼šæ¯”glm4å¤š20è¡Œå·¦å³ï¼Œå¤šä¸€äº›å¤–éƒ¨é“¾æ¥ï¼Œå¤šç”¨å¾ˆå¤šè¡¨æ ¼ï¼Œè´¨é‡è¾ƒå¥½
  - z1çš„thinkæ—¶é—´æ¯”qwen3é•¿å¾ˆå¤šï¼Œ
  - è¾“å‡ºå†…å®¹çš„é•¿åº¦æ¯”qwen3æ›´å°‘, è¾“å‡ºå†…å®¹ä¼šæœ‰è¡¨æ ¼ğŸ“ˆ

## coding-models

- tips
  - âš–ï¸ codingçš„promptä¹Ÿå°½é‡éµå¾ª plan + act çš„ç»“æ„
  - å¯¹äºaiæŒ‰ç”¨æˆ·æä¾›çš„æ¨¡ç‰ˆè¾“å‡ºhtmlçš„åœºæ™¯ï¼Œç”¨æˆ·æä¾›å’Œaiè¾“å‡ºçš„ä»£ç é€šå¸¸éƒ½æ˜¯åçŸ­çš„ã€åé™æ€çš„; 
    - å¦ä¸€ç§æ€è·¯æ˜¯æŒ‰æ¨¡ç‰ˆè¾“å‡ºmarkdownæ‰‹åŠ¨æ‹¼æ¥ï¼Œé€Ÿåº¦å¯èƒ½æ›´å¿«
  - codingæ¨¡å‹å¿…é¡»è¦ç”¨æ–°ç‰ˆæ‰èƒ½ä½¿ç”¨æœ€æ–°æ¡†æ¶çš„æ¶æ„å†™æ³•ï¼Œå¦‚tailwind.v4, reactjs.v19

- ğŸ§ª test-cases
  - landing-page + tailwind: åŸºæœ¬éƒ½èƒ½å®ç°é¡µé¢, ğŸŒ¹ glmæ“…é•¿å›¾æ ‡å’Œå›¾ç‰‡
    - kat-devå’Œqwen3éƒ½ä¸æ“…é•¿å›¾æ ‡å’Œå›¾ç‰‡
  - landing + threejs: åŸºæœ¬éƒ½èƒ½å®ç°, 
    - qwen3-32bæœ‰æ—¶æ— æ³•è¿è¡Œdemo
  - ğŸ¤” game-reaction-for-click: kat-devèƒ½æ­£ç¡®å®ç°ï¼Œ qwen3-thinkèƒ½å®ç°
    - glmå¼‚å¸¸ï¼Œqwen3å¼‚å¸¸
  - game-typing: glm/kat-devè¡Œ, qwen3éthinkä¹Ÿè¡Œ
  - ğŸ¤” dashboard-crud: qwen3å°bugï¼Œglmèƒ½åšuiï¼Œkat-devå¤±è´¥
  - ğŸ¤” weather: glmå¤§å¤šèƒ½å®ç°
    - kat-devéƒ¨åˆ†å¼‚å¸¸ï¼Œqwen3å¼‚å¸¸
  - slider: glmæ“…é•¿cssåŠ¨ç”»æ•ˆæœï¼Œå…¶ä»–aiçš„uxæ•ˆæœä¸€èˆ¬
  - threejs-earth: qwen3è¡Œ, glméƒ¨åˆ†å¼‚å¸¸, kat-devå¼‚å¸¸
  - vocabulary-card: åŸºæœ¬éƒ½èƒ½æ­£ç¡®è°ƒæ•´é¡µé¢, 
    - kat-devæœ‰æ—¶æ ·å¼å¼‚å¸¸

- qwen3-coder-30b-a3b ğŸŒ¹ /é€Ÿåº¦å¿«
  - ç”Ÿæˆå•é¡µé¢çš„æ•ˆæœå¥½é€Ÿåº¦å¿«
  - æ“…é•¿ç”¨æ¸å˜è‰²å—ä»£æ›¿å›¾ç‰‡å ä½ç¬¦
  - å†™å®Œä»£ç åä¸€èˆ¬è¿˜ä¼šè®²è§£è¯´æ˜ä¸€æ®µ

- devstral-2507-24b /æ¬§æ´²å¤šè¯­è¨€/instruct

- qwen2.5-coder-32b /å¾®è°ƒå¤š

- qwen3-32b /thinkingå¼€å…³/èƒ½åŠ›å…¨
  - æ“…é•¿æ¸å˜è‰²æ–‡å­—
  - ç”Ÿæˆå•é¡µhtmlæ—¶ï¼Œ éœ€è¦å½¢çŠ¶ç±»å›¾æ ‡çš„åœ°æ–¹ä¼šä¹±ç 
  - ä¸æ“…é•¿æ˜¾ç¤ºå¤–éƒ¨å›¾ç‰‡ï¼Œéœ€è¦å›¾ç‰‡çš„åœ°æ–¹ä¼šæ˜¾ç¤ºç¼ºçœå ä½ç¬¦

- kat-dev-32B
  - é¡µé¢ç®€æ´
  - æ¯”qwen3æ›´æ“…é•¿æ ·å¼ã€èƒ½æ˜¾ç¤ºéƒ¨åˆ†å›¾æ ‡
  - é¡µé¢ä¸æ˜¯å…¸å‹çš„tailwindé£æ ¼ï¼Œé£æ ¼æœ‰ç‚¹é™ˆæ—§
  - ä¸æ“…é•¿æ˜¾ç¤ºå¤–éƒ¨å›¾ç‰‡
  - ä¸æ“…é•¿ç”Ÿæˆå½¢çŠ¶ç±»å›¾æ ‡ï¼Œç»å¸¸ç”Ÿæˆé‡å¤å›¾æ ‡ï¼Œæœ‰æ—¶ä¼šç¼ºå¤±éƒ¨åˆ†å›¾æ ‡

- uigen-fx-4b /æ“…é•¿uiæ¡†æ¶/èƒ½å†™js
  - æ“…é•¿æ¸å˜è‰²æŒ‰é’®
  - ä¸æ“…é•¿ç”¨æ¸å˜è‰²å—ä»£æ›¿å›¾ç‰‡å ä½ç¬¦
  - æœ‰æ—¶èƒ½å†™å¾ˆå¤šjsä»£ç 

- webgen-4b /æ“…é•¿htmlé¡µé¢ä¸æ“…é•¿æ¡†æ¶å’Œjs
  - webgenç”Ÿæˆå•é¡µé¢çš„æ•ˆæœè¿œä¸å¦‚uigen/qwen3-coder
  - ä¼¼ä¹ä¸æ“…é•¿tailwind, ç”Ÿæˆé¡µé¢çš„é£æ ¼åétailwindæ ·å¼çš„ä¼ ç»Ÿç½‘é¡µ
  - ç»å¸¸å‡ºç°éƒ¨åˆ†å…ƒç´ æ ·å¼é”™ä¹±çš„é—®é¢˜
# models-features/variants
- ä¸“ç”¨æ¨¡å‹
- ocr
- tool-calling
  - lfm2-1.2b-tool
- edit-apply
- devops
- graphics
- computer-use
- cpu
  - [NanoAgent â€” A 135M Agentic LLM with Tool Calling That Runs on CPU : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1oomy4t/nanoagent_a_135m_agentic_llm_with_tool_calling/)

- reap
- https://huggingface.co/cerebras/GLM-4.6-REAP-268B-A32B
  - [I think the dataset is gonna have to be more diverse.](https://huggingface.co/cerebras/GLM-4.6-REAP-268B-A32B/discussions/1)
    - The method looks promising but as in other prunes, it may have REAPed too much to be viable.

- [starvector/starvector-1b-im2svg Â· Hugging Face _202503](https://huggingface.co/starvector/starvector-1b-im2svg)
  - StarVector is a foundation model for generating Scalable Vector Graphics (SVG) code from images and text
  - It utilizes a Vision-Language Modeling architecture to understand both visual and textual inputs, enabling high-quality vectorization and text-guided SVG creation.
  - https://github.com/joanrod/star-vector /apache2/202504/python
    - StarVector Accepted at CVPR 2025
  - [æ„Ÿè§‰ä¸å¤ªè¡Œã€‚ã€‚](https://huggingface.co/starvector/starvector-1b-im2svg/discussions/2)
    - This checkpoint is designed for converting images into SVGs. While we do have a text-to-SVG model, we plan to release it at a later stage.
    - The current model performs well with simple icons but has limitations with more complex images. Improving performance on complex cases is a key focus of our ongoing work.

- [lakhera2023/devops-slm-v1 Â· Hugging Face _202509](https://huggingface.co/lakhera2023/devops-slm-v1)
  - Based on Qwen2.5
  - a specialized language model specifically for DevOps tasks and operations only.
  - designed EXCLUSIVELY for DevOps-related tasks. It has robust filtering that will NOT respond to general questions about movies, weather, cooking, sports, music
  - [Meet the first Small Language Model built for DevOps : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ndm44z/meet_the_first_small_language_model_built_for/)

- https://huggingface.co/Alibaba-EI/SmartResume /apache2/202510
  - ä¸€ä¸ªé¢å‘ç‰ˆé¢ç»“æ„çš„æ™ºèƒ½ç®€å†è§£æç³»ç»Ÿï¼Œç³»ç»Ÿæ”¯æŒ PDFã€å›¾ç‰‡åŠå¸¸è§ Office æ–‡æ¡£æ ¼å¼ï¼Œèåˆ OCR ä¸ PDF å…ƒæ•°æ®å®Œæˆæ–‡æœ¬æå–
  - æœ¬ä»“åº“åŒ…å« SmartResume é¡¹ç›®æ‰€éœ€çš„ä¸¤ä¸ªæ ¸å¿ƒæƒé‡æ–‡ä»¶ï¼Œç”¨äºç®€å†ä¿¡æ¯æå–å’Œç‰ˆé¢åˆ†æã€‚
  - Qwen3-0.6B: ç®€å†æ–‡æœ¬ä¿¡æ¯æå–å’Œç»“æ„åŒ–å¤„ç†
  - YOLOv10: ç®€å†ç‰ˆé¢å¸ƒå±€æ£€æµ‹å’ŒåŒºåŸŸåˆ†å‰²
# models-exploring
- [nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct Â· Hugging Face](https://huggingface.co/nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct)
  - a series of ultra-long context language models designed to process extensive sequences of text (up to 1M, 2M, and 4M tokens) while maintaining competitive performance on standard benchmarks. 

- [Otakadelic/phi-4-abliterated-Orion-18B Â· Hugging Face](https://huggingface.co/Otakadelic/phi-4-abliterated-Orion-18B)
  - Orion-zhen's original wasnâ€™t just abliteration. it is capable of portraying complex personalities, delicate emotions, sharp thoughts and inner conflict, it felt more like a katana than a model: slicing through responses with cold precision.
  - This version shifts the focus slightly toward storytelling. Still emotionally complex, still powerful under duress, captivity, or obedienceâ€”but with a more narrative-friendly tone.
  - Based on Phi-4 Abliterated (40-layer model)
  - Inspired by @mlabonne's BigQwen2.5-Echo-47B-Instruct, added exact same layers to original Phi-4 14B(40-layer model) to the middle part while keep in tact first and last parts. Though its parameter count increased(14B --> 18B), it is a structural duplicate.
# model-wiki/bookmarks
- è¶…å¤§æ¨¡å‹çš„æå°é‡åŒ–ç‰ˆ
  - Llama-3.3-70B-Instruct-abliterated-Q2-mlx  22.07gb
  - Qwen3-Next-80B-A3B-Instruct-q2-mlx  24.95gb
  - Mistral-Large-Instruct-2411-Q2-MLX  45.99gb
  - gpt-oss-120b-mlx-2Bit(116.8B A5.1B)  36.61gb
  - GLM-4.5-Air-2bit(106b A12B)  33.45gb
  - GLM-4.5-Air-4bit(106b A12B)  62gb
  - DeepSeek-V3.1-Terminus-mlx-2Bit  209.89gb
  - DeepSeek-R1-2bit  251.82gb
# more
- [Qwen3: How to Run & Fine-tune | Unsloth Documentation](https://docs.unsloth.ai/models/qwen3-how-to-run-and-fine-tune)
