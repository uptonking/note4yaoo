---
title: lib-ai-app-community-model-dev-xp
tags: [large-language-model, stable-diffusion]
created: 2025-11-18T13:21:29.771Z
modified: 2025-11-18T13:22:22.078Z
---

# lib-ai-app-community-model-dev-xp

# guide

- models-variants
  - watching: openai, claude, qwen, deepseek, gemini/gemma, glm, mistral/codestral
  - variants: mlx, unsloth, quants
  - ÊµãËØïÊ®°ÂûãÊó∂ÂèØËÉΩÊõ¥Â∏åÊúõÈÄüÂ∫¶Âø´Ôºå‰ΩÜÂÅö‰ªªÂä°ÊàñËßÑÂàíÊó∂Êõ¥Â∏åÊúõË¥®ÈáèÂ•ΩÔºåÊâÄ‰ª•ÂÅèÂêëÈÄâÊã©Â§ßBÂèÇÊï∞ÁöÑÊ®°Âûã
  - üì± Á´Ø‰æßÊ®°ÂûãËøòË¶ÅËÄÉËôëÁîµÊ∫êÂèäÂäüËÄóÈóÆÈ¢ò, ÂÆûÊµãmacbook-airÂú®Ë∑ëÊ®°ÂûãÊó∂ÊéâÁîµÂæàÂø´
    - Á´Ø‰æßÊúÄÂ•ΩÁî® api-key + tiny-local-llm

- models-choices
  - ü§î LMs are tools. Describe your use cases.
    - ÂàÜÊûêÊ∏ÖÊ•öÊ†∏ÂøÉÈúÄÊ±Ç: ÈúÄË¶Åreasoning/coding/large/faster
  - moeÊ®°ÂûãÁöÑÂÆûÈôÖÊïàÊûúÂ§ßÊ¶ÇÂè™ÊúâdenseÊ®°ÂûãÁöÑ‰∏ÄÂçäÔºåÂ¶Çqwen3-30B-A3B Áõ∏ÂΩì‰∫é Qwen3-14b
  - Ê®°ÂûãÂç†Áî®VRAM‰∏çËÉΩÂ§™Â§ßÔºåËøòË¶Å‰∏∫contextÂ§ÑÁêÜ„ÄÅÂ∫îÁî®Á®ãÂ∫èÂ¶Çnextjs/comfyuiÈ¢ÑÁïôRAM/VRAM
  - ÈÄâÊã©Ê®°ÂûãÊó∂Â§öÁî®ÂÆòÊñπÁâà/‰∏ªÊµÅÁâàÔºåÂ∞è‰ºóÂæÆË∞ÉÁöÑÁâàÊú¨ÂèØËÉΩÂ≠òÂú®tool-call/overthink/Â§öËØ≠Ë®Ämultilingual/ÂØπËØùÈ£éÊ†º/llama.cpp‰∏çÊîØÊåÅÁ≠âÈóÆÈ¢ò/loop
    - ÈÄâÁî®‰∏ªÊµÅÁâàËøòÊñπ‰æø‰∏éÂÖ∂‰ªñÁî®Êà∑ÂØπÊØîÈÄüÂ∫¶/ÈÖçÁΩÆ
    - Èùû‰∏ªÊµÅÁâàÂèØËÉΩÂá∫Áé∞vision/ragÁ≠âË¢´ÂéªÊéâÁöÑÈóÆÈ¢ò
  - Â§öagentÊû∂ÊûÑÊó∂ÔºåÂèØ‰ΩøÁî®‰∏çÂêåÊû∂ÊûÑÁöÑagentÁõ∏‰∫íÈ™åËØÅ
  - non-thinkingÊàñËæìÂá∫ÁÆÄÊ¥ÅÁöÑÊ®°ÂûãÈÄÇÂêàcoding
  - Ê®°ÂûãÁöÑÂçáÁ∫ßÊç¢‰ª£ÂæàÂø´ÔºåÂñúÊñ∞‰∏çÂñúÊóßÔºåÊñ∞Ê®°ÂûãÂØπ tool-call/attention/ÈáèÂåñÁöÑÊîØÊåÅ‰∏ÄËà¨ÈÉΩÊõ¥Â•Ω
  - Â§ßÂéÇÁöÑÊ®°Âûã‰∏ÄËà¨ÈÉΩ‰ºöÂÆöÊúüÂçáÁ∫ßÔºå‰ºöÊúâÁ§æÂå∫Áî®Êà∑ÂØπÊØîËØÑÊµãÔºåÊõ¥ÂèØÈù†„ÄÅÊõ¥ÊúâÂèÇ‰∏éÊÑü

- donts
  - ÂæàÂ§öÂ∏¶thinkingÁöÑÂ§ßÊ®°Âûã‰∏çÊìÖÈïøËÆ°Êï∞ÔºåÂ¶Çwithin 18 wordsÔºå ÊúâÁöÑÊ®°ÂûãÁúüÁöÑ‰ºöÈÄê‰∏™tokenÊâìÂç∞Âá∫Êù•ÈÄê‰∏™Êï∞‰∏ÄÈÅç

- mac üçé
  - üëÄ Âú®low power modeÁúÅÁîµÊ®°Âºè‰∏ã, Ê®°ÂûãÁöÑËæìÂá∫ÈÄüÂ∫¶‰ºöÊØîÈùûÁúÅÁîµÊ®°ÂºèÊÖ¢2-3ÂÄç
# draft
- Áé∞Âú®ÁöÑÊ®°ÂûãÁº∫Â∞ë 15b-23b Èó¥ÁöÑÂ∞èÊ®°ÂûãÔºåÂèØÈÄâÊã©Á¨¨‰∏âÊñπÈ≠îÊîπÊàñÊâ©ÂÆπÂêéÁöÑÁâàÊú¨
  - ËøòÁº∫Â∞ë 40b-71b Èó¥ÁöÑ‰∏≠Ê®°Âûã

- task-specific leaderboard
  - rag
  - community users can upload benchmark results with device-specs from apps like lmstudio
    - ËØÑÊµãËÄóÊó∂ËÄóÂäõÔºåÁ§æÂå∫ÂèÇ‰∏éÊõ¥Âø´Êç∑Ôºå‰ΩÜ‰ºöÈôç‰ΩéÂáÜÁ°ÆÂ∫¶
    - ÂêåÊ†ºÂºè„ÄÅÂêåÈÖçÁΩÆ ÁöÑÊ®°ÂûãÊØîËæÉÊõ¥ÊúâÊÑè‰πâ

- 
- 
- 
- 
- 
- 

# usage-xp
- üß™ test-cases
  - Which of these objects is not like the others: orange, banana, potato, chair
  - I have 7 apples. Yesterday I ate one apple. how many apples do i have now?
  - I have 2 apples, then I buy 2 more. I bake a pie with 2 of the apples. After eating half of the pie how many apples do I have left?
  - If we lay 5 shirts out in the sun and it takes 4 hours to dry, how long would 20 shirts take to dry? Explain your reasoning step by step
  - convert inches to cm: 15.4 x 7.3 x 13.5 inches
  - what is the latest version of Node.js?
  - Â§ßÊº†Â≠§ÁÉüÁõ¥ Âá∫Ëá™Âì™Èáå? ‰∏ä‰∏ãÂè•ÊòØ‰ªÄ‰πàÔºü Ë°®Ëææ‰ªÄ‰πàÊÑèÊÄùÔºü ÊúâÂÖ∂‰ªñÂú∞ÊñπÂºïÁî®ËøáËøôÂè•Âêó?

- gemma3 üåπ /Â§öËØ≠Ë®Ä/ÂàõÊÑèÊñáÊú¨/vision
  - 27b Âíå 12b ÈÉΩËÉΩËæÉÂ•ΩÈÅµÂæ™Â∏¶ÁªìÊûÑÁöÑinstructËæìÂá∫Ôºå 27bËÉΩ‰∏ªÂä®ÁªôÂá∫Êõ¥Â§öÂ§ñÈÉ®ÁΩëÈ°µÈìæÊé•ËÄå12bÁªôÁöÑÈìæÊé•ÂæàÂ∞ë

- qwen3 üåπ /ËÉΩÂäõÂÖ®/thinkingÂºÄÂÖ≥/ÂÜÖÂÆπ‰∏∞ÂØå
  - 4bÂèä14bÁöÑËæìÂá∫ÂÜÖÂÆπÈÉΩÊØîËæÉËØ¶ÁªÜÔºåÁªèÂ∏∏ÂåÖÂê´Ë°®Ê†ºüìà

- gpt-oss-20B-A3.6B üëÄ /‰∏öÁïåÊ†áÊùÜ/ËæìÂá∫Âø´
  - ËæìÂá∫ÁöÑÂÜÖÂÆπÁâπÂà´ÂñúÊ¨¢Áî®Ë°®Ê†ºüìà, ËÆ®ËÆ∫‰ª£Á†ÅÁõ∏ÂÖ≥ÈóÆÈ¢ò‰πüÂñúÊ¨¢Áî®Ë°®Ê†º
  - unsloth-Q5ÁöÑËæìÂá∫ÈÄüÂ∫¶‰∏∫ 11.8 tops, offcial-Q4ÁöÑËæìÂá∫ÈÄüÂ∫¶‰∏∫ 11.2 tops, ÈÄüÂ∫¶ÊØîqwen3-14bÊõ¥Âø´

- magistral-small-2509-24b  üëÄ /ÂèØ‰ª•Áî®/think+vision/Ê¨ßÊ¥≤Â§öËØ≠Ë®Ä/‰∫ßÂìÅÁ∫ø‰∏∞ÂØå/censorÂº±
  - ÂõûÂ§ç‰∏ÄËà¨ÂæàÁü≠ÔºåÊÑüËßâË¥®Èáè‰∏çÈ´ò
  - mistralÁ≥ªÂàóÊ®°ÂûãÁöÑÁü•ËØÜ‰∏∞ÂØåÂ∫¶ÂæàÈ´ò, ÂèØ‰ª•Èôç‰ΩéÂØπRAGÁöÑ‰æùËµñ ü§î
  - thinkingÊó∂Èó¥Âú®~~3-10~~min(2509Â∑≤ÊîπËøõ)Â∑¶Âè≥ÔºåÊàñËÆ∏ÂØπ‰∫éplanÂà∂ÂÆöËÆ°ÂàíÊúâÁî®
  - ËæìÂá∫ÂÜÖÂÆπÂá†‰πé‰∏çÊèê‰æõÂ§ñÈÉ®ÈìæÊé•Ôºå2507‰∏ç‰πüÊèê‰æõÂ§ñÈÉ®ÈìæÊé•
  - ËæìÂá∫ÂÜÖÂÆπ‰∏≠Âá†‰πé‰∏çÊèê‰æõË°®Ê†º
  - Â∏¶thinkingÁöÑÊ®°Âûã‰∏çÊìÖÈïøËÆ°Êï∞ÔºåÂ¶Ç within 18 words

- glm4 üëÄ /ÂèØ‰ª•Áî®/ÊòØÂê¶ÂñÑÈïøhtml‰ª£Á†Å?
  - üñºÔ∏è html‰∏≠ÊîØÊåÅÊòæÁ§∫Â§ñÈÉ®ÂõæÁâáÔºåÈúÄË¶ÅÂΩ¢Áä∂Á±ªÂõæÊ†áÁöÑ‰ΩçÁΩÆËÉΩÂáÜÁ°ÆÁîüÊàêsvg
  - glm4‰∏ç‰ºöthinkÔºåËæìÂá∫ÂÜÖÂÆπË¥®ÈáèÊÑüËßâ‰∏ÄËà¨
  - ËæìÂá∫ÁöÑÈïøÂ∫¶Â§ßÊ¶ÇÂú®30-60Ë°åÔºåÁÆÄÊ¥ÅÊòØÁâπËâ≤ÔºåÂØπ‰ª£Á†ÅÊúâÁî®?
  - Âú®Â§öËΩÆËÅäÂ§©Êó∂ÔºåËæìÂá∫ÂÜÖÂÆπ‰πü‰ºöÈÄêÊ∏êÂèòÈïø?
  - ËæìÂá∫htmlÈ°µÈù¢Êó∂ËÉΩÊ∑ªÂä†Â§çÊùÇÁöÑsvg‰ª£Á†ÅÔºåÂΩ¢Áä∂Á±ªÂõæÊ†áÁöÑ‰ΩçÁΩÆËÉΩÂáÜÁ°ÆÁîüÊàêsvg
  - ÁîüÊàêÁöÑhtmlÈ°µÈù¢È£éÊ†ºÊúâÁÇπtailwindÔºå‰πüÊúâÁÇπbootstrap
  - ÁîüÊàêÁöÑslider/carouselÊØîqwen3Êõ¥ÂáÜÁ°Æ

- glm-z1 üëÄ /ÊÄùËÄÉÈùûÂ∏∏‰πÖ/‰∏çÊìÖÈïø‰ª£Á†Å
  - z1‰ºöthink5-15minÔºåthink‰∏çÊîØÊåÅdisableÔºåËæìÂá∫ÂÜÖÂÆπÁöÑÈïøÂ∫¶‰ºöÊØîglm4Â§ö20Ë°åÂ∑¶Âè≥ÔºåÂ§ö‰∏Ä‰∫õÂ§ñÈÉ®ÈìæÊé•ÔºåÂ§öÁî®ÂæàÂ§öË°®Ê†ºÔºåË¥®ÈáèËæÉÂ•Ω
  - z1ÁöÑthinkÊó∂Èó¥ÊØîqwen3ÈïøÂæàÂ§öÔºå
  - ËæìÂá∫ÂÜÖÂÆπÁöÑÈïøÂ∫¶ÊØîqwen3Êõ¥Â∞ë, ËæìÂá∫ÂÜÖÂÆπ‰ºöÊúâË°®Ê†ºüìà

## coding üî°

- tips
  - ‚öñÔ∏è codingÁöÑprompt‰πüÂ∞ΩÈáèÈÅµÂæ™ plan + act ÁöÑÁªìÊûÑ
  - ÂØπ‰∫éaiÊåâÁî®Êà∑Êèê‰æõÁöÑÊ®°ÁâàËæìÂá∫htmlÁöÑÂú∫ÊôØÔºåÁî®Êà∑Êèê‰æõÂíåaiËæìÂá∫ÁöÑ‰ª£Á†ÅÈÄöÂ∏∏ÈÉΩÊòØÂÅèÁü≠ÁöÑ„ÄÅÂÅèÈùôÊÄÅÁöÑ; 
    - Âè¶‰∏ÄÁßçÊÄùË∑ØÊòØÊåâÊ®°ÁâàËæìÂá∫markdownÊâãÂä®ÊãºÊé•ÔºåÈÄüÂ∫¶ÂèØËÉΩÊõ¥Âø´
  - codingÊ®°ÂûãÂøÖÈ°ªË¶ÅÁî®Êñ∞ÁâàÊâçËÉΩ‰ΩøÁî®ÊúÄÊñ∞Ê°ÜÊû∂ÁöÑÊû∂ÊûÑÂÜôÊ≥ïÔºåÂ¶Çtailwind.v4, reactjs.v19

- üß™ test-cases
  - landing-page + tailwind: Âü∫Êú¨ÈÉΩËÉΩÂÆûÁé∞È°µÈù¢, üåπ glmÊìÖÈïøÂõæÊ†áÂíåÂõæÁâá
    - kat-devÂíåqwen3ÈÉΩ‰∏çÊìÖÈïøÂõæÊ†áÂíåÂõæÁâá
  - landing + threejs: Âü∫Êú¨ÈÉΩËÉΩÂÆûÁé∞, 
    - qwen3-32bÊúâÊó∂Êó†Ê≥ïËøêË°ådemo
  - ü§î game-reaction-for-click: kat-devËÉΩÊ≠£Á°ÆÂÆûÁé∞Ôºå qwen3-thinkËÉΩÂÆûÁé∞
    - glmÂºÇÂ∏∏Ôºåqwen3ÂºÇÂ∏∏
  - game-typing: glm/kat-devË°å, qwen3Èùûthink‰πüË°å
  - ü§î dashboard-crud: qwen3Â∞èbugÔºåglmËÉΩÂÅöuiÔºåkat-devÂ§±Ë¥•
  - ü§î weather: glmÂ§ßÂ§öËÉΩÂÆûÁé∞
    - kat-devÈÉ®ÂàÜÂºÇÂ∏∏Ôºåqwen3ÂºÇÂ∏∏
  - slider: glmÊìÖÈïøcssÂä®ÁîªÊïàÊûúÔºåÂÖ∂‰ªñaiÁöÑuxÊïàÊûú‰∏ÄËà¨
  - threejs-earth: qwen3Ë°å, glmÈÉ®ÂàÜÂºÇÂ∏∏, kat-devÂºÇÂ∏∏
  - vocabulary-card: Âü∫Êú¨ÈÉΩËÉΩÊ≠£Á°ÆË∞ÉÊï¥È°µÈù¢, 
    - kat-devÊúâÊó∂Ê†∑ÂºèÂºÇÂ∏∏

- qwen3-coder-30b-a3b üåπ /ÈÄüÂ∫¶Âø´
  - ÁîüÊàêÂçïÈ°µÈù¢ÁöÑÊïàÊûúÂ•ΩÈÄüÂ∫¶Âø´
  - ÊìÖÈïøÁî®Ê∏êÂèòËâ≤Âùó‰ª£ÊõøÂõæÁâáÂç†‰ΩçÁ¨¶
  - ÂÜôÂÆå‰ª£Á†ÅÂêé‰∏ÄËà¨Ëøò‰ºöËÆ≤Ëß£ËØ¥Êòé‰∏ÄÊÆµ

- devstral-2507-24b /Ê¨ßÊ¥≤Â§öËØ≠Ë®Ä/instruct

- qwen2.5-coder-32b /ÂæÆË∞ÉÂ§ö

- qwen3-32b /thinkingÂºÄÂÖ≥/ËÉΩÂäõÂÖ®
  - ÊìÖÈïøÊ∏êÂèòËâ≤ÊñáÂ≠ó
  - ÁîüÊàêÂçïÈ°µhtmlÊó∂Ôºå ÈúÄË¶ÅÂΩ¢Áä∂Á±ªÂõæÊ†áÁöÑÂú∞Êñπ‰ºö‰π±Á†Å
  - ‰∏çÊìÖÈïøÊòæÁ§∫Â§ñÈÉ®ÂõæÁâáÔºåÈúÄË¶ÅÂõæÁâáÁöÑÂú∞Êñπ‰ºöÊòæÁ§∫Áº∫ÁúÅÂç†‰ΩçÁ¨¶

- kat-dev-32B
  - È°µÈù¢ÁÆÄÊ¥Å
  - ÊØîqwen3Êõ¥ÊìÖÈïøÊ†∑Âºè„ÄÅËÉΩÊòæÁ§∫ÈÉ®ÂàÜÂõæÊ†á
  - È°µÈù¢‰∏çÊòØÂÖ∏ÂûãÁöÑtailwindÈ£éÊ†ºÔºåÈ£éÊ†ºÊúâÁÇπÈôàÊóß
  - ‰∏çÊìÖÈïøÊòæÁ§∫Â§ñÈÉ®ÂõæÁâá
  - ‰∏çÊìÖÈïøÁîüÊàêÂΩ¢Áä∂Á±ªÂõæÊ†áÔºåÁªèÂ∏∏ÁîüÊàêÈáçÂ§çÂõæÊ†áÔºåÊúâÊó∂‰ºöÁº∫Â§±ÈÉ®ÂàÜÂõæÊ†á

- uigen-fx-4b /ÊìÖÈïøuiÊ°ÜÊû∂/ËÉΩÂÜôjs
  - ÊìÖÈïøÊ∏êÂèòËâ≤ÊåâÈíÆ
  - ‰∏çÊìÖÈïøÁî®Ê∏êÂèòËâ≤Âùó‰ª£ÊõøÂõæÁâáÂç†‰ΩçÁ¨¶
  - ÊúâÊó∂ËÉΩÂÜôÂæàÂ§öjs‰ª£Á†Å

- webgen-4b /ÊìÖÈïøhtmlÈ°µÈù¢‰∏çÊìÖÈïøÊ°ÜÊû∂Âíåjs
  - webgenÁîüÊàêÂçïÈ°µÈù¢ÁöÑÊïàÊûúËøú‰∏çÂ¶Çuigen/qwen3-coder
  - ‰ºº‰πé‰∏çÊìÖÈïøtailwind, ÁîüÊàêÈ°µÈù¢ÁöÑÈ£éÊ†ºÂÅèÈùûtailwindÊ†∑ÂºèÁöÑ‰º†ÁªüÁΩëÈ°µ
  - ÁªèÂ∏∏Âá∫Áé∞ÈÉ®ÂàÜÂÖÉÁ¥†Ê†∑ÂºèÈîô‰π±ÁöÑÈóÆÈ¢ò

## data-viz/charting üìä

- ÁîüÊàêÂõæË°®ÁöÑÊñπÊ°à(ÂàÜÊûêÈúÄÊ±Ç: ÂÅèÂâçÁ´ØÂ±ïÁ§∫/ÂÅèÊï∞ÊçÆÂàÜÊûê)
  - ÊÄùË∑Ø1: llmËøîÂõûÊ†∏ÂøÉÊï∞ÊçÆÔºåÂÆ¢Êà∑Á´Øtool callÁªòÂõæ
  - ÊÄùË∑Ø2: llmÁõ¥Êé•ËøîÂõû‰ª£Á†ÅÔºå‰ΩøÁî®codingÊñπÊ°àÂÆûÁé∞ÁªòÂõæ
  - ÊÄùË∑Ø3: ÁÆÄÂçïÂú∫ÊôØËÄÉËôë‰ΩøÁî®md-tableÊõø‰ª£ÔºåÁÑ∂ÂêéÂâçÁ´ØÊâãÂä®Ê∏≤ÊüìÂõæË°®ÊàñÂºïÂØºÁî®Êà∑ÂàõÂª∫ÂõæË°®
  - üí° ÊÄùË∑Ø4: ‰ΩøÁî®codeblock‰º†ËæìÂõæË°®Áõ∏ÂÖ≥Êï∞ÊçÆÔºåÂèÇËÄÉstreamdownÊµÅÂºèËß£ÊûêmermaidÁöÑÊñπÊ°à
  - ÊÄùË∑Ø5: data > sql > chart, Á±ª‰ººchat2dbÁöÑÊñπÊ°à

- [TroyDoesAI/BlackSheep-MermaidMistral-22B ¬∑ Hugging Face](https://huggingface.co/TroyDoesAI/BlackSheep-MermaidMistral-22B)
  - /202409
  - merge: TroyDoesAI/Mermaid-22B, TroyDoesAI/BlackSheep-Mistral-22B
  - [TroyDoesAI/Mermaid-Llama-3-8B ¬∑ Dataset](https://huggingface.co/TroyDoesAI/Mermaid-Llama-3-8B/discussions/1)
    - Is the dataset publicly available?
    - Yeah sure, make your own using my toolkit and my model to distill its knowledge: https://github.com/Troys-Code/AI_Research
    - [TroyDoesAI/MermaidMistral ¬∑ Dataset?](https://huggingface.co/TroyDoesAI/MermaidMistral/discussions/1)
    - I do not plan to release my ~500 example hand curated dataset as that is used for my evaluation dataset in training now as I believe these are the highest of quality compared to Generated by GPT4 and do not want that public as this is my benchmarker now for how the model is improving as it is incredibly hard to evaluate a models performance on subjective things like story flow.

- [octadion/deepseek-coder-6.7b-pandas ¬∑ Hugging Face](https://huggingface.co/octadion/deepseek-coder-6.7b-pandas)
  - /202403

- [pipizhao/Pandalyst-7B-V1.2 ¬∑ Hugging Face](https://huggingface.co/pipizhao/Pandalyst-7B-V1.2)
  - /202310
  - https://github.com/pipizhaoa/Pandalyst
  - [„ÄêPandalyst-7B-V1.2„Äë Now we can plot and much more powerful ! : r/LocalLLaMA _202310](https://www.reddit.com/r/LocalLLaMA/comments/178uxtf/pandalyst7bv12_now_we_can_plot_and_much_more/)
  - Pandalyst-7B-V1.2, which was trained on CodeLlama-7b-Python and it surpasses ChatGPT-3.5 (2023/06/13), Pandalyst-7B-V1.1 and WizardCoder-Python-13B-V1.0 in our PandaTest_V1.0.
  - 7b version is trained based on Codellama-python-7B, and achieve competitive performance with our 13B !!! (witch was trained on wizardcoder-python-13B)

## image üñºÔ∏è

- z-image-turbo ‰∏çÊìÖÈïø512Â∞∫ÂØ∏ÁöÑÂõæÁâáÔºåÊÑüËßâÂæàÁ≥ä
  - https://www.modelscope.cn/studios/Tongyi-MAI/Z-Image-Gallery
  - ÂêåÊ†∑prompt+stepÁîüÊàê1024Â∞∫ÂØ∏Â∞±ÊØîËæÉÊ∏ÖÊô∞
  - ‰ºº‰πéÂè™ËÉΩÁîüÊàêÈïøÂÆΩ1:1ÊØî‰æãÁöÑÂõæÁâá, ÈöèÊú∫ÊØî‰æã‰ºöÊäõÂá∫ÂºÇÂ∏∏
  - ÁîüÊàê 2048x2048 ÁöÑÂõæÁâáÊó∂, macbookÂÜÖÂ≠òÁàÜÁÇ∏ÔºåÁîµËÑëÈáçÂêØ
  - ‰∏çÊìÖÈïøÊ∏≤Êüì‰∏≠ÊñáÂ≠ó‰Ωì

- https://huggingface.co/AIDC-AI/Ovis-Image-7B
  - https://github.com/AIDC-AI/Ovis-Image
  - a 7B text-to-image model specifically optimized for high-quality text rendering 

## ocr

- deepseek-ocr
  - ÊûÅÂ∞ëÊï∞ÁöÑÂú∫ÊôØÔºå‰ºöÂ∞ÜÂõæÁâá‰∏≠ÁöÑËã±ÊñáÁøªËØë‰∏∫‰∏≠ÊñáËæìÂá∫, Âπ∂‰∏îLOOP
# models-features/variants
- ‰∏ìÁî®Ê®°Âûã
- ocr
- tool-calling
  - lfm2-1.2b-tool
- edit-apply
- devops
- graphics
- computer-use
- cpu
  - [NanoAgent ‚Äî A 135M Agentic LLM with Tool Calling That Runs on CPU : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1oomy4t/nanoagent_a_135m_agentic_llm_with_tool_calling/)

- reap
- https://huggingface.co/cerebras/GLM-4.6-REAP-268B-A32B
  - [I think the dataset is gonna have to be more diverse.](https://huggingface.co/cerebras/GLM-4.6-REAP-268B-A32B/discussions/1)
    - The method looks promising but as in other prunes, it may have REAPed too much to be viable.

- [starvector/starvector-1b-im2svg ¬∑ Hugging Face _202503](https://huggingface.co/starvector/starvector-1b-im2svg)
  - StarVector is a foundation model for generating Scalable Vector Graphics (SVG) code from images and text
  - It utilizes a Vision-Language Modeling architecture to understand both visual and textual inputs, enabling high-quality vectorization and text-guided SVG creation.
  - https://github.com/joanrod/star-vector /apache2/202504/python
    - StarVector Accepted at CVPR 2025
  - [ÊÑüËßâ‰∏çÂ§™Ë°å„ÄÇ„ÄÇ](https://huggingface.co/starvector/starvector-1b-im2svg/discussions/2)
    - This checkpoint is designed for converting images into SVGs. While we do have a text-to-SVG model, we plan to release it at a later stage.
    - The current model performs well with simple icons but has limitations with more complex images. Improving performance on complex cases is a key focus of our ongoing work.

- [lakhera2023/devops-slm-v1 ¬∑ Hugging Face _202509](https://huggingface.co/lakhera2023/devops-slm-v1)
  - Based on Qwen2.5
  - a specialized language model specifically for DevOps tasks and operations only.
  - designed EXCLUSIVELY for DevOps-related tasks. It has robust filtering that will NOT respond to general questions about movies, weather, cooking, sports, music
  - [Meet the first Small Language Model built for DevOps : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ndm44z/meet_the_first_small_language_model_built_for/)

- https://huggingface.co/Alibaba-EI/SmartResume /apache2/202510
  - ‰∏Ä‰∏™Èù¢ÂêëÁâàÈù¢ÁªìÊûÑÁöÑÊô∫ËÉΩÁÆÄÂéÜËß£ÊûêÁ≥ªÁªüÔºåÁ≥ªÁªüÊîØÊåÅ PDF„ÄÅÂõæÁâáÂèäÂ∏∏ËßÅ Office ÊñáÊ°£Ê†ºÂºèÔºåËûçÂêà OCR ‰∏é PDF ÂÖÉÊï∞ÊçÆÂÆåÊàêÊñáÊú¨ÊèêÂèñ
  - Êú¨‰ªìÂ∫ìÂåÖÂê´ SmartResume È°πÁõÆÊâÄÈúÄÁöÑ‰∏§‰∏™Ê†∏ÂøÉÊùÉÈáçÊñá‰ª∂ÔºåÁî®‰∫éÁÆÄÂéÜ‰ø°ÊÅØÊèêÂèñÂíåÁâàÈù¢ÂàÜÊûê„ÄÇ
  - Qwen3-0.6B: ÁÆÄÂéÜÊñáÊú¨‰ø°ÊÅØÊèêÂèñÂíåÁªìÊûÑÂåñÂ§ÑÁêÜ
  - YOLOv10: ÁÆÄÂéÜÁâàÈù¢Â∏ÉÂ±ÄÊ£ÄÊµãÂíåÂå∫ÂüüÂàÜÂâ≤
# models-exploring
- [LiquidAI/LFM2-1.2B-RAG ¬∑ Hugging Face _202509](https://huggingface.co/LiquidAI/LFM2-1.2B-RAG)
  - specialized in answering questions based on provided contextual documents, for use in RAG
  - We recommend using greedy decoding with a `temperature=0`.
  - The system prompt is optional. By default, the output's language follows the user prompt's language.
  - ‚ö†Ô∏è The model supports both single-turn and multi-turn conversations.
  - [Correct chat template ?](https://huggingface.co/LiquidAI/LFM2-1.2B-RAG/discussions/3)
    - You can use any. As mentioned, the system prompt is optional.
    - "[...]" symbolizes a truncation, which is why there's no question in the chat template example.
- [LiquidAI/LFM2-1.2B-Extract ¬∑ Hugging Face _202508](https://huggingface.co/LiquidAI/LFM2-1.2B-Extract)
  - designed to extract important information from a wide variety of unstructured documents (such as articles, transcripts, or reports) into structured outputs like JSON, XML, or YAML.
  - We strongly recommend using greedy decoding with a `temperature=0`.
  - If no system prompt is provided, the model will default to JSON outputs. We recommend providing a system prompt with a specific format (JSON, XML, or YAML) and a given schema to improve accuracy 
  - ‚ö†Ô∏è The model is intended for single-turn conversations.
- [LiquidAI/LFM2-1.2B-Tool ¬∑ Hugging Face _](https://huggingface.co/LiquidAI/LFM2-1.2B-Tool)
  - designed for concise and precise tool calling.
  - We recommend using greedy decoding with a `temperature=0`.

- [apple/CLaRa-7B-Instruct ¬∑ Hugging Face](https://huggingface.co/apple/CLaRa-7B-Instruct)
  - our instruction-tuned unified RAG model with built-in semantic document compression (16√ó & 128x).
  - It supports instruction-following QA directly from compressed document representations.
  - ‚ö†Ô∏è The model supports both single-turn and multi-turn conversations.

- [nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct ¬∑ Hugging Face](https://huggingface.co/nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct)
  - a series of ultra-long context language models designed to process extensive sequences of text (up to 1M, 2M, and 4M tokens) while maintaining competitive performance on standard benchmarks. 

- [Otakadelic/phi-4-abliterated-Orion-18B ¬∑ Hugging Face](https://huggingface.co/Otakadelic/phi-4-abliterated-Orion-18B)
  - Orion-zhen's original wasn‚Äôt just abliteration. it is capable of portraying complex personalities, delicate emotions, sharp thoughts and inner conflict, it felt more like a katana than a model: slicing through responses with cold precision.
  - This version shifts the focus slightly toward storytelling. Still emotionally complex, still powerful under duress, captivity, or obedience‚Äîbut with a more narrative-friendly tone.
  - Based on Phi-4 Abliterated (40-layer model)
  - Inspired by @mlabonne's BigQwen2.5-Echo-47B-Instruct, added exact same layers to original Phi-4 14B(40-layer model) to the middle part while keep in tact first and last parts. Though its parameter count increased(14B --> 18B), it is a structural duplicate.

- [RUC-DataLab/DeepAnalyze-8B ¬∑ Hugging Face _202510](https://huggingface.co/RUC-DataLab/DeepAnalyze-8B)
  - the first agentic LLM for autonomous data science. It can autonomously complete a wide range of data-centric tasks without human intervention
  - https://github.com/ruc-datalab/DeepAnalyze

- [DatarusAI/Datarus-R1-14B-preview ¬∑ Hugging Face _202508](https://huggingface.co/DatarusAI/Datarus-R1-14B-preview)
  - a 14B-parameter open-weights language model fine-tuned from Qwen2.5-14B-Instruct, designed to act as a virtual data analyst and graduate-level problem solver.
  - Unlike traditional models trained on isolated Q&A pairs, Datarus learns from complete analytical trajectories‚Äîincluding reasoning steps, code execution, error traces, self-corrections, and final conclusions‚Äîall captured in a ReAct-style notebook format.
  - https://github.com/DatarusAI/Datarus-JupyterAgent /MIT/python
  - [Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis | Abstract](https://www.arxiv.org/abs/2508.13382)

- [Ellbendls/Qwen-3-4b-Text_to_SQL ¬∑ Hugging Face _202509](https://huggingface.co/Ellbendls/Qwen-3-4b-Text_to_SQL)
  - a fine-tuned version of Qwen/Qwen3-4B designed to convert natural language queries into SQL statements
  - In scenarios where table schema context is missing, the model is trained to generate schema definitions along with the SQL query

- [jupyter-agent/jupyter-agent-qwen3-4b-instruct ¬∑ Hugging Face _202509](https://huggingface.co/jupyter-agent/jupyter-agent-qwen3-4b-instruct)
  - a fine-tuned version of Qwen3-4B-Thinking-2507 specifically optimized for data science agentic tasks in Jupyter notebook environments
  - This model can execute Python code, analyze datasets, and provide step-by-step reasoning with intermediate computations to solve realistic data analysis problems.
  - Dataset domains: Primarily trained on Kaggle-style data science tasks
  - https://github.com/huggingface/jupyter-agent /202509
  - https://huggingface.co/spaces/lvwerra/jupyter-agent-2
  - Context window: Limited to 32K tokens, may struggle with very large notebooks
  - Tool calling format: Requires specific scaffolding for optimal performance
  - [jupyter-agent/jupyter-agent-qwen3-4b-thinking ¬∑ Hugging Face](https://huggingface.co/jupyter-agent/jupyter-agent-qwen3-4b-thinking)
  - [Jupyter Agents: training LLMs to reason with notebooks](https://huggingface.co/blog/jupyter-agent-2)

- [mlfoundations/tabula-8b ¬∑ Hugging Face _202406](https://huggingface.co/mlfoundations/tabula-8b)
  - model for prediction (classification and binned regression) on tabular data.
  - fine-tuned from the Llama-3 8B model
  - [TabuLa-8B: a foundation model for prediction on tabular data : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1dmnlv0/tabula8b_a_foundation_model_for_prediction_on/)

- [defog/llama-3-sqlcoder-8b ¬∑ Hugging Face _202405](https://huggingface.co/defog/llama-3-sqlcoder-8b)
  - model for text to SQL generation for Postgres, Redshift and Snowflake 
  - Finetuned from model: [Meta-Llama-3-8B-Instruct]
  - üÜö [Difference between the llama-3-sqlcoder-8b model vs. sqlcoder-7b-2 model.](https://huggingface.co/defog/sqlcoder-7b-2/discussions/47)
    - Êó†ÂõûÂ§ç
  - https://x.com/rishdotblog/status/1788650171246551086 _202405
    - Llama-3 based SQLCoder 8b is out! Open weights with a commercially friendly cc-by-sa license. Probably the best <10B param model for Postgres text to SQL right now.
    - Our previous small model (sqlcoder-7b-2) was good at generating 0-shot SQL, but did terribly at following instructions. So while it was great in our evals, it was lacking in real-world use-cases where instruction following is much more important.
    - To address this, we trained this model with much more instruction data. We also made our original eval much harder to make sure we stayed on the right track.
    - Changes to prompt: You previously had to use our slightly idiosyncratic prompt for best results. Now, you can just use the standard Llama-3 instruct prompt.

- [defog/sqlcoder-7b-2 ¬∑ Hugging Face _202402](https://huggingface.co/defog/sqlcoder-7b-2)
  - model for natural language to SQL generation.
  - Finetuned from model: [CodeLlama-7B]
  - [Use ReAct prompting with this model](https://huggingface.co/defog/sqlcoder-7b-2/discussions/37)
    - unfortunately our model is not trained for such chain-of-thought (CoT) prompting; it is only meant to be used as a direct interpreter from your question + instructions + schema to the final SQL query. This is partly due to the small size of the model

- [motherduckdb/DuckDB-NSQL-7B-v0.1 ¬∑ Hugging Face _202401](https://huggingface.co/motherduckdb/DuckDB-NSQL-7B-v0.1)
  - NSQL is a family of autoregressive open-source large foundation models (FMs) designed specifically for SQL generation tasks
  - based on Meta's original Llama-2 7B model and further pre-trained on a dataset of general SQL queries and then fine-tuned on a dataset composed of DuckDB text-to-SQL pairs.
  - 200k DuckDB text-to-SQL pairs, synthetically generated using Mixtral-8x7B-Instruct-v0.1, guided by the DuckDB v0.9.2 documentation
# model-wiki/bookmarks
- Ë∂ÖÂ§ßÊ®°ÂûãÁöÑÊûÅÂ∞èÈáèÂåñÁâà
  - Llama-3.3-70B-Instruct-abliterated-Q2-mlx  22.07gb
  - Qwen3-Next-80B-A3B-Instruct-q2-mlx  24.95gb
  - Mistral-Large-Instruct-2411-Q2-MLX  45.99gb
  - gpt-oss-120b-mlx-2Bit(116.8B A5.1B)  36.61gb
  - GLM-4.5-Air-2bit(106b A12B)  33.45gb
  - GLM-4.5-Air-4bit(106b A12B)  62gb
  - DeepSeek-V3.1-Terminus-mlx-2Bit  209.89gb
  - DeepSeek-R1-2bit  251.82gb
# more
- https://github.com/NVIDIA/RULER /ÊóßÊµãËØÑÊú™Êõ¥Êñ∞
  - What‚Äôs the Real Context Size of Your Long-Context Language Models?
  - RULER generates synthetic examples to evaluate long-context language models with configurable sequence length and task complexity.

- [Qwen3: How to Run & Fine-tune | Unsloth Documentation](https://docs.unsloth.ai/models/qwen3-how-to-run-and-fine-tune)
