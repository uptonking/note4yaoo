---
title: lib-ai-app-examples
tags: [ai, examples]
created: 2023-02-08T07:20:33.048Z
modified: 2023-02-08T07:20:48.475Z
---

# lib-ai-app-examples

# guide
- not-yet â“
  - model routing
# popular
- https://github.com/mudler/LocalAI /36.2kStar/MIT/202511/go
  - https://localai.io/
  - free, Open Source OpenAI alternative. 
  - Self-hosted, community-driven and local-first. 
  - LocalAI act as a drop-in replacement REST API thatâ€™s compatible with OpenAI API specifications for local inferencing. 
  - It allows you to run LLMs, generate images, audio (and not only) locally or on-prem with consumer grade hardware, supporting multiple model families. 
  - Does not require GPU.
  - text-backend: llama.cpp, vLLM, MLX
  - audio-backend: whisper.cpp, kokoro
  - image-backend: stablediffusion.cpp, diffusers
  - [Add Local ComfyUI/SD base diffusers step with "json API" PLZ _202502](https://github.com/mudler/LocalAI/issues/4818)
    - ä¸æ”¯æŒ

- https://github.com/janhq/jan /AGPLv3/202403/ts
  - https://jan.ai/
  - an open source alternative to ChatGPT that runs 100% offline on your computer
  - Nitro is a high-efficiency C++ inference engine for edge computing. It is lightweight and embeddable

- https://github.com/Doriandarko/maestro /202406/python
  - â›“ï¸ A framework for Claude Opus to intelligently orchestrate subagents.
  - This Python script demonstrates an AI-assisted task breakdown and execution workflow using the Anthropic API. 
  - It utilizes two AI models, Opus and Haiku, to break down an objective into sub-tasks, execute each sub-task, and refine the results into a cohesive final output.
  - Run locally with LMStudio or Ollama
  - Breaks down an objective into manageable sub-tasks using the Opus model
  - Executes each sub-task using the Haiku model
  - Refines the sub-task results into a final output using the Opus model
  - Generates a detailed exchange log capturing the entire task breakdown and execution process
  - Required Python packages: anthropic and rich
  - https://x.com/tuturetom/status/1805823266671775875
    - åŸºäº Claude 3.5 Sonnet å®ç°çš„ã€Œå¤š Agent è°ƒåº¦ã€æ¡†æ¶
    - åŒæ—¶æä¾›äº† Python flask åº”ç”¨æ”¯æŒ UI Demo å®Œæˆå¤š Agent æ¼”ç¤º
    - orchestrator æ‹†è§£ä»»åŠ¡ï¼Œå…¨å±€è§†é‡è°ƒåº¦ sub_agent 

- https://github.com/run-llama/llama-agents /MIT/202407/python
  - llama-agents is an async-first framework for building, iterating, and productionizing multi-agent systems, including multi-agent communication, distributed tool execution, human-in-the-loop, and more!
  - each agent is seen as a service, endlessly processing incoming tasks. Each agent pulls and publishes messages from a message queue.
  - At the top of a llama-agents system is the control plane. The control plane keeps track of ongoing tasks, which services are in the network, and also decides which service should handle the next step of a task using an orchestrator.
  - https://x.com/tuturetom/status/1809445355832111312
    - å¤š Agent æ¡†æ¶ï¼Œä½¿ç”¨ Docker/K8S éƒ¨ç½²

- https://github.com/dstackai/dstack /MPL/202407/python
  - https://dstack.ai/
  - open-source container orchestration engine designed for running AI workloads across any cloud or data center. 
  - It simplifies dev environments, running tasks on clusters, and deployment.

- https://github.com/DataformerAI/dataformer /apache2/202410/python
  - https://dataformer.ai/
  - Dataformer empowers engineers with a robust framework for creating high-quality synthetic datasets for AI, offering speed, reliability, and scalability
  - We integrate with multiple LLM providers using one unified API and allow you to make parallel async API calls while respecting rate-limits.

- https://github.com/badboysm890/ClaraVerse /3kStar/MIT/202508/ts
  - https://claraverse.space/
  - Privacy-first, fully local AI workspace with Ollama LLM chat, tool calling, agent builder, Stable Diffusion, and embedded n8n-style automation. 
  - No backend. No API keys. Just your stack, your machine.
  - The Story Behind ClaraVerse: Why can't everything be in a single app?
  - The Solution: One App. Six Tools. Zero Compromises.
  - Built on the shoulders of giants: llama.cpp â€¢ llama-swap â€¢ faster-whisper â€¢ ComfyUI â€¢ N8N
  - ğŸ´ forks
  - https://github.com/vanvuongngo/ClaraN /MIT/202508/ts
    - This is a hardfork of ClaraVerse
    - Tauri for building Desktop- and Mobile apps 
    - Qwik web framework for building performant Web UIs
    - DaisyUI for faster, cleaner and simple tailwind CSS UI development
# LLM
- https://github.com/songquanpeng/one-api /MIT/202404/go/js
  - https://openai.justsong.cn/
  - OpenAI æ¥å£ç®¡ç† & åˆ†å‘ç³»ç»Ÿï¼Œæ”¯æŒ Azureã€Anthropic Claudeã€Google PaLM 2 & Geminiã€æ™ºè°± ChatGLMã€ç™¾åº¦æ–‡å¿ƒä¸€è¨€ã€è®¯é£æ˜Ÿç«è®¤çŸ¥ã€é˜¿é‡Œé€šä¹‰åƒé—®
  - å¯ç”¨äºäºŒæ¬¡åˆ†å‘ç®¡ç† keyï¼Œä»…å•å¯æ‰§è¡Œæ–‡ä»¶ï¼Œå·²æ‰“åŒ…å¥½ Docker é•œåƒï¼Œä¸€é”®éƒ¨ç½²ï¼Œå¼€ç®±å³ç”¨

- https://github.com/xenova/transformers.js
  - Run Transformers in your browser! We currently support BERT, ALBERT, DistilBERT, MobileBERT, SqueezeBERT, T5, T5v1.1, FLAN-T5, mT5, BART
  - [å®ç‰ on Twitter: "è¿™æœ‰å¿…è¦å—ï¼Ÿ"](https://twitter.com/dotey/status/1643801825580122115)
    - æ¯•ç«Ÿæµè§ˆå™¨æœ‰æ²™ç›’ï¼Œç›¸å¯¹åœ¨æœ¬åœ°è¿è¡Œä¸€ä¸ªæ²¡å®¡è®¡è¿‡çš„ç¨‹åºæ”¾å¿ƒä¸€ç‚¹ï¼Œä½†ç°åœ¨æµè§ˆå™¨å¯¹gpuç¡¬ä»¶æ”¯æŒè¿˜æ˜¯æ‹‰èƒ¯ã€‚
    - è¿™æ˜¯åˆ†å¸ƒå¼ç®—åŠ›å•Šï¼Œè¿™ä¸å°±æ˜¯æŒ–çŸ¿å˜›

- https://github.com/outlines-dev/outlines /apache2/202408/python
  - https://outlines-dev.github.io/outlines/
  - Robust (structured) text generation.
  - The first step towards reliability of systems that include large language models is to ensure that there is a well-defined interface between their output and user-defined code. Outlines provides ways to control the generation of language models to make their output more predictable.
  - Multiple model integrations: OpenAI, transformers, llama.cpp, exllama2, mamba
  - Simple and powerful prompting primitives based on the Jinja templating engine
  - Fast JSON generation following a JSON schema or a Pydantic model
  - Interleave completions with loops, conditionals, and custom Python functions

## benchmarks-llm

- https://github.com/bigcode-project/bigcodebench /apache2/202408/python
  - https://bigcode-bench.github.io/
  - BigCodeBench is an easy-to-use benchmark for code generation with practical and challenging programming tasks
  - It aims to evaluate the true programming capabilities of large language models (LLMs) in a more realistic setting.
  - The package is built on top of the EvalPlus framework, which is a flexible and extensible evaluation framework for code generation tasks.
  - [2024-06-18] We release BigCodeBench, a new benchmark for code generation with 1140 software-engineering-oriented programming tasks. Preprint is available here
# chatgpt
- app-site
  - https://github.com/LiLittleCat/awesome-free-chatgpt
  - [å…è´¹ chatgpt](https://twitter.com/search?q=%E5%85%8D%E8%B4%B9%20chatgpt&src=typed_query&f=live)

- api-free
  - [ä¸€å£æ°”é€äº†350ä¸ªå…è´¹çš„key](http://freeopenai.xyz/)

- https://freechatgpt.chat/
  - ç±»ä¼¼å®˜ç½‘ï¼Œéœ€è¦key
  - å¯é€‰å†…ç½®ä¸ç¨³å®škey

- [ChatGPT API Demo](https://ai.ls/)
  - https://ai.ls/
  - Powered by gpt-3.5-turbo

- [free ChatGPT](https://freegpt.one/)
  - using gpt-3.5-turbo

- [OpenGPT](https://open-gpt.app/)
  - ç«‹å³ä½¿ç”¨æµ·é‡çš„ ChatGPT åº”ç”¨ï¼Œæˆ–åœ¨å‡ ç§’é’Ÿå†…åˆ›å»ºå±äºè‡ªå·±çš„åº”ç”¨

- https://github.com/lencx/ChatGPT /rust
  - ChatGPT Desktop Application (Mac, Windows and Linux)
  - å›½å†…ipæ— æ³•è®¿é—®

## gpt-apps

- https://github.com/enricoros/big-AGI /6.6kStar/MIT/202508/ts
  - https://big-agi.com/
  - AI suite powered by state-of-the-art models and providing advanced AI/AGI functions
  - New AIX framework lets us scale features we couldn't before

- https://github.com/n4ze3m/dialoqbase /MIT/202401/ts
  - https://dialoqbase.n4ze3m.com/
  - open-source application designed to facilitate the creation of custom chatbots using a personalized knowledge base

- https://github.com/KolosalAI/Kolosal /297Star/apache2/202505/cpp/inactive
  - https://kolosal.ai/
  - OpenSource and Lightweight alternative to LM Studio to run LLMs 100% offline on your device
  - On Linux/macOS, ensure .so/.dylib are in the library search path or same folder.
  - Is this windows only? Unfortunately currently yes. But, we're using framework that mostly is crossplatform
  - https://github.com/KolosalAI/kolosal-server /apache2/202509/cpp
    - inference server for large language models with OpenAI-compatible API endpoints. Now available for both Windows and Linux systems

- https://github.com/iBz-04/offeline /MIT/202510/ts
  - https://offeline.site/
  - Offeline is a privacy-first desktop & web app for running LLMs locally on your hardware using multiple backend options
  - All AI models run locally on your hardware. No data is sent
  - Multi-Platform: Use via browser (web app) or native desktop application with Electron
  - Offline-Capable: Download models once, use them offline indefinitely (WebGPU mode)
  - Multiple AI Backends: Choose your preferred inference engine:
    - Ollama - Manage and run models with Ollama backend
    - llama.cpp - CPU/GPU optimized inference on desktop
    - WebGPU - Run models directly in your browser using GPU acceleration
  - File Embeddings: Load and ask questions about documents (PDF, MD, DOCX, TXT, CSV, RTF) - fully locally
  - Voice Support: Interact with the AI using voice messages
  - Export Conversations: Save your chats as JSON or Markdown

- https://github.com/binary-husky/gpt_academic /GPLv3/202405/python/js
  - https://github.com/binary-husky/gpt_academic/wiki/online
  - ä¸ºGPT/GLMç­‰LLMå¤§è¯­è¨€æ¨¡å‹æä¾›å®ç”¨åŒ–äº¤äº’æ¥å£ï¼Œç‰¹åˆ«ä¼˜åŒ–è®ºæ–‡é˜…è¯»/æ¶¦è‰²/å†™ä½œä½“éªŒï¼Œæ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒè‡ªå®šä¹‰å¿«æ·æŒ‰é’®&å‡½æ•°æ’ä»¶ï¼Œæ”¯æŒPythonå’ŒC++ç­‰é¡¹ç›®å‰–æ&è‡ªè¯‘è§£åŠŸèƒ½ï¼ŒPDF/LaTexè®ºæ–‡ç¿»è¯‘&æ€»ç»“åŠŸèƒ½ï¼Œæ”¯æŒå¹¶è¡Œé—®è¯¢å¤šç§LLMæ¨¡å‹ï¼Œæ”¯æŒchatglm3ç­‰æœ¬åœ°æ¨¡å‹ã€‚
  - æ¥å…¥é€šä¹‰åƒé—®, deepseekcoder, è®¯é£æ˜Ÿç«, æ–‡å¿ƒä¸€è¨€, llama2, rwkv, claude2, mossç­‰ã€‚

- https://github.com/imyuanx/ai-lawyer
  - å†™äº†ä¸ªåŸºäº ChatGPT çš„ AI ç»´æƒå¾‹å¸ˆï¼Œæ ¹æ®äº‹å®å’Œè¯‰æ±‚å¸®å½“äº‹äººç”Ÿæˆèµ·è¯‰ä¹¦ã€‚
  - https://twitter.com/imyuanx/status/1641715695762436098

- https://github.com/nicepkg/gpt-runner
  - Conversations with your files! Manage and run your AI presets!

- https://github.com/AprilNEA/ChatGPT-Admin-Web /MIT/202312/ts/inactive
  - CAW æ˜¯ä¸€ä¸ªè‡ªæ‰˜ç®¡ç½‘ç»œåº”ç”¨ç¨‹åºï¼Œæä¾›å¼€ç®±å³ç”¨çš„ç”¨æˆ·ç®¡ç†ï¼ŒåŒ…æ‹¬åå°ç•Œé¢ä»¥åŠå¯é…ç½®çš„æ”¯ä»˜è®¡åˆ’å’Œç›¸å…³æ”¯ä»˜ç•Œé¢
  - Nest.js + Next.js
  - å¼€ç®±å³ç”¨çš„ç”¨æˆ·ç®¡ç†
  - ä»˜è´¹æ–¹æ¡ˆé…ç½®ï¼Œä¸€é”®å¯¹æ¥æ”¯ä»˜æ¥å£
  - å…³é”®è¯è¿‡æ»¤ã€æ›¿æ¢ä¿è¯æ–‡æœ¬å®‰å…¨

- https://github.com/amd/gaia /MIT/202510/python
  - Run LLM Agents on Ryzen AI PCs in Minutes
  - GAIA is specifically designed for AMD Ryzen AI systems and uses Lemonade Server for optimal hardware utilization
  - GAIA UI is an optional, modern web-based interface for GAIA, built on the RAUX (Open-WebUI fork) platform.

- [Reproducing Karpathyâ€™s NanoChat on a Single GPU â€” Step by Step with AI Tools : r/LocalLLaMA _202510](https://www.reddit.com/r/LocalLLaMA/comments/1o76ev6/reproducing_karpathys_nanochat_on_a_single_gpu/)

- https://github.com/LostRuins/koboldcpp /AGPL/cpp
  - KoboldCpp is an easy-to-use AI text-generation software for GGML and GGUF models, inspired by the original KoboldAI. 
  - It's a single self-contained distributable that builds off llama.cpp and adds many additional powerful features.
  - Single file executable, with no installation required and no external dependencies
  - Runs on CPU or GPU, supports full or partial offloaded
  - Image Generation (Stable Diffusion 1.5, SDXL, SD3, Flux)
  - Speech-To-Text (Voice Recognition) via Whisper
- https://github.com/lone-cloud/gerbil /AGPL/202511/ts
  - A desktop app for running Large Language Models locally.
  - [Gerbil: An open source desktop app for running LLMs locally : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1ol9cai/gerbil_an_open_source_desktop_app_for_running/)
    - Under the hood it runs llama.cpp (via koboldcpp) backends and allows easy integration with the popular modern frontends like Open WebUI, SillyTavern, ComfyUI, StableUI (built-in) and KoboldAI Lite (built-in).

## api

- [https://www.steamship.com](https://www.steamship.com/)
  - SteamShip å¼€æ”¾äº† GPT-4 çš„æ¨¡å‹æ¥å£ï¼Œåªéœ€è¦æ³¨å†ŒSteamShip è´¦å·ï¼Œæ— éœ€ä»˜è´¹ï¼Œå‡ è¡Œä»£ç ç›´æ¥å°±èƒ½è°ƒç”¨ GPT-4

## prompts

- [Open Prompt](https://openprompt.co/)

- https://github.com/verazuo/jailbreak_llms
  - A dataset consists of 15, 140 ChatGPT prompts from Reddit, Discord, websites, and open-source datasets (including 1, 405 jailbreak prompts).
  - å¼€æºäº†è®ºæ–‡ä¸­ä½¿ç”¨çš„ 15, 140 ä¸ª ChatGPT æç¤ºï¼Œå…¶ä¸­åŒ…æ‹¬ 1, 405 ä¸ªè¶Šç‹±æç¤ºï¼Œæ”¶é›†äº Redditã€Discordã€ç½‘ç«™å’Œå¼€æºæ•°æ®é›†ã€‚
# llm-impl
- https://github.com/ideaweaver-ai/qwen3-from-scratch /apache2/202509/python
  - A complete implementation of a Qwen3-based language model trained on the TinyStories dataset. 
  - This project demonstrates modern transformer architecture with Grouped Query Attention, SwiGLU activation, and Rotary Position Embeddings.
  - Efficient Training: Mixed precision training with gradient accumulation
  - Memory Optimized: Uses memory mapping for large dataset handling
  - Text Generation: Built-in generation capabilities with temperature and top-k sampling
  - Progress Monitoring: Real-time training progress and loss visualization
  - Model Size: 283M parameters
  - [Building Qwen3 from Scratch: This Is your chance : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1ndxjsu/building_qwen3_from_scratch_this_is_your_chance/)
  - https://github.com/Emericen/tiny-qwen

- https://github.com/newhouseb/potatogpt
  - Pure Typescript, dependency free, ridiculously slow implementation of GPT2 for educational purposes

- https://github.com/openai/gpt-2 /MIT/201908/python
  - https://openai.com/blog/better-language-models/
  - Code for the paper "Language Models are Unsupervised Multitask Learners"
- https://github.com/openai/gpt-3 /202009/ä»…æ•°æ®
  - GPT-3: Language Models are Few-Shot Learners
# coding-copilot
- products
  - https://copilot.microsoft.com/

- https://github.com/Codium-ai/pr-agent /apache2/202407/python
  - CodiumAI PR-Agent: An AI-Powered Tool for Automated Pull Request Analysis, Feedback, Suggestions
  - aims to help efficiently review and handle pull requests, by providing AI feedbacks and suggestions
  - Our JSON prompting strategy enables to have modular, customizable tools. 
  - We support multiple git providers (GitHub, Gitlab, Bitbucket), multiple ways to use the tool (CLI, GitHub Action, GitHub App, Docker, ...), and multiple models (GPT-4, GPT-3.5, Anthropic, Cohere, Llama2).
  - PR-Agent Pro is a hosted version of PR-Agent, provided by CodiumAI
    - Extra features - emphasize more customization, and the usage of static code analysis, in addition to LLM logic, to improve results. See here for a list of features available in PR-Agent Pro.
  - https://x.com/tuturetom/status/1809053337825989009
    - è‡ªåŠ¨åŸºäºä½ æäº¤çš„ä»£ç è¿›è¡Œåˆ†æï¼Œç»™äºè¯„è®ºåé¦ˆä¸æ„è§ï¼Œç”Ÿæˆ PR æè¿°
    - æ”¯æŒç§æœ‰åŒ–éƒ¨ç½²å’Œå¼€æºæ¨¡å‹
    - é’ˆå¯¹å¤§çš„ PR è®¾è®¡äº† PR Compression ç­–ç•¥ï¼Œå¯ä»¥æå¤§çš„å¤„ç†å‡ ç™¾ä¸ªæ–‡ä»¶çš„åœºæ™¯

- https://github.com/TabbyML/tabby /apache2/202408/rust
  - https://tabbyml.github.io/tabby
  - Self-hosted AI coding assistant. 
  - An opensource / on-prem alternative to GitHub Copilot.
  - Self-contained, with no need for a DBMS or cloud service
  - OpenAPI interface, easy to integrate with existing infrastructure (e.g Cloud IDE).
  - Tabby æ˜¯ä¸€ä¸ªè‡ªæˆ‘æ‰˜ç®¡çš„ #GitHub #Copliot å¼€æºæ›¿ä»£å“ï¼Œè‡ªå¸¦å¯è§†åŒ–ç•Œé¢ä¸ #OpenAPI æ¥å£ï¼Œè¿˜æ”¯æŒæ¶ˆè´¹è€…çº§åˆ«çš„ #GPUï¼Œå…·æœ‰ FP-16 é‡é‡åŠ è½½å’Œå„ç§ä¼˜åŒ–åŠŸèƒ½ï¼Œæä¾›äº† #Docker é•œåƒ

- https://github.com/rjmacarthy/twinny /995Star/MIT/202403/ts
  - The most no-nonsense locally hosted (or API hosted) AI code completion plugin for Visual Studio Code, like GitHub Copilot but 100% free and 100% private.
  - designed to work seamlessly with: Ollama
# agents-multi
- https://github.com/microsoft/autogen /31.4kStar/MIT/202410/python/jupyter
  - https://microsoft.github.io/autogen/
  - AutoGen is an open-source programming framework for building AI agents and facilitating cooperation among multiple agents to solve tasks.
  - It offers features such as agents capable of interacting with each other, facilitates the use of various large language models (LLMs) and tool use support, autonomous and human-in-the-loop workflows, and multi-agent conversation patterns.

- https://github.com/om-ai-lab/OmAgent /apache2/202407/python
  - OmAgentæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œä¸“æ³¨äºåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›ä»¥åŠå…¶ä»–å¤šæ¨¡æ€ç®—æ³•æ¥åšä¸€äº›æœ‰è¶£çš„äº‹
  - åŒ…å«ä¸€ä¸ªä¸“ä¸ºè§£å†³å¤šæ¨¡æ€ä»»åŠ¡è€Œè®¾è®¡çš„è½»é‡çº§æ™ºèƒ½ä½“æ¡†æ¶omagent_coreã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸ªæ¡†æ¶æ­å»ºäº†è¶…é•¿å¤æ‚è§†é¢‘ç†è§£ç³»ç»Ÿâ€”â€”OmAgentï¼Œå½“ç„¶ä½ å¯ä»¥åˆ©ç”¨å®ƒå®ç°ä½ çš„ä»»ä½•æƒ³æ³•ã€‚
  - DnCLoop: å—åˆ°ç»å…¸ç®—æ³•æ€æƒ³Divide and Conquerå¯å‘ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé€’å½’çš„é€šç”¨ä»»åŠ¡å¤„ç†é€»è¾‘ï¼Œå®ƒå°†å¤æ‚çš„é—®é¢˜ä¸æ–­ç»†åŒ–å½¢æˆä»»åŠ¡æ ‘ï¼Œå¹¶æœ€ç»ˆä½¿å¤æ‚ä»»åŠ¡å˜æˆä¸€ç»„å¯è§£å¾—ç®€å•ä»»åŠ¡ã€‚
# agent-open
- https://github.com/AIDC-AI/Pixelle-MCP /271Star/MIT/202508/python
  - https://pixelle.ai/
  - Open-Source Multimodal AIGC Solution based on ComfyUI + MCP + LLM
  - An AIGC solution based on the MCP protocol, seamlessly converting ComfyUI workflows into MCP tools
  - Server-side is built on ComfyUI, inheriting all capabilities from the open ComfyUI ecosystem

- https://github.com/modelcontextprotocol/servers /MIT/202411/python
  - https://modelcontextprotocol.io/
  - A collection of reference implementations and community-contributed servers for the Model Context Protocol (MCP). This repository showcases the versatility and extensibility of MCP, demonstrating how it can be used to give Large Language Models (LLMs) secure, controlled access to tools and data sources.
  - Each MCP server is implemented with either the Typescript MCP SDK or Python MCP SDK.
  - https://x.com/alexalbert__/status/1861464485011300839
    - The future of MCP is truly going to be community-led and not controlled by any single entity.
    - Here are some of the highlights I'm seeing from across the industry:
    - Replit is looking into adding MCP support to Agents
    - Sourcegraph has already added MCP to Cody and you can go try it out right now!
    - Zed has added MCP support in their editor, also available to try out!
    - Github Copilot is working with MCP
    - Codeium is using MCP to improve Windsurf's capabilities
  - https://x.com/alexalbert__/status/1861453744216453309
    - we do have a few clients beside Claude desktop that have already integrated MCP in production (Zed, Sourcegraph, etc).

- https://github.com/QwenLM/Qwen-Agent /tongyi/202411/python
  - https://pypi.org/project/qwen-agent/
  - Agent framework and applications built upon Qwen>=2.0, featuring Function Calling, Code Interpreter, RAG, and Chrome extension
  - åŸºäºQwen2.0çš„agentæ¡†æ¶ï¼šQwen-Agentï¼Œå®ƒæœ‰æŒ‡ä»¤éµå¾ªã€å·¥å…·ä½¿ç”¨ã€åšè§„åˆ’å’Œè®°å¿†èƒ½åŠ›
  - åŸºäºQwen-Agentçš„ä¸€ä¸ªChromeæµè§ˆå™¨æ‰©å±•ï¼Œä¸€ä¸ªæ™ºèƒ½æµè§ˆå™¨åŠ©æ‰‹ï¼šBrowserQwen, å®ƒå¯ä»¥åŸºäºå½“å‰é¡µé¢æˆ–æ–‡æ¡£è·Ÿä½ å¯¹è¯ã€èƒ½è®°ä½ä½ æµè§ˆè¿‡çš„å†…å®¹è¿›è¡Œæ€»ç»“ã€å¯ä»¥è§£å†³æ•°å­¦é—®é¢˜ã€æ•°æ®å›¾è¡¨å¯è§†åŒ–
# llama.cpp
- https://github.com/yazon/flexllama /BSD/202510/python
  - Lightweight self-hosted tool for running multiple llama.cpp server instances with OpenAI v1 API compatibility and multi-GPU support

- https://github.com/xorbitsai/xllamacpp /MIT/202511/python/fork
  - a Python wrapper of llama.cpp
  - This project forks from cyllama and provides a Python wrapper for @ggerganov's llama.cpp which is likely the most active open-source compiled LLM inference engine.
  - https://github.com/xorbitsai/inference /apache2/202511/python
    - Xinference gives you the freedom to use any LLM you need.
    - Xllamacpp: New llama.cpp Python binding, maintained by Xinference team, supports continuous batching and is more production-ready
    - Support MLX backend for Apple Silicon chips

- https://github.com/airnsk/proxycache /202511/python
  - Smart OpenAIâ€‘compatible proxy for llama.cpp: manages slots, saves/restores KV cache to disk, routes requests by prefix similarity
  - [Faster Prompt Processing in llama.cpp: Smart Proxy + Slots + Restore : r/LocalLLaMA _202511](https://www.reddit.com/r/LocalLLaMA/comments/1ostdcn/faster_prompt_processing_in_llamacpp_smart_proxy/)
    - This service is a smart proxy in front of llama.cpp that makes longâ€‘context chat and IDE workflows much faster by managing llama.cpp slots, reusing cached context, and restoring saved caches from disk when needed.
    - llama.cpp provides â€œslots,â€ each holding a conversationâ€™s KV cache so repeated requests with the same or very similar prefix can skip recomputing the whole prompt and continue from the first mismatching token, which dramatically cuts latency for large prompts. In real teams the number of users can easily exceed the number of available slots (e.g., 20 developers but only 4 slots), so naive routing causes random slot reuse and cache overwrites that waste time and GPU/CPU cycles. This proxy solves that by steering requests to the right slot, saving evicted caches to disk, and restoring them on demand, so long prompts donâ€™t need to be recomputed from scratch each time.

- https://github.com/GeeeekExplorer/nano-vllm /MIT/202511/python
  - A lightweight vLLM implementation built from scratch.
  - Clean implementation in ~ 1, 200 lines of Python code
  - Fast offline inference - Comparable inference speeds to vLLM
  - Optimization Suite - Prefix caching, Tensor Parallelism, Torch compilation, CUDA graph, etc.

- https://github.com/lemonade-sdk/lemonade /1.6kStar/apache2/202511/python/cpp
  - https://lemonade-server.ai/
  - Lemonade helps users run local LLMs with the highest performance by configuring state-of-the-art inference engines for their NPUs and GPUs.
  - Lemonade supports both GGUF and ONNX models. You can also import custom GGUF and ONNX models from Hugging Face by using our Model Manager (requires server to be running).

## llama-rewrite

- https://github.com/projektjoe/gpt-oss /MIT/202511/python/cpp
  - From-scratch implementation of OpenAI's GPT-OSS model in Python. No Torch, No GPUs.
  - [GPTOSS: From-Scratch Implementation _202511](https://www.projektjoe.com/blog/gptoss)
    - We will rely completely on the CPU, so no need for special hardware here.
  - [I implemented GPT-OSS from scratch in pure Python, without PyTorch or a GPU : r/LLMDevs _202511](https://www.reddit.com/r/LLMDevs/comments/1ookn7f/i_implemented_gptoss_from_scratch_in_pure_python/)
  - [I implemented GPT-OSS from scratch in pure Python, without PyTorch or a GPU : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1oogvcw/i_implemented_gptoss_from_scratch_in_pure_python/)

- https://github.com/naklecha/llama3-from-scratch /MIT/202405/jupyter/inactive
  - llama3 implementation one matrix multiplication at a time

- https://github.com/karpathy/minGPT /MIT/202207/python/inactive
  - A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training
# ollama/lmstudio-like
- https://github.com/menloresearch/jan /37.5kStar/agpl > apache2/202508/ts/rust/tauri
  - https://jan.ai/
  - an AI assistant that can run 100% offline on your device
  - Local AI Models: Download and run LLMs (Llama, Gemma, Qwen, etc.) from HuggingFace
  - Cloud Integration: Connect to OpenAI, Anthropic, Mistral, Groq, and others
  - MCP integration for enhanced capabilities
  - æä¾›äº†ç±»ä¼¼lm studioçš„æ¨¡å‹ä¸‹è½½ç®€åŒ–ç‰ˆ
  - [chore: Jan's code is now under the Apache license _202505](https://github.com/menloresearch/jan/pull/5042)
    - release/v0.5.18

- https://github.com/oobabooga/text-generation-webui /45.1kStar/AGPLv3/202509/python/js
  - https://oobabooga.gumroad.com/l/deep_reason
  - A Gradio web UI for Large Language Models.
  - The definitive Web UI for local AI, with powerful features and easy setup.
  - Supports multiple local text generation backends, including llama.cpp, Transformers, ExLlamaV3, ExLlamaV2, and TensorRT-LLM (the latter via its own Dockerfile).
  - Easy setup: Choose between portable builds (zero setup, just unzip and run) for GGUF models on Windows/Linux/macOS, or the one-click installer that creates a self-contained installer_files directory.
  - 100% offline and private, with zero telemetry, external resources, or remote update requests.
  - File attachments: Upload text files, PDF documents, and .docx documents to talk about their contents.
  - Vision (multimodal models): Attach images to messages for visual understanding
  - Web search: Optionally search the internet with LLM-generated queries to add context to the conversation.
  - https://github.com/oobabooga/text-generation-webui-extensions
    - ad_discordbot (altoiddealer's discordbot)
    - Diffusion_TTS

- https://github.com/withcatai/catai /477Star/MIT/202406/ts/svelte/api+webapp/inactive
  - https://withcatai.github.io/catai/
  - Run AI assistant locally! with simple API for Node.js
  - Run GGUF models on your computer with a chat ui.
  - Inspired by Node-Llama-Cpp, Llama.cpp
  - Real time text streaming 
  - Fast model downloads
  - There is also a simple API that you can use to ask the model questions.

- https://github.com/EmreMutlu99/Ollama-Agent-Kit /202510/ts
  - https://emc-ltd.co/software-consultancy/
  - open-source Node.js toolkit for building memory-enabled AI agents powered by Ollama. 
  - It supports conversation threads, lightweight JSONL memory, and tool calls (e.g., weather, time, reservations), making it easy to integrate LLM agents into your own projects.

- https://github.com/cztomsik/ava /458Star/MIT/202508/zig/ts
  - https://avapls.com/
  - Ava is an open-source desktop application for running language models locally on your computer. 
  - It's batteries-included GUI for llama.cpp
  - Zig, C++ (llama.cpp), SQLite, Preact, Preact Signals, Twind

- https://github.com/felladrin/MiniSearch /480Star/apache2/202510/ts
  - https://felladrin-minisearch.hf.space/
  - A minimalist web-searching app with an AI assistant that runs directly from your browser.
  - Cross-platform: Models run inside the browser, both on desktop and mobile
  - Efficient: Models are loaded and cached only when needed
  - Customizable: Tweakable settings for search results and text generation
  - Can I use custom models via OpenAI-Compatible API?
    - Yes! For this, open the Menu and change the "AI Processing Location" to Remote server (API). Then configure the Base URL, and optionally set an API Key and a Model to use.
  - It's a docker-based web app built upon SearXNG.
  - https://github.com/searxng/searxng /AGPL/202510/python
    - https://docs.searxng.org/
    - SearXNG is a free internet metasearch engine which aggregates results from various search services and databases. Users are neither tracked nor profiled.

- https://github.com/pamelafox/ollama-python-playground /jupyter
  - A dev container with ollama and ollama examples with the Python OpenAI SDK
  - This project is designed to be opened in GitHub Codespaces as an easy way for anyone to try out SLMs (small language models) entirely in the browser.
  - https://x.com/pamelafox/status/1801678164969853034
    - To make it super easy for anyone to try out Ollama (esp teachers/students)
    - I've included an example notebook that walks through completions, prompt eng, few-shots, and RAG
# webgpu-ai
- https://huggingface.co/spaces/ibm-granite/Granite-4.0-Nano-WebGPU/tree/main /202510/ts
  - the demo uses Transformers.js to run the models 100% locally in your browser with WebGPU acceleration.
  - [Granite 4.0 Nano: Just how small can you go? _202510](https://huggingface.co/blog/ibm-granite/granite-4-nano)
  - [IBM releases Granite-4.0 Nano (300M & 1B), along with a local browser demo showing how the models can programmatically interact with websites and call tools/browser APIs on your behalf. : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1oifmg6/ibm_releases_granite40_nano_300m_1b_along_with_a/)
    - What will be interesting is how they handle permissioning, if the model can open URLS or trigger browser calls, sandboxing becomes key. 
    - `llama.cpp` has webasm support so they probably compiled it to webasm binary and run it via javascript.
# ml-neural-network
- https://github.com/AlloyTeam/netural
  - JavaScriptçš„å‰å‘ç¥ç»ç½‘ç»œå’Œåå‘ä¼ æ’­çš„å®ç°ã€‚

- https://github.com/TrevorBlythe/MentisJS
  - A javascript neural network library. 
  - It can train with backpropagation or neural evolution! 
  - [A better alternative to convnetjs](https://github.com/karpathy/convnetjs/issues/122)
    - This library has Deconv layers and other new features that not even convnetjs has.
    - It also doesn't require you to put your data in a custom data type.

- https://github.com/mlc-ai/web-llm
  - Introducing WebLLM, an open-source chatbot that brings language models (LLMs) directly onto web browsers. 
  - We can now run instruction fine-tuned LLaMA (Vicuna) models natively on your browser tab via @WebGPU with no server support.

- https://github.com/photopea/UNN.js
  - Deep Learning in Javascript. Alternative to ConvNetJS, that is 4x faster.
  - [A 4x faster alternative to ConvNetJS](https://github.com/karpathy/convnetjs/issues/115)
    - It has capabilities similar to ConvNetJS, but both training and testing are 4x faster (while still running in a single JS thread on the CPU).

- https://github.com/mil-tokyo/webdnn
  - The Fastest DNN Running Framework on Web Browser

- https://github.com/karpathy/convnetjs
  - Deep Learning in Javascript. Train Convolutional Neural Networks (or ordinary ones) in your browser.
  - [[Question] Why use javascript ?](https://github.com/karpathy/convnetjs/issues/72)
    - Performance-wise there is no reason to believe that JavaScript would perform significantly slower than Python. The reason why Python is used over C++ is not performance, it is the ecosystem.
    - For fast execution Python or C++ might not even be the best for neural networks as GPGPU computing could be the ultimate solution for fast parallel computing of lots of artificial neurons. In that case JavaScript would yield roughly the same performances as Python of C++.

- https://github.com/BrainJS/brain.js
  - robot GPU accelerated Neural networks in JavaScript for Browsers and Node.js
# ml-dl
- https://github.com/visheratin/web-ai /MIT/ts
  - run modern deep learning models directly in the web browser or in Node.js
  - Powered by ONNX runtime. Web AI runs the models using ONNX Runtime, which has rich support for of all kinds of operators.
  - Web worker support. All heavy operations - model creation and inference - are offloaded to a separate thread so the UI does not freeze.

- https://github.com/xenova/transformers.js /js
  - https://huggingface.co/docs/transformers.js
  - State-of-the-art Machine Learning for the web. 
  - Run Transformers directly in your browser, with no need for a server!
  - Transformers.js is designed to be functionally equivalent to Hugging Face's transformers python library
  - Transformers.js uses ONNX Runtime to run models in the browser. The best part about it, is that you can easily convert your pretrained PyTorch, TensorFlow, or JAX models to ONNX using Optimum.

- https://github.com/ml5js/ml5-library /js
  - provides access to machine learning algorithms and models in the browser, building on top of TensorFlow.js

- https://github.com/alibaba/pipcook /ts
  - https://alibaba.github.io/pipcook/
  - provides subprojects including machine learning pipeline framework, management tools, a JavaScript runtime for machine learning, and these can be also used as building blocks in conjunction with other projects.
  - provides access to Python packages by bridging the interface of `CPython` using N-API. so pytorch/tensorflow/scikit is available

- https://github.com/mljs/ml /js
  - Machine learning tools in JavaScript
# audio
- https://github.com/myshell-ai/OpenVoice /MIT/202407/python
  - https://research.myshell.ai/open-voice
  - Instant voice cloning by MyShell
  - Free Commercial Use. Starting from April 2024, both V2 and V1 are released under MIT License. 
  - OpenVoice has been powering the instant voice cloning capability of myshell.ai since May 2023. 
  - å³æ—¶è¯­éŸ³å…‹éš†å·¥å…·ï¼Œåªéœ€ä»å‚è€ƒèµ„æ–™ä¸­æˆªå–ä¸€æ®µç®€çŸ­çš„éŸ³é¢‘å³å¯å®ç°å…‹éš†ã€‚å¯è¯¦ç»†æ§åˆ¶è¯­éŸ³é£æ ¼ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿã€å£éŸ³ã€èŠ‚å¥ã€åœé¡¿å’Œè¯­è°ƒã€‚ç”Ÿæˆå¤šç§è¯­è¨€çš„è¯­éŸ³ã€‚
# rag-examples
- https://github.com/datawhalechina/all-in-rag /202511/python
  - https://datawhalechina.github.io/all-in-rag/
  - å¤§æ¨¡å‹åº”ç”¨å¼€å‘å®æˆ˜ä¸€ï¼šRAG æŠ€æœ¯å…¨æ ˆæŒ‡å—ï¼Œåœ¨çº¿é˜…è¯»
  - https://github.com/yksanjo/all-in-rag
# rag-utils
- https://github.com/rag-web-ui/rag-web-ui /apache2/202502/python/ts
  - RAG Web UI is an intelligent dialogue system based on RAG
# rag-fwk
- https://github.com/infiniflow/ragflow /68.2kStar/apache2/202511/python/ts/åäººå›¢é˜Ÿ
  - https://ragflow.io/
  - https://demo.ragflow.io/
  - open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding
  - Template-based chunking: Plenty of template options to choose from
  - Grounded citations with reduced hallucinations
  - Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.
  - Multiple recall paired with fused re-ranking.
  - ä¾èµ– Crawl4AIã€elasticsearchã€flask-loginã€minioã€pandasã€voyageaiã€pyobvector
  - æœªä½¿ç”¨langchain/aisdk, å¤šmodel-providerçš„é›†æˆå®Œå…¨è‡ªå®šä¹‰å®ç°
  - Prerequisites: RAM >= 16 GB
    - gVisor: Required only if you intend to use the code executor (sandbox) feature of RAGFlow.
  - âœ¨ å®æµ‹, ragåçš„chunkæ–‡æœ¬å¯æŸ¥çœ‹chunkä¸åŸæ–‡å¯¹åº”çš„ç»†èŠ‚
    - èŠå¤©çš„å›ç­”ä¸­, hoverå›¾æ ‡èƒ½æ˜¾ç¤ºå¼•æ–‡ï¼Œå¹¶å¯ç‚¹å‡»å¼•æ–‡åå¯åœ¨é¡µé¢é®ç½©ä¾§é¢å¼¹çª—ä¸­æŸ¥çœ‹pdfåŸæ–‡ä½ç½®
  - ğŸ› æ•´ä¸ª çŸ¥è¯†åº“/èŠå¤©/æŸ¥è¯¢ çš„åˆ›å»ºåŠä½¿ç”¨äº¤äº’è¿‡äºå¤æ‚ï¼Œ ä¸å¤Ÿç®€æ´å’Œè‡ªç„¶
  - DeepDoc (Default) - RAGFlowPdfParser
    - Full OCR, Most comprehensive but slower
    - Stream Reading Capability: âœ… YES
    - Page-by-page processing with configurable page_from/page_to
    - Uses `pdfplumber.open()` with `page_from` and `page_to` parameters
    - ä½¿ç”¨pdfplumber.open()å’Œpypdf.PdfReader(), ä¸€æ¬¡æ€§åŠ è½½æ•´ä¸ªPDFæ–‡ä»¶åˆ°å†…å­˜
  - Plain Text Parser - PlainParser
    - Uses `pypdf` for text extraction only
    - Only for text-based PDFs
    - Stream Reading Capability: âœ… YES
    - Uses `pypdf.PdfReader` with page ranges
    - Memory Usage: Very low - only text content loaded
    - å¯ä»¥é€é¡µå¤„ç†ï¼Œä½†ä»éœ€è¦åŠ è½½å®Œæ•´PDF
  - Docling Parser - DoclingParser
    -  Uses `docling.DocumentConverter` with full file
  - MinerU Parser - MinerUParser
    - External tool, entire file processing
  - TCADP Parser - TCADPParser
    - Tencent Cloud API integration
    - Cloud-based, file upload to Tencent Cloud
    - Reading Method: Converts entire file to `base64` and uploads, entire file loaded into memory for Base64 conversion
  - Vision Parser - VisionParser
    - Visual model processing, full file
    - Higher memory requirements
    - Uses `pdfplumber` with full file access
  - [HARD -- Efficient way to use enterprise dataset without uploading all files? _202509](https://github.com/orgs/infiniflow/discussions/10388)
    - You have to upload the data from Azure to RAGFlow. And currently you can do that through API. From 0.22 which is going to be launched in this Nov, we will provide some data sources and you can ingest data by just click several buttons. And more data sources could be easily added.
  - [[Question]: Why can't knowledge graphs be used? _202509](https://github.com/infiniflow/ragflow/issues/10017)
    - Knowledge graphs canâ€™t currently be used with the Table chunking method in RAGFlow. The reason is that table chunking treats each row as a separate chunk and stores field mappings for retrieval, but it does not run those chunks through the knowledge graph extraction pipeline. As a result, entityâ€“relationship extraction is skipped for tables.
    - If you need a knowledge graph from table data, youâ€™d have to implement a custom post-processing step (e.g., parsing the rows yourself and generating triples).
  - [Select PDF parser | RAGFlow](https://ragflow.io/docs/dev/select_pdf_parser)
  - [Deploy local models | RAGFlow](https://ragflow.io/docs/dev/deploy_local_llm)
    - RAGFlow supports deploying models locally using Ollama, Xinference, IPEX-LLM, or jina.
  - ğŸ› [[Bug]: Fail to access model(text-embedding-bge-reranker-v2-m3). The LmStudioRerank has not been implement _202502](https://github.com/infiniflow/ragflow/issues/5354)
    - Unable to add reranker model for LM STUDIOï¼ŒFail to access model(text-embedding-bge-reranker-v2-m3).The LmStudioRerank has not been implement
  - ğŸ› [[Question]: Support for calling the OLLAMA Reranker models ï¼Ÿ _202509](https://github.com/infiniflow/ragflow/issues/8988)
    - Ollama supports very few rerank models and is not recommended.
  - [FAQs | RAGFlow](https://ragflow.io/docs/dev/faq)
    - RAGFlow supports MinerU (>= 2.6.3) as an optional PDF parser with multiple backends. RAGFlow acts only as a client for MinerU, calling it to parse documents, reading the output files, and ingesting the parsed content.

- https://github.com/chunkhound/chunkhound /118Star/MIT/202509/python
  - https://chunkhound.github.io/
  - Deep Research for Code & Files
  - Transform your codebase into a searchable knowledge base for AI assistants using semantic search via cAST algorithm and regex search. 
  - Integrates with AI assistants via the Model Context Protocol (MCP).
  - cAST Algorithm - Research-backed semantic code chunking
  - Multi-Hop Semantic Search - Discovers interconnected code relationships beyond direct matches
  - Semantic search - Natural language queries like "find authentication code"
  - Regex search - Pattern matching without API keys
  - Local-first - Your code stays on your machine
  - 22 languages with structured parsing, via Tree-sitter

- https://github.com/HKUDS/LightRAG /20.1kStar/MIT/202508/python
  - Simple and Fast Retrieval-Augmented Generation
  - [2025.06.16]ğŸ¯ Our team has released RAG-Anything an All-in-One Multimodal RAG System for seamless text, image, table, and equation processing.
  - https://github.com/HKUDS/RAG-Anything /MIT/202508/python
    - All-in-One Multimodal Document Processing RAG system built on LightRAG.

- https://github.com/memfreeme/memfree /MIT/202409/ts
  - https://www.memfree.me/
  - MemFree is a Hybrid AI Search Engine.
  - Search and ask questions with text, images, files, and web pages.
  - Multi AI Models: ChatGPT, Claude, Gemini
  - æ”¯æŒå¤šæ¨¡æ€ï¼ˆå›¾ç‰‡ã€æ–‡å­—ã€æ–‡ä»¶ï¼‰ï¼Œå¤šæ¥æºï¼ˆTwitterã€å­¦æœ¯ã€æœ¬åœ°çŸ¥è¯†åº“ï¼‰ï¼Œå¤šç§è¡¨ç°å½¢å¼ï¼ˆæ€ç»´å¯¼å›¾ã€å›¾ç‰‡éŸ³è§†é¢‘ï¼‰ç­‰æ··åˆæœç´¢å½¢æ€

- https://github.com/KruxAI/ragbuilder /apache2/202410/python
  - https://docs.ragbuilder.io/
  - A toolkit to create optimal Production-readyRetrieval Augmented Generation(RAG) setup for your data
  - By performing hyperparameter tuning on various RAG parameters (Eg: chunking strategy: semantic, character etc., chunk size: 1000, 2000 etc.), RagBuilder evaluates these configurations against a test dataset to identify the best-performing setup for your data.

- https://github.com/llmware-ai/llmware /14.5kStar/apache2/202507/python/inactive
  - https://llmware-ai.github.io/llmware/
  - a unified framework for building LLM-based applications (e.g., RAG, Agents), using small, specialized models that can be deployed privately
  - llmware has two main components:
    - RAG Pipeline - integrated components for the full lifecycle of connecting knowledge sources to ai models
    - 50+ small, specialized models fine-tuned for key tasks in enterprise process automation, including fact-based question-answering, classification, summarization, and extraction
  - RAG-Optimized Models - 1-7B parameter models designed for RAG workflow integration and running locally.
# rag-memory
- https://github.com/mem0ai/mem0 /apache2/202409/python
  - https://mem0.ai/
  - Mem0 (pronounced as "mem-zero") enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. 
  - Mem0 remembers user preferences, adapts to individual needs, and continuously improves over time, making it ideal for customer support chatbots, AI assistants, and autonomous systems.
  - Multi-Level Memory: User, Session, and AI Agent memory retention
  - Mem0 leverages a hybrid database approach to manage and retrieve long-term memories for AI agents and assistants. 
    - Each memory is associated with a unique identifier, such as a user ID or agent ID, allowing Mem0 to organize and access memories specific to an individual or context.
    - ğŸ›¢ï¸ When a message is added to the Mem0 using add() method, the system extracts relevant facts and preferences and stores it across data stores: a vector database, a key-value database, and a graph database. 
    - This hybrid approach ensures that different types of information are stored in the most efficient manner, making subsequent searches quick and effective.
    - Mem0 performs search across these data stores, retrieving relevant information from each source

- https://github.com/getzep/zep /apache2/202409/go
  - https://docs.getzep.com/
  - a long-term memory service for AI Assistant apps. 
  - With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant, while also reducing hallucinations, latency, and cost.
  - Zep persists and recalls chat histories, and automatically generates summaries and other artifacts from these chat histories. 
  - Zep also provides a simple, easy to use abstraction for document vector search called Document Collections. This is designed to complement Zep's core memory features, but is not designed to be a general purpose vector database.
# ai-examples
- https://github.com/Zeyi-Lin/HivisionIDPhotos /apache2/202409/python
  - https://swanhub.co/ZeYiLin/HivisionIDPhotos/demo
  - ä¸€ä¸ªè½»é‡çº§çš„AIè¯ä»¶ç…§åˆ¶ä½œç®—æ³•
  - æ—¨åœ¨å¼€å‘ä¸€ç§å®ç”¨ã€ç³»ç»Ÿæ€§çš„è¯ä»¶ç…§æ™ºèƒ½åˆ¶ä½œç®—æ³•, åˆ©ç”¨ä¸€å¥—å®Œå–„çš„AIæ¨¡å‹å·¥ä½œæµç¨‹ï¼Œå®ç°å¯¹å¤šç§ç”¨æˆ·æ‹ç…§åœºæ™¯çš„è¯†åˆ«ã€æŠ å›¾ä¸è¯ä»¶ç…§ç”Ÿæˆ
  - SwanLabï¼šè®­ç»ƒäººåƒæŠ å›¾æ¨¡å‹å…¨ç¨‹ç”¨å®ƒæ¥åˆ†æå’Œç›‘æ§ï¼Œä»¥åŠå’Œå®éªŒå®¤åŒå­¦åä½œäº¤æµï¼Œå¤§å¹…æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚
  - æ”¯æŒ çº¯ç¦»çº¿ æˆ– ç«¯äº‘ æ¨ç†
# ai-devops/tooling
- https://github.com/Tencent/AI-Infra-Guard /MIT/202503/go
  - è…¾è®¯å¼€æºäº†ä¸€ä¸ªAIåŸºç¡€è®¾æ–½å®‰å…¨è¯„ä¼°å·¥å…·ï¼šAI-Infra-Guardï¼Œä¸€é”®æ£€æµ‹AIç³»ç»Ÿçš„æ½œåœ¨å®‰å…¨é£é™©
  - æ”¯æŒåŒ…æ‹¬langchainã€ollamaã€gradioã€open-webuiä»¥åŠComfyUIç­‰åœ¨å†…çš„30ç§AIç»„ä»¶
  - å¯ä»¥å¸®åŠ©è¯†åˆ«ï¼Œæ¯”å¦‚åƒOllamaåœ¨å…¶dockerä¸­é»˜è®¤ä»¥rootæƒé™è¿è¡Œä¸”å¼€æ”¾åˆ°å…¬ç½‘ä¸Šï¼Œç”±äºç¼ºä¹é‰´æƒï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹è¢«åˆ é™¤/çªƒå–/æŠ•æ¯’æˆ–ç®—åŠ›è¢«çªƒå–çš„é£é™©
  - è¿™ä¸ªå·¥å…·çš„ç‰¹ç‚¹æ˜¯è½»é‡çº§ï¼ŒäºŒè¿›åˆ¶æ–‡ä»¶8MBï¼Œå†…å­˜å ç”¨ä½ï¼Œæ”¯æŒè·¨å¹³å°è¿è¡Œï¼Œå¼€ç®±å³ç”¨

- https://github.com/exo-explore/exo /GPL
  - exo: Run your own AI cluster at home with everyday devices
  - exo supports different models including LLaMA (MLX and tinygrad), Mistral, LlaVA, Qwen, and Deepseek.
  - è¯¥é¡¹ç›®æ”¯æŒå°†ç°æœ‰è®¾å¤‡ç»Ÿä¸€åˆ°ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„GPUä¸­ï¼Œæ”¯æŒ iPhoneï¼ŒiPadï¼ŒAndroidï¼ŒMacï¼ŒNvidiaï¼Œæ ‘è“æ´¾ç­‰ç­‰å‡ ä¹æ‰€æœ‰è®¾å¤‡

- https://github.com/GhennadiiMir/ollama_server_manager /202510/ruby
  - A streamlined web interface for managing Ollama models across multiple servers. 
  - Perfect for teams running distributed Ollama instances or managing models on different machines.
  - Since I only use this on my private, trusted network, I kept it intentionally simple with no authentication required.
# more
