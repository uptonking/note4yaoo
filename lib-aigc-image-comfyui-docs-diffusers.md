---
title: lib-aigc-image-comfyui-docs-diffusers
tags: [comfyui, diffusers, docs, image]
created: 2025-08-23T06:42:00.128Z
modified: 2025-08-23T11:42:50.170Z
---

# lib-aigc-image-comfyui-docs-diffusers

# guide
- pros-comfyui ğŸ“Œ
  - easy ui to start image-gen
  - å¯æ‰©å±•: custom-node support
  - å¯¹æœ€æ–°æ¨¡å‹çš„æ”¯æŒå¾ˆå¿«
  - å¯¹å„ç§ç¡¬ä»¶çš„æ”¯æŒè¾ƒå¥½ï¼ŒåŒ…æ‹¬nvidia/amd/cpu
  - æ”¯æŒ sub-graph
  - æ”¯æŒ flow controlï¼Œå¦‚conditional
  - æä¾›äº†uiå’Œapi
  - æ¶æ„ä¸Šæ”¯æŒextension: å†…ç½®äº†managerï¼Œæ”¯æŒcustom_nodes
  - æä¾›æ‰©å±•ï¼Œæ”¯æŒæ‰¹é‡å‡ºå›¾ï¼Œå¦‚XYPlot
  - ç¤¾åŒºæ´»è·ƒï¼Œæ”¯æŒgguf
- cons-comfyui
  - license: GPLv3
  - ä¸€äº›å¤æ‚çš„workflowéš¾ä»¥ç†è§£å’Œç»´æŠ¤
  - å¯¹äºåµŒå…¥å¼æ–‡ç”Ÿå›¾/æ”¹å›¾çš„åœºæ™¯ï¼Œcomfyuiçš„å·¥ä½œæµå›¾ä¸å¦‚invokeAIçš„canvasæ˜“ç”¨
- who is using #comfyui
  - jaaz, comflowy
  - SwarmUI: æ”¯æŒcomfyui/sd-webui
  - ClaraVerse
- pros-InvokeAI ğŸ“Œ
  - license: apache2
  - canvas ux like photoshop
  - inpaint by layers
  - èƒ½è§‚å¯Ÿåˆ°å›¾åƒä»å™ªéŸ³ç‚¹åˆ°ç›®æ ‡å›¾çš„ç»˜åˆ¶è¿‡ç¨‹
  - æ”¯æŒä½¿ç”¨å·²æœ‰çš„models/lora, ä½†å¾ˆå¤šæ— æ³•å¯¼å…¥
  - æ¶æ„ä¸Šæ”¯æŒextension: æ”¯æŒcustom_nodes
  - æ”¯æŒdesktopå’Œwebæ¨¡å¼ï¼Œä½†apiæœªå¯¹å¤–éƒ¨ä½¿ç”¨åšä¼˜åŒ–
- cons-InvokeAI
  - features slow
  - å¯¹éƒ¨åˆ†model/lora/vae/controlNetçš„æ”¯æŒå¾ˆå·®, å¯¹æ–°æ¨¡å‹æ”¯æŒå¾ˆæ…¢
  - RAMå†…å­˜(éVRAM)å ç”¨å¾ˆå¤§ï¼Œæ¯”comfyuiå¤§å¾—å¤š
  - workflowå’Œæ•™ç¨‹ç¤ºä¾‹å¤ªå°‘
  - ~~models/safetensors auto converted to diffusers > duplicated~~
    - v4.2.6å¼€å§‹æ”¯æŒ Checkpoint models work without conversion to diffusers
- pros-diffusers ğŸ“Œ
  - flexible internals: pytorch, flax
  - comfyui loader éƒ¨åˆ†æ”¯æŒ
  - æ”¯æŒflux/qwen/wan
  - Hybrid Inference: VAE Encode/Decode, TextEncoders
- cons-diffusers
  - diffusersåœ¨æœªè°ƒä¼˜çš„æ¡ä»¶ä¸‹ç»“æœä¸å¦‚comfyuiï¼Œå› ä¸ºcomfyuiå†…ç½®äº†å¾ˆå¤šå‚æ•°/prompt
  - comfyui-custom_nodes ç§»æ¤åˆ° diffusers éœ€è¦æ‰‹åŠ¨è®¾ç½®å‚æ•°å’Œè½¬æ¢
- who is using #diffusers
  - SDNext
  - InvokeAI
- comfyui
  - â›“ï¸ workflowçš„ä¸šåŠ¡åœºæ™¯å¯å‚è€ƒn8n
  - comfyuiçš„å·¥ä½œæµuxå¯¹ç”¨æˆ·ä¸å‹å¥½ï¼ŒInvokeAIçš„canvasæ›´æ˜“ç”¨ã€æ˜“åµŒå…¥
  - ğŸ‘·: ComfyUI was never based on diffusers. It's a horrible library but I can't hate it that much because it's so bad that it's responsible for prematurely killing a lot of comfyui competition by catfishing poor devs into using it.
- stable-diffusion
  - ç›¸å¯¹äºä¼ ç»ŸPSè½¯ä»¶çš„ä¼˜ç‚¹: upscaleè¿˜åŸåº¦é«˜ï¼Œé€Ÿåº¦å¿«
- openrouter for image
  - å›¾åƒæ¨¡å‹çš„é…ç½®æ¯”æ–‡æœ¬llmæ›´å¤æ‚ï¼Œä¸åŒåœºæ™¯æ‰€éœ€çš„æ¨¡å‹å’Œé…ç½®éƒ½ä¸åŒ
  - éš¾ç‚¹æ˜¯sdç³»åˆ—æ¨¡å‹ç›¸å…³çš„clip/encoder/vaeç§ç±»ç¹å¤šï¼Œä¸å¦‚ç›´æ¥ç”¨comfyui-api
  - image-gençš„é€»è¾‘è¿˜éœ€è¦è€ƒè™‘ VLM ç”Ÿæˆå›¾ç‰‡æè¿°promptæ‰€é‡‡ç”¨çš„æ¨¡å‹ï¼Œè¿‡äºçµæ´»
- tips
  - éšç€æ–‡æœ¬å¤§æ¨¡å‹èƒ½åŠ›çš„å¢å¼ºï¼Œpromptè‡ªåŠ¨ç”Ÿæˆã€memoryç®¡ç†åŸºäºcodingå®ç°æ›´çµæ´»ï¼Œcomfyuiæ”¯æŒçš„èƒ½åŠ›æœ‰é™
  - åœ¨çº¿å›¾ç‰‡ç”Ÿæˆæˆ–ç¼–è¾‘çš„æ¶æ„, æ¶‰åŠåˆ°model/lora/vae/encoderçš„ä¸‹è½½ä¸ç»„åˆï¼Œç›®å‰æ²¡æœ‰ç±»ä¼¼ollamaçš„ç»Ÿä¸€æ–¹æ¡ˆ, è¿˜æ¶‰åŠåˆ°GPU/CPUç¡¬ä»¶æ”¯æŒï¼Œåªæœ‰æˆç†Ÿæ–¹æ¡ˆæ‰å¤„ç†è¿‡ç›¸å…³é—®é¢˜
  - ä¸‹è½½æ¨¡å‹æ–‡ä»¶åä¸è¦renameï¼Œæ–¹ä¾¿ä¸ç¬¬ä¸‰æ–¹åŒ…ç®¡ç†å…±äº«ï¼Œæ–¹ä¾¿ä¸äº‘ç«¯æœåŠ¡å•†çš„æ¨¡å‹å…±äº«åç§°
# dev-xp

## draft

- éœ€è¦ä¸€ä¸ªç±»ä¼¼openrouter/groqçš„apièƒ½æ”¯æŒå¤šä¸ªæœåŠ¡ç”Ÿæˆimage, éœ€è¦åŒæ—¶æ”¯æŒ sd-webuiå’Œcomfyui
  - å¯å‚è€ƒaisdkæ¥å®ç°

## comfyui

- build comfyui from source
  - [Adding UV installation instructionsUI](https://github.com/comfyanonymous/ComfyUI/pull/6349)

```sh
uv add --requirements requirements.txt

cp extra_model_paths.yaml.example extra_model_paths.yaml

uv run python main.py
```

### draft-comfy

- roadmap
  - workflow as app: å®ç°æ€è·¯åŒ…æ‹¬ç‹¬ç«‹fullstack, custom_node-api, custom_node-ui, gradio
  - vscode-comfy
  - â›“ï¸ workflowçš„ä¸šåŠ¡åœºæ™¯å¯å‚è€ƒn8n
  - é’ˆå¯¹pptä¼˜åŒ–çš„å›¾ç‰‡ç”Ÿæˆ, å¦‚è‡ªåŠ¨æ·»åŠ ç›®å½•ç¼–å·/æ°´å°/è°ƒæ•´å¤§å°
  - models: å­¦ä¹ å›¾æ ‡åº“/logoåº“çš„æ¨¡å‹
- usecase
  - upscale, anime, faceswap, bg-remove, ocr, ...
- extension-manager: batch enable/disable

## InvokeAI

- ğŸ›¢ï¸ Invoke uses a SQLite database to store image, workflow, model, and execution data.
  - è½¯ä»¶å…ƒæ•°æ®åœ¨ invokeRoot/databases/invokeai.db
  - ç”Ÿæˆçš„å›¾ç‰‡åœ¨ invokeRoot/outputs/images
- [Inpainting: How do I remove an element from an existing image? : Invoke Support Portal](https://support.invoke.ai/support/solutions/articles/151000201404)
  - å°è¯•æ ¹æ®æ•™ç¨‹ç§»é™¤è‰åœ°ä¸Šçš„é¸Ÿï¼Œå½“ Denoising Strength è®¾ä¸º0.5æ—¶æ²¡æ•ˆæœï¼Œè®¾ä¸º0.9æ—¶æ‰ç§»é™¤
# docs-sd-webui

- 
- 
- 
- 

## api-webui

- [API Â· AUTOMATIC1111/stable-diffusion-webui Wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/API)
  - [Stable Diffusion web UI txt2img img2img api example python script](https://gist.github.com/w-e-w/0f37c04c18e14e4ee1482df5c4eb9f53)
- /sdapi/v1/txt2img
  - 

- request POST
  - `override_settings`: The purpose of this parameter is to override the webui settings such as model or CLIP skip for a single request.
  - `sdapi/v1/options`: different from override_settings because this change will persist

```JSON
{
  "prompt": "",
  "negative_prompt": "",
  "styles": [
    "string"
  ],
  "seed": -1,
  "subseed": -1,
  "subseed_strength": 0,
  "seed_resize_from_h": -1,
  "seed_resize_from_w": -1,
  "sampler_name": "string",
  "scheduler": "string",
  "batch_size": 1,
  "n_iter": 1,
  "steps": 50,
  "cfg_scale": 7,
  "width": 512,
  "height": 512,
  "restore_faces": true,
  "tiling": true,
  "do_not_save_samples": false,
  "do_not_save_grid": false,
  "eta": 0,
  "denoising_strength": 0,
  "s_min_uncond": 0,
  "s_churn": 0,
  "s_tmax": 0,
  "s_tmin": 0,
  "s_noise": 0,
  "override_settings": {},
  "override_settings_restore_afterwards": true,
  "refiner_checkpoint": "string",
  "refiner_switch_at": 0,
  "disable_extra_networks": false,
  "firstpass_image": "string",
  "comments": {},
  "enable_hr": false,
  "firstphase_width": 0,
  "firstphase_height": 0,
  "hr_scale": 2,
  "hr_upscaler": "string",
  "hr_second_pass_steps": 0,
  "hr_resize_x": 0,
  "hr_resize_y": 0,
  "hr_checkpoint_name": "string",
  "hr_sampler_name": "string",
  "hr_scheduler": "string",
  "hr_prompt": "",
  "hr_negative_prompt": "",
  "hr_cfg": 1,
  "force_task_id": "string",
  "sampler_index": "Euler",
  "script_name": "string",
  "script_args": [],
  "send_images": true,
  "save_images": false,
  "alwayson_scripts": {},
  "infotext": "string"
}
```

- response-200-Successful
  - `images` is a list of base64-encoded generated images.

```JSON
{
  "images": [
    "string"
  ],
  "parameters": {},
  "info": "string"
}
```

- response-422-Validation Error

```JSON
{
  "detail": [
    {
      "loc": [
        "string",
        0
      ],
      "msg": "string",
      "type": "string"
    }
  ]
}
```

- 
- 
- 
- 

# docs-diffusers
- A diffusion model combines multiple components to generate outputs in any modality based on an input, such as a text description, image or both.
- For a standard text-to-image model:
  - A text encoder turns a prompt into embeddings that guide the denoising process. Some models have more than one text encoder.
  - A scheduler contains the algorithmic specifics for gradually denoising initial random noise into clean outputs. Different schedulers affect generation speed and quality.
  - A UNet or diffusion transformer (DiT) is the workhorse of a diffusion model. At each step, it performs the denoising predictions, such as how much noise to remove or the general direction in which to steer the noise to generate better quality outputs. The UNet or DiT repeats this loop for a set amount of steps to generate the final output.
  - A variational autoencoder (VAE) encodes and decodes pixels to a spatially compressed latent-space. Latents are compressed representations of an image and are more efficient to work with. The UNet or DiT operates on latents, and the clean latents at the end are decoded back into images.
- The `DiffusionPipeline` packages all these components into a single class for inference.
- Adapters insert a small number of trainable parameters to the original base model. 
  - Only the inserted parameters are fine-tuned while the rest of the model weights remain frozen. This makes it fast and cheap to fine-tune a model on a new style. 
  - Among adapters, LoRAâ€™s are the most popular.
- Quantization stores data in fewer bits to reduce memory usage. It may also speed up inference because it takes less time to perform calculations with fewer bits.
- 
- 
- 
- 
- 
- 

# docs-InvokeAI
- When you start working on a blank Canvas, your first image generation will be added as a Raster Layer
- Layers are a powerful set of tools enabling you to control and guide the image generation process to produce a desired outcome.
- Regional Reference Image layers use Image Prompt (IP) Adapters to inspire a new image with the content of an input image. You can use add Regional Reference Images as layers on the â€˜Layersâ€™ tab.
- Inpaint Mask layer allows you to specify a region that will be modified for generation, while preserving the rest of your raster layer data.
  - The Denoising Strength that you select will dictate how much change you want the AI model to generate in the selected region. At very high denoising strengths, the newly generated content will be very different from your original image, and at low denoising strengths, it will only make minor changes.
- Regional Guidance layers allow you more fine-tuned control over the prompt information used to guide the generation process. 
- Control Layers can provide structural control over the output of your image generations, with a number of different ways to instruct the system using visual representations like sketches, edge maps, and depth renderings.
- A Raster Layer is the image content of your canvas, similar to other Image Editing solutions. 
  - When included in your bounding box, these images serve as the base image content to start your creative process
  - This layer allows you to inspire the generation process with an initial drawing or image, which preserves the original image's rough structure, colors, and layout, while using AI to reimagine new content with your input prompt based on your denoising strength.
- Inpainting, in the context of image generation, is a process where we try to fill in parts of an image with new or modified content.
  - inpainting methods use the available information in an image (such as edges, textures, colors) to predict what the incomplete areas should look like, and then uses the selected model to regenerate that portion of the image
- 
- 
- 
- 
- 

# docs-concepts
- [IPAdapter Â· vladmandic/sdnext Wiki](https://github.com/vladmandic/sdnext/wiki/IPAdapter)
  - IP-Adapter is a tool designed for style transfer with minimal resource usage. It provides an efficient way to clone faces or apply image transformations.
  - Low Resource Usage: The IP-Adapter is lightweight, with memory requirements under 100MB for SD 1.5 and 700MB for SD-XL, making it an efficient choice for style transfer tasks.
  - Style Transfer: It offers powerful style transfer capabilities, allowing you to clone faces or apply various image styles.
  - Integration with ControlNet: IP-Adapter can be combined with ControlNet for more stable results, especially useful for batch processing or video tasks.
- 
- 
- 
- 
- 
- 
- 
- 

# more
- [Hosting a ComfyUI Workflow via API - 9elements _202402](https://9elements.com/blog/hosting-a-comfyui-workflow-via-api/)
