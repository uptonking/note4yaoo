---
title: lib-aigc-image-comfyui-docs-diffusers
tags: [comfyui, diffusers, docs, image]
created: 2025-08-23T06:42:00.128Z
modified: 2025-08-23T11:42:50.170Z
---

# lib-aigc-image-comfyui-docs-diffusers

# guide

- pros-comfyui
  - easy ui to start image-gen
  - å¯æ‰©å±•: custom-node support
  - å¯¹æœ€æ–°æ¨¡å‹çš„æ”¯æŒå¾ˆå¿«
  - æ”¯æŒ sub-graph
  - æ”¯æŒflow controlï¼Œå¦‚conditional
  - æä¾›äº†uiå’Œapi

- cons-comfyui
  - license: GPLv3
  - ä¸€äº›å¤æ‚çš„workflowéš¾ä»¥ç†è§£å’Œç»´æŠ¤

- comfyui-wrapper
  - SwarmUI
  - ClaraVerse

- pros-diffusers
  - flexible internals: pytorch, flax
  - comfyui loader éƒ¨åˆ†æ”¯æŒ
  - æ”¯æŒflux/qwen/wan
  - Hybrid Inference: VAE Encode/Decode, TextEncoders

- cons-diffusers
  - diffusersåœ¨æœªè°ƒä¼˜çš„æ¡ä»¶ä¸‹ç»“æœä¸å¦‚comfyuiï¼Œå› ä¸ºcomfyuiå†…ç½®äº†å¾ˆå¤šå‚æ•°/prompt
  - comfyui-custom-node ç§»æ¤åˆ° diffusers éœ€è¦æ‰‹åŠ¨è®¾ç½®å‚æ•°å’Œè½¬æ¢

- who is using #diffusers
  - SDNext
  - InvokeAI

- tips
  - éšç€æ–‡æœ¬å¤§æ¨¡å‹èƒ½åŠ›çš„å¢å¼ºï¼Œpromptè‡ªåŠ¨ç”Ÿæˆã€memoryç®¡ç†åŸºäºcodingå®ç°æ›´çµæ´»ï¼Œcomfyuiæ”¯æŒçš„èƒ½åŠ›æœ‰é™
  - åœ¨çº¿å›¾ç‰‡ç”Ÿæˆæˆ–ç¼–è¾‘çš„æ¶æ„, æ¶‰åŠåˆ°æ¨¡å‹ä¸‹è½½ä¸æ‰©å±•ä¸‹è½½ï¼Œç›®å‰æ²¡æœ‰ç±»ä¼¼ollamaçš„ç»Ÿä¸€æ–¹æ¡ˆ, è¿˜æ¶‰åŠåˆ°GPU/CPUç¡¬ä»¶æ”¯æŒï¼Œåªæœ‰æˆç†Ÿæ–¹æ¡ˆæ‰å¤„ç†è¿‡ç›¸å…³é—®é¢˜
  - ğŸ‘·: ComfyUI was never based on diffusers. It's a horrible library but I can't hate it that much because it's so bad that it's responsible for prematurely killing a lot of comfyui competition by catfishing poor devs into using it.
# overview
- A diffusion model combines multiple components to generate outputs in any modality based on an input, such as a text description, image or both.

- For a standard text-to-image model:
  - A text encoder turns a prompt into embeddings that guide the denoising process. Some models have more than one text encoder.
  - A scheduler contains the algorithmic specifics for gradually denoising initial random noise into clean outputs. Different schedulers affect generation speed and quality.
  - A UNet or diffusion transformer (DiT) is the workhorse of a diffusion model. At each step, it performs the denoising predictions, such as how much noise to remove or the general direction in which to steer the noise to generate better quality outputs. The UNet or DiT repeats this loop for a set amount of steps to generate the final output.
  - A variational autoencoder (VAE) encodes and decodes pixels to a spatially compressed latent-space. Latents are compressed representations of an image and are more efficient to work with. The UNet or DiT operates on latents, and the clean latents at the end are decoded back into images.

- The `DiffusionPipeline` packages all these components into a single class for inference.

- Adapters insert a small number of trainable parameters to the original base model. 
  - Only the inserted parameters are fine-tuned while the rest of the model weights remain frozen. This makes it fast and cheap to fine-tune a model on a new style. 
  - Among adapters, LoRAâ€™s are the most popular.

- Quantization stores data in fewer bits to reduce memory usage. It may also speed up inference because it takes less time to perform calculations with fewer bits.

- 
- 
- 
- 
- 
- 

# docs

- 
- 
- 
- 

# more
