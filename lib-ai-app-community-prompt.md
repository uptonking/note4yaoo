---
title: lib-ai-app-community-prompt
tags: [ai, prompt]
created: 2024-09-08T18:56:59.800Z
modified: 2024-09-08T18:57:12.231Z
---

# lib-ai-app-community-prompt

# guide

# blogs
- ğŸ§‘â€ğŸ« [OpenAI å®˜æ–¹ GPT-4.1 Prompt æŒ‡å— - çŸ¥ä¹ _202504](https://zhuanlan.zhihu.com/p/1895544303784294231)
# discuss-stars
- ## 

- ## 

- ## 

- ## [Spent 6 months deep in prompt engineering. Here's what actually moves the needle: : r/PromptEngineering _202510](https://www.reddit.com/r/PromptEngineering/comments/1ny2pff/spent_6_months_deep_in_prompt_engineering_heres/)
- Examples beat instructions 
  - Wasted weeks writing perfect instructions. Then tried 3-4 examples and got instant results. 
  - Models pattern-match better than they follow rules (except reasoning models like o1)
- Version control your prompts like code 
  - One word change broke our entire system. Now I git commit prompts, run regression tests, track performance metrics. Treat prompts as production code
- Test coverage matters more than prompt quality 
  - Built a test suite with 100+ edge cases. Found my "perfect" prompt failed 30% of the time. 
  - Now use automated evaluation with human-in-the-loop validation
- Domain expertise > prompt tricks 
  - Your medical AI needs doctors writing prompts, not engineers. 
  - Subject matter experts catch nuances that destroy generic prompts
- Temperature tuning is underrated 
  - Everyone obsesses over prompts. Meanwhile adjusting temperature from 0.7 to 0.3 fixed our consistency issues instantly
- Model-specific optimization required 
  - GPT-4o prompt â‰  Claude prompt â‰  Llama prompt. 
  - Each model has quirks. What makes GPT sing makes Claude hallucinate
- Chain-of-thought isn't always better 
  - Complex reasoning chains often perform worse than direct instructions. 
  - Start simple, add complexity only when metrics improve
- Use AI to write prompts for AI 
  - Meta but effective: Claude writes better Claude prompts than I do. Let models optimize their own instructions
- System prompts are your foundation 
  - 90% of issues come from weak system prompts. Nail this before touching user prompts
- Prompt injection defense from day one 
  - Every production prompt needs injection testing. One clever user input shouldn't break your entire system

- The biggest revelation: prompt engineering isn't about crafting perfect prompts. It's systems engineering that happens to use LLMs

- When I said prompt injection I meant more to when you are using AI inside your app and the user can talk to it (via a bot or smth similar). The two ways (as far as I know & tried) you can implement prompt injection defense are:
  - Giving very solid instruction inside your templated-prompt you are using for your LLM.
  - Fine tune your AI model to train it against prompt injections, but this a lot more time & resources, yet it's way more effective than any templated prompt.

- Examples are good, however small models will overuse them and they can really ruin the output so you have to be tactical where you use them.
  - I have a system in place that randomly picks examples from a larger set so you have more variety while keeping prompts lean.
- Nice! Iâ€™ve a number of workarounds, my favourite is using unrelated examples that the LLM would never actually use - so it copies the structure but uses the context for the actual content.

- Donâ€™t make prompts static. Dynamically write the prompt in chain so you donâ€™t have to craft a fucking system message that matters just preload hard rules and soft code other rules in the dynamic creating.
  - You guys donâ€™t think right. System prompts are not what you think. They are not rules for the system. Itâ€™s stargate.
  - You dial up your destination with your user prompts. The system message is your origin. Your perspective itâ€™s the things you believe as the environment.
# discuss-known
- ## 

- ## 

- ## 

- ## äºšé©¬é€Š aws å¼€æºäº†ç”Ÿæˆç”Ÿäº§çº§åˆ«  Claude Prompt çš„ Prompt ç”Ÿæˆå™¨ ã€ŒMeta Prompt ã€
- https://x.com/tuturetom/status/1832289213171298811
  - åŸºäº Human-in-the-Loop çš„ç†å¿µï¼Œä»åˆå§‹ Prompt å¼€å§‹ï¼Œç»“åˆäººç±»å’Œ AI åé¦ˆæ‰“åˆ†ä¼˜åŒ–ï¼Œè¿­ä»£ Prompt æ¶æ„å’Œå¡«å……ä¼˜åŒ–å¬å›ç¤ºä¾‹
- æ€æ ·ç”¨å•Šï¼Ÿ
  - ä½œä¸º System  Prompt ä¼ å…¥ï¼Œç„¶åå¯ä»¥æéœ€æ±‚ï¼Œè¿™æ ·å¯ä»¥ç”Ÿæˆä½ æƒ³è¦çš„ Prompt

# discuss-context
- ## 

- ## 

- ## 

- ## 

- ## [LM Studio and Context Caching (for API) : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1npatw9/lm_studio_and_context_caching_for_api/)
  - I'm running a Mac, so LM Studio with their MLX support is my go-to for using local models. 
  - When using the LM Studio as a local LLM server that integrates with tools and IDEs (like Zed, Roo, Cline, etc.), things get a bit annoying with the long-context slowdown. 
  - As I understand, it happens for 2 reasons:
  - The previous messages are reprocessed, the more messages, the longer it takes.
  - Especially on the Macs, the longer the context, the slower the generation speed.

# discuss-prompt-usecases
- ## 

- ## 

- ## 

- ## [My go-to prompt for analyzing stocks. Share yours! : r/PromptEngineering](https://www.reddit.com/r/PromptEngineering/comments/1oqa30m/my_goto_prompt_for_analyzing_stocks_share_yours/)
- Act as a senior equity research analyst. Your task is to compile a comprehensive investment analysis report on [Company Name] (Ticker: [Ticker Symbol]). The report should be detailed, objective, and data-driven, using financial data from the last five full fiscal years and the most recent trailing twelve months (TTM).

# discuss
- ## 

- ## 

- ## 

- ## 

- ## [å“ªäº›ä»¤ä½ æƒŠè‰³çš„AIå¤§è¯­è¨€æ¨¡å‹æç¤ºè¯ï¼ˆpromptï¼‰ï¼Ÿ - çŸ¥ä¹](https://www.zhihu.com/question/5415393695)
- æç¤ºè¯å¤§è‡´åˆ†ä¸ºä¸‹é¢å‡ ä¸ªéƒ¨ä»½ï¼š
  - å¼•å¯¼è¯­æˆ–æŒ‡ç¤ºè¯­ï¼šå‘Šè¯‰æ¨¡å‹éœ€è¦å¤„ç†ä»€ä¹ˆæ ·çš„ä»»åŠ¡ï¼Œæ‰®æ¼”ä»€ä¹ˆæ ·çš„è§’è‰²ã€‚å› ä¸ºæ˜¯ä¸€ä¸ªè¶…çº§å¤§çš„è¯­è¨€åº“ï¼Œå…ˆç»™å®šä¸€ä¸ªæ ‡ç­¾ã€‚è®©å¤§æ¨¡å‹çŸ¥é“æ˜¯å“ªä¸ªé¢†åŸŸçš„ã€‚æ–¹ä¾¿åç»­çš„å›å¤ã€‚
  - ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼šæä¾›è¶³å¤Ÿçš„èƒŒæ™¯ä¿¡æ¯ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¤„ç†è¯·æ±‚ã€‚ä¸Šä¸‹æ–‡ä¿¡æ¯å¯èƒ½åŒ…æ‹¬å…·ä½“æƒ…æ™¯ã€ç›¸å…³æ•°æ®ã€å†å²å¯¹è¯ä¿¡æ¯ç­‰å†…å®¹ã€‚å†å²å†…å®¹ï¼šå‘Šè¯‰å¤§æ¨¡å‹ä¹‹å‰åšäº†ä»€ä¹ˆã€‚
  - ä»»åŠ¡æè¿°ï¼šæ˜ç¡®åœ°æè¿°ä½ æœŸæœ›æ¨¡å‹æ‰§è¡Œçš„ä»»åŠ¡ã€‚åœ¨å¤„ç†å¤æ‚æƒ…å†µä¸‹ï¼Œä¸€ä¸¤å¥è¯´ä¸æ¸…æ¥šå°±éœ€è¦å¯¹ä»»åŠ¡è¿›è¡Œåˆ†å±‚æè¿°ã€‚
  - è¾“å‡ºæ ¼å¼æŒ‡ç¤ºï¼šå¦‚æœä½ å¯¹è¾“å‡ºç»“æœæœ‰ç‰¹å®šçš„æ ¼å¼è¦æ±‚ï¼Œå¯ä»¥ç»™å‡ºä¸€ä¸ªè¾“å‡ºç¤ºä¾‹ã€‚è®©å¤§æ¨¡å‹ç…§ç€è¾“å‡ºç¤ºä¾‹è¾“å‡ºå°±è¡Œã€‚
  - é™åˆ¶æ¡ä»¶ï¼šè®¾ç½®ä¸€äº›çº¦æŸæ¡ä»¶ï¼Œå¯ä»¥æ˜¯é£æ ¼çš„çº¦æŸï¼Œä¹Ÿå¯ä»¥æ˜¯é€»è¾‘çš„çº¦æŸã€‚
  - æ ·ä¾‹è¾“å‡ºï¼šæä¾›ä¸€ä¸ªæˆ–å¤šä¸ªä¾‹å­å¯ä»¥å¸®åŠ©LLMç†è§£æ‰€æœŸæœ›çš„è¾“å‡ºç±»å‹å’Œè´¨é‡ã€‚
  - ç»“æŸè¯­ï¼šå¦‚æœæœ‰å¿…è¦ï¼Œå¯ä»¥ä½¿ç”¨ç»“æŸè¯­æ¥æ ‡ç¤ºpromptçš„ç»“æŸï¼Œå°¤å…¶æ˜¯åœ¨è¿ç»­çš„å¯¹è¯æˆ–è€…äº¤äº’ä¸­ã€‚
- å¹¶éæ¯ä¸ªæç¤ºéƒ½è¦åŒ…å«æ‰€æœ‰è¦ç´ ã€‚æ ¹æ®å®é™…éœ€æ±‚å’Œåœºæ™¯ï¼Œæ°å½“åœ°ç»„åˆå®ƒä»¬ï¼Œå¯ä»¥å¤§å¤§æé«˜LLMè¿”å›ç»“æœçš„è´¨é‡å’Œç›¸å…³æ€§ã€‚
