---
title: lib-ai-app-community-prompt
tags: [ai, prompt]
created: 2024-09-08T18:56:59.800Z
modified: 2024-09-08T18:57:12.231Z
---

# lib-ai-app-community-prompt

# guide

# discuss-stars
- ## 

- ## 

- ## 

- ## 
# discuss-known
- ## 

- ## 

- ## 

- ## 亚马逊 aws 开源了生成生产级别  Claude Prompt 的 Prompt 生成器 「Meta Prompt 」
- https://x.com/tuturetom/status/1832289213171298811
  - 基于 Human-in-the-Loop 的理念，从初始 Prompt 开始，结合人类和 AI 反馈打分优化，迭代 Prompt 架构和填充优化召回示例
- 怎样用啊？
  - 作为 System  Prompt 传入，然后可以提需求，这样可以生成你想要的 Prompt

# discuss-context
- ## 

- ## 

- ## 

- ## 

- ## [LM Studio and Context Caching (for API) : r/LocalLLaMA _202509](https://www.reddit.com/r/LocalLLaMA/comments/1npatw9/lm_studio_and_context_caching_for_api/)
  - I'm running a Mac, so LM Studio with their MLX support is my go-to for using local models. 
  - When using the LM Studio as a local LLM server that integrates with tools and IDEs (like Zed, Roo, Cline, etc.), things get a bit annoying with the long-context slowdown. 
  - As I understand, it happens for 2 reasons:
  - The previous messages are reprocessed, the more messages, the longer it takes.
  - Especially on the Macs, the longer the context, the slower the generation speed.

# discuss
- ## 

- ## 

- ## 

- ## 
