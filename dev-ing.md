---
title: dev-ing
tags: [dev, dev-log, dev-xp]
created: 2022-05-24T17:52:58.048Z
modified: 2022-05-24T17:53:08.400Z
---

# dev-ing

# guide
- åˆ†ææ ¸å¿ƒéœ€æ±‚å’Œé—®é¢˜ï¼Œæ‹†åˆ†éœ€æ±‚ï¼Œæ¢³ç†ä»»åŠ¡ã€å­ä»»åŠ¡ï¼Œæ’æœŸå¼€å‘
é‡‘ç‘¶ é‚€è¯·æ‚¨åŠ å…¥ã€é‡‘ç‘¶çš„ä¸ªäººä¼šè®®å®¤ã€‘
ç‚¹å‡»é“¾æ¥ç›´æ¥åŠ å…¥è…¾è®¯ä¼šè®®ï¼š
https://meeting.tencent.com/p/9606972663
#è…¾è®¯ä¼šè®®ï¼š960-697-2663

# dev-summary
- module/fwk/server: çµæ´»çš„tag/bookmarkç³»ç»Ÿ, cms, tables, bi

- live-demo/play
  - [Chrome Web AI Demos](https://chrome.dev/web-ai-demos/)

- ç¼–è¾‘å™¨å‚è€ƒ
  - https://atlaskit.atlassian.com/packages/editor/editor-core
  - https://atlaskit.atlassian.com/examples/editor/editor-core/kitchen-sink
  - https://atlaskit.atlassian.com/packages/editor/editor-core/example/full-page-minimal
  - https://atlaskit.atlassian.com/packages/editor/editor-core/example/full-page-with-toolbar
  - https://ckeditor.com/docs/ckeditor5/latest/examples/builds-custom/full-featured-editor.html
  - more-editor
    - https://demo.grammarly.com/

- editor-play
  - https://ckeditor.com/docs/ckeditor5/latest/examples/builds-custom/full-featured-editor.html
  - https://prosemirror.net/examples/
    - https://tiptap.dev/docs/examples/basics/default-text-editor
  - https://codemirror.net/try/
  - https://www.slatejs.org/
  - https://quilljs.com/playground/snow
  - https://playground.lexical.dev/

- code-play
  - https://microsoft.github.io/monaco-editor/playground.html
  - https://play.tailwindcss.com/
  - https://streamlit.io/playground
  - https://arktype.io/playground
  - https://developer.mozilla.org/en-US/play
  - https://play.jqlang.org/
  - https://editor.swagger.io/
  - https://lit.dev/playground/
  - https://playground.solidjs.com/
  - https://angular.dev/playground

- lang-play
  - https://www.typescriptlang.org/play/
  - https://go.dev/play/
  - https://play.rust-lang.org/
  - https://play.kotlinlang.org/
  - https://playground.babylonjs.com/

- ai-play
  - https://www.ibm.com/granite/playground
  - https://playground.liquid.ai/chat
  - https://huggingface.co/chat/
  - https://huggingface.co/playground

- ai-deep-research
  - chat: openai, gemini, mistral, qwen, glm

- play-examples
  - https://playground.powerbi.com/en-us/
# dev-review

```shell
# delete all node_modules folders recursively
find . -name 'node_modules' -type d -prune -exec rm -rf '{}' + && find . -name 'dist' -type d -prune -exec rm -rf '{}' + && find . -name '.next' -type d -prune -exec rm -rf '{}' +
# maybe prefix sudo
find . -name 'node_modules' -type d -prune -exec rm -rf '{}' + 
# æ ¼å¼åŒ–å½“å‰åŒ…ï¼Œæ³¨æ„åœ¨å­æ–‡ä»¶å¤¹æ‰§è¡Œå‘½ä»¤ä¹Ÿä¼šä»package.jsonç›®å½•å¼€å§‹æ ¼å¼åŒ–æ•´ä¸ªåŒ…
prettier --write '**/*.{js,jsx,ts,tsx,json}' --ignore-unknown
eslint --ext .js,.ts,.tsx --quiet --fix . 
# npm i
  DEBUG=* npm i --no-audit --loglevel silly
DEBUG=* npm i --legacy-peer-deps --no-audit --loglevel=silly
DEBUG=* npm i --legacy-peer-deps --no-audit --loglevel=silly --registry=https://registry.npmmirror.com
npm --registry=https://registry.npmmirror.com install   axios
yarn add axios --registry=https://registry.npmjs.org/  
pnpm install --loglevel=debug --registry=https://registry.npmmirror.com  
export https_proxy=http://127.0.0.1:7890;export http_proxy=http://127.0.0.1:7890;export all_proxy=socks5://127.0.0.1:7890
$$('[contenteditable]')
# å…ˆæ‰“å¼€ä¸€æ¬¡discordç¡®ä¿ä¸‹è½½äº†æ›´æ–°
flatpak run com.discordapp.Discord --proxy-server="socks5://127.0.0.1:7897"
betterdiscordctl -i flatpak install
npx create-strapi@latest --ts --use-npm --git-init  --example --skip-cloud --skip-db    --quickstart ./emptyFolder
vite --host 0.0.0.0 --port 8080
serve -p 9000 --cors
HOST=0.0.0.0 PORT=8080 react-scripts start
next dev -H 0.0.0.0 -p 3000
```

- dev-goals ä¸èƒ½åœ¨äº§å“ä¸­æ£€éªŒçš„æŠ€æœ¯ä¸ç©ï¼Œæ³¨æ„äº§å“åŒ–
  - rich-editor: text/block, vanillajs
  - pivot-table: editable
  - collaboration, local-first database
  - flowchart/whiteboard/pdf/annotation/comment
  - äº‹é¡¹--æˆªæ­¢æ—¥æœŸ(0730+ä¼‘æ•´)--é‡è¦æ€§(h/m/l/s1-s3)
- deep into lib/fwk ä¹¦ç±åŸç†ä¸ä»£ç å®è·µè¦åˆ†å¼€, å¯»æ‰¾æ·±å…¥debugçš„çŠ¶æ€, learn-by-debug
  - å­¦ä¹ å·©å›º: å®è·µç»ƒä¹  > æºç /ç¤ºä¾‹ > æ–‡æ¡£/è®ºå› > ç¤¾äº¤åˆ†äº«
  - ä¸è¦ä»ä¸€ä¸ªæƒ³æ³•å¼€å§‹ï¼Œè€Œæ˜¯ä»ä¸€ä¸ªçœŸæ­£çš„é—®é¢˜å¼€å§‹
  - src-code, issues, pr, forks, extensions/alternatives
  - storage, sync/partial, conflicts, consistency
  - ç›´æ¥æ ¹æ®å…·ä½“æ¡†æ¶æˆ–äº§å“æœç´¢è§£å†³æ–¹æ¡ˆå¦‚ airtable-databaseï¼Œä¸å¿…æ‹˜æ³¥äºé€šç”¨æ–¹æ¡ˆå¦‚event-sourcing/eav, åœ¨äº§å“è®¨è®ºä¸­å¸¸æœ‰ç»†èŠ‚å’Œideas
  - è§£å†³æ–¹æ¡ˆåœ¨npm/dockerä¹Ÿå¯ä»¥æœåˆ°ï¼Œä¸”æ›´å‡†ç¡®; å¤šå…³æ³¨åŒ…ç®¡ç†å™¨ä¸Šçš„æœ€æ–°çš„åŒ…
  - github package.json ä¹Ÿèƒ½æœç´¢ç¤ºä¾‹
  - æ‹†åˆ†æ ¸å¿ƒå†…å®¹å’Œå‘¨è¾¹åŠŸèƒ½
    - split git-src and issues/pr/wiki, split txt/docx/xlsx and api
    - å°†æ›´å¤šç²¾åŠ›æŠ•å…¥ core content çš„åˆ›ä½œï¼Œä»¥åŠæ ¼å¼å…¼å®¹ã€å¹³å°å…¼å®¹ã€äº§å“é›†æˆ
  - ä¸å¿…æ‰§ç€äºvanillajsï¼Œå¸¸ç”¨æ¨¡å¼æ—©æ™šä¼šæŠ½è±¡å‡ºå·¥å…·åº“æˆ–æ¡†æ¶ï¼Œå¦‚reactive/effect/ajax/undo
# dev-2026-æ–¹å‘+æ–¹æ³•+æ—¶é—´
- ğŸ‘‰ğŸ» output: ä»£ç äº§å‡ºã€äº§å“è½åœ°ã€ç”Ÿæ€ç§¯ç´¯
- long-term-support
  - cms, airtable, pdf, lowcode/workflow/auto, drive
- techstack-to
  - async/generator, stream, buffer, binary, scheduler, arrow
  - æ ·å¼ç‰‡æ®µä¹Ÿå¯åœ¨çº¿å°è¯•: codepen, w3schools.com 
- separate storage compute example
  - `Lovefield` uses a plug-in architecture for data stores. All data stores implement `lf.BackStore` interface so that query engine can be decoupled from actual storage technology.
- cache/stream for web storage
  - å‚è€ƒ tanstack-query, falcor, localforage
- ğŸ¤” æ”¯æŒåˆ‡æ¢å†…å­˜å’Œå¼‚æ­¥æ•°æ®æºçš„ç¤ºä¾‹
  - tanstack-table external data; ag-grid server-side row model
  - abstract-level, localforage
  - tupledb, tinybase
  - tiddlywiki, react-admin
  - service worker, falcor
- collab-sync, partial-sync
  - string-crdt: ? list-crdt
  - logux: sqlite-persistence, lww-with-hlc
  - verdant/lo-fi: hlc + websocket, no-merkle
  - harika: hlc + sqlite + absurd-sql, no-merkle
  - jaredly/local-first: hlc + rga
  - evolu: hlc + merkle + worker
  - automerge: hypermerge
  - remoteStorage: google-driveã€ç½‘ç›˜ã€ä¸ƒç‰›å¯¹è±¡å­˜å‚¨
  - ä½¿ç”¨hlc: idbsidesync, verdant, harika
  - ç»“åˆhlc+crdt: idbsidesync, evolu, rga-crdt
  - ç»“åˆhlc+db: piratedb, tinybase, kappa-db-stream, linvodb
  - hypercore: partial-sync
- undo/redoä¸branchingå¯æ‹†åˆ†å®ç°
  - undoä¸versioning/historyåŸºäºpersistent data structure
  - branchingä¸mergeå¯åœ¨åº”ç”¨å±‚å®ç°
  - å¤šä¸ªbranchingå¯é€šè¿‡structural sharingå…±äº«æ•°æ®ç»“æ„
- ui: headless-architecture
  - state + action: å‚è€ƒautocompleteã€search-ui
- headlessç»„ä»¶æ˜¯å¦è¡¨æ˜reactå°†viewä¸logicè€¦åˆåœ¨ä¸€èµ·å°è£…ä¸ºcomponentçš„æ€è·¯æ˜¯é”™è¯¯çš„?
  - ä¸viewè§†å›¾æ— å…³çš„componentæœ¬èº«å°±æ˜¯ä¸ªç®€å•çš„å‡½æ•°æˆ–eventemitter-pattern
- è‹¥slate-modelå±‚é‡‡ç”¨æ‰å¹³åŒ–Node(æ‰å¹³åŒ–çš„æ€è·¯å¯å‚è€ƒevent-sourcing/orm/tinybase)
  - å¦‚ä½•ä¿æŒpathå’ŒkeyåŒæ­¥ï¼Œå‚è€ƒ getKeysToPathsTable, getByKeyå®ç°ä¸ŠåŸºäºgetByPath
  - ä¼˜åŒ–æ–¹å‘å¯å‚è€ƒtreeçš„crudåŠåä½œ
  - åä½œæ—¶è¿˜åº”è¯¥è€ƒè™‘ json patch + last-write-win
  - Nodeå®šä¹‰é‡‡ç”¨ast, å¦‚ unist
  - lwwçš„å­—ç¬¦ä¸²æ”¹ä¸ºé’ˆå¯¹crdtä¼˜åŒ–çš„ç±»å‹
- flat-data-modelçš„ç¤ºä¾‹
  - frontend/in-memory databaseï¼Œå¦‚rxdb/pouchdb/tupledb
  - è¿˜å¯ä»¥å‚è€ƒindexeddbç›¸å…³ç¤ºä¾‹ï¼Œå¦‚dexie
  - sqlite-react: vlcn-orm
  - astå¦‚ä½•æ‰å¹³åŒ–
  - å‚è€ƒæ¡ˆä¾‹: treeã€react-admin
- å†…å®¹çš„å­˜å‚¨ä¸æ›´æ–°å¦‚ä½•ä¸æ•°æ®åº“é›†æˆ
  - ç¼–è¾‘å™¨å†…å®¹è‡ªåŠ¨ä¿å­˜ä¸€èˆ¬é€šè¿‡åœ¨onChangeæ–¹æ³•ä¸­æ‰§è¡ŒsaveToDB
    - ä¹Ÿå¯ä»¥åœ¨onChangeæ–¹æ³•ä¸­åˆ›å»ºå†…å­˜dbã€æ›´æ–°ç´¢å¼•ï¼Œé€šè¿‡ç´¢å¼•æé«˜è®¡ç®—æ•ˆç‡
    - åº”è¯¥é¿å…ç»´æŠ¤2ä»½æ•°æ®
  - å°†ç¼–è¾‘å™¨çš„è®¡ç®—å¯†é›†éƒ¨åˆ†çš„æ•°æ®æ¨¡å‹ä¸ä½¿ç”¨æ™®é€šjsonå¯¹è±¡ï¼Œè€Œç›´æ¥ç”¨ç±»ä¼¼æ•°æ®åº“æ¨¡å‹çš„è®¾è®¡
  - ä¸ºäº†æ€§èƒ½ï¼Œå°½é‡ä¸è¦ç›´æ¥è¯»å†™æŒä¹…åŒ–æ•°æ®æºï¼Œè¦ä½¿ç”¨ç¼“å­˜object pool
- why use es6 class
  - è¿è¡Œæ—¶ç±»å‹æ£€æŸ¥ï¼Œinstanceof
    - ä¸šå†…ä¸»æµæ–¹æ¡ˆæ˜¯zod/valibot
  - æ—¢åŒ…å«ç±»å‹å®šä¹‰ï¼ŒåˆåŒ…å«é€»è¾‘å·¥å…·æ–¹æ³•
    - æ³¨æ„classæœ‰æ—¶ä¹Ÿé‡‡ç”¨å…ˆå®šä¹‰interfaceå†å®ç°ï¼Œæ­¤æ—¶ts typeä¹Ÿåˆç†äº†
    - ä½†åº”ç”¨å±‚ä¸šåŠ¡ä»£ç ä¸€èˆ¬ä¸éœ€è¦å®šä¹‰å•ç‹¬interface
  - æ–¹ä¾¿è°ƒè¯•ï¼Œå¯ç›´æ¥logåˆ°å¯¹è±¡åŠæ–¹æ³•ï¼Œå‡½æ•°é‡Œé¢çš„é—­åŒ…å˜é‡æ›´æ–°éš¾ä»¥å®šä½
    - ä¹Ÿå¯æå‰å°†éœ€è¦è°ƒè¯•çš„å±æ€§æˆ–æ–¹æ³•æ·»åŠ åˆ°é—­åŒ…æš´éœ²çš„å¯¹è±¡æˆ–windowä¸Š
    - é—­åŒ…å®ç°çš„ç§æœ‰å±æ€§æ›´å®‰å…¨
- dev-xp-editor
  - ä¸ä»…è¦ä¿æŒç¼–è¾‘å™¨å†…å®¹å’Œè§†å›¾åŒæ­¥ï¼Œè¿˜è¦ä¿æŒé€‰åŒºå’Œå†…å®¹åŒæ­¥
  - ç¼–è¾‘å™¨å¤–éƒ¨ç›¸å…³é¢æ¿çš„ååŒäº§å“è¾ƒå°‘ï¼Œå¦‚è¯„è®º

## ing

- yaoo-proj
  - ~~prosemirror/codemirror + comfyui~~
  - ~~codemirror-devtools~~
- not-yet
  - ~~elmesque-editor~~, åŸºäºimmutableæ€æƒ³å®ç°çš„ç¼–è¾‘å™¨å¤§å¤šé‡‡ç”¨redux/elmé£æ ¼
  - branching/versioned-doc
  - pouchdb + kappa-crdt + eav => pouchdb-crdt-eav
  - åšå®Œtailwind-tableå°±é¢è¯•
- dev-to æç‚¼æ ¸å¿ƒ`éœ€æ±‚+äº§å‡º`å·¥ä½œæµï¼Œä¸èƒ½åœ¨äº§å“ä¸­æ£€éªŒçš„æŠ€æœ¯ä¸ç©
# dev-03

```log //dev-xp
console.log('; ; task ', taskState, runningTaskAction, task?.task_steps)
^((?!(42\["heartbeat|resourceMonit|refreshXtermCols)).)*$
^(?!42\["resourceMonit).* 
/syncUpdates|syncOTUpdates/

```
```log //ai
- introduce reactjs in less than 90 words
- introduce reactjs with hello world code example
- count from 1 to 80, every number on a separate line
- count from zz1 to zz40, every item on a separate line  /no_think
- when did deepseek v3.1 model release?
- when did qwen3-coder model release?
- what is the weather in guangzhou china? give me some food and outdoor-activities suggestions according to weather temperature

- what is the weather in Hainan? what clothes should i wear?

- what is the dom api compatibility in common browsers

- explain code at ./playground/quick-sort.js
- for file ./playground/quick-sort.js, add a simple test case

- image-lawn
  - a big park for resting and relaxing, there are little trees around a big lawn, some birds are resting in the lawn, The lawn and the trees around it both need pruning
- image-logo-excel-like
  - create a product logo for my excel-like webapp, 
  - the logo brand color should be like green/teal/indigo/..., or any good color that giving a cold and formal feeling, 
  - the logo should express rows or columns or grid, but logo should not be complicated,
add action to add datetime at top of readme.md
add action to create quickSort1.mjs and try to implement quick sort algorithm in less than 60 lines
<!-- ğŸ› -->
use vanilla html/css/javascript to create a simplistic personal profile landing page: homepage shows a big welcoming greeting, then shows 2 example personal projects, then a simple get in touch example email below it
use vanilla html/css/javascript to create a personal profile landing page: homepage shows a cool welcoming animation, then shows 4 example personal projects, then a simple get in touch form below it
use react to create a homepage shows a list of frontend frameworks like react/vue/angular, when clicking the framework, navigate to the route to show its introduction

- You are a professional python/typescript/nodejs fullstack developer.
  - provide a comprehensive overview of this project. analyze the project architecture and primary user journey use cases. then find some important source code to explain the core architecture and data flow.
  - i want more details about rag ingest/chunking/embedding/persistence logic
  - how to refactor related source code to support more vector db including qdrant/chromadb/pgvector? show me some code and explain the architecture migration
- analyze the project architecture and related code. explain the core architecture and data flow

- You are a professional python/typescript/nodejs fullstack developer and a system administrator.
  - I want to run this project fully locally without docker and nginx for local development and debugging.
  - please read setup-related files like docker-compose.yaml and README.md, and tell me step by step how to configure and run frontend/backend locally.
  - i have already installed python/uv/npm/chromadb/Ollama/postgresql/mysql/redis on my local macos.
  - just tell me steps and commands to set up backend/frontend, i will start services like db/redis later.

- ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šä¸”å‹å–„çš„ AI åŠ©æ‰‹ã€‚ä½ çš„å›åº”åº”è¯¥ï¼š 1. ä½¿ç”¨ç®€ä½“ä¸­æ–‡å›ç­” 2. å½“éœ€è¦å±•ç¤ºä»£ç æ—¶ï¼Œä½¿ç”¨é€‚å½“çš„è¯­æ³•é«˜äº®ï¼ˆå¦‚ typescript, python, javascript ç­‰ï¼‰ 3. å½“éœ€è¦è§£é‡Šå¤æ‚æ¦‚å¿µæ—¶ï¼Œå¯ä»¥ä½¿ç”¨ Mermaid å›¾è¡¨ 4. å½“æ¶‰åŠæ•°å­¦å…¬å¼æ—¶ï¼Œä½¿ç”¨ LaTeX è¯­æ³• 5. ä¿æŒå›åº”ç®€æ´æ˜ç¡®ï¼Œé€‚æ—¶ä½¿ç”¨åˆ—è¡¨å’Œè¡¨æ ¼æ¥ç»„ç»‡ä¿¡æ¯. 
  - è¯·æŒ‰ä»¥ä¸Šè¦æ±‚ä»‹ç»reactjså‰ç«¯æ¡†æ¶

- the northwinds excel contains the sales data for a company called Northwind Traders, which imports and exports specialty foods from around the world. 
  - The sales team wants to identify for which month they perform well and bad in 2014.
  - please give me the result with tables and plots

- you can add more debugging logs to make it easy to trace data flow, then i restarted and give the log to you. 

```

```rules
- when writing or editing large files, it's easy to fail if the content to write is large. you can write small edits many times to avoid large edits.

```

- [guide : running gpt-oss with llama.cpp Â· ggml-org/llama.cpp Â· Discussion _202508](https://github.com/ggml-org/llama.cpp/discussions/15396)

```sh /llm

# cd ~/Documents/repos/ai-ml-llm/done-hub-local && dist/one-api --config config.yaml

launchctl stop com.donehub && launchctl start com.donehub

ollama run --verbose gemma3:4b
OLLAMA_DEBUG=2 ollama serve gemma3:4b

# llama-server -m model.gguf --port 8080 --alias name

./build/bin/llama-server -m ~/.lmstudio/models/ 

# llama-cli -m model.gguf -cnv --chat-template chatml

./build/bin/llama-cli -m ~/.lmstudio/models/mradermacher/merged-mermaid-7b-GGUF/merged-mermaid-7b. Q6_K.gguf
llama-cli -hf ggml-org/gemma-3-1b-it-GGUF

VLLM_LOGGING_LEVEL=debug VLLM_CONFIGURE_LOGGING=1 vllm serve RUC-DataLab/DeepAnalyze-8B --max-num-batched-tokens 40000 --max-model-len 28000 --enable-log-requests --enable-log-outputs --enable-prompt-tokens-details --uvicorn-log-level debug 

cd ~/Documents/opt/compiled/qdrant && ./qdrant

```

```sh /image
cd ~/Documents/opt/compiled/zimage && ./ZImageCLI -m mzbac/Z-Image-Turbo-8bit -o ~/Pictures/test11.png -W 512 -H 512 -s 7 -p "ä¸€ä½å¸…å“¥å’Œä»–çš„å® ç‰©ç‹—ç©¿ç€é…å¥—çš„æœè£…å‚åŠ ç‹—ç‹—ç§€èŠ‚ç›®ï¼Œå®¤å†…ç¯å…‰ï¼ŒèƒŒæ™¯ä¸­æœ‰è§‚ä¼—ã€‚"

cd ~/Documents/opt/compiled/zimage && ./ZImageCLI -m mzbac/Z-Image-Turbo-8bit -o ~/Pictures/test11.png -W 1024 -H 1024 -s 9 -p "
```

- goal-to å¢å¼ºç‰¹è‰²
  - editor
  - crud
  - ä¸šåŠ¡ç³»ç»Ÿä¸æ¶æ„å¢å¼º: lasuite-docs, knowledgebase, cms
  - ğŸ“Œ pdf-editor
  - ğŸ“Œ export-ppt
  - ğŸ“Œ fix text/Chinese
  - translation: ä¸€é”®ç¿»è¯‘ pdf/ppt
  - reuse bg/graphics/fonts

- dev-to
  - ?
- dev-log
  - ?

## 0301

- latest chromium/chrome has experimental features like ai/split-view, how can i enable these features automatically on my custom electron app? explain to me and give me some docs and urls for references
  - enable the Chromium flags you need at app startup (via `app.commandLine.appendSwitch`) and/or turn on Renderer-level runtime flags via BrowserWindow's `webPreferences` (experimentalFeatures / enableBlinkFeatures).
- Electron is built on Chromium's web rendering engine, not the Chrome browser UI. Because "Split Tabs" is strictly a Chrome Browser UI feature, appending the split-view flag in Electron will do absolutely nothing.
  - To implement a split-view in Electron, you don't use flags; you build it natively using Electron's `WebContentsView` API (the modern replacement for BrowserView). This allows you to embed multiple independent web views side-by-side in a single window.
- Google Chrome recently introduced a built-in AI API (window.ai.languageModel) powered by a local Gemini Nano model.
  - While appending the flags above will successfully expose the window.ai JavaScript API bindings in your Electron renderer process, it will not actually work out-of-the-box.
  - Google Chrome downloads the massive gigabyte-sized Gemini Nano model weights dynamically using Chrome's proprietary "Component Updater" (chrome://components). Electron does not include this component updater or Google's proprietary server connections, meaning the model will never download and the API will return a "No model available" error
- the official Electron maintainers created an experimental library called @electron/llm
  - It perfectly mimics Chromium's window.ai API but allows you to bundle and load your own local .gguf open-source models (like Llama 3) entirely offline
# dev-02-vlm-powerful-than-ocr-&-code-indexing-mcp-&-unify-code-search-and-rag-with-skills-&-context-engine-and-memsearch-qmd

## 0228

- [How do I bring back the full URL in the address bar? I want to see https://www.example.com : r/firefox](https://www.reddit.com/r/firefox/comments/1moorm2/how_do_i_bring_back_the_full_url_in_the_address/)
  - In about:config, toggle browser.urlbar.trimURLs to false.  

## 0224

- ä»‹ç»ä¸‹ MINISFORUM é“­å‡¡ è¿™å®¶å…¬å¸ï¼ŒåŠä¸»åŠ›äº§å“ï¼Œ èèµ„å†ç¨‹ï¼ŒæŠ•èµ„å…³ç³»ï¼Œ å‘å±•å‰æ™¯
  - MINISFORUMé“­å‡¡æ˜¯æ·±åœ³å¸‚ç¾é«˜ç”µå­è®¾å¤‡æœ‰é™å…¬å¸æ——ä¸‹çš„æ•°ç å“ç‰Œï¼Œæˆç«‹äº2012å¹´ï¼ˆå…¬å¸ä¸»ä½“ï¼‰ï¼Œå¹¶äº2018å¹´æ­£å¼ä»¥MINISFORUMå“ç‰Œè¿è¥ã€‚å“ç‰Œäº2022å¹´11æœˆå…¬å¸ƒäº†ä¸­æ–‡åâ€œé“­å‡¡â€ã€‚å…¬å¸æ€»éƒ¨ä½äºæ·±åœ³ï¼Œæ˜¯ä¸€å®¶ä¸“æ³¨äºé«˜æ€§èƒ½å¾®å‹è®¡ç®—æœºç ”å‘ã€ç”Ÿäº§å’Œé”€å”®çš„æŠ€æœ¯ä¼ä¸š
  - ä¸“æ³¨AMDå¹³å°ï¼šç›¸æ¯”Intel NUCï¼Œé“­å‡¡æ›´ä¸“æ³¨äºAMDé”é¾™å¤„ç†å™¨ï¼Œæä¾›æ›´é«˜æ€§ä»·æ¯”çš„CPUé…ç½® 
  - æå®¢å®šä½ï¼šäº§å“å‡ä»·é«˜äºè”æƒ³ã€æˆ´å°”ç­‰å·¨å¤´ï¼Œå¸¸é‡‡ç”¨300ç¾å…ƒçº§åˆ«çš„é«˜ç«¯CPUï¼ˆå¦‚R9 7940HSï¼‰ï¼Œè€Œéä¸»æµçš„150ç¾å…ƒCPU
  - è¶…è¿‡80%çš„æ”¶å…¥æ¥è‡ªæµ·å¤–å¸‚åœº
  - å…¬å¸æ›´å¤šä»¥è‡ªæœ‰å“ç‰Œä¸ OEM/ODM ç”Ÿäº§ã€å¹¶é€šè¿‡å…¨çƒåˆ†é”€ä¸ç›´é”€æ¸ é“ï¼ˆå®˜ç½‘ã€æµ·å¤–é›¶å”®å•†ä¸å¹³å°ï¼‰æ”¯æ’‘æˆé•¿ï¼Œè€Œéæ˜æ˜¾é æœŸæƒ/é£æŠ•é©±åŠ¨çš„æ‰©å¼ è·¯å¾„ã€‚

## 0223

- search rag-skill, + rag-cli

## 0222

- aionui + notebooklm-like-search

## 0221

- coding-agentçš„
  - ä»£ç æœç´¢å¢å¼º, å¯å‚è€ƒ (augment-)context-engine, chunkhound
  - æ–‡æœ¬æœç´¢å¢å¼º, å¯å‚è€ƒ qmd(ä»…å‘½ä»¤è¡Œ), mgrep, notebooklm

- this project provides MCP/cli for code search.
  - the legacy cli `scripts/ctx.py` only implements partial features of mcp servers.
  - in current git uncommited files, it's a new cli in a new separate folder `cli` that has implemented full features of code search and auto reindexing, using programatic function call and import useful existing code/lib instead of using mcp api call. features related to memory/context enhancement are not required to implement, focusing on good code search first.
  - there is a new skill at `skills/context-engine-cli/SKILL.md` to provide the new cli for other agent clients.

- the goal is to review the uncommited code and refactor the new cli at `cli` to be pip installable, while making logic correct, code reuse, extensible, maintainable.
  - after refacotring, the new cli should be installable by `pip install -e .` or `uv tool install -e .` , and the cli name should be `power-context`, usage should be like `power-context search "auth middleware"`. rename the skill name to be `power-context`.
  - analyze the architecture and core data flow, refactor the new cli and make sure the new cli code indexing and search is correct and clear. finally, provide updated agent skills .

## 0220

- `pip install -e .` can install a package globally in every terminal cli. when i run `uv pip install -e .`, my package is not available from terminal . how to use `uv` to install a package Install in editable mode globally?

```sh
# --editable, -e: Install the target package in editable mode, such that changes in the package's source directory are reflected without reinstallation
uv tool install -e .

uv tool update-shell

uv tool list

```

- `git add -A` vs `git add .`
  - git add .: Stages changes in the current directory and its subfolders. If you are in a subfolder (e.g., /src/components), it will only add files inside that folder. It ignores changes in the root or other sibling folders.
  - git add -A(--all): Stages changes in the entire repository.

- this project provides MCP/cli for code search. the existing cli `scripts/ctx.py` only implements partial features of mcp servers. please write a new cli in a new separate folder that implements full features of code search and auto reindexing, using programatic function call and import useful existing code/lib instead of using mcp api call. features related to memory/context enhancement are not required to implement, focusing on good code search first. then write a new skill at `skills/context-engine-cli/SKILL.md` to provide the new cli for other agent clients.

- this project provides MCP/cli for code search.
  - the existing cli `scripts/ctx.py` only implements partial features of mcp servers.
  - in current git uncommited files, it's a new cli in a new separate folder `cli` that has implemented full features of code search and auto reindexing as `power-context`, using programatic function call and import useful existing code/lib instead of using mcp api call. features related to memory/context enhancement are not required to implement, focusing on good code search first.
  - there is a new skill at `skills/power-context/SKILL.md` to provide the new cli for other agent clients.

- the goal is to review the uncommited code and provide feedback for improvements, focusing on logic correctness, code reuse, extensibility, maintainability.
  - review the mcp implementation and the new `power-context` cli implementation, recheck code resuse and extensibility. analyze the architecture and core data flow, make sure the new cli code indexing and search is correct and clear. provide concise agent skills at `skills/power-context/SKILL.md`, removing commands that are unrelated to code indexing and search.

## 0219

- ğŸ¤” There are so many cli coding agents like claude-code/codex-cli/gemini-cli, is there any popular solution to unify the chat message into an interoperable format? for example, unify/export/manage claude code message and codex cli message.  Is there any popular open source solution to this problem? if there is, list github repo and provide description for each.
  - thereâ€™s no single â€œone trueâ€ universal format yet â€” but two practical standards have emerged and are already used by many projects, and a few open-source tools already act as adapters/bridges.
  - If you want one place to start, treat the Model Context Protocol (MCP) as the canonical message/tool-call wire format and use AGENTS.md for repo-level agent instructions; then plug adapters (or use projects that already do this) so different CLIs/agents can interoperate.
- AGENTS.md doesnâ€™t replace a wire protocol, but it unifies how repositories present instructions and constraints to different agents. Good to combine with MCP for behavior + tooling.
- https://github.com/open-voice-interoperability/openfloor-docs
  - This repository contains the specification and schemas for the Open-Floor Protocol family of Vendor-Independent specifications and supporting schemas developed by the Voiceinteroperability.ai initiative
  - The Open-Floor Conversation Envelope is a universal JSON structure whose purpose is to allow human or automatic agents (assistants) to interoperably participate in a conversation. When coupled with a specific protocol, such as HTTPS, a dialog agent that can generate and send Conversation Envelopes is capable of inter-operating with any other Open-Floor-compliant agent, regardless of the technology or architecture on which that other agent is based.

- Model Context Protocol (MCP) : An open-source standard (maintained by Anthropic) for connecting AI applications to external systems. While not specifically for chat history export, it provides a standardized way for agents to interact with tools and data sources, creating a foundation for interoperability. Claude Code, Gemini CLI, and Codex CLI all support MCP integrations.

## 0216

- ğŸ¤” when i add a mcp to claude code like `claude mcp add n8n-mcp`, how does agent know when to use the mcp tools and what mcp tools are available? is it required to add  explicit mcp instructions to CLAUDE.md ?  there are many open source mcp on github, after adding some by commands like `claude mcp add n8n-mcp`, i am curious the internals about the mcp integration. does opencode-cli work the similar way?
- How the Agent Discovers Available MCP Tools
  - When you run claude mcp add n8n-mcp, here's what happens under the hood:
  - Registration Phase: This stores the MCP server configuration in Claude Code's settings 
  - Tool Discovery Phase (At Startup): When Claude Code starts a session, it Launches each registered MCP server as a subprocess (or connects via SSE/HTTP for remote servers), Calls tools/list on each MCP server via the MCP protocol â€” this is a standardized JSON-RPC method. These tool definitions are injected into the model's context â€” they become part of the tool-use schema that Claude (the LLM) sees. The model sees them exactly the same way it sees built-in tools like Read, Write, Bash, etc.
- How the Agent Knows When to Use MCP Tools
  - The agent relies entirely on the tool descriptions and names. There's no magic routing â€” it works the same way all tool use works with LLMs. This is why good tool descriptions in MCP servers are critical.
- Do You Need to Add Instructions to CLAUDE.md?
  - It's NOT required, but it's highly recommended for complex MCP setups.
  - Without CLAUDE.md instructions: The model only has the tool names and descriptions from the MCP server
  - It may not know the preferred workflow or when to prefer one tool over another

- Does opencode-cli Work the Same Way?
  - Yes, opencode-cli follows the same MCP protocol, since MCP is a standardized protocol. The core mechanics are the same:
  - You configure MCP servers in opencode's config (typically opencode.json or similar)
  - At startup, it connects to MCP servers and calls tools/list
  - Tool definitions are injected into the LLM's context
  - The LLM decides when to call them based on descriptions
- The MCP protocol is model-agnostic â€” any agent framework that implements the MCP client spec will discover and call tools the same way. The only difference is which LLM is doing the reasoning about when to use which tool.

## 0215

- Still the same category of errors â€” WiredMemoryTicket, ToolCallFormat, ToolCallParser are all types that exist in mlx-swift 0.30.6 but not in 0.30.3. Even this older commit of mlx-swift-lm already depends on them.
  - The root cause is clear: the Package.resolved in the repo is stale. The CI passes because it resolves fresh and gets mlx-swift 0.30.6. Trying to find an mlx-swift-lm commit old enough to work with 0.30.3 would mean going back very far and likely breaking FlowDown's own code that depends on newer mlx-swift-lm APIs.
  - The correct fix is the other direction â€” update Package.resolved to use mlx-swift 0.30.6. Run these commands:
  - rm FlowDown.xcworkspace/xcshareddata/swiftpm/Package.resolved
  - rm FlowDown.xcodeproj/project.xcworkspace/xcshareddata/swiftpm/Package.resolved
  - rm -rf build 
  - xcodebuild build

```sh
# FlowDown built from source and debugging

xcodebuild build \
    -workspace FlowDown.xcworkspace \
    -scheme FlowDown \
    -configuration Debug \
    -destination 'generic/platform=macOS' \
    -derivedDataPath ./build \
    CODE_SIGN_IDENTITY=- \
    CODE_SIGN_STYLE=Manual \
    CODE_SIGNING_REQUIRED=NO \
    CODE_SIGNING_ALLOWED=NO \
    COMPILER_INDEX_STORE_ENABLE=NO

lldb ./build/Build/Products/Debug-maccatalyst/FlowDown.app/Contents/MacOS/FlowDown

run

bt

cp -R ./build/Build/Products/Release-maccatalyst/FlowDown.app /Applications/

xattr -cr /Applications/FlowDown.app
```

## 0213

- {"type":"10163", "error":{"type":"system_error", "message":"Provider API error: xunfei response error: sid: cht000bd483@dx19c584c088ab8aa700 msg: RequestParamsError:(02:38:24.169) '$.parameter.cbm.max_tokens' value must be less or equal than 16384; (request id: 20260214023823724232000Scx7MLQe)"}}
  - set max_tokens or remove it

## 0211

- the `README.md` shows how to setup app with swift impelmentation at `Build from Source`. please add `docs-mlx-py.md` to guide the user how to setup with mlx python backend.

- file `dev-arch-gguf-mlx.md` describes the core architecture for this llm chat app. analyze how the mlx swift server is implemented and integrated into the backend. please write a similar python implementation to replace the swift implementation. You should keep the existing features still working, and keep as many code as possible unchanged, then add a environment variable `ENABLE_MLX_PY_BACKEND`, if it's set to true, use python impelementation instead of the swift implementation. if it's set to false or unset, existing swift implementation is still used. 
  - it's better to implement the python package as a separate module so that it's easy to maintain and keep changes to existing code as small as possible.
  - avoid writing python implementation from scratch, use project mlx-openai-server at folder `/Users/yaoo/Documents/repos/ai-ml-llm/all-runtime-mlx/mlx-openai-server` as a util/library, analyze its architecture, write a wrapper or glue code to implement the features like swift. make it easy for me to update the wrapper/glue code if mlx-openai-server code is updated.

- this project is a llm chat app built with tauri. it supports using external llm api like openai/mistral api, as well as running local .gguf models and mlx models to provide api. 
  - please analyze the project architecture and related code. explain the core architecture and data flow for the extensible architecture to support local models in different formats. write your result to dev-arch-gguf-mlx.md. 
  - there are different approaches to run local models. is it possible to write a extensible module to make the logic of running swift mlx server swappable. i want to use python mlx-server implementation to replace the swift implementation.

- ğŸ¤” when i use claude-code-cli on my mac, why is it always using sonnet? will it switch between sonnet and opus automatically according to task complexity just like it uses haiku automatically ? 
  - Claude Code does NOT automatically switch between Sonnet and Opus based on task complexity.
  - Main model (Sonnet 4) â€” Used for the primary agent loop (reasoning, coding, tool use)
  - Haiku â€” Used automatically for lightweight/cheap subtasks like: File summarization Background context gathering Simple classifications
  - Unlike the Sonnet â†’ Haiku auto-switching, there is no automatic escalation to Opus for complex tasks. Opus is only used if you explicitly request it.

- i want to run a local openai-compatible api . project mlx-lm and project mlx-openai-server both provide a solution. you can read mlx-lm/SERVER.md and mlx-openai-server/README.md. please analyze the code and architecture, compare the  solutions to help me choose one.

- ğŸ¤” can i open multiple terminals on different folders, and run multiple claude code at the same time to work on different projects? will messages get messy and confused?
  - Yes, You Can Run Multiple Claude Code Instances Simultaneously
  - Each terminal session runs as a completely independent instance.
- what if i open multiple terminals on the same folder, and run claude code on each terminal? 
  - Yes, you can do it, but it gets risky.
  - Each terminal is still its own independent process with its own conversation. Claude won't confuse who said what across sessions.
  - But the Real Problem: File Conflicts

- ğŸ¤” i want to package python backend code into stand-alone executables.  what's the most popular solutions? pyinstaller is licensed under GPL, why is it popular?
  - PyInstaller comes with a special "Bootloader Exception" (similar to the GCC Runtime Library Exception). 
  - The Bootloader Exception: The actual executable you generate contains a small piece of binary code called the "bootloader" (which sets up the environment and runs your script). The authors explicitly grant an exception that allows you to link this bootloader with your own proprietary code without triggering the GPL requirement to open-source your app.
  - PyInstaller has a massive library of "hooks" that automatically find hidden imports for complex libraries like pandas or matplotlib, which often break in other packagers.

## 0208

- context-engine for claude/MCP-client

```sh
cd ~/Documents/opt/compiled/qdrant && ./qdrant

uv run --env-file .env -- python -m scripts.mcp_memory_server
uv run --env-file .env -- python -m scripts.mcp_indexer_server

FASTMCP_TRANSPORT=http FASTMCP_PORT=8002 uv run --env-file .env -- python -m scripts.mcp_memory_server

FASTMCP_TRANSPORT=http FASTMCP_INDEXER_PORT=8003 uv run --env-file .env -- python -m scripts.mcp_indexer_server

uv run --env-file .env -- python -m scripts.ingest_code --root /Users/yaoo/Documents/repos/ai-ml-llm/aionui-office

# æ›´æ”¹vectorç›¸å…³é…ç½®æ—¶ï¼Œéœ€è¦
curl -X DELETE http://localhost:6333/collections/codebase
uv run --env-file .env -- python -m scripts.ingest_code --root /Users/yaoo/Documents/repos/ai-ml-llm/aionui-office --recreate

# --scope local (default): Available only to you in the current project
# --scope project: Shared with everyone in the project via .mcp.json file
claude mcp add --transport http --scope project qdrant-indexer http://localhost:8003/mcp

```

- test mcp
  - Both servers are responding correctly â€” the initialize handshake succeeds on both ports. So the servers themselves are fine.
  - ğŸ’¡ The issue is on the Claude Code side. 
  - æ’æŸ¥æ–¹å‘è½¬ä¸º æ¨¡å‹çš„ mcp tool call æ˜¯å¦èƒ½æˆåŠŸè°ƒç”¨ï¼Œ å®æµ‹è®¯é£æ¨¡å‹kimiè°ƒç”¨ä¼šå¤±è´¥
  - Claude Code won't automatically use MCP tools for every question. The model decides based on relevance. Try being explicit: "Use the repo_search tool to search for 'authentication'" or "Use the qdrant-indexer MCP server to find code related to embeddings".

```sh
# test mcp connection
curl -s -X POST -H "Content-Type: application/json" -H "Accept: application/json, text/event-stream" http://localhost:8003/mcp -d '{"jsonrpc":"2.0","method":"initialize","params":{"protocolVersion":"2024- 11-05","capabilities":{},"clientInfo":{"name":"test","version":"0.1"}},"id":1}'

# event: message
# data: {"jsonrpc":"2.0","id":1,"result":{"protocolVersion":"2025-06-18","capabilities":{"experimental":{},"prompts":{"listChanged":false},"resources":{"subscribe":false,"listChanged":false},"tools":{"listChanged":false}},"serverInfo":{"name":"qdrant-indexer-mcp","version":"1.17.0"}}}
```

- You are a professional python/typescript/nodejs fullstack developer and a system administrator.
  - I want to run this project fully locally without docker and nginx for local development and debugging.
  - please read setup-related files like docker-compose.yml/README.md/docs/ARCHITECTURE.md or other files, and tell me step by step how to configure and run this project as MCP server locally. i want to self host this mcp server and use it with local claude-code.
  - i have already installed python/uv/npm/QdrantDB/Ollama/postgresql/mysql/redis on my local macos.
  - just tell me steps and commands to set up backend/frontend, i will start services like db/redis later.

- [Chroma Architecture](https://docs.trychroma.com/docs/overview/architecture)
  - Chroma delegates, as much as possible, problems of data durability to trusted sub-systems such as SQLite and Cloud Object Storage, focusing the system design on core problems of data management and information retrieval.
  - In Local and Single Node mode, all components share a process and use the local filesystem for durability.
  - v0.4.0_202307
    - drops duckdb and clickhouse in favor of sqlite for metadata storage. 

## 0206

- stream disconnected before completion: stream closed before response.completed
  - ä½¿ç”¨ custom type è€Œä¸æ˜¯ codex, ä¸è¦å¼€å¯ responses api çš„å…¼å®¹æ¨¡å¼

- stream disconnected before completion: failed to parse ResponseCompleted: missing field `id`.
  - ä¸è¦å¼€å¯ responses api çš„å…¼å®¹æ¨¡å¼ï¼Œ è€Œåº”è¯¥æ”¹å˜provider typeä¸º codex

- sensitive_words_detected
  - æ¢ç”¨å›½äº§æ¨¡å‹

- stream disconnected before completion: error sending request for url (https://127.0.0.1:4090/v1/ responses)
  - claude codeåˆ†æåˆ°åŸå› , åº”è¯¥ç”¨http, è€Œä¸æ˜¯https

- ğŸ¤” i want to download and run gguf model locally. which one should i use 
  - https://huggingface.co/unsloth/Qwen3-1.7B-GGUF
  - https://huggingface.co/unsloth/Qwen3-1.7B-unsloth-bnb-4bit
- bnb 4-bit safetensors release: 
  - Itâ€™s a bitsandbytes 4-bit quant thatâ€™s intended for GPU inference with transformers + bitsandbytes
  - bnb-4bit: smaller VRAM usage and faster on GPU, but requires CUDA, bitsandbytes, and recent transformers/accelerate. Not suitable for CPU-only runs.

- [ä¸­å›½é€ å‡ºâ€œæ··çººAIé«˜å…‰è°±å…‰é€‰æœºâ€ï¼Œçˆ†æ”¹å…¨çƒåƒåœ¾åœºï¼Œè€å¤–çœ‹æ‡µäº†ï¼_å“”å“©å“”å“©_bilibili](https://www.bilibili.com/video/BV1gCBSBDEPB/)
  - å¼“å¶ç§‘æŠ€
  - [ä¸­å›½å¥³åšå£«â€œæ‹£åƒåœ¾â€ï¼Œç™»ä¸Šã€Šæ—¶ä»£ã€‹å¹´åº¦æ¦œå• - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/2000182014125564584)

## 0201

- ç¾å›½æœ€å¤§ç”µå­é›¶å”®å•†å¤„ç†æ‘¸æ‘¸æœº, æŒ‡çš„æ˜¯é‚£ä¸ªå¹³å°æˆ–ç½‘ç«™
  - Best Buy (ç™¾æ€ä¹°)ï¼šæ˜¯ç¾å›½æœ€å¤§çš„æ¶ˆè´¹ç”µå­é›¶å”®å•†ï¼ˆå®ä½“+çº¿ä¸Šï¼‰ã€‚
  - è€Œåœ¨æµ·æ·˜åœˆæˆ–æ•°ç çˆ±å¥½è€…å£ä¸­ï¼Œä¸“é—¨å¤„ç†â€œæ‘¸æ‘¸æœºâ€ï¼ˆå³è¢«é¡¾å®¢é€€è´§ã€ä»…å¼€ç®±æˆ–çŸ­æš‚ä½¿ç”¨è¿‡çš„äº§å“ï¼‰çš„å¹³å°ï¼Œé€šå¸¸æŒ‡çš„æ˜¯ Best Buy å®˜ç½‘æ——ä¸‹çš„ Open-Box æ¿å—ã€‚
  - Open-Box (å¼€ç®±ç‰ˆ)ï¼šè¿™æ˜¯ Best Buy å®˜ç½‘ä¸Šä¸€ä¸ªä¸“é—¨çš„åˆ†ç±»ã€‚ç”±äºç¾å›½é€€è´§æ”¿ç­–å®½æ¾ï¼ˆé€šå¸¸ä¸º14-30å¤©æ— ç†ç”±é€€è´§ï¼‰ï¼Œè®¸å¤šé¡¾å®¢ä¹°å›å®¶æ‹†å°è¯•ç”¨å‡ å¤©åé€€è´§ï¼Œè¿™äº›äº§å“ç»è¿‡æ£€æµ‹åä¼šé‡æ–°ä¸Šæ¶é”€å”®ï¼Œä»·æ ¼é€šå¸¸æ¯”å…¨æ–°ç‰ˆä¾¿å®œå¾ˆå¤šã€‚
  - ä¸ºä»€ä¹ˆå«â€œæ‘¸æ‘¸æœºâ€ï¼šå› ä¸ºè¿™äº›æœºå™¨è¢«ä¸Šä¸€ä»»ä¹°å®¶â€œæ‘¸è¿‡â€ï¼ˆä½¿ç”¨è¿‡ï¼‰ï¼Œæ‰€ä»¥ä¸­æ–‡ç¤¾åŒºæˆç§°ä¸ºâ€œæ‘¸æ‘¸æœºâ€ã€‚
  - CowBoom æ›¾æ˜¯ Best Buy æ——ä¸‹çš„ç‹¬ç«‹æ¸…ä»“ç½‘ç«™ï¼Œä¸“é—¨ç”¨æ¥ä»¥æä½çš„ä»·æ ¼ç”©å–äºŒæ‰‹ã€ç¿»æ–°å’Œå¼€ç®±äº§å“ã€‚ ç°çŠ¶ï¼šè¯¥ç½‘ç«™å·²äºå‡ å¹´å‰è¢«å…³é—­ï¼Œç›¸å…³ä¸šåŠ¡å·²å®Œå…¨æ•´åˆå› Best Buy å®˜ç½‘çš„ Outlet / Open-Box é¢‘é“ ä»¥åŠ Best Buy åœ¨ eBay ä¸Šçš„å®˜æ–¹åº—ã€‚
- åœ¨ Best Buy è´­ä¹°â€œæ‘¸æ‘¸æœºâ€æ—¶ï¼Œé€šå¸¸ä¼šçœ‹åˆ°ä»¥ä¸‹ç­‰çº§ï¼Œæˆè‰²è¶Šå·®ä»·æ ¼è¶Šä½ï¼š
  - Excellent - Certifiedï¼šå®˜æ–¹è®¤è¯ï¼Œæ¥è¿‘å…¨æ–°ï¼Œé€šå¸¸æœ‰åŸç›’å’Œå®Œæ•´é…ä»¶ã€‚
  - Excellentï¼šå¤–è§‚çœ‹èµ·æ¥åƒæ–°çš„ï¼Œæ— æ˜æ˜¾åˆ’ç—•ï¼Œé…ä»¶é½å…¨ã€‚
  - Satisfactoryï¼šæœ‰è½»å¾®åˆ’ç—•æˆ–ä½¿ç”¨ç—•è¿¹ï¼Œå¯èƒ½ç¼ºå°‘éå¿…è¦é…ä»¶ã€‚
  - Fairï¼šæœ‰æ˜æ˜¾åˆ’ç—•æˆ–å‡¹ç—•ï¼Œå¯èƒ½ç¼ºå°‘é‡è¦é…ä»¶ï¼Œä½†åŠŸèƒ½æ­£å¸¸ã€‚
- å¦ä¸€ä¸ªå¸¸è§å¹³å°ï¼šAmazon Warehouse (ç°æ›´åä¸º Amazon Resale)
  - äºšé©¬é€Šå¤„ç†é€€è´§äº§å“çš„éƒ¨é—¨åŸåå« Amazon Warehouseï¼ˆæœ€è¿‘æ”¹åä¸º Amazon Resale/Renewedï¼‰ã€‚å¾ˆå¤šæµ·æ·˜ç”¨æˆ·ä¹Ÿä¼šå»é‚£é‡Œæ·˜äºŒæ‰‹æ•°ç äº§å“ã€‚
# dev-01-ppt-edit-eg-&-promptfoo-eval-&-llm-backend-llamacpp-mlx-&-donehub-llm-api

## 0130

- ğŸ¤” i want to implement a rag app for pdf files. To support scanned pdfs, i want to create a solution that support different ocr models like mineru and paddleocr. but the outputs of mineru and paddleocr is different . Is there a popular ocr protocol or  specification that unifies the outputs of different ocr models? analyze popular apps or open source projects, and explain to me what's the best practice
  - there is no single "official" industry standard protocol (like ALTO or hOCR) that modern RAG applications use. While legacy XML formats exist, they are too verbose for LLMs.
  - The best practice today, adopted by popular open-source frameworks (like Docling, DeepDoctection, and Unstructured), is to normalize everything into an internal Intermediate Representation (IR)â€”usually a structured JSON objectâ€”and then export that to Markdown for the LLM.
- Since MinerU and PaddleOCR speak different languages, you should build a Wrapper / Adapter Layer that converts their raw output into your own Unified Data Class.

- How Popular Projects Handle Multi-OCR Unification
- Pattern A: Adapter/Plugin Architecture (RAGFlow) 
  - RAGFlow treats OCR as layout parsers rather than text extractors
  - RAGFlow doesn't try to unify at the OCR output level; instead, it unifies at the document object level after full parsing. They treat MinerU as a "layout analyzer" that feeds into chunking strategies
- Pattern B: Unified Interface Wrapper (Upsonic OCR)
  - Upsonic provides a factory pattern that normalizes at the API level
- Pattern C: Pipeline Abstraction (ocrpy)
  - ocrpy library takes a configuration-driven approach
  - They abstract the storage layer (S3, GCS, local) alongside OCR backends, treating document processing as an ETL pipeline

- For a RAG application supporting MinerU and PaddleOCR, I recommend creating a canonical internal representation rather than forcing both into an existing standard.
  - Step 1: Define a Canonical Schema (Pydantic)
  - Step 2: Implement Adapter Pattern
  - Step 3: Factory & Configuration
- IBM's Docling implements exactly this pattern with their `DoclingDocument` classâ€”a unified internal representation that can export to Markdown, HTML, or JSON. They support plugging in different layout models and OCR backends while maintaining a consistent document object model 

## 0129

- I run backend and the frontend locally by readme.  if I upload a PDF file and mineru API parsing succeeded, I can see the original PDF and parsed text content at http://localhost:8080/viewer?task_id=a9da721c. If I killed the server and started the server again and visit the same URL http://localhost:8080/viewer?task_id=a9da721c, will the original pdf and text content show again? Analyze related code and architecture explain to me where the original PDF and first text content are stored.

- [MinerU API Docs](https://mineru.net/apiManage/docs)

```JSON
{
    "code": 0,
    "msg": "ok",
    "trace_id": "ff0527169d6eeda17d5b558b2f650580",
    "data": {
        "batch_id": "1820a77f-ee2b-4d7c-ba09-9029b6d254df",
        "file_urls": [
            "https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/extract/2026-01-29/1820a77f-ee2b-4d7c-ba09-9029b6d254df/9a9ea9b9-8b3a-4a63-994c-0a7a79b697d3.pdf?Expires=1769765834&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=0ctVyoNI14t0qGVziHlkTDpmAW4%3D"
        ]
    }
}
```

- â€œbasedâ€ åœ¨è‹±ç¾äº’è”ç½‘é‡Œæ˜¯è¤’ä¹‰ä¿šè¯­ï¼Œæ„æ€å¤§è‡´æ˜¯â€œçœŸè¯šã€ä¸è¿åˆæ½®æµã€å€¼å¾—å°Šæ•¬/èµåŒâ€ï¼ˆåä¹‰æ˜¯ â€œcringeâ€ï¼‰ã€‚
  - åœ¨ç°ä»£è‹±æ–‡ç½‘ç»œæ–‡åŒ–ï¼ˆå°¤å…¶æ˜¯ 4chan, Reddit, Twitch, Twitterï¼‰ä¸­ï¼Œ"Based" æ˜¯ä¸€ä¸ªéå¸¸æµè¡Œä¸”é«˜åº¦è¤’ä¹‰çš„å½¢å®¹è¯ã€‚
  - æ‰€ä»¥ â€œIs drinking water based?â€ æ›´åƒæ˜¯åœ¨å¼€ç©ç¬‘åœ°é—®â€œå–æ°´è¿™äº‹å„¿é…·ä¸é…·/å€¼å¾—èµï¼Ÿâ€ â€”â€” å…¸å‹çš„å¹½é»˜/è®½åˆºæé—®ã€‚
  - å­—é¢æ„æ€ï¼š å–æ°´è¿™ä»¶äº‹å¾ˆé…·/å¾ˆèµ/å¾ˆç¡¬æ ¸å—ï¼Ÿ

## 0128

- the goal is opencode-cli bundled just like the existing gemini-cli. and you provide a new environment variable for user to choose which cli to use. you have aleady implemented it, but with bugs. so opencode-cli is just implemented by referring to gemini-cli, but with bugs. 
- i start electron app with opencode-cli by `npm run opencode:start`.  when i select a custom model from chatbox models dropdown and send chat message, ai answers well. then i click "New Chat" button, and send a new message to start a new chat, ai also answers well but the terminal often shows a error log like below

- when i start electron app by `npm start`, it uses gemini-cli by default, not opencode-cli. when i select a folder fo1 at chatbox and start chatting, why is a new `.opencode` folder generated at the folder fo1? analyze related code and architecture, find the reason and fix it. .opencode folder should only be generated when i set environment variable to use opencode-cli.

- your solution should make it work for starting a new chat with the same model or with a different model that user selected from the chatbox models dropdown menu.
  - Looking at the architecture, I see the real issue: each chat is trying to use its own workspace, but the OpenCode server is shared across all chats. When the second chat creates a config in its workspace, it's either: - Overwriting the config in the same workspace (if workspaces are accidentally shared) - Creating a config in a different workspace that the server can't see
  - The solution is to: 1. Use a single global workspace for all OpenCode chats (since OpenCode has built-in session management) 2. Merge model configs instead of overwriting when adding a new model 3. Track which models are already configured to avoid unnecessary config updates

## 0127

- Uses @ai-sdk/openai-compatible: This is the key - OpenCode has built-in support for OpenAI-compatible APIs

```moonshotai/kimi-k2-thinking

[Config] Using OpenCode CLI provider
[OpenCode] Configuration:
  - Base URL: https://integrate.api.nvidia.com/v1
  - Custom Provider ID: custom-nvidia
  - Model String: moonshotai/kimi-k2-thinking
[OpenCode] Creating config: {
  "provider": {

    "custom-nvidia": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Custom Provider (nvidia)",
      "options": {
        "baseURL": "https://integrate.api.nvidia.com/v1",
        "apiKey": "***MASKED***"
      },
      "models": {
        "moonshotai/kimi-k2-thinking": {
          "name": "moonshotai/kimi-k2-thinking"
        }
      }
    }

  }, 
  "model": "custom-nvidia/moonshotai/kimi-k2-thinking"
}
[OpenCode] Created config at: /Users/yaoo/.aionui/gemini-temp-1769495685105/.opencode/opencode.json
[OpenCode] Configured provider: custom-nvidia, model: moonshotai/kimi-k2-thinking
[OpenCode] Starting server on port 4096
[OpenCode] Using binary: /Users/yaoo/Documents/repos/office/aionui-office/node_modules/.bin/opencode
[OpenCode] Working directory: /Users/yaoo/.aionui/gemini-temp-1769495685105
[OpenCode Server] INFO  2026-01-27T06:34:45 +18ms service=models.dev file={} refreshing
[OpenCode Server] Error: Unexpected error, check log file at /Users/yaoo/.local/share/opencode/log/2026-01-27T063445.log for more details

Failed to start server on port 4096
[OpenCode] Stopping server
[OpenCode] Force killing server

```

```moonshotai/kimi-k2-instruct-0905
[Config] Using OpenCode CLI provider
[OpenCode] Configuration:
  - Base URL: https://integrate.api.nvidia.com/v1
  - Custom Provider ID: custom-nvidia
  - Model String: moonshotai/kimi-k2-instruct-0905
[OpenCode] Creating config: {
  "provider": {

    "custom-nvidia": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Custom Provider (nvidia)",
      "options": {
        "baseURL": "https://integrate.api.nvidia.com/v1",
        "apiKey": "***MASKED***"
      },
      "models": {
        "moonshotai/kimi-k2-instruct-0905": {
          "name": "moonshotai/kimi-k2-instruct-0905"
        }
      }
    }

  }, 
  "model": "custom-nvidia/moonshotai/kimi-k2-instruct-0905"
}
[OpenCode] Created config at: /Users/yaoo/.aionui/gemini-temp-1769495507065/.opencode/opencode.json
[OpenCode] Configured provider: custom-nvidia, model: moonshotai/kimi-k2-instruct-0905
[OpenCode] Starting server on port 4096
[OpenCode] Using binary: /Users/yaoo/Documents/repos/office/aionui-office/node_modules/.bin/opencode
[OpenCode] Working directory: /Users/yaoo/.aionui/gemini-temp-1769495507065
[OpenCode Server] INFO  2026-01-27T06:31:47 +20ms service=models.dev file={} refreshing
[OpenCode] Server ready
[OpenCode] Session created: ses_401d94c64ffeqICmuZ6Vvhzs8V
[OpenCode] Initialization complete
[OpenCode] Already initialized
[OpenCode] refreshAuth called (no-op for OpenCode)
[OpenCode] Tool registration skipped: aionui_web_fetch
Loaded cached credentials.
[OpenCode] Tool registration skipped: gemini_web_search
[OpenCode] setTools called (no-op for OpenCode)
[StreamMonitor] State changed to: connecting 
[OpenCode] sendMessageStream - Query text: tell a joke within 20 words...
[OpenCode] Sending request to: http://localhost:4096/session/ses_401d94c64ffeqICmuZ6Vvhzs8V/message
[OpenCode] Request body: {
  "parts": [

    {
      "type": "text",
      "text": "tell a joke within 20 words"
    }

  ], 
  "model": {

    "providerID": "custom-nvidia",
    "modelID": "moonshotai/kimi-k2-instruct-0905"

  }
}
Loaded cached credentials. jinyaoo86@gmail.com
[OpenCode] Response status: 200 OK
[OpenCode] Response received, info: {
  "id": "msg_bfe26bd6a001GvLaLYy51OVjXt", 
  "sessionID": "ses_401d94c64ffeqICmuZ6Vvhzs8V", 
  "role": "assistant", 
  "time": {

    "created": 1769495510378,
    "completed": 1769495513270

  }, 
  "parentID": "msg_bfe26bd51001ZT2WptJDT1M2ue", 
  "modelID": "moonshotai/kimi-k2-instruct-0905", 
  "providerID": "custom-nvidia", 
  "mode": "build", 
  "agent": "build", 
  "path": {

    "cwd": "/Users/yaoo/.aionui/gemini-temp-1769495507065",
    "root": "/"

  }, 
  "cost": 0, 
  "tokens": {

    "input": 10420,
    "output": 13,
    "reasoning": 0,
    "cache": {
      "read": 0,
      "write": 0
    }

  }, 
  "finish": "stop"
}
[OpenCode] Response parts count: 3
[OpenCode] Processing 3 parts
[OpenCode] Processing part: step-start
[OpenCode] Processing part: text
[OpenCode] Yielding text part, length: 66
[StreamMonitor] State changed to: connected 
[OpenCode] Processing part: step-finish
[OpenCode] Token usage: {
  input: 10420, 
  output: 13, 
  reasoning: 0, 
  cache: { read: 0, write: 0 }
}
[OpenCode] Returning final turn, hasContent: true parts: 1
[StreamMonitor] State changed to: disconnected 
Loaded cached credentials. jinyaoo86@gmail.com

```

## 0126

- ğŸ¯ the goal is opencode-cli bundled just like the existing gemini-cli. and you provide a new environment variable for user to choose which cli to use.  you aleady have implemented it, but with bugs. so opencode-cli is just implemented by referring to gemini-cli, but with bugs.
- i start electron app with opencode-cli by `npm run opencode:start`. when i chatted, it not work. 

- when i restarted and chatted again, it not work. the terminal log is 

- the original workflow for gemini-cli is that user selects a custom provider and model from chatbox dropdown, and gemini-cli will use it for chatting. please analyze the code and dataflow, and make it work for opencode-cli.
- user can configure custom provider and custom model from settings ui, then use it from chatbox dropdown menu. please assume user is always using custom provider and custom model, do not rely on the naming pattern of model-name or provier-name.
- analyze related code and review the data flow. the original gemini-cli works well when chatting with custom model. please make the chatting with opencode-cli work in a similar way.

- your previous fix by searching "think" in model name is not robust. when i use a thinking model that does not contain think in the model name, it will not work. a better solution is to parse the think event in ai response. you can read gemini-cli related code and analyze how it deals with thinking parts in response . then read opencode-cli related code , try to parse and show thinking content in electron ui in a similar way.

- review your implementation. when i  select custom model from chatbox dropdown ui, will chat work with opencode-cli? the original gemini-cli works well.  recheck the data flow, and make it work for opencode-cli

- opencode-cli is not required to install globally. you already include it via a wrapper like the original gemini-cli wrapper "@office-ai/aioncli-core".

- please refer to how gemini-cli supports openai-compatible provider, and make opencode-cli supports openai-compatible provider in the similar way.

- the original gemini-cli works well. for the opencode implementation, please read related code and architecture to make text-chat/streaming/tool-call/file-read/file-write more robust
- the original gemini-cli is imported as @office-ai/aioncli-core as an npm package dependency, and i can use the default gemini-cli without installing gemini-cli. is your implementation for opencode can be used in a similar way? if i set the environment variable to opencode, can i use it without installing opencode-cli?
- the task has already been implemented by you . i want you to review the existing gemini-cli implementation and improve  your opencode-cli implementation. just use git diff to see what you have edited
- the goal is opencode-cli bundled just like the existing gemini-cli. and you provide a  new environment variable for user to choose which cli to use.  the opencode-cli architecture should be like gemini-cli. analyze related code and architecture, review your edits and improve your code.

- remove testing step in your plan. you should do the core implementation first, we will write tests later. 

- this project provides llm electron app and web app that can chat with cli agent like claude-code/codex-cli/gemini-cli/opencode-cli. a modified gemini-cli is bundled by default, other cli needs to be installed manually. Chatting with the built-in bundled gemini-cli is implemented internally, while chatting with other cli is handled by ACP protocol.
- the goal is to add a new opencode-cli-mini bundled just like the existing gemini-cli. and add a new icon of opencode-cli-mini after the first default gemini-cli icon for user to select which cli to use. when using the app, if the first gemini-cli icon is selected, bundled gemini-cli should be used, and existing features should still work. if the second opencode-cli-mini icon is selected, bundled opencode-cli should be used, and most existing features should still work. features for the existing bundled gemini-cli should also work for the new bundled opencode-cli-mini. the original gemini-cli is imported by `@office-ai/aioncli-core` as an npm package dependency, and i can use the default gemini-cli without installing gemini-cli. make your implementation for opencode-cli can be used in a similar way.
- The implementation should be extensible so that it should be easy to add more bundled cli in the future. creating a standalone package like gemini-cli wrapper `@office-ai/aioncli-core` for opencode-cli-mini may be an approach.  when user configures the llm model at Settings--Model page and select the model at chatbox models dropdown list , it should work for all bundled cli like gemini-cli/opencode-cli-mini. 
- the existing acp impelmentation with opencode-cli should be kept unchaged for backward compatibility, it will not be used anymore. do not use the existing opencode acp integration.
- you should keep as many existing code as possible to stay unchanged so that it is easy to merge upstream changes later.

- the following resources may be useful for your reference:
- the source code for gemini-cli wrapper `@office-ai/aioncli-core` is at folder /Users/yaoo/Documents/repos/office/aioncli/packages/core. you can find api or docs and more details at /Users/yaoo/Documents/repos/office/aioncli . 
- opencode cli can be installed by `npm install opencode-ai`. the source code for opencode-cli is at /Users/yaoo/Documents/repos/ai-ml-llm/opencode/packages/opencode/src/cli. you can find api or docs and more details at /Users/yaoo/Documents/repos/ai-ml-llm/opencode .

- analyze related code and architecture , make a plan and write your plan to plan-opencode-cli-mini.md, ask for approval before implement it.

- when writing or editing large files, it's easy to fail if the content to write is large. you can write small edits multiple times to avoid large edits.

- review the goal, analyze related code and architecture , finally update your plan at plan-opencode-updated.md

- I want the opencode-cli bundled just like the existing gemini-cli. and you provide a  new environment variable for user to choose which cli to use. if the environment  variable is not set, gemini-cli should be used, and existing features still work. if environment variable is set to opencode, opencode-cli should be used, and the most existing features still work. The implementation should be extensible so that other environment variable value can be used to support more  cli in the future. reiew your previous plan at plan-opencode-updated.md and recheck the goal. optimize and update the plan.

- I want you to make a standalone package like `@office-ai/aioncli-core` for opencode-cli-mini. the source code for "@office-ai/aioncli-core" is at folder /Users/yaoo/Documents/repos/office/aioncli/packages/core. you can find api/docs and more details at /Users/yaoo/Documents/repos/office/aioncli. Now review your previous plan at plan-opencode.md and updated your plan. I have reverted all your previous edits.

## 0125

- [error: No solution found when resolving dependencies for split Â· Issue #14231 Â· astral-sh/uv](https://github.com/astral-sh/uv/issues/14231)

- [[Bug]: Error : module 'httpx' has no attribute 'AsyncHTTPTransport' Â· Issue #459 Â· acheong08/EdgeGPT](https://github.com/acheong08/EdgeGPT/issues/459)
  - httpxå›ºå®šç‰ˆæœ¬ä¸ºæ—§ç‰ˆæœ¬0.28.1, è€Œä¸æ˜¯1.0dev

## 0124

- [Error: 404 No allowed providers are available for the selected model. : r/RooCode](https://www.reddit.com/r/RooCode/comments/1kz57n0/error_404_no_allowed_providers_are_available_for/)
  - If you are using open router, going to open router and double check the settings. I was getting that error because I had set my api key to only do one model provider . sometimes it seems like you get this error if you leave open router totally empty so you can pull any model

## 0123

- [Some love for MacOS gpu Â· Issue Â· microsoft/onnxruntime _202207](https://github.com/microsoft/onnxruntime/issues/12252)
  - onnxruntime-gpu depends on CUDA.
  - Nvidia only provide CUDA downloads for two operating system: Linux and Windows. 
  - macOS users need to use coreml for GPU acceleration, or webgpu which we are still developing the support.

- dev-to
  - ocr api as openai api
- dev-log
  - ocr solution: deepseek-ocr api as openai api

## 0122

- `claude "query"` vs `claude -p "query"`:
  - claude "query" opens the interactive REPL with that query as the initial prompt; 
  - claude -p "query" runs a one-off (print/SDK) query, prints the result, and exits â€” useful for scripts or piping.

- ğŸ¤” is there any popular apps or open source projects that is a cli tool wrapper? vlc is a good example, as ffmpeg wrapper. I'm trying to build something similar and need some references. find related examples and provide each with description/github-repo...
  - ollama
  - git ui

## 0121

- ğŸ¤” is there any popular apps or open source projects that use database as storage and files as cache? git is a good example, because the the internal human-unreadable data is storage in .git, and files are just a cache. Is there any other apps or projects use this architecture or design? and how to make data/db and files sync? I am looking for a good reference for implement my own. find related examples and provide each with description
- Git â€” content-addressable object DB + working tree as cache
  - Git stores everything (blobs, trees, commits) inside .git (object database and refs); the working directory is a checked-out, human-readable cache of that data. You can reconstruct the repo entirely from .git.
- git-annex â€” content stored in annex (key/value) tracked by Git refs, working files optional
  - git-annex stores file contents in a separate annex directory and keeps metadata/location information in Git. Files in the working tree can be pointers or actual content; clones collectively host the distributed key/value store.
- DVC (Data Version Control) â€” metadata in Git, large files in remote store, local files as cache
  - DVC keeps small pointer/metadata files in Git (hashes, remotes, versions). Large data (datasets, models) is stored in remote object stores (S3/GCS/etc). The local filesystem holds a cache/checkout of the currently needed blobs.
- IPFS (content-addressed block store) â€” block/object DB + file DAGs + local pin/cache
  - IPFS stores content-addressed blocks in a blockstore; files are DAGs of those blocks. Nodes pin the blocks they want to keep; a local filesystem view (FUSE mounts, HTTP gateways) materializes files from the blockstore.

## 0120

- [[BUG] Mistral LLM Fails with Tools Due to Role Expectation Â· Issue Â· crewAIInc/crewAI](https://github.com/crewAIInc/crewAI/issues/2194)
  - [Fix Mistral Issue: by Vidit-Ostwal Â· Pull Request #2580 Â· crewAIInc/crewAI](https://github.com/crewAIInc/crewAI/pull/2580)
  - Added an additional check to convert any `system` role to `assistant` role.
  - [[BUG] Problem with the user and assistant roles when using the Mistarl api. Â· Issue #757 Â· huggingface/smolagents](https://github.com/huggingface/smolagents/issues/757)
    - When using the Mistarl API via LiteLLM or OpenAIServerModel, an error occurs due to the fact that the Mistral API expects a message with the User or Tool role, and receives it with the Assistant role.

- ğŸ§© in local mlx llm, q5.5bit quant achieved 1.25 perplexity in our testing.  what does `perplexity` mean for model?
  - Perplexity (PPL) is the standard metric used to evaluate how well a model predicts the next token in a sequence.
  - Perplexity is a measurement of how well a probability model (like your LLM) predicts a sample of text. In simple terms, it measures the model's "surprise" or "uncertainty" when seeing new text.
  - Lower is better. A lower score means the model is less surprised by the text it sees.
  - PPL = 1.0 is perfect (model always assigns probability 1 to the true token).
  - A PPL of 1.25 implies the model is highly confident and usually puts ~80% probability mass on the true next token â€” unusually good for typical open-domain text.
  - Higher is worse. A high score means the model is confused and struggling to guess what comes next.
  - If the Perplexity is 100, the model is as confused as if it had to pick blindly from a bag of 100 equally likely words.
  - If the Perplexity is 1.25, the model is effectively choosing between 1.25 options.
  - A perplexity of 1.25 is extremely low: it means the modelâ€™s geometric mean probability for the true next token is 1 / 1.25 = 0.8 (80%).

## 0116

- ğŸ¤” i want to develop a electron app with python backend server bundled as a single package  . user downloads the package and use it , without separate commands for starting server.   what's the best practice to achieve this? is there any open source app or github repo that already implemented this? if there is, give me the app or github repo url with description. comfyui desktop app seems to be a good example, but it's too complicated. find more examples and explain to me.
  - Most robust option: bundle a standalone Python runtime (or a PyInstallerâ€™d single executable) into the app, and have the Electron main process spawn that Python server on app start, wait for it to become ready, then load the renderer
- Approaches
- ğŸ’¡ Bundle a full Python runtime (standalone) + spawn it at startup
  - include a ready-to-run Python folder (or embeddable Python) in your app resources and call it directly. The app does not require the user to install Python.
  - full Python features, supports dynamic `pip install` if needed.
  - Cons: large binary size.
  - ğŸŒ° Real project pattern: Datasette Desktop (Simon Willison) describes bundling a standalone Python in the app.
  - [Bundling Python inside an Electron app | Simon Willisonâ€™s TILs _202109](https://til.simonwillison.net/electron/python-inside-electron)
- ğŸ’¡ Compile your Python backend to a single executable (PyInstaller / pyoxidize / py2exe) and include that executable
  - produce a single exe (or macOS binary) and include it in extraResources of your Electron build.
  - smaller than full runtime in some cases; simpler startup (one binary to spawn).
  - Cons: platform build complexity, native deps can be tricky.
  - Electron packaging guides and many community posts recommend this flow.
- ğŸ’¡ Sidecar / managed install on first run (ComfyUI style)
  - ship a lightweight bootstrap/electron wrapper that installs Python dependencies at first start (or downloads model files) and then starts the server. Useful for huge dep sets (e.g., ML models).
  - Pros: smaller initial download; flexible updating of backend and models.
  - Cons: first-run complexity and long install times; network required.
- ğŸ’¡ Use Tauri with a sidecar binary (alternative to Electron)
  - Tauri has official sidecar support and first-class bundling for external binaries
  - Cons: different stack (Rust toolchain) and ecosystem differences.

- https://github.com/lmstudio-ai/venvstacks
  - venvstacks is a tooling / packaging model that layers Python virtual environments into three conceptual layers â€” runtime (the Python interpreter), framework (big shared frameworks such as PyTorch/CUDA/MLX), and application (your app code). 
  - Layers are built and published as deterministic, self-contained archives and then composed on the target machine so multiple apps can share large framework/runtime pieces instead of duplicating them.
- LM Studio needed to ship frequent app features that depend on heavy Python ML stacks, but they didnâ€™t want every release to include a full copy of PyTorch/CUDA or duplicate runtime binaries.
  - On the client, layers are unpacked into a shared place, post-install scripts are run starting from the runtime up, and then the app layer is launched with the runtimeâ€™s interpreter
  - This enables smaller per-app downloads, and independent upgrades of framework/runtime layers.
- Building layer archives and maintaining cross-platform build automation is more complex than a single PyInstaller build.
  - There are constraints on relocatability, native extension linking, and you must run postinstall scripts carefully on the client. venvstacks documents these limitations.

- ComfyUI / Portable style, you are looking for the Bundled Runtime (Portable Python) architecture.
  - This differs from the PyInstaller approach. Instead of compiling your code into a frozen .exe, you ship a raw, miniature Python folder (interpreter) inside your app.
  - This approach is popular for AI apps (like ComfyUI, Automatic1111) because it allows the app to be extensible. Users can add custom nodes or modify Python files because the code isn't locked inside a compiled binary.

## 0115

- electron python runtime
- transformerlab-app model site:reddit.com
- multiple inference mlx llama.cpp github

- which expression is more natural in english: "please use the articles above to create a  summary" vs "please use the above articles to create a  summary" 
  - "Please use the articles above..." (Most Common/Natural)
  - This is the most standard way to say it in modern English. Using "above" after the noun (as a post-positive modifier) indicates location clearly.
  - "Please summarize the provided articles."

## 0114

- electron æ‰“åŒ… python runtime çš„ç¤ºä¾‹

- å¹¿å·å•†ç§‘é›†å›¢æ˜¯ä¸€å®¶åˆ›ç«‹äº1994å¹´çš„å…¬å¸ï¼Œæ——ä¸‹ä¸»è¦è¿è¥ç€é“­ç‘„(Maxsun)ã€æ¢…æ·(SOYO)å’Œå°ç”µï¼ˆTECLASTï¼‰ è¿™ä¸‰å¤§å“ç‰Œ
  - æ¢…æ·å“ç‰Œï¼šæºè‡ªå°æ¹¾ï¼Œåˆ›ç«‹äº1985å¹´ï¼Œä¸“æ³¨äºä¸»æ¿ç ”å‘ã€‚ç›®å‰ï¼Œè¯¥å“ç‰Œåœ¨ä¸­å›½å¤§é™†çš„è¿è¥æƒå½’å±äºå¹¿å·å•†ç§‘é›†å›¢ã€‚
  - æ¢…æ·çš„äº§å“çº¿éå¸¸ä¸“æ³¨äºä¸»æ¿é¢†åŸŸï¼Œå…¶ç‰¹ç‚¹æ˜¯åœ¨ä¸»æµå’Œå…¥é—¨çº§ä»·ä½æ®µ
  - â€œæ¢…æ·â€å°±æ˜¯å“ç‰Œå SOYOï¼ˆä¸­æ–‡å¸¸å†™ä½œâ€œæ¢…æ·â€/â€œæ¢…æ·ç§‘æŠ€â€ï¼‰ï¼ŒåŸä¸ºå°æ¹¾ä¸»æ¿å‚ï¼Œåæ¥å¤§é™†ä¸šåŠ¡è¢«å•†ç§‘é›†å›¢ï¼ˆä¹Ÿå°±æ˜¯é“­ç‘„/å°ç”µåŒé›†å›¢ï¼‰æ”¶è´­å¹¶åœ¨å¤§é™†ä»¥ SOYO/æ¢…æ· åä¹‰ç»§ç»­é”€å”®ä¸»æ¿ã€æ˜¾å¡ç­‰äº§å“

## 0113

- it's hard to find llm server example that supports pluggable backend like .gguf/.mlx. can you find some other server example that supports pluggable backend? you can find examples for other cases like  reading/conversion/editing for document/image/pdf ... .  I want to see the best practice on how to  implement pluggable backends. if you can find, give me some github repos and description
- Docling â€” document converter with format-specific backends
  - Study how it maps file-type â†’ backend and orchestrates pipelines. Very relevant for mapping model file format (.gguf/.mlx) â†’ runtime backend.

- Dynamic Loading: Use Python's `importlib` if you want to drop new backends into a folder without restarting the server, or simple if/else factories if you only have 2-3 backends.

- How to Design Your Own (Best Practices)
  - you should use the Registry + Adapter Pattern.
  - Step 1: Define the Protocol (The "Interface") Don't write code for Llama or MLX yet. First, define what a "Model" is to your server.
  - Step 2: Implement the Adapters (The "Plugins") Create separate files for each backend. This isolates dependencies. If a user doesn't have Apple Silicon, the MLXBackend import will fail, but the server will still run LlamaCppBackend.
  - Step 3: The Factory (The "Router") This is the piece most tutorials skip. You need a function that looks at the file extension and picks the right class.

## 0111

- to use openai compatible api like http://localhost:1234/v1/chat/completions, how can i pass prompt text to this api? give me the api data structure and some api usage examples
  - the chat completions endpoint expects an array of message objects, not a plain string.
  - `"messages": [{ "role": "user", "content": "Your single prompt here" }]`

- [error TS6133: 'functions' is declared but its value is never read - Stack Overflow](https://stackoverflow.com/questions/57815268/src-index-ts11-error-ts6133-functions-is-declared-but-its-value-is-never)

```JSON
{
  "compilerOptions": {
    "noUnusedLocals": false
  }
}
```

- ğŸ¤” when i do some edits and run `git push`, i hope tags and commits are both pushed and the built win/macos/linux artifacts can be seen on github release page https://github.com/examples-hub/electron-tanstack-shadcn/releases. tell me how to do it manually with shell commands step by step

```sh
# After making edits and committing:
git add .
git commit -m "your message"
git tag v1.0.3
# pushes both the main branch commits AND the v1.0.3 tag in one command
git push origin main v1.0.3

```

- When you push a tag, both build.yml and release.yml will trigger. They both build and release, which is redundant. You may want to disable one of them later, but for now both will create releases.

## 0110

- ğŸ¤” is there any electron examples that use client/server architecture so that it supports both windows/mac desktop app and webapp? i want to support both web ui and desktop app usage. if there's any open source repo, show the url and description
- Actual Budget (Best "Local Server" Example) 
  - Repository: github.com/actualbudget/actual
  - It uses a client/server architecture where the "server" can run in the cloud (for the web app) or locally (inside the Electron app)
  - Web Mode: The frontend runs in the browser and talks to a remote server API.
  - Desktop Mode: The Electron process spawns a local instance of the server code. The frontend (running in the Electron window) connects to this local server via localhost.
- Hoppscotch (Best "Shared Frontend" Example)
  - Repository: github.com/hoppscotch/hoppscotch
  - It is built as a Progressive Web App (PWA) first. The desktop version is essentially a wrapper around the web application but adds "native" capabilities (like bypassing CORS) that a browser cannot do.
  - It treats the "Client" as a completely separate entity from the environment it runs in.
  - It uses a "Proxy" server approach to handle requests that would otherwise fail in a pure web environment due to browser security restrictions.
  - It shows how to structure a massive Vue.js application so it works flawlessly in a browser and as an installed desktop app.
- Visual Studio Code (The "Gold Standard")
  - Architecture: Process Separation. They separate the Workbench (UI) from the Extension Host (Logic/Server).
  - When you run the Desktop app, the "Server" runs locally on your machine (accessing your local files).
  - When you run the Web version, the "Server" runs remotely (like in GitHub Codespaces) or in a limited browser sandbox.
  - The UI code communicates with the Server code via JSON-RPC, meaning the UI doesn't care if the server is local or remote.
- Next.js + Electron Boilerplate (Best for Starting New)
  - Repository: github.com/saltyshiomix/nextron
  - If you are starting from scratch and want to build a React/Next.js app that serves as both, this is the most popular boilerplate (often called "Nextron").
  - It configures the build system (Webpack/Turbo) to output two targets from your code:
  - Web: A standard Next.js build for your website.
  - Desktop: An Electron renderer build that loads the Next.js pages.
  - It doesn't strictly force a "server" architecture, but it sets up the Shared Code structure for you. You can put your logic in a shared/ folder and import it into both the renderer/ (Frontend) and main/ (Electron Backend) folders.

- ğŸ¤” for a github repo with a package.json file, for one author, it's "author": { "name": "Jonathan Wheeler", "email": "jonathan.wheeler87@gmail.com" }, . if there are 2 authors, how to write?
  - If you have two authors, you should use the `contributors` array
  - Option 1: Co-Authors (Equal Status)
  - Option 2: Author + Contributor

## 0109

- [Conversation Compaction Failure: Summary Generation Error Â· Issue Â· anthropics/claude-code](https://github.com/anthropics/claude-code/issues/5778)
  - I launched the compact command for a second try and it worked without issues

- ğŸ¤” when i have used claude code for a long time , context is too long but todos are still in progress. can i interrupt it and execute /compact to reduce context and continue to finish todos? if it's possible, how to do it in claude code?
  - If Claude is currently running a long task or stuck in a loop, you can interrupt it by pressing: Ctrl + C
  - While you can just type /compact, the safest way to ensure your specific todos are not lost in the summary is to provide a prompt to the compact command.

## 0108

- ğŸ†š i want to build an online image editor webapp. I want to add text and resize image. knova and fabric.js js seems to be good library. please compare them. or do you have any better suggestion?
- Choose Konva.js if you are using React and need high performance (e.g., thousands of objects). However, you will have to build your own "resize handles" and "text editor overlay" (using a hidden HTML `<textarea>`), as Konva does not have these native UI controls.
  - If you want high-performance shape/animation-heavy editors and a very polished React integration â†’ Konva (with react-konva).
  - Scene Graph Architecture (like game engines) . Objects are organized in a hierarchical tree (layers, groups, shapes).
- Choose Fabric.js if you want "out-of-the-box" editor features. It has built-in support for resizing handles, rotating, and a real text cursor (selectable text), saving you weeks of coding.
  - If you want ready-made canvas object-model features like inline text editing, SVG import/export, image filters/cropping and lots of editor examples â†’ Fabric.js.
  - Object-Oriented Canvas Model. It provides a rich object model over the native Canvas API, treating everything (images, text, shapes) as objects that can be manipulated individually
- Konva: modern canvas abstraction with layers, nodes and Transformers. Works best if you treat canvas like a scenegraph; editing text is usually done via a DOM `<input>` overlay (Konva recommends using native inputs for editing).
  - Konva. Text for rendering. For user editing you typically overlay an input/textarea and sync it back to the Konva node (Konva has examples).
- Fabric: a higher-level object model over canvas (objects, groups, IText/Textbox, image primitives) and built-in inline text editing (IText/Textbox). Easier to get an editable text UX fully on-canvas.
  - IText/Textbox support native editing behavior on canvas (selection, caret, copy/paste, wrapping). If editable-on-canvas text is a big requirement, Fabric is simpler out of the box.
- Both libraries can run in Node (via node-canvas / explicit canvas backends) and can export canvases to images; 

## 0106

- ğŸ¤” i want to run ocr llm model locally. why does llama.cpp not support deepseek-ocr gguf, but ollama supports deepseek-ocr since ollama uses llama.cpp under the hood? 
- [Ollama's new engine for multimodal models Â· Ollama Blog _202505](https://ollama.com/blog/multimodal-models)
  - Ollama has so far relied on the ggml-org/llama.cpp project for model support and has instead focused on ease of use and model portability.
  - As more multimodal models are released by major research labs, the task of supporting these models the way Ollama intends became more and more challenging.
  - We set out to support a new engine that makes multimodal models first-class citizens, and getting Ollamaâ€™s partners to contribute more directly to the community via the GGML tensor library.
  - Today, ggml/llama.cpp offers first-class support for text-only models. For multimodal systems, however, the text decoder and vision encoder are split into separate models and executed independently. Passing image embeddings from the vision model into the text model therefore demands model-specific logic in the orchestration layer that can break specific model implementations.
  - Within Ollama, each model is fully self-contained and can expose its own projection layer, aligned with how that model was trained. This isolation lets model creators implement and ship their code without patching multiple files or adding cascading if statements. They no longer need to understand a shared multimodal projection function or worry about breaking other modelsâ€”they can focus solely on their own model and its training.

## 0104

- ğŸ¤” https://huggingface.co/datalab-to/chandra this model uses [OpenRAIL ](https://huggingface.co/blog/open_rail) license. if i use this model locally on ocr saas webapp, is it free to use? is OpenRAIL license commercial friendly? 
  - æ­¤é—®é¢˜deepseekå›ç­”æ— æ³•è®¿é—®ç½‘é¡µè€Œæ‹’ç»ï¼Œgemini/grok/chat-glmå¹»è§‰ï¼Œchatgptè¯¢é—®æ˜¯å¦è¦fetch licenseæ–‡ä»¶çš„å†…å®¹å› è€Œæœ€å‡†ç¡®
  - https://huggingface.co/datalab-to/chandra/blob/main/LICENSE
  - check the exact OpenRAIL variant and its prohibited-use clauses. OpenRAIL licenses are explicitly designed to permit reuse, distribution and commercialization of models while adding responsibility / prohibited-use rules.
  - itâ€™s a Modified OpenRAIL-M license, with explicit restrictions
  - Attachment A - USE RESTRICTIONS

    - Commercial and broader use licenses may be available from Licensor
    - for any purpose if You (your employer, or the entity you are affiliated with) generated/raised more than two million US Dollars

- 65é£æ ¼å‘è¨€æŒ‡çš„æ˜¯ä»€ä¹ˆ
  - â€œ65é£æ ¼å‘è¨€â€æºè‡ªä¸­å›½ CS: GOï¼ˆåæç²¾è‹±ï¼‰ åœˆå­ï¼Œç‰¹æŒ‡çŸ¥åä¸»æ’­QUQUçš„è€æ¿/é˜Ÿå‹ B65ï¼ˆäººç§°Bå“¥ã€65ï¼‰çš„è¯´è¯æ–¹å¼ã€‚
  - è¦æŒæ¡æˆ–è¯†åˆ«â€œçœŸæ­£çš„65é£æ ¼â€ï¼Œä½ éœ€è¦æŠ“ä½ä»¥ä¸‹å‡ ä¸ªæ ¸å¿ƒè¦ç´ ï¼šæåº¦çš„è‡ªä¿¡ã€ç‹¬ç‰¹çš„å£å¤´ç¦…ã€å¯¹â€œæ¸¸æˆç†è§£â€çš„è‡ªæˆ‘å¹æ§ï¼Œä»¥åŠä¸€ç§â€œè€æ¿â€ç‰¹æœ‰çš„æŒ‡ç‚¹æ±Ÿå±±çš„æ°”è´¨ã€‚
  - çœŸæ­£çš„65é£æ ¼ç¦»ä¸å¼€è¿™å‡ ä¸ªè¯ï¼Œå¦‚æœç¼ºå°‘äº†å®ƒä»¬ï¼Œå°±æ²¡æœ‰é‚£ä¸ªå‘³å„¿ï¼š 
  - â€œå¸¸è§„â€ï¼šè¿™æ˜¯65é£æ ¼çš„çµé­‚ã€‚æ— è®ºåšä»€ä¹ˆæ“ä½œï¼ˆå“ªæ€•æ˜¯ç™½ç»™ï¼‰ï¼Œéƒ½è¦è¯´æ˜¯â€œå¸¸è§„æ“ä½œâ€ã€‚
  - â€œé¡¶çº§â€ï¼šå½¢å®¹è‡ªå·±çš„æ„è¯†ã€é“å…·æˆ–æªæ³•ã€‚
  - â€œç†è§£â€ï¼šæŒ‡æ¸¸æˆé˜…è¯»èƒ½åŠ›ã€‚
  - â€œæ»¡æ˜â€ï¼šå³â€œæ»¡åˆ†â€ï¼Œå£éŸ³æ¢—ã€‚
  - â€œç”šè‡³â€ï¼šç”¨æ¥é€’è¿›å¹å˜˜è‡ªå·±çš„æ“ä½œã€‚
  - â€œæ‹¿ä¸‹â€ï¼šè¡¨ç¤ºèƒœåˆ©æˆ–å‡»æ€ã€‚
